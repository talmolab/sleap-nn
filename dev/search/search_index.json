{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SLEAP-NN","text":"<p>SLEAP-NN is the deep learning engine that powers SLEAP (Social LEAP Estimates Animal Poses), providing neural network architectures for multi-instance animal pose estimation and tracking. Built on PyTorch, SLEAP-NN offers an end-to-end training workflow, supporting multiple model types (Single Instance, Top-Down, Bottom-Up, Multi-Class), and seamless integration with SLEAP's GUI and command-line tools.</p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>End-to-end workflow: Streamlined training and inference pipelines, including a multi-instance tracking worklow with a flow-shift-based tracker.</li> <li>Multiple model architectures: Support for single-instance, top-down, and bottom-up pose estimation models using a variety of backbones, including highly-customizable UNet, ConvNeXt, and Swin Transformer.</li> <li>PyTorch Lightning integration: Built on PyTorch Lightning for fast and scalable training, with support for multi-GPU and distributed training.</li> <li>Flexible configuration: Hydra and OmegaConf based config system to validate training parameters and enable reproducible experiments.</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Let's start SLEAPiNNg !!! \ud83d\udc2d\ud83d\udc2d</p>"},{"location":"#1-install-sleap-nn","title":"1. Install <code>sleap-nn</code>","text":"<ul> <li>Windows/Linux with NVIDIA GPU (CUDA 11.8):</li> </ul> <pre><code>pip install sleap-nn[torch-cuda118]\n</code></pre> <ul> <li>Windows/Linux with NVIDIA GPU (CUDA 12.8):</li> </ul> <pre><code>pip install sleap-nn[torch-cuda128]\n</code></pre> <ul> <li>macOS with Apple Silicon (M1, M2, M3, M4) or CPU-only (no GPU or unsupported GPU):  Note: Even if torch-cpu is used on macOS, the MPS backend will be available. <pre><code>pip install sleap-nn[torch-cpu]\n</code></pre></li> </ul> <p>Refer Installation for more details on how to install sleap-nn package for your specific hardware.</p>"},{"location":"#2-set-up-your-configuration","title":"2. Set Up Your Configuration","text":"<p>Create a <code>config.yaml</code> file for your experiment.</p> <ul> <li>Refer to the Configuration Guide for detailed options.</li> <li>Or, use a sample config from <code>docs/sample_configs</code>.</li> </ul>"},{"location":"#3-train-a-model","title":"3. Train a model","text":"<p>Download sample training data from here and validation data from here for quick experimentation.</p> <pre><code>sleap-nn-train --config-name config.yaml --config-dir configs/ \"data_config.train_labels_path=[labels.pkg.slp]\"\n</code></pre> <p>For detailed information on training workflows, configuration options, and advanced usage, please refer to the Training Guide.</p>"},{"location":"#4-run-inference-on-the-trained-model","title":"4. Run inference on the trained model","text":"<p>To run inference: <pre><code>sleap-nn-track --data-path video.mp4 --model-paths model_ckpt_dir/\n</code></pre></p> <p>More options for running inference and tracking workflows are available in the Inference Guide.</p> <p>Note</p> <p>Note: For step-by-step tutorial on our entire workflow, check out Step--by-Step guide.</p> <p>Note</p> <p>Note: For tutorials, sample notebooks, and instructions on how to use the interactive notebooks, please see Example notebooks.</p>"},{"location":"#core-components","title":"\ud83d\udee0\ufe0f Core Components","text":"<p>SLEAP-NN provides a modular, PyTorch-based architecture:</p>"},{"location":"#data-pipeline","title":"Data Pipeline","text":"<ul> <li>Efficient loading of data from SLEAP label files</li> <li>Parallelized data loading using PyTorch's multiprocessing for high throughput</li> <li>Caching (memory/ disk) to accelerate repeated data access and minimize I/O bottlenecks</li> </ul>"},{"location":"#model-system","title":"Model System","text":"<ul> <li>Pluggable Design: Easy to add new backbones/ head modules</li> <li>Backbone Networks: UNet, ConvNeXt, Swin Transformer  <ul> <li>See Backbone Architectures for more details.</li> </ul> </li> <li>Model Types: Single Instance, Top-Down, Bottom-Up, Multi-Class (Supervised ID models) variants</li> </ul> <ul> <li> <p>Single Instance: Direct pose prediction for single animals</p> </li> <li> <p>Top-Down: Two-stage (centroid \u2192 centered-instance) for multi-animal scenarios</p> </li> <li> <p>Bottom-Up: Simultaneous keypoint detection and association using Part Affinity Fields (PAFs).</p> </li> <li> <p>Supervised ID or Multi-Class: Pose estimation + ID assignment for multi-instance scenarios</p> </li> </ul> <p>Explore detailed descriptions of all supported architectures in the Model Types Guide.</p>"},{"location":"#training-engine","title":"Training Engine","text":"<ul> <li>PyTorch Lightning integration with custom callbacks</li> <li>In-built multi-GPU and distributed training support</li> <li>Experiment tracking with visualizers and WandB</li> </ul>"},{"location":"#inference-pipeline","title":"Inference Pipeline","text":"<ul> <li>Optimized inference workflow for different model types</li> <li>Integration with SLEAP's labeling interface</li> </ul>"},{"location":"#tracking-system","title":"Tracking System","text":"<ul> <li>Multi-instance tracking across frames</li> <li>Flow-shift based tracker for robust tracking</li> </ul>"},{"location":"#get-help","title":"Get Help","text":"<p>If you encounter issues or have questions about SLEAP-NN:</p> <p>Report Bugs:</p> <p>Found a bug? Please create an issue on GitHub: - Create a new issue - Include details about your environment, error messages, and steps to reproduce</p> <p>Start a Discussion:</p> <p>Have questions about usage, feature requests, or want to share your experience? - Start a discussion</p> <p>Additional Resources:</p> <ul> <li>SLEAP Documentation - Main SLEAP documentation</li> <li>SLEAP-NN GitHub Repository - Source code and releases</li> <li>SLEAP Community - General SLEAP discussions</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide</li> <li>Configuration Guide</li> <li>Training Models</li> <li>Running Inference / Tracking</li> <li>API Reference</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"config/","title":"Configuration Guide","text":""},{"location":"config/#overview","title":"Overview","text":"<p>This document provides a detailed guide to all configuration options available for training and running inference with sleap-nn models.</p> <p>The config file has three main sections:</p> <ol> <li><code>data_config</code>: Creating a data pipeline  </li> <li><code>model_config</code>: Initialize the sleap-nn backbone and head models  </li> <li><code>trainer_config</code>: Hyperparameters required to train the model with Lightning</li> </ol>"},{"location":"config/#basic-configuration-structure","title":"Basic Configuration Structure","text":""},{"location":"config/#sample-configuration-format","title":"Sample configuration format","text":"<pre><code>data_config:\n  train_labels_path: \n    - path/to/your/training_data.slp\n  val_labels_path:\n    - path/to/your/validation_data.slp\n  validation_fraction: 0.1\n  provider: LabelsReader\n  user_instances_only: true\n  data_pipeline_fw: torch_dataset\n  preprocessing:\n    ensure_rgb: false\n    ensure_grayscale: false\n    scale: 1.0\n    crop_hw: null\n    min_crop_size: 100\n  use_augmentations_train: true\n  augmentation_config:\n    intensity:\n      uniform_noise_p: 0.0\n      gaussian_noise_p: 0.0\n      contrast_p: 0.0\n      brightness_p: 0.0\n    geometric:\n      rotation_min: -15.0\n      rotation_max: 15.0\n      scale_min: 0.9\n      scale_max: 1.1\n      affine_p: 1.0\n\nmodel_config:\n  init_weights: default\n  pretrained_backbone_weights: null\n  pretrained_head_weights: null\n  backbone_config:\n    unet:\n      in_channels: 1\n      kernel_size: 3\n      filters: 16\n      filters_rate: 2.0\n      max_stride: 16\n      middle_block: true\n      up_interpolate: true\n      stacks: 1\n      convs_per_block: 2\n      output_stride: 2\n    convnext: null\n    swint: null\n  head_configs:\n    single_instance:\n      confmaps:\n        part_names: null\n        sigma: 2.5\n        output_stride: 2\n    centroid: null\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown: null\n\ntrainer_config:\n  train_data_loader:\n    batch_size: 4\n    shuffle: true\n    num_workers: 0\n  val_data_loader:\n    batch_size: 4\n    shuffle: false\n    num_workers: 0\n  model_ckpt:\n    save_top_k: 1\n    save_last: false\n  trainer_devices: auto\n  trainer_accelerator: auto\n  enable_progress_bar: true\n  min_train_steps_per_epoch: 200\n  visualize_preds_during_training: true\n  keep_viz: false\n  max_epochs: 200\n  seed: 0\n  use_wandb: false\n  save_ckpt: true\n  save_ckpt_path: your_model_name\n  optimizer_name: Adam\n  optimizer:\n    lr: 0.0001\n    amsgrad: false\n  lr_scheduler:\n    step_lr: null\n    reduce_lr_on_plateau:\n      threshold: 1.0e-06\n      threshold_mode: rel\n      cooldown: 3\n      patience: 5\n      factor: 0.5\n      min_lr: 1.0e-08\n  early_stopping:\n    min_delta: 1.0e-08\n    patience: 10\n    stop_training_on_plateau: true\n  online_hard_keypoint_mining:\n    online_mining: false\n    hard_to_easy_ratio: 2.0\n    min_hard_keypoints: 2\n    max_hard_keypoints: null\n    loss_scale: 5.0\n  zmq:\n    publish_address:\n    controller_address:\n    controller_polling_timeout: 10\n\nname: 'your_experiment_name'\ndescription: 'Brief description of your experiment'\nsleap_nn_version: '0.0.1'\nfilename: ''\n</code></pre>"},{"location":"config/#available-sample-configurations","title":"Available Sample Configurations","text":"<p>Refer to the sample configuration files here.</p> <ul> <li><code>config_single_instance_unet.yaml</code> - Basic single instance detection with UNet backbone</li> <li><code>config_centroid_unet.yaml</code> - Centroid-based detection with UNet backbone</li> <li> <p><code>config_centroid_swint.yaml</code> - Centroid-based detection with Swin Transformer backbone</p> </li> <li> <p><code>config_topdown_centered_instance_unet.yaml</code> - Top-down centered instance detection with UNet backbone</p> </li> <li> <p><code>config_topdown_multi_class_centered_instance_unet.yaml</code> - Top-down multi-class centered instance detection with UNet backbone</p> </li> <li> <p><code>config_bottomup_unet.yaml</code> - Bottom-up detection with UNet backbone</p> </li> <li><code>config_bottomup_convnext.yaml</code> - Bottom-up detection with ConvNeXt backbone</li> <li><code>config_multi_class_bottomup_unet.yaml</code> - Multi-class bottom-up detection with UNet backbone</li> </ul>"},{"location":"config/#converting-legacy-sleap-14-configjson-to-sleap-nn-yaml","title":"Converting legacy SLEAP (\u22641.4) <code>config.json</code> to SLEAP-NN YAML","text":"<p>If you have a SLEAP (v1.4 or earlier) <code>config.json</code> file from a previous project, you can easily convert it to a SLEAP-NN-compatible YAML configuration using the following code snippet:</p> <pre><code>from sleap_nn.config.training_job_config import TrainingJobConfig\nfrom omegaconf import OmegaConf\n\nconfig = TrainingJobConfig.load_sleap_config(\"/path/to/config/json\")\nOmegaConf.save(config, \"config.yaml\")\n</code></pre>"},{"location":"config/#data-configuration-data_config","title":"Data Configuration (<code>data_config</code>)","text":"<p>The data configuration section controls how training and validation data is loaded, preprocessed, and augmented.</p>"},{"location":"config/#core-data-settings","title":"Core Data Settings","text":"<ul> <li><code>provider</code>: (str) Provider class to read the input sleap files. Only \"LabelsReader\" is currently supported for the training pipeline. Default: <code>\"LabelsReader\"</code></li> <li><code>train_labels_path</code>: (list) List of paths to training data (<code>.slp</code> file(s)). Default: <code>[]</code></li> <li><code>val_labels_path</code>: (list) List of paths to validation data (<code>.slp</code> file(s)). Default: <code>None</code></li> <li><code>validation_fraction</code>: (float) Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the <code>validation_labels</code> are already specified, this has no effect. Default: <code>0.1</code></li> <li><code>test_file_path</code>: (str) Path to test dataset (<code>.slp</code> file or <code>.mp4</code> file). Note: This is used only with CLI to get evaluation on test set once training is completed. Default: <code>None</code></li> <li><code>user_instances_only</code>: (bool) <code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used. Default: <code>True</code></li> </ul>"},{"location":"config/#example","title":"Example:","text":"<p>Single training file: <pre><code>data_config:\n  train_labels_path:\n    - path/to/your/single_training_data.slp\n  val_labels_path:\n    - path/to/your/validation_data.slp\n</code></pre></p> <p>Multiple training files: <pre><code>data_config:\n  train_labels_path:\n    - path/to/your/training_data_1.slp\n    - path/to/your/training_data_2.slp\n    - path/to/your/training_data_3.slp\n  val_labels_path:\n    - path/to/your/validation_data_1.slp\n    - path/to/your/validation_data_2.slp\n</code></pre></p>"},{"location":"config/#data-pipeline-framework","title":"Data Pipeline Framework","text":"<ul> <li><code>data_pipeline_fw</code>: (str) Framework to create the data loaders. One of [<code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code>]. Default: <code>\"torch_dataset\"</code>. (Note: When using <code>torch_dataset</code>, <code>num_workers</code> in <code>trainer_config</code> should be set to 0 as multiprocessing doesn't work with pickling video backends.)</li> <li><code>cache_img_path</code>: (str) Path to save <code>.jpg</code> images created with <code>torch_dataset_cache_img_disk</code> data pipeline framework. If <code>None</code>, the path provided in <code>trainer_config.save_ckpt</code> is used. The <code>train_imgs</code> and <code>val_imgs</code> dirs are created inside this path. Default: <code>None</code></li> <li><code>use_existing_imgs</code>: (bool) Use existing train and val images/ chunks in the <code>cache_img_path</code> for <code>torch_dataset_cache_img_disk</code> frameworks. If <code>True</code>, the <code>cache_img_path</code> should have <code>train_imgs</code> and <code>val_imgs</code> dirs. Default: <code>False</code></li> <li><code>delete_cache_imgs_after_training</code>: (bool) If <code>False</code>, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. Default: <code>True</code></li> </ul>"},{"location":"config/#image-preprocessing","title":"Image Preprocessing","text":"<ul> <li><code>preprocessing</code>:<ul> <li><code>ensure_rgb</code>: (bool) True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to <code>True</code>, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: <code>False</code></li> <li><code>ensure_grayscale</code>: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: <code>False</code></li> <li><code>max_height</code>: (int) Maximum height the image should be padded to. If not provided, the original image size will be retained. Default: <code>None</code></li> <li><code>max_width</code>: (int) Maximum width the image should be padded to. If not provided, the original image size will be retained. Default: <code>None</code></li> <li><code>scale</code>: (float) Factor to resize the image dimensions by, specified as a float. Default: <code>1.0</code></li> <li><code>crop_hw</code>: (Tuple[int]) Crop height and width of each instance (h, w) for centered-instance model. If <code>None</code>, this would be automatically computed based on the largest instance in the <code>sio.Labels</code> file. Default: <code>None</code></li> <li><code>min_crop_size</code>: (int) Minimum crop size to be used if <code>crop_hw</code> is <code>None</code>. Default: <code>100</code></li> </ul> </li> </ul>"},{"location":"config/#example-common-preprocessing-configurations","title":"Example: Common Preprocessing Configurations","text":"<p>Default img channels from source: <pre><code>data_config:\n  preprocessing:\n    ensure_rgb: false\n    ensure_grayscale: false\n    scale: 1.0\n    crop_hw: null\n    min_crop_size: 100\n</code></pre></p> <p>RGB images: <pre><code>data_config:\n  preprocessing:\n    ensure_rgb: true\n    ensure_grayscale: false\n    scale: 1.0\n    crop_hw: null\n    min_crop_size: 100\n</code></pre></p> <p>Fixed crop size for centered instance models: <pre><code>data_config:\n  preprocessing:\n    ensure_rgb: false\n    ensure_grayscale: false\n    scale: 1.0\n    crop_hw: [256, 256]  # Fixed 256x256 crop\n    min_crop_size: 100\n</code></pre></p> <p>Resize/ Pad images with maximum dimensions: <pre><code>data_config:\n  preprocessing:\n    ensure_rgb: false\n    ensure_grayscale: false\n    scale: 1.0\n    max_height: 512\n    max_width: 512\n    crop_hw: null\n    min_crop_size: 100\n</code></pre></p>"},{"location":"config/#data-augmentation","title":"Data Augmentation","text":"<ul> <li><code>use_augmentations_train</code>: (bool) True if the data augmentation should be applied to the training data, else False. Default: <code>False</code></li> <li><code>augmentation_config</code>: (only if <code>use_augmentations</code> is <code>True</code>)</li> <li><code>intensity</code>: (Optional)<ul> <li><code>uniform_noise_min</code>: (float) Minimum value for uniform noise (uniform_noise_min &gt;=0). Default: <code>0.0</code></li> <li><code>uniform_noise_max</code>: (float) Maximum value for uniform noise (uniform_noise_max &lt;&gt;=1). Default: <code>1.0</code></li> <li><code>uniform_noise_p</code>: (float) Probability of applying random uniform noise. Default: <code>0.0</code></li> <li><code>gaussian_noise_mean</code>: (float) The mean of the gaussian noise distribution. Default: <code>0.0</code></li> <li><code>gaussian_noise_std</code>: (float) The standard deviation of the gaussian noise distribution. Default: <code>1.0</code></li> <li><code>gaussian_noise_p</code>: (float) Probability of applying random gaussian noise. Default: <code>0.0</code></li> <li><code>contrast_min</code>: (float) Minimum contrast factor to apply. Default: <code>0.9</code></li> <li><code>contrast_max</code>: (float) Maximum contrast factor to apply. Default: <code>1.1</code></li> <li><code>contrast_p</code>: (float) Probability of applying random contrast. Default: <code>0.0</code></li> <li><code>brightness_min</code>: (float) Minimum brightness factor to apply. Default: <code>1.0</code></li> <li><code>brightness_max</code>: (float) Maximum brightness factor to apply. Default: <code>1.0</code></li> <li><code>brightness_p</code>: (float) Probability of applying random brightness. Default: <code>0.0</code></li> </ul> </li> <li><code>geometric</code>: (Optional)<ul> <li><code>rotation_min</code>: (float) Minimum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. Default: <code>-15.0</code>.</li> <li><code>rotation_max</code>: (float) Maximum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. Default: <code>15.0</code>.</li> <li><code>scale_min</code>: (float) Minimum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. Default: <code>0.9</code>.</li> <li><code>scale_max</code>: (float) Maximum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. Default: <code>1.1</code>.</li> <li><code>translate_width</code>: (float) Maximum absolute fraction for horizontal translation. For example, if translate_width=a, then horizontal shift is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a. Will not translate by default. Default: <code>0.0</code></li> <li><code>translate_height</code>: (float) Maximum absolute fraction for vertical translation. For example, if translate_height=a, then vertical shift is randomly sampled in the range -img_height * a &lt; dy &lt; img_height * a. Will not translate by default. Default: <code>0.0</code></li> <li><code>affine_p</code>: (float) Probability of applying random affine transformations. Default: <code>0.0</code></li> <li><code>erase_scale_min</code>: (float) Minimum value of range of proportion of erased area against input image. Default: <code>0.0001</code></li> <li><code>erase_scale_max</code>: (float) Maximum value of range of proportion of erased area against input image. Default: <code>0.01</code></li> <li><code>erase_ratio_min</code>: (float) Minimum value of range of aspect ratio of erased area. Default: <code>1.0</code></li> <li><code>erase_ratio_max</code>: (float) Maximum value of range of aspect ratio of erased area. Default: <code>1.0</code></li> <li><code>erase_p</code>: (float) Probability of applying random erase. Default: <code>0.0</code></li> <li><code>mixup_lambda_min</code>: (float) Minimum mixup strength value. Default: <code>0.01</code></li> <li><code>mixup_lambda_max</code>: (float) Maximum mixup strength value. Default: <code>0.05</code></li> <li><code>mixup_p</code>: (float) Probability of applying random mixup v2. Default: <code>0.0</code></li> </ul> </li> </ul>"},{"location":"config/#example-common-augmentation-configurations","title":"Example: Common Augmentation Configurations","text":"<p>No augmentation: <pre><code>data_config:\n  use_augmentations_train: false\n  # augmentation_config is not needed when use_augmentations_train is false\n</code></pre></p> <p>Only intensity augmentations: <pre><code>data_config:\n  use_augmentations_train: true\n  augmentation_config:\n    intensity:\n      contrast_p: 0.3\n      brightness_p: 0.3\n      gaussian_noise_p: 0.1\n      gaussian_noise_std: 0.1\n    geometric: null  # No geometric augmentations\n</code></pre></p> <p>Only geometric augmentations: <pre><code>data_config:\n  use_augmentations_train: true\n  augmentation_config:\n    intensity: null  # No intensity augmentations\n    geometric:\n      rotation_min: -15.0\n      roration_max: 15.0\n      scale_min: 0.9\n      scale_max: 1.1\n      translate_width: 0.1\n      translate_height: 0.1\n      affine_p: 0.5\n</code></pre></p> <p>Both intensity and geometric augmentations: <pre><code>data_config:\n  use_augmentations_train: true\n  augmentation_config:\n    intensity:\n      contrast_p: 0.3\n      brightness_p: 0.3\n      gaussian_noise_p: 0.1\n      gaussian_noise_std: 0.1\n    geometric:\n      rotation_min: -15.0\n      rotation_max: 15.0\n      scale_min: 0.9\n      scale_max: 1.1\n      translate_width: 0.1\n      translate_height: 0.1\n      affine_p: 0.5\n      erase_p: 0.1\n</code></pre></p>"},{"location":"config/#model-configuration-model_config","title":"Model Configuration (<code>model_config</code>)","text":"<p>The model configuration section defines the neural network architecture, including backbone and head configurations.</p>"},{"location":"config/#model-initialization","title":"Model Initialization","text":"<ul> <li><code>init_weights</code>: (str) Model weights initialization method. \"default\" uses kaiming uniform initialization and \"xavier\" uses Xavier initialization method. Default: <code>\"default\"</code></li> <li><code>pretrained_backbone_weights</code>: (str) Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the backbone is initialized. If <code>None</code>, random init is used. Default: <code>None</code></li> <li><code>pretrained_head_weights</code>: (str) Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the head layers are initialized. If <code>None</code>, random init is used. Default: <code>None</code></li> </ul>"},{"location":"config/#backbone-configuration","title":"Backbone Configuration","text":"<p>Note: Configs should be provided only for the model to train and others should be <code>None</code>.</p>"},{"location":"config/#unet-backbone","title":"UNet Backbone","text":"<ul> <li><code>backbone_config.unet</code>:<ul> <li><code>in_channels</code>: (int) Number of input channels. Default: <code>1</code></li> <li><code>kernel_size</code>: (int) Size of the convolutional kernels. Default: <code>3</code></li> <li><code>filters</code>: (int) Base number of filters in the network. Default: <code>32</code></li> <li><code>filters_rate</code>: (float) Factor to adjust the number of filters per block. Default: <code>1.5</code></li> <li><code>max_stride</code>: (int) Scalar integer specifying the maximum stride which is used to compute the number of down blocks. Default: <code>16</code></li> <li><code>stem_stride</code>: (int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. Default: <code>None</code></li> <li><code>middle_block</code>: (bool) If True, add an additional block at the end of the encoder. Default: <code>True</code></li> <li><code>up_interpolate</code>: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code></li> <li><code>stacks</code>: (int) Number of upsampling blocks in the decoder. Default: <code>1</code></li> <li><code>convs_per_block</code>: (int) Number of convolutional layers per block. Default: <code>2</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Ideally, this should be minimum of the output strides of all head layers. Default: <code>1</code></li> </ul> </li> </ul> <p>Example UNet configuration: <pre><code>model_config:\n  backbone_config:\n    unet:\n      in_channels: 1\n      kernel_size: 3\n      filters: 32\n      filters_rate: 1.5\n      max_stride: 16\n      stem_stride: null\n      middle_block: true\n      up_interpolate: true\n      stacks: 1\n      convs_per_block: 2\n      output_stride: 1\n    convnext: null\n    swint: null\n</code></pre></p>"},{"location":"config/#convnext-backbone","title":"ConvNeXt Backbone","text":"<ul> <li><code>backbone_config.convnext</code>:</li> <li><code>pre_trained_weights</code>: (str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"]. Default: <code>None</code></li> <li><code>arch</code>: (Default is <code>Tiny</code> architecture config. No need to provide if <code>model_type</code> is provided)<ul> <li><code>depths</code>: (List[int]) Number of layers in each block. Default: <code>[3, 3, 9, 3]</code></li> <li><code>channels</code>: (List[int]) Number of channels in each block. Default: <code>[96, 192, 384, 768]</code></li> </ul> </li> <li><code>model_type</code>: (str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: <code>\"tiny\"</code></li> <li><code>max_stride</code>: (int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures provided stem_stride is 2. Default: <code>32</code></li> <li><code>stem_patch_kernel</code>: (int) Size of the convolutional kernels in the stem layer. Default: <code>4</code></li> <li><code>stem_patch_stride</code>: (int) Convolutional stride in the stem layer. Default: <code>2</code></li> <li><code>in_channels</code>: (int) Number of input channels. Default: <code>1</code></li> <li><code>kernel_size</code>: (int) Size of the convolutional kernels. Default: <code>3</code></li> <li><code>filters_rate</code>: (float) Factor to adjust the number of filters per block. Default: <code>2</code></li> <li><code>convs_per_block</code>: (int) Number of convolutional layers per block. Default: <code>2</code></li> <li><code>up_interpolate</code>: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Ideally, this should be minimum of the output strides of all head layers. Default: <code>1</code></li> </ul> <p>Example ConvNeXt configuration: <pre><code>model_config:\n  backbone_config:\n    unet: null\n    convnext:\n      pre_trained_weights: \"ConvNeXt_Tiny_Weights\"\n      model_type: \"tiny\"\n      max_stride: 32\n      stem_patch_kernel: 4\n      stem_patch_stride: 2\n      in_channels: 1\n      kernel_size: 3\n      filters_rate: 2\n      convs_per_block: 2\n      up_interpolate: true\n      output_stride: 1\n    swint: null\n</code></pre></p>"},{"location":"config/#swin-transformer-backbone","title":"Swin Transformer Backbone","text":"<ul> <li><code>backbone_config.swint</code>:</li> <li><code>pre_trained_weights</code>: (str) Pretrained weights file name supported only for SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]. Default: <code>None</code></li> <li><code>model_type</code>: (str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. Default: <code>\"tiny\"</code></li> <li><code>arch</code>: Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. Default: <code>None</code></li> <li><code>max_stride</code>: (int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all swint architectures provided stem_stride is 2. Default: <code>32</code></li> <li><code>patch_size</code>: (int) Patch size for the stem layer of SwinT. Default: <code>4</code></li> <li><code>stem_patch_stride</code>: (int) Stride for the patch. Default: <code>2</code></li> <li><code>window_size</code>: (int) Window size. Default: <code>7</code></li> <li><code>in_channels</code>: (int) Number of input channels. Default: <code>1</code></li> <li><code>kernel_size</code>: (int) Size of the convolutional kernels. Default: <code>3</code></li> <li><code>filters_rate</code>: (float) Factor to adjust the number of filters per block. Default: <code>2</code></li> <li><code>convs_per_block</code>: (int) Number of convolutional layers per block. Default: <code>2</code></li> <li><code>up_interpolate</code>: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Ideally, this should be minimum of the output strides of all head layers. Default: <code>1</code></li> </ul> <p>Example SwinT configuration: <pre><code>model_config:\n  backbone_config:\n    unet: null\n    convnext: null\n    swint:\n      pre_trained_weights: \"Swin_T_Weights\"\n      model_type: \"tiny\"\n      max_stride: 32\n      patch_size: 4\n      stem_patch_stride: 2\n      window_size: 7\n      in_channels: 1\n      kernel_size: 3\n      filters_rate: 2\n      convs_per_block: 2\n      up_interpolate: true\n      output_stride: 1\n</code></pre></p>"},{"location":"config/#head-configuration","title":"Head Configuration","text":"<p>Note: Configs should be provided only for the model to train and others should be <code>None</code>.</p>"},{"location":"config/#single-instance-head","title":"Single Instance Head","text":"<ul> <li><code>head_configs.single_instance.confmaps</code>:<ul> <li><code>part_names</code>: (List[str]) <code>None</code> if nodes from <code>sio.Labels</code> file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'.</li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>5.0</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code></li> </ul> </li> </ul> <p>Example Single Instance configuration: <pre><code>model_config:\n  head_configs:\n    single_instance:\n      confmaps:\n        part_names: null  # Uses all body parts from skeleton\n        sigma: 5.0\n        output_stride: 1\n    centroid: null\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown: null\n</code></pre></p>"},{"location":"config/#centroid-head","title":"Centroid Head","text":"<ul> <li><code>head_configs.centroid.confmaps</code>:<ul> <li><code>anchor_part</code>: (str) Node name (as per skeleton) to use as the anchor point. If None, the midpoint of the bounding box of all visible instance points will be used as the anchor. The bounding box midpoint will also be used if the anchor part is specified but not visible in the instance. Setting a reliable anchor point can significantly improve topdown model accuracy as they benefit from a consistent geometry of the body parts relative to the center of the image. Default: <code>None</code></li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>5.0</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code></li> </ul> </li> </ul> <p>Example Centroid configuration: <pre><code>model_config:\n  head_configs:\n    single_instance: null\n    centroid:\n      confmaps:\n        anchor_part: null  # Uses bounding box midpoint\n        sigma: 5.0\n        output_stride: 1\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown: null\n</code></pre></p>"},{"location":"config/#centered-instance-head","title":"Centered Instance Head","text":"<ul> <li><code>head_configs.centered_instance.confmaps</code>:<ul> <li><code>part_names</code>: (List[str]) <code>None</code> if nodes from <code>sio.Labels</code> file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'. Default: <code>None</code></li> <li><code>anchor_part</code>: (str) Node name (as per skeleton) to use as the anchor point. If None, the midpoint of the bounding box of all visible instance points will be used as the anchor. The bounding box midpoint will also be used if the anchor part is specified but not visible in the instance. Setting a reliable anchor point can significantly improve topdown model accuracy as they benefit from a consistent geometry of the body parts relative to the center of the image. Default: <code>None</code></li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>5.0</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code></li> </ul> </li> </ul> <p>Example Centered Instance configuration: <pre><code>model_config:\n  head_configs:\n    single_instance: null\n    centroid: null\n    centered_instance:\n      confmaps:\n        part_names: null  # Uses all body parts from skeleton\n        anchor_part: null  # Uses bounding box midpoint\n        sigma: 5.0\n        output_stride: 1\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown: null\n</code></pre></p>"},{"location":"config/#bottom-up-head","title":"Bottom-Up Head","text":"<ul> <li> <p><code>head_configs.bottomup.confmaps</code>:</p> <ul> <li><code>part_names</code>: (List[str]) <code>None</code> if nodes from <code>sio.Labels</code> file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'. Default: <code>None</code></li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>5.0</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code></li> <li><code>loss_weight</code>: (float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models. Default: <code>None</code></li> </ul> </li> <li> <p><code>head_configs.bottomup.pafs</code>: (same structure as that of <code>confmaps</code>. Note: This section is only for BottomUp model.)</p> <ul> <li><code>edges</code>: (List[str]) <code>None</code> if edges from <code>sio.Labels</code> file can be used directly. Note: Only for 'PartAffinityFieldsHead'. List of indices <code>(src, dest)</code> that form an edge. Default: <code>None</code></li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>15.0</code> Note: For PAFs, a higher sigma is usually used compared to confidence maps, as the vector fields benefit from a broader spatial spread.</li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code></li> <li><code>loss_weight</code>: (float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models. Default: <code>None</code></li> </ul> </li> </ul> <p>Example Bottom-Up configuration: <pre><code>model_config:\n  head_configs:\n    single_instance: null\n    centroid: null\n    centered_instance: null\n    bottomup:\n      confmaps:\n        part_names: null  # Uses all body parts from skeleton\n        sigma: 5.0\n        output_stride: 1\n        loss_weight: 1.0\n      pafs:\n        edges: null  # Uses all edges from skeleton\n        sigma: 15.0\n        output_stride: 1\n        loss_weight: 1.0\n    multi_class_bottomup: null\n    multi_class_topdown: null\n</code></pre></p>"},{"location":"config/#multi-class-bottom-up-head","title":"Multi-Class Bottom-Up Head","text":"<ul> <li> <p><code>head_configs.multi_class_bottomup.confmaps</code>:</p> <ul> <li><code>part_names</code>: (List[str]) <code>None</code> if nodes from <code>sio.Labels</code> file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'. Default: <code>None</code></li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>5.0</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code></li> <li><code>loss_weight</code>: (float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models. Default: <code>None</code></li> </ul> </li> <li> <p><code>head_configs.multi_class_bottomup.class_maps</code>:</p> <ul> <li><code>classes</code>: (List[str]) List of class (track) names. Default: <code>None</code>. When <code>None</code>, these are inferred from the track names in the labels file.</li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>5.0</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code></li> <li><code>loss_weight</code>: (float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models. Default: <code>None</code></li> </ul> </li> </ul> <p>Example Multi-Class Bottom-Up configuration: <pre><code>model_config:\n  head_configs:\n    single_instance: null\n    centroid: null\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup:\n      confmaps:\n        part_names: null  # Uses all body parts from skeleton\n        sigma: 5.0\n        output_stride: 1\n        loss_weight: 1.0\n      class_maps:\n        classes: null  # Inferred from track names in labels file\n        sigma: 5.0\n        output_stride: 1\n        loss_weight: 1.0\n    multi_class_topdown: null\n</code></pre></p>"},{"location":"config/#multi-class-top-down-head","title":"Multi-Class Top-Down Head","text":"<ul> <li> <p><code>head_configs.multi_class_topdown.confmaps</code>:</p> <ul> <li><code>part_names</code>: (List[str]) <code>None</code> if nodes from <code>sio.Labels</code> file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'. Default: <code>None</code></li> <li><code>anchor_part</code>: (str) Node name to use as the anchor point. If None, the midpoint of the bounding box of all visible instance points will be used as the anchor. The bounding box midpoint will also be used if the anchor part is specified but not visible in the instance. Setting a reliable anchor point can significantly improve topdown model accuracy as they benefit from a consistent geometry of the body parts relative to the center of the image. Default: <code>None</code></li> <li><code>sigma</code>: (float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied. Default: <code>5.0</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>. (Ideally this should be same as the backbone's maxstride).</li> <li><code>loss_weight</code>: (float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models. Default: <code>None</code></li> </ul> </li> <li> <p><code>head_configs.multi_class_topdown.class_vectors</code>:</p> <ul> <li><code>classes</code>: (List[str]) List of class (track) names. Default: <code>None</code>. When <code>None</code>, these are inferred from the track names in the labels file.</li> <li><code>num_fc_layers</code>: (int) Number of fully connected layers after flattening input features. Default: <code>1</code></li> <li><code>num_fc_units</code>: (int) Number of units (dimensions) in fully connected layers prior to classification output. Default: <code>64</code></li> <li><code>global_pool</code>: (bool) Enable global pooling. Default: <code>True</code></li> <li><code>output_stride</code>: (int) The stride of the output confidence maps relative to the input image. These should be the same as max stride set in the backbone config as we take the feature maps from the last layer of the encoder. Default: <code>16</code></li> <li><code>loss_weight</code>: (float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models. Default: <code>None</code></li> </ul> </li> </ul> <p>Example Multi-Class Top-Down configuration: <pre><code>model_config:\n  head_configs:\n    single_instance: null\n    centroid: null\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown:\n      confmaps:\n        part_names: null  # Uses all body parts from skeleton\n        anchor_part: null  # Uses bounding box midpoint\n        sigma: 5.0\n        output_stride: 1\n        loss_weight: 1.0\n      class_vectors:\n        classes: null  # Inferred from track names in labels file\n        num_fc_layers: 1\n        num_fc_units: 64\n        global_pool: true\n        output_stride: 16 # should be same as `max_stride` in `backbone_config`\n        loss_weight: 1.0\n</code></pre></p>"},{"location":"config/#trainer-configuration-trainer_config","title":"Trainer Configuration (<code>trainer_config</code>)","text":"<p>The trainer configuration section controls the training process, including data loading, optimization, and monitoring.</p>"},{"location":"config/#data-loader-settings","title":"Data Loader Settings","text":"<ul> <li><code>train_data_loader</code>:<ul> <li><code>batch_size</code>: (int) Number of samples per batch or batch size for training data. Default: <code>1</code></li> <li><code>shuffle</code>: (bool) True to have the data reshuffled at every epoch. Default: <code>False</code></li> <li><code>num_workers</code>: (int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: <code>0</code></li> </ul> </li> <li><code>val_data_loader</code>: (Similar to <code>train_data_loader</code>)</li> </ul> <p>Example Data Loader configurations:</p> <p>Basic configuration (single GPU): <pre><code>trainer_config:\n  train_data_loader:\n    batch_size: 4\n    shuffle: true\n    num_workers: 0\n  val_data_loader:\n    batch_size: 4\n    shuffle: false\n    num_workers: 0\n</code></pre></p>"},{"location":"config/#model-checkpointing","title":"Model Checkpointing","text":"<ul> <li><code>model_ckpt</code>:<ul> <li><code>save_top_k</code>: (int) If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. Default: <code>1</code></li> <li><code>save_last</code>: (bool) When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. Default: <code>None</code></li> </ul> </li> </ul>"},{"location":"config/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li><code>trainer_devices</code>: (int) Number of devices to train on (int), which devices to train on (list or str), or \"auto\" to select automatically. Default: <code>\"auto\"</code></li> <li><code>trainer_accelerator</code>: (str) One of the (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the <code>Trainer</code> to be connected to. Default: <code>\"auto\"</code></li> <li><code>profiler</code>: (str) Profiler for pytorch Trainer. One of [\"advanced\", \"passthrough\", \"pytorch\", \"simple\"]. Default: <code>None</code></li> <li><code>trainer_strategy</code>: (str) Training strategy, one of [\"auto\", \"ddp\", \"fsdp\", \"ddp_find_unused_parameters_false\", \"ddp_find_unused_parameters_true\", ...]. This supports any training strategy that is supported by <code>lightning.Trainer</code>. Default: <code>\"auto\"</code></li> </ul>"},{"location":"config/#training-control","title":"Training Control","text":"<ul> <li><code>enable_progress_bar</code>: (bool) When True, enables printing the logs during training. Default: <code>True</code></li> <li><code>min_train_steps_per_epoch</code>: (int) Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer <code>limit_train_batches</code> parameter of Torch <code>Trainer</code>. Default: <code>200</code></li> <li><code>train_steps_per_epoch</code>: (int) Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data or <code>min_train_steps_per_epoch</code>, whichever is largest. Default: <code>None</code>. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>. </li> <li><code>visualize_preds_during_training</code>: (bool) If set to <code>True</code>, sample predictions (keypoints + confidence maps) are saved to <code>viz</code> folder in the ckpt dir and in wandb table. Default: <code>False</code></li> <li><code>keep_viz</code>: (bool) If set to <code>True</code>, the <code>viz</code> folder containing training visualizations will be kept after training completes. If <code>False</code>, the folder will be deleted. This parameter only has an effect when <code>visualize_preds_during_training</code> is <code>True</code>. Default: <code>False</code></li> <li><code>max_epochs</code>: (int) Maximum number of epochs to run. Default: <code>10</code></li> <li><code>seed</code>: (int) Seed value for the current experiment. Default: <code>0</code></li> <li><code>use_wandb</code>: (bool) True to enable wandb logging. Default: <code>False</code></li> <li><code>save_ckpt</code>: (bool) True to enable checkpointing. Default: <code>False</code></li> <li><code>save_ckpt_path</code>: (str) Directory path to save the training config and checkpoint files. Default: <code>None</code></li> <li><code>resume_ckpt_path</code>: (str) Path to <code>.ckpt</code> file from which training is resumed. Default: <code>None</code></li> </ul>"},{"location":"config/#optimizer-configuration","title":"Optimizer Configuration","text":"<ul> <li><code>optimizer_name</code>: (str) Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: <code>\"Adam\"</code></li> <li><code>optimizer</code>:<ul> <li><code>lr</code>: (float) Learning rate of type float. Default: <code>1e-3</code></li> <li><code>amsgrad</code>: (bool) Enable AMSGrad with the optimizer. Default: <code>False</code></li> </ul> </li> </ul>"},{"location":"config/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Note: Configs should only be provided for one scheduler. Others should be <code>None</code>.</p>"},{"location":"config/#step-lr-scheduler","title":"Step LR Scheduler","text":"<ul> <li><code>lr_scheduler.step_lr</code>:<ul> <li><code>step_size</code>: (int) Period of learning rate decay. If <code>step_size</code>=10, then every 10 epochs, learning rate will be reduced by a factor of <code>gamma</code>. Default: <code>10</code></li> <li><code>gamma</code>: (float) Multiplicative factor of learning rate decay. Default: <code>0.1</code></li> </ul> </li> </ul>"},{"location":"config/#reduce-lr-on-plateau","title":"Reduce LR on Plateau","text":"<ul> <li><code>lr_scheduler.reduce_lr_on_plateau</code>:<ul> <li><code>threshold</code>: (float) Threshold for measuring the new optimum, to only focus on significant changes. Default: <code>1e-4</code></li> <li><code>threshold_mode</code>: (str) One of \"rel\", \"abs\". In rel mode, dynamic_threshold = best * ( 1 + threshold ) in max mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: <code>\"rel\"</code></li> <li><code>cooldown</code>: (int) Number of epochs to wait before resuming normal operation after lr has been reduced. Default: <code>0</code></li> <li><code>patience</code>: (int) Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the third epoch if the loss still hasn't improved then. Default: <code>10</code></li> <li><code>factor</code>: (float) Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: <code>0.1</code></li> <li><code>min_lr</code>: (float or List[float]) A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: <code>0.0</code></li> </ul> </li> </ul> <p>Example Learning Rate Scheduler configurations:</p> <p>No scheduler (constant learning rate): <pre><code>trainer_config:\n  lr_scheduler:\n    step_lr: null\n    reduce_lr_on_plateau: null\n</code></pre></p> <p>Step LR Scheduler: <pre><code>trainer_config:\n  lr_scheduler:\n    step_lr:\n      step_size: 20\n      gamma: 0.5\n    reduce_lr_on_plateau: null\n</code></pre></p> <p>Reduce LR on Plateau: <pre><code>trainer_config:\n  lr_scheduler:\n    step_lr: null\n    reduce_lr_on_plateau:\n      threshold: 1e-6\n      threshold_mode: \"rel\"\n      cooldown: 3\n      patience: 5\n      factor: 0.5\n      min_lr: 1e-8\n</code></pre></p>"},{"location":"config/#early-stopping","title":"Early Stopping","text":"<ul> <li><code>early_stopping</code>:<ul> <li><code>stop_training_on_plateau</code>: (bool) True if early stopping should be enabled. Default: <code>False</code></li> <li><code>min_delta</code>: (float) Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. Default: <code>0.0</code></li> <li><code>patience</code>: (int) Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. Default: <code>1</code></li> </ul> </li> </ul>"},{"location":"config/#online-hard-keypoint-mining-ohkm","title":"Online Hard Keypoint Mining (OHKM)","text":"<ul> <li><code>online_hard_keypoint_mining</code>:<ul> <li><code>online_mining</code>: (bool) If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss. Default: <code>False</code></li> <li><code>hard_to_easy_ratio</code>: (float) The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training. Default: <code>2.0</code></li> <li><code>min_hard_keypoints</code>: (int) The minimum number of keypoints that will be considered as \"hard\", even if they are not below the <code>hard_to_easy_ratio</code>. Default: <code>2</code></li> <li><code>max_hard_keypoints</code>: (int) The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether. Default: <code>None</code></li> <li><code>loss_scale</code>: (float) Factor to scale the hard keypoint losses by. Default: <code>5.0</code></li> </ul> </li> </ul>"},{"location":"config/#wandb-configuration","title":"WandB Configuration","text":"<p>Note: Only required if <code>use_wandb</code> is <code>True</code>.</p> <ul> <li><code>wandb</code>:<ul> <li><code>entity</code>: (str) Entity of wandb project. Default: <code>None</code></li> <li><code>project</code>: (str) Project name for the wandb project. Default: <code>None</code></li> <li><code>name</code>: (str) Name of the current run. Default: <code>None</code></li> <li><code>api_key</code>: (str) API key. The API key is masked when saved to config files. Default: <code>None</code></li> <li><code>wandb_mode</code>: (str) \"offline\" if only local logging is required. Default: <code>\"None\"</code></li> <li><code>prv_runid</code>: (str) Previous run ID if training should be resumed from a previous ckpt. Default: <code>None</code></li> <li><code>group</code>: (str) Group name for the run.</li> </ul> </li> </ul> <p>Example WandB configurations:</p> <p>Basic WandB logging: <pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: \"your_username\"\n    project: \"sleap_nn_experiments\"\n    name: \"single_instance_unet_training\"\n    api_key: null  # Set via environment variable WANDB_API_KEY\n    wandb_mode: \"online\"\n    prv_runid: null\n    group: null\n</code></pre></p> <p>Offline WandB logging: <pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: \"your_username\"\n    project: \"sleap_nn_experiments\"\n    name: \"offline_experiment\"\n    api_key: null\n    wandb_mode: \"offline\"\n    prv_runid: null\n    group: \"experiment_group\"\n</code></pre></p> <p>Resume from previous run: <pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: \"your_username\"\n    project: \"sleap_nn_experiments\"\n    name: \"continued_training\"\n    api_key: null\n    wandb_mode: \"online\"\n    prv_runid: \"abc123def456\"\n    group: \"continued_experiments\"\n</code></pre></p> <p>No WandB logging: <pre><code>trainer_config:\n  use_wandb: false\n  # wandb section not needed when use_wandb is false\n</code></pre></p>"},{"location":"config/#zmq-configuration","title":"ZMQ Configuration","text":"<ul> <li><code>zmq</code>:<ul> <li><code>publish_address</code>: (str) Specifies the address and port to which the training logs (loss values) should be sent to. Default: <code>None</code></li> <li><code>controller_address</code>: (str) Specifies the address and port to listen to to stop the training (specific to SLEAP GUI). Default: <code>None</code></li> <li><code>controller_polling_timeout</code>: (int) Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed. Default: <code>10</code></li> </ul> </li> </ul>"},{"location":"config/#next-steps","title":"Next Steps","text":"<ul> <li>Model Architectures</li> <li>Training Guide</li> <li>API Reference</li> </ul>"},{"location":"example_notebooks/","title":"Example Notebooks","text":"<p>This page provides information on how to access and use the example notebooks from the sleap-nn GitHub repository.</p>"},{"location":"example_notebooks/#accessing-example-notebooks","title":"Accessing Example Notebooks","text":"<p>The example notebooks are available in the sleap-nn GitHub repository in the <code>example_notebooks/</code> folder. These notebooks are created with marimo and provide interactive tutorials for various sleap-nn workflows.</p>"},{"location":"example_notebooks/#getting-started-with-marimo","title":"Getting Started with Marimo","text":"<p>The example notebooks use marimo, which provides a sandboxed environment that automatically handles all dependencies. You don't need to create a separate sleap-nn environment.</p>"},{"location":"example_notebooks/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install uv:     <pre><code>pip install uv\n</code></pre></p> </li> <li> <p>Create a new folder:</p> <p>Clone the <code>sleap-nn</code> repo   <pre><code>cd sleap-nn/example_notebooks\n</code></pre></p> <p>Or make a new directory   <pre><code>mkdir sleap-nn-tutorials\n</code></pre></p> </li> <li> <p>Initialize uv:</p> <p>Ensure the working directory where you run <code>uv init</code> doesn't have an existing <code>pyproject.toml</code>.   <pre><code>uv init\n</code></pre></p> </li> </ol>"},{"location":"example_notebooks/#setup","title":"Setup","text":"<ol> <li>Add marimo to your project:</li> </ol> <p>Ensure the working directory where you run <code>uv init</code> doesn't have an existing <code>pyproject.toml</code> as <code>uv add</code> will try to add marimo to the existing <code>.toml</code>.</p> <pre><code>uv add marimo\n</code></pre> <ol> <li>Run the marimo notebooks</li> </ol> <p>Ensure the python scripts are in your current working directory!</p> <p>(i) Training Demo Notebook</p> <ul> <li>Description: End-to-end demo on creating config files and running training, inference, and evaluation using sleap-nn APIs.</li> </ul> <pre><code>   uvx marimo edit --sandbox training_demo.py\n</code></pre> <p>(ii) Augmentation Guide Notebook</p> <ul> <li>Description: Visualize the different data augmentation techniques available in sleap-nn.</li> </ul> <pre><code>   uvx marimo edit --sandbox augmentation_guide.py\n</code></pre> <p>(iii) Receptive Field Guide Notebook</p> <ul> <li>Description: Visualize how the receptive field could be set by changing the config parameters.</li> </ul> <pre><code>   uvx marimo edit --sandbox receptive_field_guide.py\n</code></pre>"},{"location":"inference/","title":"Running Inference","text":"<p>SLEAP-NN provides powerful inference capabilities for pose estimation with support for multiple model types, tracking, and legacy SLEAP model compatibility. SLEAP-NN supports inference on:</p> <ul> <li>Videos: Any format supported by OpenCV</li> <li>SLEAP Labels: <code>.slp</code> files</li> </ul>"},{"location":"inference/#run-inference-with-cli","title":"Run Inference with CLI","text":"<pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --model_paths models/ckpt_folder/\n</code></pre> <p>To run inference on video files with specific frames <pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --frames \"1-100,200-300\" \\\n    --model_paths models/ckpt_folder/\n</code></pre></p> <p>To run inference with different backbone weights than the one in <code>models/ckpt_folder/</code> <pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --frames \"1-100,200-300\" \\\n    --model_paths models/ckpt_folder/ \\\n    --backbone_ckpt_path models/backbone.ckpt\n</code></pre></p> <p>For two-stage models (topdown and multiclass topdown), both the centroid and centered-instance model ckpts should be provided as given below: <pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --model_paths models/centroid_unet/ \\\n    --model_paths models/centered_instance_unet/\n</code></pre></p> <p>Note</p> <p>Note (for centroid-only inference): The centroid model is essentially the first stage of TopDown model workflow, which only predicts centers, not keypoints. In centroid-only inference, each predicted centroid is matched (by Euclidean distance) to the nearest ground-truth instance, and the ground-truth keypoints are copied for display. Therefore, an OKS mAP of 1.0 just means all instances were detected\u2014it does not reflect pose/keypoint accuracy. To evaluate keypoints, run the second stage (the pose model) rather than centroid-only inference.</p>"},{"location":"inference/#arguments-for-cli","title":"Arguments for CLI","text":""},{"location":"inference/#essential-parameters","title":"Essential Parameters","text":"Parameter Description Default <code>--data_path</code> Path to <code>.slp</code> file or <code>.mp4</code> to run inference on Required <code>--model_paths</code> List of paths to the directory where the best.ckpt and training_config.yaml are saved Required <code>--output_path</code> The output filename to use for the predicted data. If not provided, defaults to '[data_path].slp' <code>[input].slp</code> <code>--device</code> Device on which torch.Tensor will be allocated. One of ('cpu', 'cuda', 'mps', 'auto', 'opencl', 'ideep', 'hip', 'msnpu'). Default: 'auto' (based on available backend either cuda, mps or cpu is chosen) <code>auto</code> <code>--batch_size</code> Number of frames to predict at a time. Larger values result in faster inference speeds, but require more memory <code>4</code> <code>--peak_threshold</code> Minimum confidence map value to consider a peak as valid <code>0.2</code> <code>--integral_refinement</code> If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>'integral'</code>, peaks will be refined with integral regression. Default: 'integral'. <code>integral</code>"},{"location":"inference/#model-configuration","title":"Model Configuration","text":"Parameter Description Default <code>--backbone_ckpt_path</code> To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here <code>None</code> <code>--head_ckpt_path</code> Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided) <code>None</code> <code>--max_instances</code> Limit maximum number of instances in multi-instance models. Not available for ID models <code>None</code>"},{"location":"inference/#image-pre-processing","title":"Image Pre-processing","text":"Parameter Description Default <code>--max_height</code> Maximum height the image should be padded to. If not provided, the values from the training config are used. Default: None. <code>None</code> <code>--max_width</code> Maximum width the image should be padded to. If not provided, the values from the training config are used. Default: None. <code>None</code> <code>--input_scale</code> Scale factor to apply to the input image. If not provided, the values from the training config are used. Default: None. <code>None</code> <code>--ensure_rgb</code> True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to <code>True</code>, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the values from the training config are used. Default: <code>None</code>. <code>False</code> <code>--ensure_grayscale</code> True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the values from the training config are used. Default: <code>None</code>. <code>False</code> <code>--crop_size</code> Crop size. If not provided, the crop size from training_config.yaml is used <code>None</code> <code>--anchor_part</code> The node name to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used. Default: <code>None</code>. <code>None</code>"},{"location":"inference/#data-selection","title":"Data Selection","text":"Parameter Description Default <code>--only_labeled_frames</code> <code>True</code> if inference should be run only on user-labeled frames <code>False</code> <code>--only_suggested_frames</code> <code>True</code> if inference should be run only on unlabeled suggested frames <code>False</code> <code>--video_index</code> Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path <code>None</code> <code>--video_dataset</code> The dataset for HDF5 videos <code>None</code> <code>--video_input_format</code> The input_format for HDF5 videos <code>channels_last</code> <code>--frames</code> List of frames indices. If <code>None</code>, all frames in the video are used All frames"},{"location":"inference/#performance","title":"Performance","text":"Parameter Description Default <code>--queue_maxsize</code> Maximum size of the frame buffer queue <code>8</code>"},{"location":"inference/#tracking-parameters","title":"Tracking Parameters","text":"Parameter Description Default <code>--tracking</code> If True, runs tracking on the predicted instances <code>False</code> <code>--tracking_window_size</code> Number of frames to look for in the candidate instances to match with the current detections <code>5</code> <code>--min_new_track_points</code> We won't spawn a new track for an instance with fewer than this many points <code>0</code> <code>--candidates_method</code> Either of <code>fixed_window</code> or <code>local_queues</code>. In fixed window method, candidates from the last <code>window_size</code> frames. In local queues, last <code>window_size</code> instances for each track ID is considered for matching against the current detection <code>fixed_window</code> <code>--min_match_points</code> Minimum non-NaN points for match candidates <code>0</code> <code>--features</code> Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>] <code>keypoints</code> <code>--scoring_method</code> Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>] <code>oks</code> <code>--scoring_reduction</code> Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>] <code>mean</code> <code>--robust_best_instance</code> If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value <code>1.0</code> <code>--track_matching_method</code> Track matching algorithm. One of <code>hungarian</code>, <code>greedy</code> <code>hungarian</code> <code>--max_tracks</code> Maximum number of new tracks to be created to avoid redundant tracks (only for local queues candidate) <code>None</code> <code>--use_flow</code> If True, <code>FlowShiftTracker</code> is used, where the poses are matched using optical flow shifts <code>False</code> <code>--of_img_scale</code> Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements (only if <code>use_flow</code> is True) <code>1.0</code> <code>--of_window_size</code> Optical flow window size to consider at each pyramid scale level (only if <code>use_flow</code> is True) <code>21</code> <code>--of_max_levels</code> Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling (only if <code>use_flow</code> is True) <code>3</code> <code>--post_connect_single_breaks</code> If True and <code>max_tracks</code> is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame <code>False</code>"},{"location":"inference/#run-inference-with-api","title":"Run inference with API","text":"<p>To return predictions as list of dictionaries, </p> <pre><code>from sleap_nn.predict import run_inference\n\n# Run inference\nlabels = run_inference(\n    data_path=\"video.mp4\",\n    model_paths=[\"models/unet/\"],\n    make_labels=False,\n    return_confmaps=True\n)\n</code></pre> <p>To return predictions as a <code>sleap_io.Labels</code> object, </p> <pre><code>from sleap_nn.predict import run_inference\n\n# Run inference\nlabels = run_inference(\n    data_path=\"video.mp4\",\n    model_paths=[\"models/unet/\"],\n    output_path=\"predictions.slp\",\n    make_labels=True,\n)\n</code></pre> <p>For a detailed explanation of all available parameters for <code>run_inference</code>, see the API docs for run_inference().</p>"},{"location":"inference/#tracking","title":"Tracking","text":"<p>SLEAP-NN includes sophisticated tracking capabilities for multi-instance scenarios. The output labels object (or <code>.slp</code> file) will have track IDs associated to the predicted instances (or user-labeled instances).</p>"},{"location":"inference/#tracking-methods","title":"Tracking Methods","text":""},{"location":"inference/#fixed-window-tracking","title":"Fixed Window Tracking","text":"<p>This method maintains a fixed-size window of the last N frames and uses all instances from those frames as candidates for matching.</p> <pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --model_paths models/bottomup_unet/ \\\n    --tracking \\\n    --candidates_method fixed_window \\\n    --tracking_window_size 10\n</code></pre>"},{"location":"inference/#local-queues-tracking","title":"Local Queues Tracking","text":"<p>This method maintains separate queues for each track ID, keeping the last N instances per track. It's more robust to track breaks but requires more memory and computation.</p> <pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --model_paths models/bottomup_unet/ \\\n    --tracking \\\n    --candidates_method local_queues \\\n    --tracking_window_size 5\n</code></pre>"},{"location":"inference/#optical-flow-tracking","title":"Optical Flow Tracking","text":"<p>This method uses optical flow to shift the candidates onto the frame to be tracked and then associates the untracked instances to the shifted instances.</p> <pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --model_paths models/bottomup_unet/ \\\n    --tracking \\\n    --use_flow\n</code></pre>"},{"location":"inference/#track-only-workflow","title":"Track-only workflow","text":"<p>You can perform tracking on existing user-labeled instances\u2014without running inference to get new predictions\u2014by enabling tracking (<code>--tracking</code>) and omitting the <code>--model-paths</code> argument. This will associate tracks using only the provided labels.</p> <pre><code>sleap-nn-track \\\n    --data_path video.mp4 \\\n    --tracking \\\n    --candidates_method fixed_window \\\n    --tracking_window_size 10\n</code></pre>"},{"location":"inference/#legacy-sleap-model-support","title":"Legacy SLEAP Model Support","text":"<p>SLEAP-NN supporting running inference with models trained using the original SLEAP TensorFlow/Keras backend (sleap &lt;= 1.4), but only for UNet backbone models. To run inference with legacy SLEAP models (trained with TensorFlow/Keras, sleap \u2264 1.4), simply use the same CLI or API workflows described above in the With CLI and With API sections. </p> <p>Just provide the path to the directory containing both <code>best_model.h5</code> and <code>training_config.json</code> from your SLEAP training run as your <code>--model-paths</code> (CLI) or <code>model_paths</code> (API) argument. SLEAP-NN will automatically detect and load these legacy models for inference.</p>"},{"location":"inference/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>SLEAP-NN provides comprehensive evaluation capabilities to assess model performance against ground truth labels.</p> <p>Using CLI: <pre><code>sleap-nn-eval \\\n    --ground_truth_path gt_labels.slp \\\n    --predicted_path pred_labels.slp \\\n    --save_metrics pred_metrics.npz \\\n</code></pre></p> <p>Using <code>Evaluator</code> API: <pre><code>import sleap_io as sio\nfrom sleap_nn.evaluation import Evaluator\n\n# Load ground truth and predictions\ngt_labels = sio.load_slp(\"gt_labels.slp\")\npred_labels = sio.load_slp(\"pred_labels.slp\")\n\n# Create evaluator and compute metrics\nevaluator = Evaluator(gt_labels, pred_labels)\nmetrics = evaluator.evaluate()\n\n# Print results\nprint(f\"OKS mAP: {metrics['voc_metrics']['oks_voc.mAP']}\")\nprint(f\"mOKS: {metrics['mOKS']}\")\nprint(f\"Dist. error @90th percentile: {metrics['distance_metrics']['p90']}\")\n</code></pre></p>"},{"location":"inference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"inference/#common-issues","title":"Common Issues","text":""},{"location":"inference/#out-of-memory","title":"Out of Memory","text":"<ul> <li>Reduce <code>--batch_size</code></li> <li>Use <code>--device cpu</code> for CPU-only inference</li> </ul>"},{"location":"inference/#slow-inference","title":"Slow Inference","text":"<ul> <li>Increase <code>--batch_size</code> (if memory allows)</li> <li>Use <code>--device cuda</code> for GPU acceleration</li> </ul>"},{"location":"inference/#poor-predictions","title":"Poor Predictions","text":"<ul> <li>Adjust <code>--peak_threshold</code></li> <li>Check model compatibility</li> <li>Verify input preprocessing matches training</li> </ul>"},{"location":"inference/#tracking-issues","title":"Tracking Issues","text":"<ul> <li>Adjust <code>--tracking_window_size</code></li> <li>Try different feature representation and scoring method.</li> </ul>"},{"location":"inference/#next-steps","title":"Next Steps","text":"<ul> <li>Training Models</li> <li>Configuration Guide</li> <li>Model Architectures</li> <li>API Reference</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Prerequisites: Python 3.11</p>"},{"location":"installation/#from-pypi","title":"From PyPI","text":"<ul> <li>Windows/Linux with NVIDIA GPU (CUDA 11.8):</li> </ul> <pre><code>pip install sleap-nn[torch-cuda118]\n</code></pre> <ul> <li>Windows/Linux with NVIDIA GPU (CUDA 12.8):</li> </ul> <pre><code>pip install sleap-nn[torch-cuda128]\n</code></pre> <ul> <li>macOS with Apple Silicon (M1, M2, M3, M4) or CPU-only (no GPU or unsupported GPU):  Note: Even if torch-cpu is used on macOS, the MPS backend will be available. <pre><code>pip install sleap-nn[torch-cpu]\n</code></pre></li> </ul>"},{"location":"installation/#for-development-setup","title":"For development setup","text":"<ol> <li>Clone the sleap-nn repo</li> </ol> <pre><code>git clone https://github.com/talmolab/sleap-nn.git\ncd sleap-nn\n</code></pre> <ol> <li> <p>Install <code>uv</code> and development dependencies <code>uv</code> is a fast and modern package manager for <code>pyproject.toml</code>-based projects. Refer installation docs to install uv.</p> </li> <li> <p>Install sleap-nn dependencies based on your platform\\</p> </li> <li> <p>Sync all dependencies based on your correct wheel using <code>uv sync</code>:</p> <ul> <li>Windows/Linux with NVIDIA GPU (CUDA 11.8):</li> </ul> <pre><code>uv sync --extra dev --extra torch-cuda118\n</code></pre> <ul> <li>Windows/Linux with NVIDIA GPU (CUDA 12.8):</li> </ul> <pre><code>uv sync --extra dev --extra torch-cuda128\n</code></pre> <ul> <li>macOS with Apple Silicon (M1, M2, M3, M4) or CPU-only (no GPU or unsupported GPU):   Note: Even if torch-cpu is used on macOS, the MPS backend will be available.  <code>bash   uv sync --extra dev --extra torch-cpu</code></li> </ul> </li> </ol> <p>You can find the correct wheel for your system at:\\    \ud83d\udc49 https://pytorch.org/get-started/locally</p> <ol> <li> <p>Run tests <pre><code>uv run pytest tests\n</code></pre></p> </li> <li> <p>(Optional) Lint and format code <pre><code>uv run black --check sleap_nn tests\nuv run ruff check sleap_nn/\n</code></pre></p> </li> </ol>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors:</p> <ol> <li>Verify PyTorch is installed: <code>python -c \"import torch; print(torch.__version__)\"</code></li> <li>Try reinstalling with torch extras: <code>uv pip install -e \".[torch-cuda118]\"</code></li> </ol>"},{"location":"installation/#cuda-issues","title":"CUDA Issues","text":"<p>If you encounter CUDA-related errors:</p> <ol> <li>Verify your NVIDIA drivers are up to date</li> <li>Check CUDA compatibility with PyTorch version</li> <li>Try the CPU-only installation as a fallback</li> </ol>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Step-by-step guide on training models</li> <li>Configuration Guide</li> <li>Training models</li> </ul>"},{"location":"models/","title":"Models","text":""},{"location":"models/#model-types","title":"Model Types","text":""},{"location":"models/#single-instance","title":"\ud83d\udd39 Single Instance","text":"<ul> <li>The model predicts the pose of a single animal per frame.</li> <li>Useful for datasets where there is exactly one animal present in each frame.</li> <li>Simple and fast\u2014no need for instance detection or tracking.</li> </ul>"},{"location":"models/#top-down","title":"\ud83d\udd39 Top-Down","text":"<ul> <li>Stage 1: Centroid detection \u2013 The model first predicts the centroid (center point) of each animal in the frame, providing candidate locations for each instance.</li> <li>Stage 2: Centered instance pose estimation \u2013 For each detected centroid, a second model predicts the pose of the animal within a cropped region centered on that centroid.</li> <li>This approach enables accurate pose estimation in crowded scenes by focusing on one animal at a time.</li> <li>Particularly effective for datasets with moderate to high animal density, where animals are not heavily overlapping.</li> </ul>"},{"location":"models/#bottom-up","title":"\ud83d\udd39 Bottom-Up","text":"<ul> <li>Predicts all body part locations (keypoints) and their pairwise associations (Part Affinity Fields, PAFs) for all animals in the frame simultaneously.</li> <li>PAFs encode the direction and strength of connections between body parts, enabling the model to group keypoints into individual animals even when they overlap or are occluded.</li> <li>Assembles detected keypoints into individual animal instances by solving a global assignment problem based on the predicted PAFs.</li> <li>Effective for challenging scenarios with frequent occlusions, close physical contact, or overlapping animals.</li> </ul>"},{"location":"models/#top-down-id-model","title":"\ud83d\udd39 Top-Down ID model","text":"<ul> <li>Stage 1: Centroid detection \u2013 The model predicts the centroid of each animal instance (same as standard top-down, without classification).</li> <li>Stage 2: Centered instance pose estimation with classification \u2013 For each detected centroid, a second model predicts the pose of the animal using the image cropped around the centroid and also classifies the instance into predefined classes using supervised learning with ground truth track IDs from the training data.</li> <li>Training Requirement: Multi-class models require ground truth track IDs during training and are used to assign persistent IDs to animals across frames.</li> </ul>"},{"location":"models/#bottom-up-id-model","title":"\ud83d\udd39 Bottom-Up ID model","text":"<ul> <li>Predicts all body part locations (keypoints) and their class labels for all animals simultaneously.</li> <li>Directly classifies keypoints and groups them into instances with class assignments.</li> <li>Assembles detected keypoints into individual animal instances by solving a global assignment problem, while maintaining class-specific groupings.</li> <li>Training Requirement: Multi-class models require ground truth track IDs during training and are used to assign persistent IDs to animals across frames.</li> </ul>"},{"location":"models/#backbone-architectures","title":"Backbone Architectures","text":"<p>SLEAP-NN supports three different backbone architectures for feature extraction, each offering unique advantages for pose estimation tasks.</p>"},{"location":"models/#unet","title":"UNet","text":"<p>UNet is based on the original U-Net architecture from Ronneberger et al. (2015), which was made modular and adapted for pose estimation in Pereira et al. (2019). It uses an encoder-decoder structure with skip connections to capture both fine-grained details and high-level features. UNet offers the highest flexibility with configurable depth and filters. It supports stem blocks for initial downsampling, middle blocks for additional processing, and variable convolution layers per block, making it ideal for custom architectures.</p>"},{"location":"models/#swin-transformer-swint","title":"Swin Transformer (SwinT)","text":"<p>Swin Transformer (SwinT) is based on Liu et al. (2021) \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" and uses a hierarchical vision transformer with shifted windows to efficiently process images at multiple scales while maintaining the benefits of self-attention mechanisms. SwinT is also a large model that divides images into non-overlapping windows and applies self-attention within each window, with shifted window mechanisms allowing information flow between windows. It's available in Tiny, Small, and Base configurations and is particularly effective for capturing complex spatial relationships and global context understanding, though it requires significant computational resources. Pretrained ImageNet weights are available for all Tiny, Small, and Base configurations, enabling transfer learning for improved performance.</p>"},{"location":"models/#convnext","title":"ConvNeXt","text":"<p>ConvNeXt is based on Liu et al. (2022) \"A ConvNet for the 2020s\" and modernizes traditional convolutional networks by incorporating design principles from vision transformers while maintaining CNN efficiency. It uses a hierarchical structure with depth-wise convolutions, layer normalization, and modern activation functions. ConvNeXt is a large model that supports ImageNet pre-trained weights for transfer learning and is available in Tiny, Small, Base, and Large configurations, making it ideal for high-performance applications with standard image sizes. Pretrained ImageNet weights are available for all Tiny, Small, Base, and Large configurations, allowing easy initialization for transfer learning.</p>"},{"location":"models/#how-to-configure-models","title":"How to configure models?","text":"<p>To train or use a specific model type, you must set the corresponding <code>head_configs</code> in the <code>model_config</code> section of your configuration file. Different model types require different head configurations:</p> <ul> <li>Single Instance models use one head (<code>single_instance</code>)</li> <li>Top-Down models each model (centroid and centered-instance) use one head (<code>confmaps</code>)</li> <li>Bottom-Up models use two heads (<code>bottomup</code> with both <code>confmaps</code> and <code>pafs</code>)</li> <li>ID models use two heads (e.g., <code>multi_class_bottomup</code> with both <code>confmaps</code> and <code>class_maps</code>/ <code>class_vectors</code>)</li> </ul> <p>The choice and configuration of these heads determine the model's outputs and behavior, so it's important to set them carefully according to your task.</p> <p>For backbone configuration, you can specify the architecture type and parameters in the <code>backbone_config</code> section. Each backbone type (UNet, ConvNeXt, SwinT) has its own configuration options that control the network architecture, such as filter counts, depths, and other architectural parameters.</p> <p>For a detailed explanation of all <code>model_config</code>, <code>head_configs</code>, and <code>backbone_config</code> options\u2014including how to specify multiple heads and the meaning of each parameter\u2014see the backbone_config section and head_configs section.</p>"},{"location":"step_by_step_tutorial/","title":"Step-by-Step Training Tutorial","text":"<p>This tutorial will walk you through the complete process of training a pose estimation model from scratch.</p>"},{"location":"step_by_step_tutorial/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before starting, make sure you have <code>SLEAP-NN</code> installed (Refer <code>Installation docs</code>)</p>"},{"location":"step_by_step_tutorial/#step-1-configuration-setup","title":"\ud83d\ude80 Step 1: Configuration Setup","text":"<p>The first step is to set-up our configuration file, which configures the parameters required to train a pose estimation model with sleap-nn.</p>"},{"location":"step_by_step_tutorial/#11-load-a-sample-configuration","title":"1.1 Load a Sample Configuration","text":"<p>Start by loading a sample (<code>.yaml</code>) configuration:</p> <ul> <li>Available sample configs</li> </ul> <pre><code>data_config:\n  train_labels_path: \n    - path/to/your/training_data.slp\n  val_labels_path:\n    - path/to/your/validation_data.slp\n  validation_fraction: 0.1\n  user_instances_only: true\n  data_pipeline_fw: torch_dataset\n  preprocessing:\n    ensure_rgb: false\n    ensure_grayscale: false\n    scale: 1.0\n    crop_hw: null\n    min_crop_size: 100\n  use_augmentations_train: true\n  augmentation_config:\n    intensity:\n      contrast_p: 0.5\n      brightness_p: 0.5\n    geometric:\n      rotation_min: -15.0\n      rotation_max: 15.0\n      scale_min: 0.9\n      scale_max: 1.1\n      affine_p: 1.0\nmodel_config:\n  pretrained_backbone_weights: null\n  pretrained_head_weights: null\n  backbone_config:\n    unet:\n      in_channels: 1\n      kernel_size: 3\n      filters: 16\n      filters_rate: 2.0\n      max_stride: 16\n      middle_block: true\n      up_interpolate: true\n      stacks: 1\n      convs_per_block: 2\n      output_stride: 2\n    convnext: null\n    swint: null\n  head_configs:\n    single_instance:\n      confmaps:\n        part_names: null\n        sigma: 2.5\n        output_stride: 2\n    centroid: null\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown: null\ntrainer_config:\n  train_data_loader:\n    batch_size: 4\n    shuffle: true\n    num_workers: 0\n  val_data_loader:\n    batch_size: 4\n    shuffle: false\n    num_workers: 0\n  model_ckpt:\n    save_top_k: 1\n    save_last: false\n  trainer_devices: auto\n  trainer_accelerator: auto\n  min_train_steps_per_epoch: 200\n  visualize_preds_during_training: true\n  keep_viz: false\n  max_epochs: 200\n  use_wandb: false\n  save_ckpt: true\n  save_ckpt_path: your_model_name\n  optimizer_name: Adam\n  optimizer:\n    lr: 0.0001\n    amsgrad: false\n  lr_scheduler:\n    step_lr: null\n    reduce_lr_on_plateau:\n      threshold: 1.0e-06\n      threshold_mode: rel\n      cooldown: 3\n      patience: 5\n      factor: 0.5\n      min_lr: 1.0e-08\n  early_stopping:\n    min_delta: 1.0e-08\n    patience: 10\n    stop_training_on_plateau: true\n</code></pre>"},{"location":"step_by_step_tutorial/#12-understanding-the-configuration-structure","title":"1.2 Understanding the Configuration Structure","text":"<p>Your config file has three main sections:</p> <pre><code>data_config:      # How to load and process your data\nmodel_config:     # What model architecture to use\ntrainer_config:   # How to train the model\n</code></pre>"},{"location":"step_by_step_tutorial/#13-key-parameters-to-modify","title":"1.3 Key Parameters to Modify","text":""},{"location":"step_by_step_tutorial/#data-configuration-data_config","title":"Data Configuration (<code>data_config</code>)","text":"<p>Set the <code>train_labels_path</code> to the path of your training <code>.slp</code> file, or a list of <code>.slp</code> files if you have multiple datasets. The <code>val_labels_path</code> is optional\u2014if you leave it out, the training data will be automatically split into training and validation sets based on the <code>validation_fraction</code> parameter. </p> <p>Download sample <code>train.pkg.slp</code> and <code>val.pkg.slp</code>.</p> <p>Choose the appropriate <code>data_pipeline_fw</code> based on your dataset size and hardware:</p> <ul> <li>Use <code>torch_dataset_cache_img_memory</code> for small datasets that fit comfortably in RAM. This will cache all source images in memory for faster training.</li> <li>Use <code>torch_dataset_cache_img_disk</code> for larger datasets that don't fit in memory. This caches images to disk, enabling efficient loading even for very large datasets. You can reuse the disk cache across different model types, since only the raw source images are cached (not model-specific data).</li> </ul> <p>You can customize data loading, preprocessing, and augmentation options in this section. For a full explanation of all available parameters and augmentation settings, see the Data config section of the Configuration Guide.</p> <pre><code>data_config:\n  train_labels_path: \n    - path/to/your/training_data.slp\n  val_labels_path:\n    - path/to/your/validation_data.slp\n  validation_fraction: 0.1\n  data_pipeline_fw: torch_dataset\n  preprocessing:\n    ensure_rgb: false\n    ensure_grayscale: false\n    scale: 1.0\n    crop_hw: null # only for centered-instance model\n    min_crop_size: 100 # only for centered-instance model\n  use_augmentations_train: true\n  augmentation_config:\n    intensity:\n      contrast_p: 0.5\n      brightness_p: 0.5\n    geometric:\n      rotation_min: -15.0\n      rotation_max: 15.0\n      scale_min: 0.9\n      scale_max: 1.1\n      affine_p: 1.0\n</code></pre>"},{"location":"step_by_step_tutorial/#model-configuration-model_config","title":"Model Configuration (<code>model_config</code>)","text":"<p>When configuring your model, you\u2019ll need to select both a backbone architecture and a model type:</p> <ul> <li>Backbone options: <code>unet</code>, <code>swint</code>, or <code>convnext</code></li> <li>Model type options: <code>single_instance</code>, <code>centroid</code>, <code>centered_instance</code>, or <code>bottomup</code></li> </ul> <p>For a detailed explanation of each backbone and model type, see the Model Architectures Guide.</p> <p>Tips for configuring your model:</p> <ul> <li>Input channels (<code>in_channels</code>): Set this to match your input image format (e.g., 1 for grayscale, 3 for RGB). The training pipeline will also infer and adjust this automatically.</li> <li>Max stride (<code>max_stride</code>): This parameter controls the number of downsampling (encoder) blocks in the backbone, which directly affects the receptive field size. For a deeper dive into how receptive field is affected, check out the Receptive Field Guide.</li> <li>Special note for <code>convnext</code> and <code>swint</code>: For these backbones, <code>max_stride</code> is determined by <code>stem_patch_stride * 16</code> and cannot be set arbitrarily.</li> </ul> <p>For ready-to-use configuration examples for each backbone and model type, see the Model Config Guide.</p> <pre><code>model_config:\n  pretrained_backbone_weights: null\n  pretrained_head_weights: null\n  backbone_config:\n    unet:\n      in_channels: 1\n      kernel_size: 3\n      filters: 16\n      filters_rate: 2.0\n      max_stride: 16\n      middle_block: true\n      up_interpolate: true\n      stacks: 1\n      convs_per_block: 2\n      output_stride: 2\n    convnext: null\n    swint: null\n  head_configs:\n    single_instance:\n      confmaps:\n        part_names: null\n        sigma: 2.5\n        output_stride: 2\n    centroid: null\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown: null\n</code></pre>"},{"location":"step_by_step_tutorial/#trainer-configuration-trainer_config","title":"Trainer Configuration (<code>trainer_config</code>)","text":"<p>The <code>trainer_config</code> section controls the training process, including key hyperparameters and device settings.</p> <p>Key tips for configuring <code>trainer_config</code>:</p> <ul> <li> <p>Data Loader Workers (<code>num_workers</code>): </p> <ul> <li>For the default data pipeline (<code>torch_dataset</code>), set <code>num_workers: 0</code> because <code>.slp</code> video objects cannot be pickled for multiprocessing.</li> <li>If you use a caching data pipeline (e.g., <code>torch_dataset_cache_img_memory</code> or <code>torch_dataset_cache_img_disk</code>), you can increase <code>num_workers</code> (&gt;0) to speed up data loading.</li> </ul> </li> <li> <p>Epochs and Checkpoints: </p> <ul> <li>Set <code>max_epochs</code> to control how many epochs to train for.</li> <li>Use <code>save_ckpt_path</code> to specify where model checkpoints are saved. If not set, a default path will be created using a timestamp and model type.</li> <li>For multi-GPU training, always set a static <code>save_ckpt_path</code> so all workers write to the same location.</li> </ul> </li> <li> <p>Device and Accelerator: </p> <ul> <li><code>trainer_accelerator</code> can be <code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code>, or <code>\"auto\"</code>.         - <code>\"auto\"</code> lets Lightning choose the best device based on your hardware.</li> <li><code>trainer_devices</code> can be set to specify the number of devices (e.g., GPUs) to use.</li> </ul> </li> <li> <p>Other Tips: </p> <ul> <li>Adjust <code>batch_size</code> and learning rate (<code>optimizer.lr</code>) as needed for your dataset and hardware.</li> <li>Enable <code>visualize_preds_during_training</code> to see predictions during training.</li> <li>Use <code>use_wandb: true</code> to log training metrics to Weights &amp; Biases (optional).</li> </ul> </li> </ul> <p>For a full list of options and explanations, see the Config Reference.</p> <pre><code>trainer_config:\n  train_data_loader:\n    batch_size: 4\n    shuffle: true\n    num_workers: 0\n  val_data_loader:\n    batch_size: 4\n    shuffle: false\n    num_workers: 0\n  model_ckpt:\n    save_top_k: 1\n    save_last: false\n  trainer_devices: auto\n  trainer_accelerator: auto\n  min_train_steps_per_epoch: 200\n  visualize_preds_during_training: true\n  keep_viz: false\n  max_epochs: 200\n  use_wandb: false\n  save_ckpt: true\n  save_ckpt_path: my_model_ckpt_dir\n  optimizer_name: Adam\n  optimizer:\n    lr: 0.0001\n    amsgrad: false\n  lr_scheduler:\n    step_lr: null\n    reduce_lr_on_plateau:\n      threshold: 1.0e-06\n      threshold_mode: rel\n      cooldown: 3\n      patience: 5\n      factor: 0.5\n      min_lr: 1.0e-08\n  early_stopping:\n    min_delta: 1.0e-08\n    patience: 10\n    stop_training_on_plateau: true\n</code></pre>"},{"location":"step_by_step_tutorial/#step-2-training-your-model","title":"\ud83e\udd16 Step 2: Training Your Model","text":"<p>Now that you have your configuration file, let's train your model! SLEAP-NN provides two ways to train: command-line interface (CLI) and Python API.</p>"},{"location":"step_by_step_tutorial/#21-training-with-command-line-interface-cli","title":"2.1 Training with Command Line Interface (CLI)","text":"<p>The CLI is perfect for quick training runs and automation:</p> <pre><code>sleap-nn-train \\\n    --config-name my_config.yaml \\\n    --config-dir /path/to/config/directory\n</code></pre>"},{"location":"step_by_step_tutorial/#22-training-with-python-api","title":"2.2 Training with Python API","text":"<p>The Python API gives you more control and is great for custom training workflows:</p> <pre><code>from omegaconf import OmegaConf\nfrom sleap_nn.training.model_trainer import ModelTrainer\n\n# Load configuration\nconfig = OmegaConf.load(\"my_config.yaml\")\n\n# Create trainer\ntrainer = ModelTrainer.get_model_trainer_from_config(config=config)\n\n# Start training\ntrainer.train()\n</code></pre> <p>If you want to use custom <code>sleap_io.Labels</code> objects,</p> <pre><code>from sleap_nn.training.model_trainer import ModelTrainer\nfrom sleap_io import Labels\n\n# Load your labels\ntrain_labels = Labels.load(\"my_data.slp\")\nval_labels = Labels.load(\"my_validation.slp\")\n\n# Create trainer with custom labels\ntrainer = ModelTrainer.get_model_trainer_from_config(\n    config=config,\n    train_labels=[train_labels],\n    val_labels=[val_labels]\n)\n\n# Train\ntrainer.train()\n</code></pre>"},{"location":"step_by_step_tutorial/#23-training-output","title":"2.3 Training Output","text":"<p>After training, you'll find: <pre><code>my_model_ckpt_dir/\n\u251c\u2500\u2500 best.ckpt                  # Best model weights\n\u251c\u2500\u2500 initial_config.yaml        # Initial training configuration\n\u251c\u2500\u2500 training_config.yaml       # Final training configuration\n\u251c\u2500\u2500 labels_train_gt_0.slp      # Ground-truth train data split\n\u251c\u2500\u2500 labels_val_gt_0.slp        # Ground-truth val data split\n\u251c\u2500\u2500 pred_train_0.slp           # Predictions on training data\n\u251c\u2500\u2500 pred_val_0.slp             # Predictions on validation data\n\u251c\u2500\u2500 train_0_pred_metrics.npz   # Metrics on train preds\n\u251c\u2500\u2500 val_0_pred_metrics.npz     # Metrics on val preds\n\u2514\u2500\u2500 training_log.csv           # CSV that tracks the train/ val losses and epoch time\n</code></pre></p>"},{"location":"step_by_step_tutorial/#step-3-running-inference","title":"\ud83d\udd0d Step 3: Running Inference","text":"<p>Now that you have a trained model, let's use it to make predictions on new data!</p>"},{"location":"step_by_step_tutorial/#31-inference-on-videos","title":"3.1 Inference on Videos","text":"<pre><code># Basic inference on a slp file\nsleap-nn-track \\\n    --data_path test.slp \\\n    --model_paths my_model \\\n    --output_path my_predictions.slp\n\n# Inference on specific frames on a video\nsleap-nn-track \\\n    --data_path video.mp4 \\\n    --frames \"1-100\" \\\n    --model_paths my_model \\\n    --output_path my_predictions.slp\n\n# Inference on a video + tracking\nsleap-nn-track \\\n    --data_path video.mp4 \\\n    --frames \"1-100\" \\\n    --model_paths my_model \\\n    --output_path my_predictions.slp \\\n    --tracking\n</code></pre>"},{"location":"step_by_step_tutorial/#32-inference-parameters","title":"3.2 Inference Parameters","text":""},{"location":"step_by_step_tutorial/#essential-parameters","title":"Essential Parameters:","text":"<ul> <li><code>--data_path</code>: Input video or labels file</li> <li><code>--model_paths</code>: Path to your trained model directory</li> <li><code>--output_path</code>: Where to save predictions</li> <li><code>--batch_size</code>: Number of frames to process at once</li> <li><code>--device</code>: Hardware to use (cpu, cuda, mps, auto)</li> <li><code>--peak_threshold</code>: Confidence threshold for detections</li> <li><code>--frames</code>: Specific frame ranges (e.g., \"1-100,200-300\")</li> <li><code>--tracking</code>: To enable tracking</li> </ul>"},{"location":"step_by_step_tutorial/#step-4-evaluation-and-visualization","title":"\ud83d\udcca Step 4: Evaluation and Visualization","text":"<p>Let's evaluate how well your model performed and visualize the results!</p>"},{"location":"step_by_step_tutorial/#41-evaluating-model-performance","title":"4.1 Evaluating Model Performance","text":"<pre><code>from sleap_nn.evaluation import Evaluator\nimport sleap_io as sio\n\n# Load labels\nground_truth = sio.load_slp(\"ground_truth.slp\")\npredictions = sio.load_slp(\"predictions.slp\")\n\n# Create evaluator\nevaluator = Evaluator(\n    ground_truth=ground_truth,\n    predicted=predictions\n)\n\n# Run evaluation\nmetrics = evaluator.evaluate()\n\n# Print results\nprint(f\"OKS mAP: {metrics['voc_metrics']['oks_voc.mAP']:.3f}\")\nprint(f\"Dist p90: {metrics['distance_metrics']['p90']:.3f}\")\n</code></pre>"},{"location":"step_by_step_tutorial/#42-visualizing-results","title":"4.2 Visualizing Results","text":"<pre><code>import sleap_io as sio\nimport matplotlib.pyplot as plt\n\ndef plot_preds(gt_labels, lf_index, pred_labels):\n    _fig, _ax = plt.subplots(1, 1, figsize=(5 * 1, 5 * 1))\n\n    # Plot each frame\n    gt_lf = gt_labels[lf_index.value]\n    pred_lf = pred_labels[lf_index.value]\n\n    # Ensure we're plotting keypoints for the same frame\n    assert (\n        gt_lf.frame_idx == pred_lf.frame_idx\n    ), f\"Frame mismatch at {lf_index.value}: GT={gt_lf.frame_idx}, Pred={pred_lf.frame_idx}\"\n\n    _ax.imshow(gt_lf.image, cmap=\"gray\")\n    _ax.set_title(\n        f\"Frame {gt_lf.frame_idx} (lf idx: {lf_index.value})\",\n        fontsize=12,\n        fontweight=\"bold\",\n    )\n\n    # Plot ground truth instances\n    for idx, instance in enumerate(gt_lf.instances):\n        if not instance.is_empty:\n            gt_pts = instance.numpy()\n            _ax.plot(\n                gt_pts[:, 0],\n                gt_pts[:, 1],\n                \"go\",\n                markersize=6,\n                alpha=0.8,\n                label=\"GT\" if idx == 0 else \"\",\n            )\n\n    # Plot predicted instances\n    for idx, instance in enumerate(pred_lf.instances):\n        if not instance.is_empty:\n            pred_pts = instance.numpy()\n            _ax.plot(\n                pred_pts[:, 0],\n                pred_pts[:, 1],\n                \"rx\",\n                markersize=6,\n                alpha=0.8,\n                label=\"Pred\" if idx == 0 else \"\",\n            )\n\n    # Add legend\n    _ax.legend(loc=\"upper right\", fontsize=8)\n\n    _ax.axis(\"off\")\n\n    plt.suptitle(f\"Ground Truth vs Predictions\", fontsize=16, fontweight=\"bold\", y=0.98)\n\n    plt.tight_layout()\n    plt.show()\n    return\n\n\n# Overlay results\ngt_labels = sio.load_slp(\"groundtruth.slp\")\npred_labels = sio.load_slp(\"my_predictions.slp\")\nplot_preds(gt_labels, pred_labels, lf_index=0)\n</code></pre>"},{"location":"step_by_step_tutorial/#43-metrics-interpretation","title":"4.3 Metrics Interpretation","text":""},{"location":"step_by_step_tutorial/#key-metrics-to-understand","title":"Key Metrics to Understand:","text":"<ul> <li>PCK (Percentage of Correct Keypoints): How many keypoints are within a certain distance threshold</li> <li>OKS (Object Keypoint Similarity): How similar are the predicted keypoints to the ground-truth</li> <li>mAP (mean Average Precision): Mean of average precisions across match thresholds (where OKS or PCK could be the matching score).</li> <li>Distance Metrics: Average euclidean distance between predicted and true keypoints</li> </ul>"},{"location":"step_by_step_tutorial/#next-steps","title":"\u2728 Next Steps","text":"<p>Now that you have the basics, you can:</p> <ol> <li>Experiment with different model architectures (UNet, ConvNeXt, SwinT)</li> <li>Try different detection methods (single instance, bottom-up, top-down)</li> <li>Optimize hyperparameters for better performance</li> <li>Use data augmentation to improve model robustness</li> </ol>"},{"location":"step_by_step_tutorial/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Configuration Guide: Detailed configuration options</li> <li>Training Documentation: Advanced training features</li> <li>Inference Guide: Complete inference options</li> <li>Model Architectures: Available model types</li> <li>Example Notebooks: Interactive tutorials</li> </ul> <p>Happy SLEAPiNNg! \ud83d\udc2d\ud83d\udc2d</p>"},{"location":"training/","title":"Training Models","text":""},{"location":"training/#overview","title":"Overview","text":"<p>SLEAP-NN leverages a flexible, configuration-driven training workflow built on Hydra and OmegaConf. This guide will walk you through the essential steps for training pose estimation models using SLEAP-NN, whether you prefer the command-line interface or Python APIs.</p> <p>Note</p> <p>Training is only supported with SLEAP label files with ground truth annotations in <code>.slp</code> or <code>.pkg.slp</code> format.</p>"},{"location":"training/#training-with-config","title":"Training with Config","text":"<p>This section explains how to train a model using an existing configuration file. If you need help creating or editing a config, see the configuration guide. </p>"},{"location":"training/#using-cli","title":"Using CLI","text":"<p>To train a model using CLI,  <pre><code>sleap-nn-train --config-name config --config-dir path/to/config_dir\n</code></pre></p> <ul> <li><code>config-name</code>: Name of the config file</li> <li><code>config-dir</code>: Path to the config file</li> </ul> <p>If yor config file is in the path: <code>/path/to/config/file/config.yaml</code>, then <code>config-name</code> would be <code>config.yaml</code> and <code>config-dir</code> would be <code>/path/to/config/file</code>.</p> <p>Override any configuration from command line:</p> <pre><code># Train on list of .slp files\nsleap-nn-train --config-name config --config-dir path/to/config_dir \"data_config.train_labels_path=[labels.pkg.slp,labels.pkg.slp]\"\n\n# Change batch size\nsleap-nn-train --config-name config --config-dir path/to/config_dir trainer_config.train_data_loader.batch_size=8 trainer_config.val_data_loader.batch_size=8 \"data_config.train_labels_path=[labels.pkg.slp]\"\n\n# Set number of GPUs to be used\nsleap-nn-train --config-name config --config-dir path/to/config_dir trainer_config.trainer_devices=1 \"data_config.train_labels_path=[labels.pkg.slp]\"\n\n# Change learning rate\nsleap-nn-train --config-name config --config-dir path/to/config_dir trainer_config.optimizer.lr=5e-4 \"data_config.train_labels_path=[labels.pkg.slp]\"\n</code></pre> <p>Note</p> <p>For topdown, we need to train two models (centroid \u2192 instance):</p> <pre><code># Train centroid model\nsleap-nn-train \\\n    --config-dir configs \\\n    --config-name centroid_unet \\\n    \"data_config.train_labels_path=[labels.pkg.slp]\"\n\n# Train centered instance model\nsleap-nn-train \\\n    --config-dir configs \\\n    --config-name centered_instance_unet \\ \n    \"data_config.train_labels_path=[labels.pkg.slp]\"\n</code></pre>"},{"location":"training/#using-modeltrainer-api","title":"Using <code>ModelTrainer</code> API","text":"<pre><code>from omegaconf import OmegaConf\nfrom sleap_nn.training.model_trainer import ModelTrainer\n\nconfig = OmegaConf.load(\"config.yaml\")\ntrainer = ModelTrainer.get_model_trainer_from_config(config=config)\ntrainer.train()\n</code></pre> <p>If you have a cutsom labels object which is not in a slp file: <pre><code>from omegaconf import OmegaConf\nfrom sleap_nn.training.model_trainer import ModelTrainer\n\nconfig = OmegaConf.load(\"config.yaml\")\ntrainer = ModelTrainer.get_model_trainer_from_config(config=config, train_labels=[train_labels], val_labels=[val_labels])\ntrainer.train()\n</code></pre></p>"},{"location":"training/#training-without-config","title":"Training without Config","text":"<p>If you prefer not to create a large custom config file, you can quickly train a model by calling the <code>train()</code> function directly and passing your desired parameters as arguments.</p> <p>This approach is much simpler than manually specifying every parameter for each model component. For example, instead of defining all the details for a UNet backbone, you can just set <code>backbone_config=\"unet_medium_rf\"</code> or <code>\"unet_large_rf\"</code>, and the appropriate preset values will be used automatically. The same applies to head configurations\u2014just specify the desired preset (e.g., <code>\"bottomup\"</code>), and the defaults are handled for you. To look into the preset values for each of the backbones and heads, refer the model configs.</p> <p>For a full list of available arguments and their descriptions, see the <code>train()</code> API reference in the documentation.</p> <pre><code>from sleap_nn.train import train\n\ntrain(\n    train_labels_path=[\"labels.slp\"], # or list of labels\n    backbone_config=\"unet_medium_rf\",\n    head_configs=\"bottomup\",\n    save_ckpt=True,\n)\n</code></pre> <p>Applying data augmentation is also much simpler\u2014you can just specify the augmentation names directly (as a string or list), instead of writing out a full configuration.</p> <pre><code>from sleap_nn.train import train\n\ntrain(\n    train_labels_path=[\"labels.slp\"], # or list of labels\n    backbone_config=\"unet_medium_rf\",\n    head_configs=\"bottomup\",\n    save_ckpt=True,\n    use_augmentations_train=True,\n    intensity_aug=\"uniform_noise\",\n    geometric_aug=[\"rotation\", \"scale\"]\n)\n</code></pre>"},{"location":"training/#monitoring-training","title":"Monitoring Training","text":""},{"location":"training/#weights-biases-wandb-integration","title":"Weights &amp; Biases (WandB) Integration","text":"<p>If you set <code>trainer_config.use_wandb = True</code> and provide a valid <code>trainer_config.wandb_config</code>, all key training metrics\u2014including losses, training/validation times, and visualizations\u2014are automatically logged to your WandB project. This makes it easy to monitor progress and compare runs.</p>"},{"location":"training/#checkpointing-artifacts","title":"Checkpointing &amp; Artifacts","text":"<p>For every training run, a dedicated checkpoint directory is created. This directory contains:</p> <ul> <li>The original user-provided config (<code>initial_config.yaml</code>)</li> <li>The full training config with computed values (<code>training_config.yaml</code>)</li> <li>The best model weights (<code>best.ckpt</code>) when <code>trainer_config.save_ckpt</code> is set to <code>True</code></li> <li>The training and validation SLP files used</li> <li>A CSV log tracking train/validation loss, times, and learning rate across epochs</li> </ul>"},{"location":"training/#training-visualization","title":"Training Visualization","text":"<p>To help understand model performance, SLEAP-NN can generate visualizations of model predictions (e.g., confidence maps) after each epoch when <code>visualize_preds_during_training</code> is set to <code>True</code>. By default, these images are saved temporarily (deleted after training is completed), but you can configure the system to keep them by setting <code>trainer_config.keep_viz</code> to <code>True</code>. If WandB logging is enabled, these visualizations are also uploaded to your WandB dashboard.</p>"},{"location":"training/#advanced-options","title":"Advanced Options","text":""},{"location":"training/#resume-training","title":"Resume Training","text":"<p>To resume training from a previous checkpoint, </p> <pre><code>sleap-nn-train \\\n    --config-name config \\\n    --config-dir path/to/config_dir \\ \n    trainer_config.ckpt_path=/path/to/checkpoint.ckpt \\\n    trainer_config.resume_ckpt_path=/path/to/prv_trained/checkpoint.ckpt \\\n    \"data_config.train_labels_path=[labels.pkg.slp]\"\n</code></pre>"},{"location":"training/#multi-gpu-training","title":"Multi-GPU Training","text":"<p>To automatically configure the accelerator and number of devices, set: <pre><code>trainer_config:\n  save_ckpt_path: multi_gpu_training\n  trainer_accelerator: \"auto\"\n  trainer_devices: \"auto\"\n  trainer_strategy: \"auto\"\n</code></pre></p> <p>To set the number of gpus to be used and the accelerator: <pre><code>trainer_config:\n  save_ckpt_path: multi_gpu_training\n  trainer_accelerator: \"gpu\"\n  trainer_devices: 4\n  trainer_strategy: \"ddp\"\n</code></pre></p> <p>Note</p> <p>In a multi-gpu training setup, the effective steps during training would be <code>config.trainer_config.trainer_steps_per_epoch</code> / <code>config.trainer_config.trainer_devices</code>. </p> <p>Note</p> <p>Multi-node trainings have not been validated and should be considered experimental.</p>"},{"location":"training/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with default configurations</li> <li>Cache data: If you want to get faster training time, consider caching the images on memory (or disk) by setting the relevant <code>data_config.data_pipeline_fw</code>.</li> <li>Monitor Overfitting: Watch validation metrics</li> <li>Adjust Learning Rate: Use learning rate scheduling</li> <li>Data Augmentation: Enable augmentations for better generalization</li> <li>Early Stopping: Prevent overfitting with early stopping callback.</li> </ol>"},{"location":"training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/#out-of-memory","title":"Out of Memory","text":"<p>For large models or datasets:</p> <ul> <li>Reduce <code>batch_size</code></li> <li>Enable gradient accumulation</li> <li>Use mixed precision training</li> <li>Reduce model size (fewer filters/layers)</li> </ul>"},{"location":"training/#slow-training","title":"Slow Training","text":"<ul> <li>Increase <code>num_workers</code> for data loading</li> <li>Enable mixed precision</li> <li>Use SSD for data storage</li> <li>Check GPU utilization</li> </ul>"},{"location":"training/#poor-performance","title":"Poor Performance","text":"<ul> <li>Increase training data</li> <li>Adjust augmentation parameters</li> <li>Try different architectures</li> <li>Tune hyperparameters</li> </ul>"},{"location":"training/#next-steps","title":"Next Steps","text":"<ul> <li>Running Inference</li> <li>Configuration Guide</li> <li>Model Architecture Guide</li> </ul>"},{"location":"api/","title":"sleap_nn","text":""},{"location":"api/#sleap_nn","title":"<code>sleap_nn</code>","text":"<p>Main module for sleap_nn package.</p> <p>Modules:</p> Name Description <code>architectures</code> <p>Modules related to model architectures.</p> <code>cli</code> <p>Unified CLI for SLEAP-NN using Click.</p> <code>config</code> <p>Configuration modules for sleap-nn.</p> <code>data</code> <p>Modules related to data loading and processing.</p> <code>evaluation</code> <p>This module is to compute evaluation metrics for trained models.</p> <code>inference</code> <p>Inference-related modules.</p> <code>legacy_models</code> <p>Utilities for loading legacy SLEAP models.</p> <code>predict</code> <p>Entry point for running inference.</p> <code>tracking</code> <p>Tracker related modules.</p> <code>train</code> <p>Entry point for sleap_nn training.</p> <code>training</code> <p>Training-related modules.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>sleap_nn<ul> <li>architectures<ul> <li>common</li> <li>convnext</li> <li>encoder_decoder</li> <li>heads</li> <li>model</li> <li>swint</li> <li>unet</li> <li>utils</li> </ul> </li> <li>cli</li> <li>config<ul> <li>data_config</li> <li>get_config</li> <li>model_config</li> <li>trainer_config</li> <li>training_job_config</li> <li>utils</li> </ul> </li> <li>data<ul> <li>augmentation</li> <li>confidence_maps</li> <li>custom_datasets</li> <li>edge_maps</li> <li>identity</li> <li>instance_centroids</li> <li>instance_cropping</li> <li>normalization</li> <li>providers</li> <li>resizing</li> <li>utils</li> </ul> </li> <li>evaluation</li> <li>inference<ul> <li>bottomup</li> <li>identity</li> <li>paf_grouping</li> <li>peak_finding</li> <li>predictors</li> <li>single_instance</li> <li>topdown</li> <li>utils</li> </ul> </li> <li>legacy_models</li> <li>predict</li> <li>tracking<ul> <li>candidates<ul> <li>fixed_window</li> <li>local_queues</li> </ul> </li> <li>track_instance</li> <li>tracker</li> <li>utils</li> </ul> </li> <li>train</li> <li>training<ul> <li>callbacks</li> <li>lightning_modules</li> <li>losses</li> <li>model_trainer</li> <li>utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/cli/","title":"cli","text":""},{"location":"api/cli/#sleap_nn.cli","title":"<code>sleap_nn.cli</code>","text":"<p>Unified CLI for SLEAP-NN using Click.</p> <p>Functions:</p> Name Description <code>cli</code> <p>SLEAP-NN: Neural network backend for training and inference for animal pose estimation.</p> <code>eval</code> <p>Run evaluation workflow.</p> <code>track</code> <p>CLI command that handles argument conversion and calls run_inference.</p> <code>train</code> <p>Run training workflow.</p>"},{"location":"api/cli/#sleap_nn.cli.cli","title":"<code>cli()</code>","text":"<p>SLEAP-NN: Neural network backend for training and inference for animal pose estimation.</p> <p>Use subcommands to run different workflows:</p> <p>train    - Run training workflow track    - Run inference/ tracking workflow eval     - Run evaluation workflow</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@click.group()\ndef cli():\n    \"\"\"SLEAP-NN: Neural network backend for training and inference for animal pose estimation.\n\n    Use subcommands to run different workflows:\n\n    train    - Run training workflow\n    track    - Run inference/ tracking workflow\n    eval     - Run evaluation workflow\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.eval","title":"<code>eval(**kwargs)</code>","text":"<p>Run evaluation workflow.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--ground_truth_path\",\n    type=str,\n    required=True,\n    help=\"Path to ground truth labels file (.slp)\",\n)\n@click.option(\n    \"--predicted_path\",\n    type=str,\n    required=True,\n    help=\"Path to predicted labels file (.slp)\",\n)\n@click.option(\n    \"--oks_stddev\",\n    type=float,\n    default=0.05,\n    help=\"Standard deviation for OKS calculation\",\n)\n@click.option(\"--oks_scale\", type=float, help=\"Scale factor for OKS calculation\")\n@click.option(\n    \"--match_threshold\", type=float, default=0.5, help=\"Threshold for instance matching\"\n)\n@click.option(\n    \"--user_labels_only\", is_flag=True, help=\"Only evaluate user-labeled frames\"\n)\n@click.option(\"--save_metrics\", type=str, help=\"Path to save metrics (.npz file)\")\ndef eval(**kwargs):\n    \"\"\"Run evaluation workflow.\"\"\"\n    run_evaluation(**kwargs)\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.track","title":"<code>track(**kwargs)</code>","text":"<p>CLI command that handles argument conversion and calls run_inference.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--data_path\",\n    type=str,\n    required=True,\n    help=\"Path to data to predict on. This can be a labels (.slp) file or any supported video format.\",\n)\n@click.option(\n    \"--model_paths\",\n    multiple=True,\n    help=\"Path to trained model directory (with training_config.json). Multiple models can be specified, each preceded by --model_paths.\",\n)\n@click.option(\n    \"--backbone_ckpt_path\",\n    type=str,\n    default=None,\n    help=\"To run inference on any `.ckpt` other than `best.ckpt` from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\",\n)\n@click.option(\n    \"--head_ckpt_path\",\n    type=str,\n    default=None,\n    help=\"Path to `.ckpt` file if a different set of head layer weights are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt from `backbone_ckpt_path` if provided.)\",\n)\n@click.option(\n    \"-n\",\n    \"--max_instances\",\n    type=int,\n    default=None,\n    help=\"Limit maximum number of instances in multi-instance models. Not available for ID models. Defaults to None.\",\n)\n@click.option(\n    \"--max_height\",\n    type=int,\n    default=None,\n    help=\"Maximum height the image should be padded to. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--max_width\",\n    type=int,\n    default=None,\n    help=\"Maximum width the image should be padded to. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--input_scale\",\n    type=float,\n    default=None,\n    help=\"Scale factor to apply to the input image. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--ensure_rgb\",\n    is_flag=True,\n    default=False,\n    help=\"True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to `True`, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the values from the training config are used. Default: `None`.\",\n)\n@click.option(\n    \"--ensure_grayscale\",\n    is_flag=True,\n    default=False,\n    help=\"True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the values from the training config are used. Default: `None`.\",\n)\n@click.option(\n    \"--anchor_part\",\n    type=str,\n    default=None,\n    help=\"The node name to use as the anchor for the centroid. If not provided, the anchor part in the `training_config.yaml` is used. Default: `None`.\",\n)\n@click.option(\n    \"--only_labeled_frames\",\n    is_flag=True,\n    default=False,\n    help=\"Only run inference on user labeled frames when running on labels dataset. This is useful for generating predictions to compare against ground truth.\",\n)\n@click.option(\n    \"--only_suggested_frames\",\n    is_flag=True,\n    default=False,\n    help=\"Only run inference on unlabeled suggested frames when running on labels dataset. This is useful for generating predictions for initialization during labeling.\",\n)\n@click.option(\n    \"--video_index\",\n    type=int,\n    default=None,\n    help=\"Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.\",\n)\n@click.option(\n    \"--video_dataset\", type=str, default=None, help=\"The dataset for HDF5 videos.\"\n)\n@click.option(\n    \"--video_input_format\",\n    type=str,\n    default=\"channels_last\",\n    help=\"The input_format for HDF5 videos.\",\n)\n@click.option(\n    \"--frames\",\n    type=str,\n    default=\"\",\n    help=\"List of frames to predict when running on a video. Can be specified as a comma separated list (e.g. 1,2,3) or a range separated by hyphen (e.g., 1-3, for 1,2,3). If not provided, defaults to predicting on the entire video.\",\n)\n@click.option(\n    \"--batch_size\",\n    type=int,\n    default=4,\n    help=\"Number of frames to predict at a time. Larger values result in faster inference speeds, but require more memory.\",\n)\n@click.option(\n    \"--integral_patch_size\",\n    type=int,\n    default=5,\n    help=\"Size of patches to crop around each rough peak as an integer scalar. Default: 5.\",\n)\n@click.option(\n    \"--return_confmaps\",\n    is_flag=True,\n    default=False,\n    help=\"If True, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.\",\n)\n@click.option(\n    \"--return_pafs\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the part affinity fields will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model. Default: False.\",\n)\n@click.option(\n    \"--return_paf_graph\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the part affinity field graph will be returned together with the predicted instances. The graph is obtained by parsing the part affinity fields with the paf_scorer instance and is an intermediate representation used during instance grouping. Default: False.\",\n)\n@click.option(\n    \"--max_edge_length_ratio\",\n    type=float,\n    default=0.25,\n    help=\"The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching. Default: 0.25.\",\n)\n@click.option(\n    \"--dist_penalty_weight\",\n    type=float,\n    default=1.0,\n    help=\"A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly. Default: 1.0.\",\n)\n@click.option(\n    \"--n_points\",\n    type=int,\n    default=10,\n    help=\"Number of points to sample along the line integral. Default: 10.\",\n)\n@click.option(\n    \"--min_instance_peaks\",\n    type=float,\n    default=0,\n    help=\"Minimum number of peaks the instance should have to be considered a real instance. Instances with fewer peaks than this will be discarded (useful for filtering spurious detections). Default: 0.\",\n)\n@click.option(\n    \"--min_line_scores\",\n    type=float,\n    default=0.25,\n    help=\"Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones. Default: 0.25.\",\n)\n@click.option(\n    \"--return_class_maps\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the class maps will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.\",\n)\n@click.option(\n    \"--return_class_vectors\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the classification probabilities will be returned together with the predicted peaks. This will not line up with the grouped instances, for which the associated class probabilities will always be returned in instance_scores.\",\n)\n@click.option(\n    \"--make_labels\",\n    is_flag=True,\n    default=True,\n    help=\"If True (the default), returns a sio.Labels instance with sio.PredictedInstances. If False, just return a list of dictionaries containing the raw arrays returned by the inference model. Default: True.\",\n)\n@click.option(\n    \"--queue_maxsize\",\n    type=int,\n    default=8,\n    help=\"Maximum size of the frame buffer queue.\",\n)\n@click.option(\n    \"--crop_size\",\n    type=int,\n    default=None,\n    help=\"Crop size. If not provided, the crop size from training_config.yaml is used.\",\n)\n@click.option(\n    \"--peak_threshold\",\n    type=float,\n    default=0.2,\n    help=\"Minimum confidence map value to consider a peak as valid.\",\n)\n@click.option(\n    \"--integral_refinement\",\n    type=str,\n    default=\"integral\",\n    help=\"If `None`, returns the grid-aligned peaks with no refinement. If `'integral'`, peaks will be refined with integral regression. Default: 'integral'.\",\n)\n@click.option(\n    \"-o\",\n    \"--output_path\",\n    type=str,\n    default=None,\n    help=\"The output filename to use for the predicted data. If not provided, defaults to '[data_path].slp'.\",\n)\n@click.option(\n    \"--device\",\n    type=str,\n    default=\"auto\",\n    help=\"Device on which torch.Tensor will be allocated. One of the ('cpu', 'cuda', 'mps', 'auto', 'opencl', 'ideep', 'hip', 'msnpu'). Default: 'auto' (based on available backend either cuda, mps or cpu is chosen).\",\n)\n@click.option(\n    \"--tracking\",\n    is_flag=True,\n    default=False,\n    help=\"If True, runs tracking on the predicted instances.\",\n)\n@click.option(\n    \"--tracking_window_size\",\n    type=int,\n    default=5,\n    help=\"Number of frames to look for in the candidate instances to match with the current detections.\",\n)\n@click.option(\n    \"--min_new_track_points\",\n    type=int,\n    default=0,\n    help=\"We won't spawn a new track for an instance with fewer than this many points.\",\n)\n@click.option(\n    \"--candidates_method\",\n    type=str,\n    default=\"fixed_window\",\n    help=\"Either of `fixed_window` or `local_queues`. In fixed window method, candidates from the last `window_size` frames. In local queues, last `window_size` instances for each track ID is considered for matching against the current detection.\",\n)\n@click.option(\n    \"--min_match_points\",\n    type=int,\n    default=0,\n    help=\"Minimum non-NaN points for match candidates.\",\n)\n@click.option(\n    \"--features\",\n    type=str,\n    default=\"keypoints\",\n    help=\"Feature representation for the candidates to update current detections. One of [`keypoints`, `centroids`, `bboxes`, `image`].\",\n)\n@click.option(\n    \"--scoring_method\",\n    type=str,\n    default=\"oks\",\n    help=\"Method to compute association score between features from the current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`, `euclidean_dist`].\",\n)\n@click.option(\n    \"--scoring_reduction\",\n    type=str,\n    default=\"mean\",\n    help=\"Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [`mean`, `max`, `robust_quantile`].\",\n)\n@click.option(\n    \"--robust_best_instance\",\n    type=float,\n    default=1.0,\n    help=\"If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.\",\n)\n@click.option(\n    \"--track_matching_method\",\n    type=str,\n    default=\"hungarian\",\n    help=\"Track matching algorithm. One of `hungarian`, `greedy`.\",\n)\n@click.option(\n    \"--max_tracks\",\n    type=int,\n    default=None,\n    help=\"Maximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate)\",\n)\n@click.option(\n    \"--use_flow\",\n    is_flag=True,\n    default=False,\n    help=\"If True, `FlowShiftTracker` is used, where the poses are matched using optical flow shifts.\",\n)\n@click.option(\n    \"--of_img_scale\",\n    type=float,\n    default=1.0,\n    help=\"Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements.\",\n)\n@click.option(\n    \"--of_window_size\",\n    type=int,\n    default=21,\n    help=\"Optical flow window size to consider at each pyramid scale level.\",\n)\n@click.option(\n    \"--of_max_levels\",\n    type=int,\n    default=3,\n    help=\"Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling.\",\n)\n@click.option(\n    \"--post_connect_single_breaks\",\n    is_flag=True,\n    default=False,\n    help=\"If True and `max_tracks` is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.\",\n)\ndef track(**kwargs):\n    \"\"\"CLI command that handles argument conversion and calls run_inference.\"\"\"\n    # Convert model_paths from tuple to list\n    if \"model_paths\" in kwargs and kwargs[\"model_paths\"]:\n        kwargs[\"model_paths\"] = list(kwargs[\"model_paths\"])\n    else:\n        kwargs[\"model_paths\"] = None\n\n    # Convert frames string to list\n    if \"frames\" in kwargs and kwargs[\"frames\"]:\n        kwargs[\"frames\"] = frame_list(kwargs[\"frames\"])\n    else:\n        kwargs[\"frames\"] = None\n\n    # Call the original function\n    return run_inference(**kwargs)\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.train","title":"<code>train(config_name, config_dir)</code>","text":"<p>Run training workflow.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@cli.command()\n@click.option(\"--config-name\", \"-c\", type=str, help=\"Configuration file name\")\n@click.option(\"--config-dir\", \"-d\", type=str, help=\"Configuration directory path\")\ndef train(config_name, config_dir):\n    \"\"\"Run training workflow.\"\"\"\n    config_name = (\n        config_name if config_name.endswith(\".yaml\") else f\"{config_name}.yaml\"\n    )\n    config_path = Path(config_dir) / config_name if config_dir else config_name\n    cfg = OmegaConf.load(config_path)\n    logger.info(\"Input config:\")\n    logger.info(\"\\n\" + OmegaConf.to_yaml(cfg))\n    run_training(cfg)\n</code></pre>"},{"location":"api/evaluation/","title":"evaluation","text":""},{"location":"api/evaluation/#sleap_nn.evaluation","title":"<code>sleap_nn.evaluation</code>","text":"<p>This module is to compute evaluation metrics for trained models.</p> <p>Classes:</p> Name Description <code>Evaluator</code> <p>Compute the standard evaluation metrics with the predicted and the ground-truth Labels.</p> <code>MatchInstance</code> <p>Class to have a new structure for sio.Instance object.</p> <p>Functions:</p> Name Description <code>compute_dists</code> <p>Compute Euclidean distances between matched pairs of instances.</p> <code>compute_instance_area</code> <p>Compute the area of the bounding box of a set of keypoints.</p> <code>compute_oks</code> <p>Compute the object keypoints similarity between sets of points.</p> <code>find_frame_pairs</code> <p>Find corresponding frames across two sets of labels.</p> <code>get_instances</code> <p>Get a list of instances of type MatchInstance from the Labeled Frame.</p> <code>main</code> <p>Run evaluation.</p> <code>match_frame_pairs</code> <p>Match all ground truth and predicted instances within each pair of frames.</p> <code>match_instances</code> <p>Match pairs of instances between ground truth and predictions in a frame.</p> <code>run_evaluation</code> <p>Evaluate SLEAP-NN model predictions against ground truth labels.</p>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator","title":"<code>Evaluator</code>","text":"<p>Compute the standard evaluation metrics with the predicted and the ground-truth Labels.</p> <p>This class is used to calculate the common metrics for pose estimation models which includes voc metrics (with oks and pck), mOKS, distance metrics, pck metrics and visibility metrics.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_instances</code> <code>Labels</code> <p>The <code>sio.Labels</code> dataset object with ground truth labels.</p> required <code>predicted_instances</code> <code>Labels</code> <p>The <code>sio.Labels</code> dataset object with predicted labels.</p> required <code>oks_stddev</code> <code>float</code> <p>The standard deviation to use for calculating object keypoint similarity; see <code>compute_oks</code> function for details.</p> <code>0.025</code> <code>oks_scale</code> <code>Optional[float]</code> <p>The scale to use for calculating object keypoint similarity; see <code>compute_oks</code> function for details.</p> <code>None</code> <code>match_threshold</code> <code>float</code> <p>The threshold to use on oks scores when determining which instances match between ground truth and predicted frames.</p> <code>0</code> <code>user_labels_only</code> <code>bool</code> <p>If False, predicted instances in the ground truth frame may be considered for matching.</p> <code>True</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the Evaluator class with ground-truth and predicted labels.</p> <code>distance_metrics</code> <p>Compute the Euclidean distance error at different percentiles using the pairwise distances.</p> <code>evaluate</code> <p>Return the evaluation metrics.</p> <code>mOKS</code> <p>Return the meanOKS value.</p> <code>pck_metrics</code> <p>Compute PCK across a range of thresholds using the pair-wise distances.</p> <code>visibility_metrics</code> <p>Compute node visibility metrics for the matched pair of instances.</p> <code>voc_metrics</code> <p>Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>class Evaluator:\n    \"\"\"Compute the standard evaluation metrics with the predicted and the ground-truth Labels.\n\n    This class is used to calculate the common metrics for pose estimation models which\n    includes voc metrics (with oks and pck), mOKS, distance metrics, pck metrics and\n    visibility metrics.\n\n    Args:\n        ground_truth_instances: The `sio.Labels` dataset object with ground truth labels.\n        predicted_instances: The `sio.Labels` dataset object with predicted labels.\n        oks_stddev: The standard deviation to use for calculating object\n            keypoint similarity; see `compute_oks` function for details.\n        oks_scale: The scale to use for calculating object\n            keypoint similarity; see `compute_oks` function for details.\n        match_threshold: The threshold to use on oks scores when determining\n            which instances match between ground truth and predicted frames.\n        user_labels_only: If False, predicted instances in the ground truth frame may be\n            considered for matching.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ground_truth_instances: sio.Labels,\n        predicted_instances: sio.Labels,\n        oks_stddev: float = 0.025,\n        oks_scale: Optional[float] = None,\n        match_threshold: float = 0,\n        user_labels_only: bool = True,\n    ):\n        \"\"\"Initialize the Evaluator class with ground-truth and predicted labels.\"\"\"\n        self.ground_truth_instances = ground_truth_instances\n        self.predicted_instances = predicted_instances\n        self.match_threshold = match_threshold\n        self.oks_stddev = oks_stddev\n        self.oks_scale = oks_scale\n        self.user_labels_only = user_labels_only\n\n        self._process_frames()\n\n    def _process_frames(self):\n        self.frame_pairs = find_frame_pairs(\n            self.ground_truth_instances, self.predicted_instances, self.user_labels_only\n        )\n        if not self.frame_pairs:\n            message = \"Empty Frame Pairs. No match found for the video frames\"\n            logger.error(message)\n            raise Exception(message)\n\n        self.positive_pairs, self.false_negatives = match_frame_pairs(\n            self.frame_pairs,\n            stddev=self.oks_stddev,\n            scale=self.oks_scale,\n            threshold=self.match_threshold,\n        )\n\n        self.dists_dict = compute_dists(self.positive_pairs)\n\n    def voc_metrics(\n        self,\n        match_score_by=\"oks\",\n        match_score_thresholds: np.ndarray = np.linspace(\n            0.5, 0.95, 10\n        ),  # 0.5:0.05:0.95\n        recall_thresholds: np.ndarray = np.linspace(0, 1, 101),  # 0.0:0.01:1.00\n    ):\n        \"\"\"Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.\n\n        Args:\n            match_score_by: The score to be used for computing the metrics. \"ock\" or \"pck\"\n            match_score_thresholds: Score thresholds at which to consider matches as a true\n                positive match.\n            recall_thresholds: Recall thresholds at which to evaluate Average Precision.\n\n        Returns:\n            A dictionary of VOC metrics.\n        \"\"\"\n        if match_score_by == \"oks\":\n            match_scores = np.array([oks for _, _, oks in self.positive_pairs])\n            name = \"oks_voc\"\n        elif match_score_by == \"pck\":\n            pck_metrics = self.pck_metrics()\n            match_scores = pck_metrics[\"pcks\"].mean(axis=-1).mean(axis=-1)\n            name = \"pck_voc\"\n        else:\n            message = \"Invalid Option for match_score_by. Choose either `oks` or `pck`\"\n            logger.error(message)\n            raise Exception(message)\n\n        detection_scores = np.array(\n            [pp[1].instance.score for pp in self.positive_pairs]\n        )\n\n        inds = np.argsort(-detection_scores, kind=\"mergesort\")\n        detection_scores = detection_scores[inds]\n        match_scores = match_scores[inds]\n\n        precisions = []\n        recalls = []\n\n        npig = len(self.positive_pairs) + len(\n            self.false_negatives\n        )  # total number of GT instances\n\n        for match_score_threshold in match_score_thresholds:\n            tp = np.cumsum(match_scores &gt;= match_score_threshold)\n            fp = np.cumsum(match_scores &lt; match_score_threshold)\n\n            if tp.size == 0:\n                return {\n                    name + \".match_score_thresholds\": 0,\n                    name + \".recall_thresholds\": 0,\n                    name + \".match_scores\": 0,\n                    name + \".precisions\": 0,\n                    name + \".recalls\": 0,\n                    name + \".AP\": 0,\n                    name + \".AR\": 0,\n                    name + \".mAP\": 0,\n                    name + \".mAR\": 0,\n                }\n\n            rc = tp / npig\n            pr = tp / (fp + tp + np.spacing(1))\n\n            recall = rc[-1]  # best recall at this OKS threshold\n\n            # Ensure strictly decreasing precisions.\n            for i in range(len(pr) - 1, 0, -1):\n                if pr[i] &gt; pr[i - 1]:\n                    pr[i - 1] = pr[i]\n\n            # Find best precision at each recall threshold.\n            rc_inds = np.searchsorted(rc, recall_thresholds, side=\"left\")\n            precision = np.zeros(rc_inds.shape)\n            is_valid_rc_ind = rc_inds &lt; len(pr)\n            precision[is_valid_rc_ind] = pr[rc_inds[is_valid_rc_ind]]\n\n            precisions.append(precision)\n            recalls.append(recall)\n\n        precisions = np.array(precisions)\n        recalls = np.array(recalls)\n\n        AP = precisions.mean(\n            axis=1\n        )  # AP = average precision over fixed set of recall thresholds\n        AR = recalls  # AR = max recall given a fixed number of detections per image\n\n        mAP = precisions.mean()  # mAP = mean over all OKS thresholds\n        mAR = recalls.mean()  # mAR = mean over all OKS thresholds\n\n        return {\n            name + \".match_score_thresholds\": match_score_thresholds,\n            name + \".recall_thresholds\": recall_thresholds,\n            name + \".match_scores\": match_scores,\n            name + \".precisions\": precisions,\n            name + \".recalls\": recalls,\n            name + \".AP\": AP,\n            name + \".AR\": AR,\n            name + \".mAP\": mAP,\n            name + \".mAR\": mAR,\n        }\n\n    def mOKS(self):\n        \"\"\"Return the meanOKS value.\"\"\"\n        pair_oks = np.array([oks for _, _, oks in self.positive_pairs])\n        return {\"mOKS\": pair_oks.mean()}\n\n    def distance_metrics(self):\n        \"\"\"Compute the Euclidean distance error at different percentiles using the pairwise distances.\n\n        Returns:\n            A dictionary of distance metrics.\n        \"\"\"\n        dists = self.dists_dict[\"dists\"]\n        results = {\n            \"frame_idxs\": self.dists_dict[\"frame_idxs\"],\n            \"video_paths\": self.dists_dict[\"video_paths\"],\n            \"dists\": dists,\n            \"avg\": np.nanmean(dists),\n            \"p50\": np.nan,\n            \"p75\": np.nan,\n            \"p90\": np.nan,\n            \"p95\": np.nan,\n            \"p99\": np.nan,\n        }\n\n        is_non_nan = ~np.isnan(dists)\n        if np.any(is_non_nan):\n            non_nans = dists[is_non_nan]\n            for ptile in (50, 75, 90, 95, 99):\n                results[f\"p{ptile}\"] = np.percentile(non_nans, ptile)\n\n        return results\n\n    def pck_metrics(self, thresholds: np.ndarray = np.linspace(1, 10, 10)):\n        \"\"\"Compute PCK across a range of thresholds using the pair-wise distances.\n\n        Args:\n            thresholds: A list of distance thresholds in pixels.\n\n        Returns:\n            A dictionary of PCK metrics evaluated at each threshold.\n        \"\"\"\n        dists = self.dists_dict[\"dists\"]\n        dists = np.copy(dists)\n        dists[np.isnan(dists)] = np.inf\n        pcks = np.expand_dims(dists, -1) &lt; np.reshape(thresholds, (1, 1, -1))\n        mPCK_parts = pcks.mean(axis=0).mean(axis=-1)\n        mPCK = mPCK_parts.mean()\n\n        return {\n            \"thresholds\": thresholds,\n            \"pcks\": pcks,\n            \"mPCK_parts\": mPCK_parts,\n            \"mPCK\": mPCK,\n        }\n\n    def visibility_metrics(self):\n        \"\"\"Compute node visibility metrics for the matched pair of instances.\n\n        Returns:\n            A dictionary of visibility metrics, including the confusion matrix.\n        \"\"\"\n        vis_tp = 0\n        vis_fn = 0\n        vis_fp = 0\n        vis_tn = 0\n\n        for instance_gt, instance_pr, _ in self.positive_pairs:\n            missing_nodes_gt = np.isnan(instance_gt.instance.numpy()).any(axis=-1)\n            missing_nodes_pr = np.isnan(instance_pr.instance.numpy()).any(axis=-1)\n\n            vis_tn += ((missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n            vis_fn += ((~missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n            vis_fp += ((missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n            vis_tp += ((~missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n\n        return {\n            \"tp\": vis_tp,\n            \"fp\": vis_fp,\n            \"tn\": vis_tn,\n            \"fn\": vis_fn,\n            \"precision\": vis_tp / (vis_tp + vis_fp) if (vis_tp + vis_fp) else np.nan,\n            \"recall\": vis_tp / (vis_tp + vis_fn) if (vis_tp + vis_fn) else np.nan,\n        }\n\n    def evaluate(self):\n        \"\"\"Return the evaluation metrics.\"\"\"\n        metrics = {}\n        metrics[\"voc_metrics\"] = self.voc_metrics()\n        metrics[\"mOKS\"] = self.mOKS()\n        metrics[\"distance_metrics\"] = self.distance_metrics()\n        metrics[\"pck_metrics\"] = self.pck_metrics()\n        metrics[\"visibility_metrics\"] = self.visibility_metrics()\n\n        return metrics\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.__init__","title":"<code>__init__(ground_truth_instances, predicted_instances, oks_stddev=0.025, oks_scale=None, match_threshold=0, user_labels_only=True)</code>","text":"<p>Initialize the Evaluator class with ground-truth and predicted labels.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def __init__(\n    self,\n    ground_truth_instances: sio.Labels,\n    predicted_instances: sio.Labels,\n    oks_stddev: float = 0.025,\n    oks_scale: Optional[float] = None,\n    match_threshold: float = 0,\n    user_labels_only: bool = True,\n):\n    \"\"\"Initialize the Evaluator class with ground-truth and predicted labels.\"\"\"\n    self.ground_truth_instances = ground_truth_instances\n    self.predicted_instances = predicted_instances\n    self.match_threshold = match_threshold\n    self.oks_stddev = oks_stddev\n    self.oks_scale = oks_scale\n    self.user_labels_only = user_labels_only\n\n    self._process_frames()\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.distance_metrics","title":"<code>distance_metrics()</code>","text":"<p>Compute the Euclidean distance error at different percentiles using the pairwise distances.</p> <p>Returns:</p> Type Description <p>A dictionary of distance metrics.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def distance_metrics(self):\n    \"\"\"Compute the Euclidean distance error at different percentiles using the pairwise distances.\n\n    Returns:\n        A dictionary of distance metrics.\n    \"\"\"\n    dists = self.dists_dict[\"dists\"]\n    results = {\n        \"frame_idxs\": self.dists_dict[\"frame_idxs\"],\n        \"video_paths\": self.dists_dict[\"video_paths\"],\n        \"dists\": dists,\n        \"avg\": np.nanmean(dists),\n        \"p50\": np.nan,\n        \"p75\": np.nan,\n        \"p90\": np.nan,\n        \"p95\": np.nan,\n        \"p99\": np.nan,\n    }\n\n    is_non_nan = ~np.isnan(dists)\n    if np.any(is_non_nan):\n        non_nans = dists[is_non_nan]\n        for ptile in (50, 75, 90, 95, 99):\n            results[f\"p{ptile}\"] = np.percentile(non_nans, ptile)\n\n    return results\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.evaluate","title":"<code>evaluate()</code>","text":"<p>Return the evaluation metrics.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def evaluate(self):\n    \"\"\"Return the evaluation metrics.\"\"\"\n    metrics = {}\n    metrics[\"voc_metrics\"] = self.voc_metrics()\n    metrics[\"mOKS\"] = self.mOKS()\n    metrics[\"distance_metrics\"] = self.distance_metrics()\n    metrics[\"pck_metrics\"] = self.pck_metrics()\n    metrics[\"visibility_metrics\"] = self.visibility_metrics()\n\n    return metrics\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.mOKS","title":"<code>mOKS()</code>","text":"<p>Return the meanOKS value.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def mOKS(self):\n    \"\"\"Return the meanOKS value.\"\"\"\n    pair_oks = np.array([oks for _, _, oks in self.positive_pairs])\n    return {\"mOKS\": pair_oks.mean()}\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.pck_metrics","title":"<code>pck_metrics(thresholds=np.linspace(1, 10, 10))</code>","text":"<p>Compute PCK across a range of thresholds using the pair-wise distances.</p> <p>Parameters:</p> Name Type Description Default <code>thresholds</code> <code>ndarray</code> <p>A list of distance thresholds in pixels.</p> <code>linspace(1, 10, 10)</code> <p>Returns:</p> Type Description <p>A dictionary of PCK metrics evaluated at each threshold.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def pck_metrics(self, thresholds: np.ndarray = np.linspace(1, 10, 10)):\n    \"\"\"Compute PCK across a range of thresholds using the pair-wise distances.\n\n    Args:\n        thresholds: A list of distance thresholds in pixels.\n\n    Returns:\n        A dictionary of PCK metrics evaluated at each threshold.\n    \"\"\"\n    dists = self.dists_dict[\"dists\"]\n    dists = np.copy(dists)\n    dists[np.isnan(dists)] = np.inf\n    pcks = np.expand_dims(dists, -1) &lt; np.reshape(thresholds, (1, 1, -1))\n    mPCK_parts = pcks.mean(axis=0).mean(axis=-1)\n    mPCK = mPCK_parts.mean()\n\n    return {\n        \"thresholds\": thresholds,\n        \"pcks\": pcks,\n        \"mPCK_parts\": mPCK_parts,\n        \"mPCK\": mPCK,\n    }\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.visibility_metrics","title":"<code>visibility_metrics()</code>","text":"<p>Compute node visibility metrics for the matched pair of instances.</p> <p>Returns:</p> Type Description <p>A dictionary of visibility metrics, including the confusion matrix.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def visibility_metrics(self):\n    \"\"\"Compute node visibility metrics for the matched pair of instances.\n\n    Returns:\n        A dictionary of visibility metrics, including the confusion matrix.\n    \"\"\"\n    vis_tp = 0\n    vis_fn = 0\n    vis_fp = 0\n    vis_tn = 0\n\n    for instance_gt, instance_pr, _ in self.positive_pairs:\n        missing_nodes_gt = np.isnan(instance_gt.instance.numpy()).any(axis=-1)\n        missing_nodes_pr = np.isnan(instance_pr.instance.numpy()).any(axis=-1)\n\n        vis_tn += ((missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n        vis_fn += ((~missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n        vis_fp += ((missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n        vis_tp += ((~missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n\n    return {\n        \"tp\": vis_tp,\n        \"fp\": vis_fp,\n        \"tn\": vis_tn,\n        \"fn\": vis_fn,\n        \"precision\": vis_tp / (vis_tp + vis_fp) if (vis_tp + vis_fp) else np.nan,\n        \"recall\": vis_tp / (vis_tp + vis_fn) if (vis_tp + vis_fn) else np.nan,\n    }\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.voc_metrics","title":"<code>voc_metrics(match_score_by='oks', match_score_thresholds=np.linspace(0.5, 0.95, 10), recall_thresholds=np.linspace(0, 1, 101))</code>","text":"<p>Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.</p> <p>Parameters:</p> Name Type Description Default <code>match_score_by</code> <p>The score to be used for computing the metrics. \"ock\" or \"pck\"</p> <code>'oks'</code> <code>match_score_thresholds</code> <code>ndarray</code> <p>Score thresholds at which to consider matches as a true positive match.</p> <code>linspace(0.5, 0.95, 10)</code> <code>recall_thresholds</code> <code>ndarray</code> <p>Recall thresholds at which to evaluate Average Precision.</p> <code>linspace(0, 1, 101)</code> <p>Returns:</p> Type Description <p>A dictionary of VOC metrics.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def voc_metrics(\n    self,\n    match_score_by=\"oks\",\n    match_score_thresholds: np.ndarray = np.linspace(\n        0.5, 0.95, 10\n    ),  # 0.5:0.05:0.95\n    recall_thresholds: np.ndarray = np.linspace(0, 1, 101),  # 0.0:0.01:1.00\n):\n    \"\"\"Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.\n\n    Args:\n        match_score_by: The score to be used for computing the metrics. \"ock\" or \"pck\"\n        match_score_thresholds: Score thresholds at which to consider matches as a true\n            positive match.\n        recall_thresholds: Recall thresholds at which to evaluate Average Precision.\n\n    Returns:\n        A dictionary of VOC metrics.\n    \"\"\"\n    if match_score_by == \"oks\":\n        match_scores = np.array([oks for _, _, oks in self.positive_pairs])\n        name = \"oks_voc\"\n    elif match_score_by == \"pck\":\n        pck_metrics = self.pck_metrics()\n        match_scores = pck_metrics[\"pcks\"].mean(axis=-1).mean(axis=-1)\n        name = \"pck_voc\"\n    else:\n        message = \"Invalid Option for match_score_by. Choose either `oks` or `pck`\"\n        logger.error(message)\n        raise Exception(message)\n\n    detection_scores = np.array(\n        [pp[1].instance.score for pp in self.positive_pairs]\n    )\n\n    inds = np.argsort(-detection_scores, kind=\"mergesort\")\n    detection_scores = detection_scores[inds]\n    match_scores = match_scores[inds]\n\n    precisions = []\n    recalls = []\n\n    npig = len(self.positive_pairs) + len(\n        self.false_negatives\n    )  # total number of GT instances\n\n    for match_score_threshold in match_score_thresholds:\n        tp = np.cumsum(match_scores &gt;= match_score_threshold)\n        fp = np.cumsum(match_scores &lt; match_score_threshold)\n\n        if tp.size == 0:\n            return {\n                name + \".match_score_thresholds\": 0,\n                name + \".recall_thresholds\": 0,\n                name + \".match_scores\": 0,\n                name + \".precisions\": 0,\n                name + \".recalls\": 0,\n                name + \".AP\": 0,\n                name + \".AR\": 0,\n                name + \".mAP\": 0,\n                name + \".mAR\": 0,\n            }\n\n        rc = tp / npig\n        pr = tp / (fp + tp + np.spacing(1))\n\n        recall = rc[-1]  # best recall at this OKS threshold\n\n        # Ensure strictly decreasing precisions.\n        for i in range(len(pr) - 1, 0, -1):\n            if pr[i] &gt; pr[i - 1]:\n                pr[i - 1] = pr[i]\n\n        # Find best precision at each recall threshold.\n        rc_inds = np.searchsorted(rc, recall_thresholds, side=\"left\")\n        precision = np.zeros(rc_inds.shape)\n        is_valid_rc_ind = rc_inds &lt; len(pr)\n        precision[is_valid_rc_ind] = pr[rc_inds[is_valid_rc_ind]]\n\n        precisions.append(precision)\n        recalls.append(recall)\n\n    precisions = np.array(precisions)\n    recalls = np.array(recalls)\n\n    AP = precisions.mean(\n        axis=1\n    )  # AP = average precision over fixed set of recall thresholds\n    AR = recalls  # AR = max recall given a fixed number of detections per image\n\n    mAP = precisions.mean()  # mAP = mean over all OKS thresholds\n    mAR = recalls.mean()  # mAR = mean over all OKS thresholds\n\n    return {\n        name + \".match_score_thresholds\": match_score_thresholds,\n        name + \".recall_thresholds\": recall_thresholds,\n        name + \".match_scores\": match_scores,\n        name + \".precisions\": precisions,\n        name + \".recalls\": recalls,\n        name + \".AP\": AP,\n        name + \".AR\": AR,\n        name + \".mAP\": mAP,\n        name + \".mAR\": mAR,\n    }\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.MatchInstance","title":"<code>MatchInstance</code>","text":"<p>Class to have a new structure for sio.Instance object.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>@attrs.define(auto_attribs=True, slots=True)\nclass MatchInstance:\n    \"\"\"Class to have a new structure for sio.Instance object.\"\"\"\n\n    instance: sio.Instance\n    frame_idx: int\n    video_path: str\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.compute_dists","title":"<code>compute_dists(positive_pairs)</code>","text":"<p>Compute Euclidean distances between matched pairs of instances.</p> <p>Parameters:</p> Name Type Description Default <code>positive_pairs</code> <code>List[Tuple[Instance, PredictedInstance, Any]]</code> <p>A list of tuples of the form <code>(instance_gt, instance_pr, _)</code> containing the matched pair of instances.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, List[int], List[str]]]</code> <p>A dictionary with the following keys:     dists: An array of pairwise distances of shape <code>(n_positive_pairs, n_nodes)</code>     frame_idxs: A list of frame indices corresponding to the <code>dists</code>     video_paths: A list of video paths corresponding to the <code>dists</code></p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def compute_dists(\n    positive_pairs: List[Tuple[sio.Instance, sio.PredictedInstance, Any]],\n) -&gt; Dict[str, Union[np.ndarray, List[int], List[str]]]:\n    \"\"\"Compute Euclidean distances between matched pairs of instances.\n\n    Args:\n        positive_pairs: A list of tuples of the form `(instance_gt, instance_pr, _)`\n            containing the matched pair of instances.\n\n    Returns:\n        A dictionary with the following keys:\n            dists: An array of pairwise distances of shape `(n_positive_pairs, n_nodes)`\n            frame_idxs: A list of frame indices corresponding to the `dists`\n            video_paths: A list of video paths corresponding to the `dists`\n    \"\"\"\n    dists = []\n    frame_idxs = []\n    video_paths = []\n    for instance_gt, instance_pr, _ in positive_pairs:\n        points_gt = instance_gt.instance.numpy()\n        points_pr = instance_pr.instance.numpy()\n\n        dists.append(np.linalg.norm(points_pr - points_gt, axis=-1))\n        frame_idxs.append(instance_gt.frame_idx)\n        video_paths.append(instance_gt.video_path)\n\n    dists = np.array(dists)\n\n    # Bundle everything into a dictionary\n    dists_dict = {\n        \"dists\": dists,\n        \"frame_idxs\": frame_idxs,\n        \"video_paths\": video_paths,\n    }\n\n    return dists_dict\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.compute_instance_area","title":"<code>compute_instance_area(points)</code>","text":"<p>Compute the area of the bounding box of a set of keypoints.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>A numpy array of coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The area of the bounding box of the points.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def compute_instance_area(points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the area of the bounding box of a set of keypoints.\n\n    Args:\n        points: A numpy array of coordinates.\n\n    Returns:\n        The area of the bounding box of the points.\n    \"\"\"\n    if points.ndim == 2:\n        points = np.expand_dims(points, axis=0)\n\n    min_pt = np.nanmin(points, axis=-2)\n    max_pt = np.nanmax(points, axis=-2)\n\n    return np.prod(max_pt - min_pt, axis=-1)\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.compute_oks","title":"<code>compute_oks(points_gt, points_pr, scale=None, stddev=0.025, use_cocoeval=True)</code>","text":"<p>Compute the object keypoints similarity between sets of points.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>Ground truth instances of shape (n_gt, n_nodes, n_ed), where n_nodes is the number of body parts/keypoint types, and n_ed is the number of Euclidean dimensions (typically 2 or 3). Keypoints that are missing/not visible should be represented as NaNs.</p> required <code>points_pr</code> <code>ndarray</code> <p>Predicted instance of shape (n_pr, n_nodes, n_ed).</p> required <code>use_cocoeval</code> <code>bool</code> <p>Indicates whether the OKS score is calculated like cocoeval method or not. True indicating the score is calculated using the cocoeval method (widely used and the code can be found here at https://github.com/cocodataset/cocoapi/blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/PythonAPI/pycocotools/cocoeval.py#L192C5-L233C20) and False indicating the score is calculated using the method exactly as given in the paper referenced in the Notes below.</p> <code>True</code> <code>scale</code> <code>Optional[float]</code> <p>Size scaling factor to use when weighing the scores, typically the area of the bounding box of the instance (in pixels). This should be of the length n_gt. If a scalar is provided, the same number is used for all ground truth instances. If set to None, the bounding box area of the ground truth instances will be calculated.</p> <code>None</code> <code>stddev</code> <code>float</code> <p>The standard deviation associated with the spread in the localization accuracy of each node/keypoint type. This should be of the length n_nodes. \"Easier\" keypoint types will have lower values to reflect the smaller spread expected in localizing it.</p> <code>0.025</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The object keypoints similarity between every pair of ground truth and predicted instance, a numpy array of of shape (n_gt, n_pr) in the range of [0, 1.0], with 1.0 denoting a perfect match.</p> Notes <p>It's important to set the stddev appropriately when accounting for the difficulty of each keypoint type. For reference, the median value for all keypoint types in COCO is 0.072. The \"easiest\" keypoint is the left eye, with stddev of 0.025, since it is easy to precisely locate the eyes when labeling. The \"hardest\" keypoint is the left hip, with stddev of 0.107, since it's hard to locate the left hip bone without external anatomical features and since it is often occluded by clothing.</p> <p>The implementation here is based off of the descriptions in: Ronch &amp; Perona. \"Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation.\" ICCV (2017).</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def compute_oks(\n    points_gt: np.ndarray,\n    points_pr: np.ndarray,\n    scale: Optional[float] = None,\n    stddev: float = 0.025,\n    use_cocoeval: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Compute the object keypoints similarity between sets of points.\n\n    Args:\n        points_gt: Ground truth instances of shape (n_gt, n_nodes, n_ed),\n            where n_nodes is the number of body parts/keypoint types, and n_ed\n            is the number of Euclidean dimensions (typically 2 or 3). Keypoints\n            that are missing/not visible should be represented as NaNs.\n        points_pr: Predicted instance of shape (n_pr, n_nodes, n_ed).\n        use_cocoeval: Indicates whether the OKS score is calculated like cocoeval\n            method or not. True indicating the score is calculated using the\n            cocoeval method (widely used and the code can be found here at\n            https://github.com/cocodataset/cocoapi/blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/PythonAPI/pycocotools/cocoeval.py#L192C5-L233C20)\n            and False indicating the score is calculated using the method exactly\n            as given in the paper referenced in the Notes below.\n        scale: Size scaling factor to use when weighing the scores, typically\n            the area of the bounding box of the instance (in pixels). This\n            should be of the length n_gt. If a scalar is provided, the same\n            number is used for all ground truth instances. If set to None, the\n            bounding box area of the ground truth instances will be calculated.\n        stddev: The standard deviation associated with the spread in the\n            localization accuracy of each node/keypoint type. This should be of\n            the length n_nodes. \"Easier\" keypoint types will have lower values\n            to reflect the smaller spread expected in localizing it.\n\n    Returns:\n        The object keypoints similarity between every pair of ground truth and\n        predicted instance, a numpy array of of shape (n_gt, n_pr) in the range\n        of [0, 1.0], with 1.0 denoting a perfect match.\n\n    Notes:\n        It's important to set the stddev appropriately when accounting for the\n        difficulty of each keypoint type. For reference, the median value for\n        all keypoint types in COCO is 0.072. The \"easiest\" keypoint is the left\n        eye, with stddev of 0.025, since it is easy to precisely locate the\n        eyes when labeling. The \"hardest\" keypoint is the left hip, with stddev\n        of 0.107, since it's hard to locate the left hip bone without external\n        anatomical features and since it is often occluded by clothing.\n\n        The implementation here is based off of the descriptions in:\n        Ronch &amp; Perona. \"Benchmarking and Error Diagnosis in Multi-Instance Pose\n        Estimation.\" ICCV (2017).\n    \"\"\"\n    if points_gt.ndim == 2:\n        points_gt = np.expand_dims(points_gt, axis=0)\n    if points_pr.ndim == 2:\n        points_pr = np.expand_dims(points_pr, axis=0)\n\n    if scale is None:\n        scale = compute_instance_area(points_gt)\n\n    n_gt, n_nodes, n_ed = points_gt.shape  # n_ed = 2 or 3 (euclidean dimensions)\n    n_pr = points_pr.shape[0]\n\n    # If scalar scale was provided, use the same for each ground truth instance.\n    if np.isscalar(scale):\n        scale = np.full(n_gt, scale)\n\n    # If scalar standard deviation was provided, use the same for each node.\n    if np.isscalar(stddev):\n        stddev = np.full(n_nodes, stddev)\n\n    # Compute displacement between each pair.\n    displacement = np.reshape(points_gt, (n_gt, 1, n_nodes, n_ed)) - np.reshape(\n        points_pr, (1, n_pr, n_nodes, n_ed)\n    )\n    assert displacement.shape == (n_gt, n_pr, n_nodes, n_ed)\n\n    # Convert to pairwise Euclidean distances.\n    distance = (displacement**2).sum(axis=-1)  # (n_gt, n_pr, n_nodes)\n    assert distance.shape == (n_gt, n_pr, n_nodes)\n\n    # Compute the normalization factor per keypoint.\n    if use_cocoeval:\n        # If use_cocoeval is True, then compute normalization factor according to cocoeval.\n        spread_factor = (2 * stddev) ** 2\n        scale_factor = 2 * (scale + np.spacing(1))\n    else:\n        # If use_cocoeval is False, then compute normalization factor according to the paper.\n        spread_factor = stddev**2\n        scale_factor = 2 * ((scale + np.spacing(1)) ** 2)\n    normalization_factor = np.reshape(spread_factor, (1, 1, n_nodes)) * np.reshape(\n        scale_factor, (n_gt, 1, 1)\n    )\n    assert normalization_factor.shape == (n_gt, 1, n_nodes)\n\n    # Since a \"miss\" is considered as KS &lt; 0.5, we'll set the\n    # distances for predicted points that are missing to inf.\n    missing_pr = np.any(np.isnan(points_pr), axis=-1)  # (n_pr, n_nodes)\n    assert missing_pr.shape == (n_pr, n_nodes)\n    distance[:, missing_pr] = np.inf\n\n    # Compute the keypoint similarity as per the top of Eq. 1.\n    ks = np.exp(-(distance / normalization_factor))  # (n_gt, n_pr, n_nodes)\n    assert ks.shape == (n_gt, n_pr, n_nodes)\n\n    # Set the KS for missing ground truth points to 0.\n    # This is equivalent to the visibility delta function of the bottom\n    # of Eq. 1.\n    missing_gt = np.any(np.isnan(points_gt), axis=-1)  # (n_gt, n_nodes)\n    assert missing_gt.shape == (n_gt, n_nodes)\n    ks[np.expand_dims(missing_gt, axis=1)] = 0\n\n    # Compute the OKS.\n    n_visible_gt = np.sum(\n        (~missing_gt).astype(\"float32\"), axis=-1, keepdims=True\n    )  # (n_gt, 1)\n    oks = np.sum(ks, axis=-1) / n_visible_gt\n    assert oks.shape == (n_gt, n_pr)\n\n    return oks\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.find_frame_pairs","title":"<code>find_frame_pairs(labels_gt, labels_pr, user_labels_only=True)</code>","text":"<p>Find corresponding frames across two sets of labels.</p> <p>Parameters:</p> Name Type Description Default <code>labels_gt</code> <code>Labels</code> <p>A <code>sio.Labels</code> instance with ground truth instances.</p> required <code>labels_pr</code> <code>Labels</code> <p>A <code>sio.Labels</code> instance with predicted instances.</p> required <code>user_labels_only</code> <code>bool</code> <p>If False, frames with predicted instances in <code>labels_gt</code> will also be considered for matching.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Tuple[LabeledFrame, LabeledFrame]]</code> <p>A list of pairs of <code>sio.LabeledFrame</code>s in the form <code>(frame_gt, frame_pr)</code>.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def find_frame_pairs(\n    labels_gt: sio.Labels, labels_pr: sio.Labels, user_labels_only: bool = True\n) -&gt; List[Tuple[sio.LabeledFrame, sio.LabeledFrame]]:\n    \"\"\"Find corresponding frames across two sets of labels.\n\n    Args:\n        labels_gt: A `sio.Labels` instance with ground truth instances.\n        labels_pr: A `sio.Labels` instance with predicted instances.\n        user_labels_only: If False, frames with predicted instances in `labels_gt` will\n            also be considered for matching.\n\n    Returns:\n        A list of pairs of `sio.LabeledFrame`s in the form `(frame_gt, frame_pr)`.\n    \"\"\"\n    frame_pairs = []\n    for video_gt in labels_gt.videos:\n        # Find matching video instance in predictions.\n        video_pr = None\n        for video in labels_pr.videos:\n            if (\n                isinstance(video.backend, type(video_gt.backend))\n                and video.filename == video_gt.filename\n            ):\n                same_dataset = (\n                    (video.backend.dataset == video_gt.backend.dataset)\n                    if hasattr(video.backend, \"dataset\")\n                    else True\n                )  # `dataset` attr exists only for hdf5 backend not for mediavideo\n                if same_dataset:\n                    video_pr = video\n                    break\n\n        if video_pr is None:\n            continue\n\n        # Find labeled frames in this video.\n        labeled_frames_gt = labels_gt.find(video_gt)\n        if user_labels_only:\n            for lf in labeled_frames_gt:\n                lf.instances = lf.user_instances\n            labeled_frames_gt = [\n                lf for lf in labeled_frames_gt if len(lf.user_instances) &gt; 0\n            ]\n\n        # Attempt to match each labeled frame in the ground truth.\n        for labeled_frame_gt in labeled_frames_gt:\n            labeled_frames_pr = labels_pr.find(\n                video_pr, frame_idx=labeled_frame_gt.frame_idx\n            )\n\n            if not labeled_frames_pr:\n                # No match\n                continue\n            elif len(labeled_frames_pr) == 1:\n                # Match!\n                frame_pairs.append((labeled_frame_gt, labeled_frames_pr[0]))\n\n    return frame_pairs\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.get_instances","title":"<code>get_instances(labeled_frame)</code>","text":"<p>Get a list of instances of type MatchInstance from the Labeled Frame.</p> <p>Parameters:</p> Name Type Description Default <code>labeled_frame</code> <code>LabeledFrame</code> <p>Input Labeled frame of type sio.LabeledFrame.</p> required <p>Returns:</p> Type Description <code>List[MatchInstance]</code> <p>List of MatchInstance objects for the given labeled frame.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def get_instances(labeled_frame: sio.LabeledFrame) -&gt; List[MatchInstance]:\n    \"\"\"Get a list of instances of type MatchInstance from the Labeled Frame.\n\n    Args:\n        labeled_frame: Input Labeled frame of type sio.LabeledFrame.\n\n    Returns:\n        List of MatchInstance objects for the given labeled frame.\n    \"\"\"\n    instance_list = []\n    frame_idx = labeled_frame.frame_idx\n    video_path = (\n        labeled_frame.video.backend.source_filename\n        if hasattr(labeled_frame.video.backend, \"source_filename\")\n        else labeled_frame.video.backend.filename\n    )\n    for instance in labeled_frame.instances:\n        match_instance = MatchInstance(\n            instance=instance, frame_idx=frame_idx, video_path=video_path\n        )\n        instance_list.append(match_instance)\n    return instance_list\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.main","title":"<code>main(**kwargs)</code>","text":"<p>Run evaluation.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>@click.command()\n@click.option(\n    \"--ground_truth_path\",\n    type=str,\n    required=True,\n    help=\"Path to ground truth labels file (.slp)\",\n)\n@click.option(\n    \"--predicted_path\",\n    type=str,\n    required=True,\n    help=\"Path to predicted labels file (.slp)\",\n)\n@click.option(\n    \"--oks_stddev\",\n    type=float,\n    default=0.025,\n    help=\"Standard deviation for OKS computation. Default: 0.025\",\n)\n@click.option(\n    \"--oks_scale\",\n    type=float,\n    default=None,\n    help=\"Scale for OKS computation. If None, bounding box area will be used. Default: None\",\n)\n@click.option(\n    \"--match_threshold\",\n    type=float,\n    default=0.0,\n    help=\"Threshold for OKS scores when determining instance matches. Default: 0.0\",\n)\n@click.option(\n    \"--user_labels_only\",\n    is_flag=True,\n    default=True,\n    help=\"If True, only user-labeled frames in ground truth are considered. Default: True\",\n)\n@click.option(\n    \"--save_metrics\",\n    type=str,\n    default=None,\n    help=\"Path to save metrics as .npz file. If not provided, metrics will not be saved.\",\n)\ndef main(**kwargs):\n    \"\"\"Run evaluation.\"\"\"\n    run_evaluation(**kwargs)\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.match_frame_pairs","title":"<code>match_frame_pairs(frame_pairs, stddev=0.025, scale=None, threshold=0)</code>","text":"<p>Match all ground truth and predicted instances within each pair of frames.</p> <p>This is a wrapper for <code>match_instances()</code> but operates on lists of frames.</p> <p>Parameters:</p> Name Type Description Default <code>frame_pairs</code> <code>List[Tuple[LabeledFrame, LabeledFrame]]</code> <p>A list of pairs of <code>sleap.LabeledFrame</code>s in the form <code>(frame_gt, frame_pr)</code>. These can be obtained with <code>find_frame_pairs()</code>.</p> required <code>stddev</code> <code>float</code> <p>The expected spread of coordinates for OKS computation.</p> <code>0.025</code> <code>scale</code> <code>Optional[float]</code> <p>The scale for normalizing the OKS. If not set, the bounding box area will be used.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The minimum OKS between a candidate pair of instances to be considered a match.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[List[Tuple[Instance, PredictedInstance, float]], List[Instance]]</code> <p>A tuple of (<code>positive_pairs</code>, <code>false_negatives</code>).</p> <p><code>positive_pairs</code> is a list of 3-tuples of the form <code>(instance_gt, instance_pr, oks)</code> containing the matched pair of instances and their OKS.</p> <p><code>false_negatives</code> is a list of ground truth <code>sio.Instance</code>s that could not be matched.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def match_frame_pairs(\n    frame_pairs: List[Tuple[sio.LabeledFrame, sio.LabeledFrame]],\n    stddev: float = 0.025,\n    scale: Optional[float] = None,\n    threshold: float = 0,\n) -&gt; Tuple[List[Tuple[sio.Instance, sio.PredictedInstance, float]], List[sio.Instance]]:\n    \"\"\"Match all ground truth and predicted instances within each pair of frames.\n\n    This is a wrapper for `match_instances()` but operates on lists of frames.\n\n    Args:\n        frame_pairs: A list of pairs of `sleap.LabeledFrame`s in the form\n            `(frame_gt, frame_pr)`. These can be obtained with `find_frame_pairs()`.\n        stddev: The expected spread of coordinates for OKS computation.\n        scale: The scale for normalizing the OKS. If not set, the bounding box area will\n            be used.\n        threshold: The minimum OKS between a candidate pair of instances to be\n            considered a match.\n\n    Returns:\n        A tuple of (`positive_pairs`, `false_negatives`).\n\n        `positive_pairs` is a list of 3-tuples of the form\n        `(instance_gt, instance_pr, oks)` containing the matched pair of instances and\n        their OKS.\n\n        `false_negatives` is a list of ground truth `sio.Instance`s that could not be\n        matched.\n    \"\"\"\n    positive_pairs = []\n    false_negatives = []\n    for frame_gt, frame_pr in frame_pairs:\n        positive_pairs_frame, false_negatives_frame = match_instances(\n            frame_gt,\n            frame_pr,\n            stddev=stddev,\n            scale=scale,\n            threshold=threshold,\n        )\n        positive_pairs.extend(positive_pairs_frame)\n        false_negatives.extend(false_negatives_frame)\n\n    return positive_pairs, false_negatives\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.match_instances","title":"<code>match_instances(frame_gt, frame_pr, stddev=0.025, scale=None, threshold=0)</code>","text":"<p>Match pairs of instances between ground truth and predictions in a frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame_gt</code> <code>LabeledFrame</code> <p>A <code>sio.LabeledFrame</code> with ground truth instances.</p> required <code>frame_pr</code> <code>LabeledFrame</code> <p>A <code>sio.LabeledFrame</code> with predicted instances.</p> required <code>stddev</code> <code>float</code> <p>The expected spread of coordinates for OKS computation.</p> <code>0.025</code> <code>scale</code> <code>Optional[float]</code> <p>The scale for normalizing the OKS. If not set, the bounding box area will be used.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The minimum OKS between a candidate pair of instances to be considered a match.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[List[Tuple[Instance, PredictedInstance, float]], List[Instance]]</code> <p>A tuple of (<code>positive_pairs</code>, <code>false_negatives</code>).</p> <p><code>positive_pairs</code> is a list of 3-tuples of the form <code>(instance_gt, instance_pr, oks)</code> containing the matched pair of instances and their OKS.</p> <p><code>false_negatives</code> is a list of ground truth <code>sleap.Instance</code>s that could not be matched.</p> Notes <p>This function uses the approach from the PASCAL VOC scoring procedure. Briefly, predictions are sorted descending by their instance-level prediction scores and greedily matched to ground truth instances which are then removed from the pool of available instances.</p> <p>Ground truth instances that remain unmatched are considered false negatives.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def match_instances(\n    frame_gt: sio.LabeledFrame,\n    frame_pr: sio.LabeledFrame,\n    stddev: float = 0.025,\n    scale: Optional[float] = None,\n    threshold: float = 0,\n) -&gt; Tuple[List[Tuple[sio.Instance, sio.PredictedInstance, float]], List[sio.Instance]]:\n    \"\"\"Match pairs of instances between ground truth and predictions in a frame.\n\n    Args:\n        frame_gt: A `sio.LabeledFrame` with ground truth instances.\n        frame_pr: A `sio.LabeledFrame` with predicted instances.\n        stddev: The expected spread of coordinates for OKS computation.\n        scale: The scale for normalizing the OKS. If not set, the bounding box area will\n            be used.\n        threshold: The minimum OKS between a candidate pair of instances to be\n            considered a match.\n\n    Returns:\n        A tuple of (`positive_pairs`, `false_negatives`).\n\n        `positive_pairs` is a list of 3-tuples of the form\n        `(instance_gt, instance_pr, oks)` containing the matched pair of instances and\n        their OKS.\n\n        `false_negatives` is a list of ground truth `sleap.Instance`s that could not be\n        matched.\n\n    Notes:\n        This function uses the approach from the PASCAL VOC scoring procedure. Briefly,\n        predictions are sorted descending by their instance-level prediction scores and\n        greedily matched to ground truth instances which are then removed from the pool\n        of available instances.\n\n        Ground truth instances that remain unmatched are considered false negatives.\n    \"\"\"\n    # Sort predicted instances by score.\n    frame_pr_match_instances = get_instances(frame_pr)\n\n    scores_pr = np.array(\n        [\n            m.instance.score\n            for m in frame_pr_match_instances\n            if hasattr(m.instance, \"score\")\n        ]\n    )\n    idxs_pr = np.argsort(-scores_pr, kind=\"mergesort\")  # descending\n    scores_pr = scores_pr[idxs_pr]\n\n    available_instances_gt = get_instances(frame_gt)\n    available_instances_gt_idxs = list(range(len(available_instances_gt)))\n\n    positive_pairs = []\n    for idx_pr in idxs_pr:\n        # Pull out predicted instance.\n        instance_pr = frame_pr_match_instances[idx_pr]\n\n        # Convert instances to point arrays.\n        points_pr = np.expand_dims(instance_pr.instance.numpy(), axis=0)\n        points_gt = np.stack(\n            [\n                available_instances_gt[idx].instance.numpy()\n                for idx in available_instances_gt_idxs\n            ],\n            axis=0,\n        )\n\n        # Find the best match by computing OKS.\n        oks = compute_oks(points_gt, points_pr, stddev=stddev, scale=scale)\n        oks = np.squeeze(oks, axis=1)\n        assert oks.shape == (len(points_gt),)\n\n        oks[oks &lt;= threshold] = np.nan\n        best_match_gt_idx = np.argsort(-oks, kind=\"mergesort\")[0]\n        best_match_oks = oks[best_match_gt_idx]\n        if np.isnan(best_match_oks):\n            continue\n\n        # Remove matched ground truth instance and add as a positive pair.\n        instance_gt_idx = available_instances_gt_idxs.pop(best_match_gt_idx)\n        instance_gt = available_instances_gt[instance_gt_idx]\n        positive_pairs.append((instance_gt, instance_pr, best_match_oks))\n\n        # Stop matching lower scoring instances if we run out of candidates in the\n        # ground truth.\n        if not available_instances_gt_idxs:\n            break\n\n    # Any remaining ground truth instances are considered false negatives.\n    false_negatives = [\n        available_instances_gt[idx] for idx in available_instances_gt_idxs\n    ]\n\n    return positive_pairs, false_negatives\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.run_evaluation","title":"<code>run_evaluation(ground_truth_path, predicted_path, oks_stddev=0.025, oks_scale=None, match_threshold=0, user_labels_only=True, save_metrics=None)</code>","text":"<p>Evaluate SLEAP-NN model predictions against ground truth labels.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def run_evaluation(\n    ground_truth_path: str,\n    predicted_path: str,\n    oks_stddev: float = 0.025,\n    oks_scale: Optional[float] = None,\n    match_threshold: float = 0,\n    user_labels_only: bool = True,\n    save_metrics: Optional[str] = None,\n):\n    \"\"\"Evaluate SLEAP-NN model predictions against ground truth labels.\"\"\"\n    logger.info(\"Loading ground truth labels...\")\n    ground_truth_instances = sio.load_slp(ground_truth_path)\n\n    logger.info(\"Loading predicted labels...\")\n    predicted_instances = sio.load_slp(predicted_path)\n\n    logger.info(\"Creating evaluator...\")\n    evaluator = Evaluator(\n        ground_truth_instances=ground_truth_instances,\n        predicted_instances=predicted_instances,\n        oks_stddev=oks_stddev,\n        oks_scale=oks_scale,\n        match_threshold=match_threshold,\n        user_labels_only=user_labels_only,\n    )\n\n    logger.info(\"Computing evaluation metrics...\")\n    metrics = evaluator.evaluate()\n\n    # Print key metrics\n    logger.info(\"Evaluation Results:\")\n    logger.info(f\"mOKS: {metrics['mOKS']['mOKS']:.4f}\")\n    logger.info(f\"mAP (OKS VOC): {metrics['voc_metrics']['oks_voc.mAP']:.4f}\")\n    logger.info(f\"mAR (OKS VOC): {metrics['voc_metrics']['oks_voc.mAR']:.4f}\")\n    logger.info(f\"Average Distance: {metrics['distance_metrics']['avg']:.4f}\")\n    logger.info(f\"mPCK: {metrics['pck_metrics']['mPCK']:.4f}\")\n    logger.info(\n        f\"Visibility Precision: {metrics['visibility_metrics']['precision']:.4f}\"\n    )\n    logger.info(f\"Visibility Recall: {metrics['visibility_metrics']['recall']:.4f}\")\n\n    # Save metrics if path provided\n    if save_metrics:\n        logger.info(f\"Saving metrics to {save_metrics}...\")\n        save_path = Path(save_metrics)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Convert metrics to numpy arrays for saving\n        np.savez(\n            save_path,\n            mOKS=metrics[\"mOKS\"][\"mOKS\"],\n            mAP=metrics[\"voc_metrics\"][\"oks_voc.mAP\"],\n            mAR=metrics[\"voc_metrics\"][\"oks_voc.mAR\"],\n            avg_distance=metrics[\"distance_metrics\"][\"avg\"],\n            mPCK=metrics[\"pck_metrics\"][\"mPCK\"],\n            visibility_precision=metrics[\"visibility_metrics\"][\"precision\"],\n            visibility_recall=metrics[\"visibility_metrics\"][\"recall\"],\n            # Save full metrics dict as well\n            voc_metrics=metrics[\"voc_metrics\"],\n            distance_metrics=metrics[\"distance_metrics\"],\n            pck_metrics=metrics[\"pck_metrics\"],\n            visibility_metrics=metrics[\"visibility_metrics\"],\n        )\n        logger.info(f\"Metrics saved successfully to {save_path}\")\n\n    return metrics\n</code></pre>"},{"location":"api/legacy_models/","title":"legacy_models","text":""},{"location":"api/legacy_models/#sleap_nn.legacy_models","title":"<code>sleap_nn.legacy_models</code>","text":"<p>Utilities for loading legacy SLEAP models.</p> <p>This module provides functions to convert SLEAP models trained with the TensorFlow/Keras backend to PyTorch format compatible with sleap-nn.</p> <p>Functions:</p> Name Description <code>convert_keras_to_pytorch_conv2d</code> <p>Convert Keras Conv2D weights to PyTorch format.</p> <code>convert_keras_to_pytorch_conv2d_transpose</code> <p>Convert Keras Conv2DTranspose weights to PyTorch format.</p> <code>create_model_from_legacy_config</code> <p>Create a PyTorch model from a legacy training config.</p> <code>get_keras_first_layer_channels</code> <p>Extract the number of input channels from the first layer of a Keras model.</p> <code>load_keras_weights</code> <p>Load all weights from a Keras HDF5 model file.</p> <code>load_legacy_model</code> <p>Load a complete legacy SLEAP model including weights.</p> <code>load_legacy_model_weights</code> <p>Load legacy Keras weights into a PyTorch model.</p> <code>map_legacy_to_pytorch_layers</code> <p>Create mapping between legacy Keras layers and PyTorch model layers.</p> <code>parse_keras_layer_name</code> <p>Parse a Keras layer path to extract basic information.</p> <code>update_backbone_in_channels</code> <p>Update the backbone configuration's in_channels if it's different from the Keras model.</p>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.convert_keras_to_pytorch_conv2d","title":"<code>convert_keras_to_pytorch_conv2d(keras_weight)</code>","text":"<p>Convert Keras Conv2D weights to PyTorch format.</p> <p>Parameters:</p> Name Type Description Default <code>keras_weight</code> <code>ndarray</code> <p>Numpy array with shape (H, W, C_in, C_out) from Keras</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>PyTorch tensor with shape (C_out, C_in, H, W)</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def convert_keras_to_pytorch_conv2d(keras_weight: np.ndarray) -&gt; torch.Tensor:\n    \"\"\"Convert Keras Conv2D weights to PyTorch format.\n\n    Args:\n        keras_weight: Numpy array with shape (H, W, C_in, C_out) from Keras\n\n    Returns:\n        PyTorch tensor with shape (C_out, C_in, H, W)\n    \"\"\"\n    if keras_weight.ndim != 4:\n        raise ValueError(\n            f\"Expected 4D array for Conv2D weights, got shape {keras_weight.shape}\"\n        )\n\n    # Keras: (H, W, C_in, C_out) -&gt; PyTorch: (C_out, C_in, H, W)\n    pytorch_weight = keras_weight.transpose(3, 2, 0, 1)\n    return torch.from_numpy(pytorch_weight.copy()).float()\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.convert_keras_to_pytorch_conv2d_transpose","title":"<code>convert_keras_to_pytorch_conv2d_transpose(keras_weight)</code>","text":"<p>Convert Keras Conv2DTranspose weights to PyTorch format.</p> <p>Parameters:</p> Name Type Description Default <code>keras_weight</code> <code>ndarray</code> <p>Numpy array with shape (H, W, C_out, C_in) from Keras</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>PyTorch tensor with shape (C_in, C_out, H, W)</p> Note <p>Keras stores transposed conv weights differently than regular conv.</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def convert_keras_to_pytorch_conv2d_transpose(keras_weight: np.ndarray) -&gt; torch.Tensor:\n    \"\"\"Convert Keras Conv2DTranspose weights to PyTorch format.\n\n    Args:\n        keras_weight: Numpy array with shape (H, W, C_out, C_in) from Keras\n\n    Returns:\n        PyTorch tensor with shape (C_in, C_out, H, W)\n\n    Note:\n        Keras stores transposed conv weights differently than regular conv.\n    \"\"\"\n    if keras_weight.ndim != 4:\n        raise ValueError(\n            f\"Expected 4D array for Conv2DTranspose weights, got shape {keras_weight.shape}\"\n        )\n\n    # Keras: (H, W, C_out, C_in) -&gt; PyTorch: (C_in, C_out, H, W)\n    pytorch_weight = keras_weight.transpose(3, 2, 0, 1)\n    return torch.from_numpy(pytorch_weight.copy()).float()\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.create_model_from_legacy_config","title":"<code>create_model_from_legacy_config(config_path)</code>","text":"<p>Create a PyTorch model from a legacy training config.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the legacy training_config.json file</p> required <p>Returns:</p> Type Description <code>Model</code> <p>Model instance configured to match the legacy architecture</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def create_model_from_legacy_config(config_path: str) -&gt; Model:\n    \"\"\"Create a PyTorch model from a legacy training config.\n\n    Args:\n        config_path: Path to the legacy training_config.json file\n\n    Returns:\n        Model instance configured to match the legacy architecture\n    \"\"\"\n    # Load config using existing functionality\n    config_path = Path(config_path)\n    if config_path.is_dir():\n        config_path = config_path / \"training_config.json\"\n\n    # Use the existing config loader\n    config = TrainingJobConfig.load_sleap_config(str(config_path))\n\n    # Determine backbone type from config\n    backbone_type = \"unet\"  # Default for legacy models\n\n    # Get backbone config (should be under the unet key for legacy models)\n    backbone_config = config.model_config.backbone_config.unet\n\n    # Check if there's a corresponding .h5 file to extract input channels\n    model_dir = config_path.parent\n    h5_path = model_dir / \"best_model.h5\"\n\n    if h5_path.exists():\n        keras_in_channels = get_keras_first_layer_channels(str(h5_path))\n        if keras_in_channels is not None:\n            backbone_config = update_backbone_in_channels(\n                backbone_config, keras_in_channels\n            )\n\n    # Determine model type from head configs\n    head_configs = config.model_config.head_configs\n    model_type = None\n    active_head_config = None\n\n    if head_configs.centroid is not None:\n        model_type = \"centroid\"\n        active_head_config = head_configs.centroid\n    elif head_configs.centered_instance is not None:\n        model_type = \"centered_instance\"\n        active_head_config = head_configs.centered_instance\n    elif head_configs.single_instance is not None:\n        model_type = \"single_instance\"\n        active_head_config = head_configs.single_instance\n    elif head_configs.bottomup is not None:\n        model_type = \"bottomup\"\n        active_head_config = head_configs.bottomup\n    elif head_configs.multi_class_topdown is not None:\n        model_type = \"multi_class_topdown\"\n        active_head_config = head_configs.multi_class_topdown\n    elif head_configs.multi_class_bottomup is not None:\n        model_type = \"multi_class_bottomup\"\n        active_head_config = head_configs.multi_class_bottomup\n    else:\n        raise ValueError(\"Could not determine model type from head configs\")\n\n    # Create model using the from_config method\n    model = Model.from_config(\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=active_head_config,\n        model_type=model_type,\n    )\n\n    return model\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.get_keras_first_layer_channels","title":"<code>get_keras_first_layer_channels(h5_path)</code>","text":"<p>Extract the number of input channels from the first layer of a Keras model.</p> <p>Parameters:</p> Name Type Description Default <code>h5_path</code> <code>str</code> <p>Path to the .h5 model file</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Number of input channels in the first layer, or None if not found</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def get_keras_first_layer_channels(h5_path: str) -&gt; Optional[int]:\n    \"\"\"Extract the number of input channels from the first layer of a Keras model.\n\n    Args:\n        h5_path: Path to the .h5 model file\n\n    Returns:\n        Number of input channels in the first layer, or None if not found\n    \"\"\"\n    try:\n        with h5py.File(h5_path, \"r\") as f:\n            # Look for the first convolutional layer weights\n            kernel_weights = []\n\n            def collect_kernel_weights(name, obj):\n                if isinstance(obj, h5py.Dataset) and name.startswith(\"model_weights/\"):\n                    # Skip optimizer weights\n                    if \"optimizer_weights\" in name:\n                        return\n\n                    # Look for kernel weights (not bias)\n                    if \"kernel\" in name and obj.ndim == 4:\n                        kernel_weights.append((name, obj.shape))\n\n            # Visit all items to collect kernel weights\n            f.visititems(collect_kernel_weights)\n\n            if not kernel_weights:\n                return None\n\n            # Look for the known first layer patterns (stem0_conv0 or stack0_enc0_conv0)\n            for name, shape in kernel_weights:\n                input_channels = shape[2]\n                layer_name = name.split(\"/\")[1] if len(name.split(\"/\")) &gt; 1 else name\n\n                # Check for the known first layer patterns\n                if \"stem0_conv0\" in layer_name or \"stack0_enc0_conv0\" in layer_name:\n                    logger.info(\n                        f\"Found first layer '{name}' with {input_channels} input channels\"\n                    )\n                    return input_channels\n\n            # If no known first layer patterns are found, return None\n            logger.warning(\n                f\"No known first layer patterns (stem0_conv0 or stack0_enc0_conv0) found in {h5_path}\"\n            )\n            return None\n\n    except Exception as e:\n        logger.warning(f\"Could not extract first layer channels from {h5_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.load_keras_weights","title":"<code>load_keras_weights(h5_path)</code>","text":"<p>Load all weights from a Keras HDF5 model file.</p> <p>Parameters:</p> Name Type Description Default <code>h5_path</code> <code>str</code> <p>Path to the .h5 model file</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary mapping layer paths to weight arrays</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def load_keras_weights(h5_path: str) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Load all weights from a Keras HDF5 model file.\n\n    Args:\n        h5_path: Path to the .h5 model file\n\n    Returns:\n        Dictionary mapping layer paths to weight arrays\n    \"\"\"\n    weights = {}\n\n    with h5py.File(h5_path, \"r\") as f:\n\n        def extract_weights(name, obj):\n            if isinstance(obj, h5py.Dataset) and name.startswith(\"model_weights/\"):\n                # Skip optimizer weights\n                if \"optimizer_weights\" in name:\n                    return\n                weights[name] = obj[:]\n\n        f.visititems(extract_weights)\n\n    return weights\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.load_legacy_model","title":"<code>load_legacy_model(model_dir, load_weights=True)</code>","text":"<p>Load a complete legacy SLEAP model including weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Path to the legacy model directory containing        training_config.json and best_model.h5</p> required <code>load_weights</code> <code>bool</code> <p>Whether to load the weights. If False, only           creates the model architecture.</p> <code>True</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model instance with loaded weights</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def load_legacy_model(model_dir: str, load_weights: bool = True) -&gt; Model:\n    \"\"\"Load a complete legacy SLEAP model including weights.\n\n    Args:\n        model_dir: Path to the legacy model directory containing\n                   training_config.json and best_model.h5\n        load_weights: Whether to load the weights. If False, only\n                      creates the model architecture.\n\n    Returns:\n        Model instance with loaded weights\n    \"\"\"\n    model_dir = Path(model_dir)\n\n    # Create model from config\n    model = create_model_from_legacy_config(str(model_dir))\n    model.eval()\n\n    # Load weights if requested\n    if load_weights:\n        h5_path = model_dir / \"best_model.h5\"\n        if h5_path.exists():\n            load_legacy_model_weights(model, str(h5_path))\n\n        else:\n            message = f\"Model weights not found at {h5_path}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    return model\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.load_legacy_model_weights","title":"<code>load_legacy_model_weights(pytorch_model, h5_path, mapping=None)</code>","text":"<p>Load legacy Keras weights into a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>pytorch_model</code> <code>Module</code> <p>PyTorch model to load weights into</p> required <code>h5_path</code> <code>str</code> <p>Path to the legacy .h5 model file</p> required <code>mapping</code> <code>Optional[Dict[str, str]]</code> <p>Optional manual mapping of layer names. If None,      will attempt automatic mapping.</p> <code>None</code> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def load_legacy_model_weights(\n    pytorch_model: torch.nn.Module,\n    h5_path: str,\n    mapping: Optional[Dict[str, str]] = None,\n) -&gt; None:\n    \"\"\"Load legacy Keras weights into a PyTorch model.\n\n    Args:\n        pytorch_model: PyTorch model to load weights into\n        h5_path: Path to the legacy .h5 model file\n        mapping: Optional manual mapping of layer names. If None,\n                 will attempt automatic mapping.\n    \"\"\"\n    # Load legacy weights\n    legacy_weights = load_keras_weights(h5_path)\n\n    if mapping is None:\n        # Attempt automatic mapping\n        try:\n            mapping = map_legacy_to_pytorch_layers(legacy_weights, pytorch_model)\n        except Exception as e:\n            logger.error(f\"Failed to create weight mappings: {e}\")\n            return\n\n    # Apply weights\n    loaded_count = 0\n    errors = []\n\n    for legacy_path, pytorch_name in mapping.items():\n        if legacy_path not in legacy_weights:\n            logger.warning(f\"Legacy weight not found: {legacy_path}\")\n            continue\n\n        weight = legacy_weights[legacy_path]\n        info = parse_keras_layer_name(legacy_path)\n\n        # Convert weight format if needed\n        if info[\"weight_type\"] == \"kernel\":\n            if \"trans_conv\" in legacy_path:\n                weight = convert_keras_to_pytorch_conv2d_transpose(weight)\n            else:\n                weight = convert_keras_to_pytorch_conv2d(weight)\n        else:\n            # Bias weights don't need conversion\n            weight = torch.from_numpy(weight).float()\n\n        # Set the parameter using state_dict\n        try:\n            state_dict = pytorch_model.state_dict()\n            if pytorch_name not in state_dict:\n                logger.warning(f\"PyTorch parameter not found: {pytorch_name}\")\n                continue\n\n            # Check shape compatibility\n            pytorch_shape = state_dict[pytorch_name].shape\n            if weight.shape != pytorch_shape:\n                logger.warning(\n                    f\"Shape mismatch for {pytorch_name}: \"\n                    f\"legacy {weight.shape} vs pytorch {pytorch_shape}\"\n                )\n                continue\n\n            # Update the parameter in the model\n            with torch.no_grad():\n                param = pytorch_model\n                for attr in pytorch_name.split(\".\")[:-1]:\n                    param = getattr(param, attr)\n                param_name = pytorch_name.split(\".\")[-1]\n                setattr(param, param_name, torch.nn.Parameter(weight))\n\n            loaded_count += 1\n\n        except Exception as e:\n            error_msg = f\"Error loading {pytorch_name}: {e}\"\n            logger.error(error_msg)\n            errors.append(error_msg)\n\n    # Log summary\n    if loaded_count == 0:\n        logger.info(\n            f\"No weights were successfully loaded. \"\n            f\"Attempted to load {len(mapping)} weights, but all failed.\"\n        )\n    else:\n        logger.info(\n            f\"Successfully loaded {loaded_count}/{len(mapping)} weights from legacy model\"\n        )\n\n    # Log any errors that occurred\n    if errors:\n        logger.info(\n            f\"Weight loading completed with {len(errors)} errors: {'; '.join(errors[:5])}\"\n        )\n\n    # Verify all loaded weights by comparing means\n    logger.info(\"Verifying weight assignments...\")\n    verification_errors = []\n\n    for legacy_path, pytorch_name in mapping.items():\n        if legacy_path not in legacy_weights:\n            continue\n\n        try:\n            original_weight = legacy_weights[legacy_path]\n            info = parse_keras_layer_name(legacy_path)\n\n            if info[\"weight_type\"] == \"kernel\":\n                # Convert Keras to PyTorch format\n                torch_weight = convert_keras_to_pytorch_conv2d(original_weight)\n                # Keras: (H, W, C_in, C_out), PyTorch: (C_out, C_in, H, W)\n                keras_cout = original_weight.shape[-1]\n                torch_cout = torch_weight.shape[0]\n                assert (\n                    keras_cout == torch_cout\n                ), f\"Output channel mismatch: {keras_cout} vs {torch_cout}\"\n\n                # Check each output channel\n                channel_errors = []\n                for i in range(keras_cout):\n                    keras_ch_mean = np.mean(original_weight[..., i])\n                    torch_ch_mean = torch.mean(torch_weight[i]).item()\n                    diff = abs(keras_ch_mean - torch_ch_mean)\n                    if diff &gt; 1e-6:\n                        channel_errors.append(\n                            f\"channel {i}: keras={keras_ch_mean:.6f}, torch={torch_ch_mean:.6f}, diff={diff:.6e}\"\n                        )\n\n                if channel_errors:\n                    message = f\"Channel verification failed for {pytorch_name}: {'; '.join(channel_errors)}\"\n                    logger.error(message)\n                    verification_errors.append(message)\n\n            else:\n                # Bias: just compare all values\n                keras_mean = np.mean(original_weight)\n                torch_mean = torch.mean(\n                    torch.from_numpy(original_weight).float()\n                ).item()\n                diff = abs(keras_mean - torch_mean)\n                if diff &gt; 1e-6:\n                    message = f\"Weight verification failed for {pytorch_name} bias): keras={keras_mean:.6f}, torch={torch_mean:.6f}, diff={diff:.6e}\"\n                    logger.error(message)\n                    verification_errors.append(message)\n\n        except Exception as e:\n            error_msg = f\"Error verifying {pytorch_name}: {e}\"\n            logger.error(error_msg)\n            verification_errors.append(error_msg)\n\n    if not verification_errors:\n        logger.info(\"\u2713 All weight assignments verified successfully\")\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.map_legacy_to_pytorch_layers","title":"<code>map_legacy_to_pytorch_layers(legacy_weights, pytorch_model)</code>","text":"<p>Create mapping between legacy Keras layers and PyTorch model layers.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_weights</code> <code>Dict[str, ndarray]</code> <p>Dictionary of legacy weights from load_keras_weights()</p> required <code>pytorch_model</code> <code>Module</code> <p>PyTorch model instance to map to</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping legacy layer paths to PyTorch parameter names</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def map_legacy_to_pytorch_layers(\n    legacy_weights: Dict[str, np.ndarray], pytorch_model: torch.nn.Module\n) -&gt; Dict[str, str]:\n    \"\"\"Create mapping between legacy Keras layers and PyTorch model layers.\n\n    Args:\n        legacy_weights: Dictionary of legacy weights from load_keras_weights()\n        pytorch_model: PyTorch model instance to map to\n\n    Returns:\n        Dictionary mapping legacy layer paths to PyTorch parameter names\n    \"\"\"\n    mapping = {}\n\n    # Get all PyTorch parameters with their shapes\n    pytorch_params = {}\n    for name, param in pytorch_model.named_parameters():\n        pytorch_params[name] = param.shape\n\n    # For each legacy weight, find the corresponding PyTorch parameter\n    for legacy_path, weight in legacy_weights.items():\n        # Extract the layer name from the legacy path\n        # Legacy path format: \"model_weights/stack0_enc0_conv0/stack0_enc0_conv0/kernel:0\"\n        clean_path = legacy_path.replace(\"model_weights/\", \"\")\n        parts = clean_path.split(\"/\")\n\n        if len(parts) &lt; 2:\n            continue\n\n        layer_name = parts[0]  # e.g., \"stack0_enc0_conv0\" or \"CentroidConfmapsHead_0\"\n        weight_name = parts[-1]  # e.g., \"kernel:0\" or \"bias:0\"\n\n        # Convert Keras weight type to PyTorch weight type\n        weight_type = \"weight\" if \"kernel\" in weight_name else \"bias\"\n\n        # For head layers, strip numeric suffixes (e.g., \"CentroidConfmapsHead_0\" -&gt; \"CentroidConfmapsHead\")\n        # This handles cases where Keras uses suffixes like _0, _1, etc.\n        if \"Head\" in layer_name:\n            # Remove trailing _N where N is a number\n            import re\n\n            layer_name_clean = re.sub(r\"_\\d+$\", \"\", layer_name)\n        else:\n            layer_name_clean = layer_name\n\n        # Find the PyTorch parameter that contains this layer name\n        # PyTorch names will be like: \"backbone.enc.encoder_stack.0.blocks.0.stack0_enc0_conv0.weight\"\n        matching_pytorch_name = None\n\n        for pytorch_name in pytorch_params.keys():\n            # Check if the PyTorch parameter name contains the layer name (or cleaned layer name for heads)\n            # and has the correct weight type\n            search_name = layer_name_clean if \"Head\" in layer_name else layer_name\n            if search_name in pytorch_name and pytorch_name.endswith(f\".{weight_type}\"):\n                # For kernel weights, we need to check shape after conversion\n                if weight_type == \"weight\":\n                    # Convert Keras kernel to PyTorch format for shape comparison\n                    if \"trans_conv\" in legacy_path:\n                        converted_weight = convert_keras_to_pytorch_conv2d_transpose(\n                            weight\n                        )\n                    else:\n                        converted_weight = convert_keras_to_pytorch_conv2d(weight)\n                    shape_to_check = converted_weight.shape\n                else:\n                    # Bias weights don't need conversion\n                    shape_to_check = weight.shape\n\n                # Verify shape compatibility\n                if shape_to_check == pytorch_params[pytorch_name]:\n                    matching_pytorch_name = pytorch_name\n                    break\n\n        if matching_pytorch_name:\n            mapping[legacy_path] = matching_pytorch_name\n        else:\n            logger.warning(f\"No matching PyTorch parameter found for {legacy_path}\")\n\n    # Log mapping results\n    if not mapping:\n        logger.info(\n            f\"No mappings could be created between legacy weights and PyTorch model. \"\n            f\"Legacy weights: {len(legacy_weights)}, PyTorch parameters: {len(pytorch_params)}\"\n        )\n    else:\n        logger.info(\n            f\"Successfully mapped {len(mapping)}/{len(legacy_weights)} legacy weights to PyTorch parameters\"\n        )\n\n    return mapping\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.parse_keras_layer_name","title":"<code>parse_keras_layer_name(layer_path)</code>","text":"<p>Parse a Keras layer path to extract basic information.</p> <p>Parameters:</p> Name Type Description Default <code>layer_path</code> <code>str</code> <p>Full path like \"model_weights/stack0_enc0_conv0/stack0_enc0_conv0/kernel:0\"</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parsed information: - layer_name: Base layer name (e.g., \"stack0_enc0_conv0\") - weight_type: \"kernel\" or \"bias\"</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def parse_keras_layer_name(layer_path: str) -&gt; Dict[str, Any]:\n    \"\"\"Parse a Keras layer path to extract basic information.\n\n    Args:\n        layer_path: Full path like \"model_weights/stack0_enc0_conv0/stack0_enc0_conv0/kernel:0\"\n\n    Returns:\n        Dictionary with parsed information:\n        - layer_name: Base layer name (e.g., \"stack0_enc0_conv0\")\n        - weight_type: \"kernel\" or \"bias\"\n    \"\"\"\n    # Remove model_weights prefix and split\n    clean_path = layer_path.replace(\"model_weights/\", \"\")\n    parts = clean_path.split(\"/\")\n\n    if len(parts) &lt; 2:\n        raise ValueError(f\"Invalid layer path: {layer_path}\")\n\n    layer_name = parts[0]\n    weight_name = parts[-1]  # e.g., \"kernel:0\" or \"bias:0\"\n\n    info = {\n        \"layer_name\": layer_name,\n        \"weight_type\": \"kernel\" if \"kernel\" in weight_name else \"bias\",\n    }\n\n    return info\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.update_backbone_in_channels","title":"<code>update_backbone_in_channels(backbone_config, keras_in_channels)</code>","text":"<p>Update the backbone configuration's in_channels if it's different from the Keras model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone_config</code> <p>The backbone configuration object</p> required <code>keras_in_channels</code> <code>int</code> <p>Number of input channels from the Keras model</p> required Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def update_backbone_in_channels(backbone_config, keras_in_channels: int):\n    \"\"\"Update the backbone configuration's in_channels if it's different from the Keras model.\n\n    Args:\n        backbone_config: The backbone configuration object\n        keras_in_channels: Number of input channels from the Keras model\n    \"\"\"\n    if backbone_config.in_channels != keras_in_channels:\n        logger.info(\n            f\"Updating backbone in_channels from {backbone_config.in_channels} to {keras_in_channels}\"\n        )\n        backbone_config.in_channels = keras_in_channels\n\n    return backbone_config\n</code></pre>"},{"location":"api/predict/","title":"predict","text":""},{"location":"api/predict/#sleap_nn.predict","title":"<code>sleap_nn.predict</code>","text":"<p>Entry point for running inference.</p> <p>Functions:</p> Name Description <code>frame_list</code> <p>Converts 'n-m' string to list of ints.</p> <code>main</code> <p>CLI command that handles argument conversion and calls run_inference.</p> <code>run_inference</code> <p>Entry point to run inference on trained SLEAP-NN models.</p>"},{"location":"api/predict/#sleap_nn.predict.frame_list","title":"<code>frame_list(frame_str)</code>","text":"<p>Converts 'n-m' string to list of ints.</p> <p>Parameters:</p> Name Type Description Default <code>frame_str</code> <code>str</code> <p>string representing range</p> required <p>Returns:</p> Type Description <code>Optional[List[int]]</code> <p>List of ints, or None if string does not represent valid range.</p> Source code in <code>sleap_nn/predict.py</code> <pre><code>def frame_list(frame_str: str) -&gt; Optional[List[int]]:\n    \"\"\"Converts 'n-m' string to list of ints.\n\n    Args:\n        frame_str: string representing range\n\n    Returns:\n        List of ints, or None if string does not represent valid range.\n    \"\"\"\n    # Handle ranges of frames. Must be of the form \"1-200\" (or \"1,-200\")\n    if \"-\" in frame_str:\n        min_max = frame_str.split(\"-\")\n        min_frame = int(min_max[0].rstrip(\",\"))\n        max_frame = int(min_max[1])\n        return list(range(min_frame, max_frame + 1))\n\n    return [int(x) for x in frame_str.split(\",\")] if len(frame_str) else None\n</code></pre>"},{"location":"api/predict/#sleap_nn.predict.main","title":"<code>main(**kwargs)</code>","text":"<p>CLI command that handles argument conversion and calls run_inference.</p> Source code in <code>sleap_nn/predict.py</code> <pre><code>@click.command()\n@click.option(\n    \"--data_path\",\n    type=str,\n    required=True,\n    help=\"Path to data to predict on. This can be a labels (.slp) file or any supported video format.\",\n)\n@click.option(\n    \"--model_paths\",\n    multiple=True,\n    help=\"Path to trained model directory (with training_config.json). Multiple models can be specified, each preceded by --model_paths.\",\n)\n@click.option(\n    \"--backbone_ckpt_path\",\n    type=str,\n    default=None,\n    help=\"To run inference on any `.ckpt` other than `best.ckpt` from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\",\n)\n@click.option(\n    \"--head_ckpt_path\",\n    type=str,\n    default=None,\n    help=\"Path to `.ckpt` file if a different set of head layer weights are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt from `backbone_ckpt_path` if provided.)\",\n)\n@click.option(\n    \"-n\",\n    \"--max_instances\",\n    type=int,\n    default=None,\n    help=\"Limit maximum number of instances in multi-instance models. Not available for ID models. Defaults to None.\",\n)\n@click.option(\n    \"--max_height\",\n    type=int,\n    default=None,\n    help=\"Maximum height the image should be padded to. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--max_width\",\n    type=int,\n    default=None,\n    help=\"Maximum width the image should be padded to. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--input_scale\",\n    type=float,\n    default=None,\n    help=\"Scale factor to apply to the input image. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--ensure_rgb\",\n    is_flag=True,\n    default=False,\n    help=\"True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to `True`, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the values from the training config are used. Default: `None`.\",\n)\n@click.option(\n    \"--ensure_grayscale\",\n    is_flag=True,\n    default=False,\n    help=\"True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the values from the training config are used. Default: `None`.\",\n)\n@click.option(\n    \"--anchor_part\",\n    type=str,\n    default=None,\n    help=\"The node name to use as the anchor for the centroid. If not provided, the anchor part in the `training_config.yaml` is used. Default: `None`.\",\n)\n@click.option(\n    \"--only_labeled_frames\",\n    is_flag=True,\n    default=False,\n    help=\"Only run inference on user labeled frames when running on labels dataset. This is useful for generating predictions to compare against ground truth.\",\n)\n@click.option(\n    \"--only_suggested_frames\",\n    is_flag=True,\n    default=False,\n    help=\"Only run inference on unlabeled suggested frames when running on labels dataset. This is useful for generating predictions for initialization during labeling.\",\n)\n@click.option(\n    \"--video_index\",\n    type=int,\n    default=None,\n    help=\"Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.\",\n)\n@click.option(\n    \"--video_dataset\", type=str, default=None, help=\"The dataset for HDF5 videos.\"\n)\n@click.option(\n    \"--video_input_format\",\n    type=str,\n    default=\"channels_last\",\n    help=\"The input_format for HDF5 videos.\",\n)\n@click.option(\n    \"--frames\",\n    type=str,\n    default=\"\",\n    help=\"List of frames to predict when running on a video. Can be specified as a comma separated list (e.g. 1,2,3) or a range separated by hyphen (e.g., 1-3, for 1,2,3). If not provided, defaults to predicting on the entire video.\",\n)\n@click.option(\n    \"--batch_size\",\n    type=int,\n    default=4,\n    help=\"Number of frames to predict at a time. Larger values result in faster inference speeds, but require more memory.\",\n)\n@click.option(\n    \"--integral_patch_size\",\n    type=int,\n    default=5,\n    help=\"Size of patches to crop around each rough peak as an integer scalar. Default: 5.\",\n)\n@click.option(\n    \"--return_confmaps\",\n    is_flag=True,\n    default=False,\n    help=\"If True, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.\",\n)\n@click.option(\n    \"--return_pafs\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the part affinity fields will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model. Default: False.\",\n)\n@click.option(\n    \"--return_paf_graph\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the part affinity field graph will be returned together with the predicted instances. The graph is obtained by parsing the part affinity fields with the paf_scorer instance and is an intermediate representation used during instance grouping. Default: False.\",\n)\n@click.option(\n    \"--max_edge_length_ratio\",\n    type=float,\n    default=0.25,\n    help=\"The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching. Default: 0.25.\",\n)\n@click.option(\n    \"--dist_penalty_weight\",\n    type=float,\n    default=1.0,\n    help=\"A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly. Default: 1.0.\",\n)\n@click.option(\n    \"--n_points\",\n    type=int,\n    default=10,\n    help=\"Number of points to sample along the line integral. Default: 10.\",\n)\n@click.option(\n    \"--min_instance_peaks\",\n    type=float,\n    default=0,\n    help=\"Minimum number of peaks the instance should have to be considered a real instance. Instances with fewer peaks than this will be discarded (useful for filtering spurious detections). Default: 0.\",\n)\n@click.option(\n    \"--min_line_scores\",\n    type=float,\n    default=0.25,\n    help=\"Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones. Default: 0.25.\",\n)\n@click.option(\n    \"--return_class_maps\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the class maps will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.\",\n)\n@click.option(\n    \"--return_class_vectors\",\n    is_flag=True,\n    default=False,\n    help=\"If True, the classification probabilities will be returned together with the predicted peaks. This will not line up with the grouped instances, for which the associated class probabilities will always be returned in instance_scores.\",\n)\n@click.option(\n    \"--make_labels\",\n    is_flag=True,\n    default=True,\n    help=\"If True (the default), returns a sio.Labels instance with sio.PredictedInstances. If False, just return a list of dictionaries containing the raw arrays returned by the inference model. Default: True.\",\n)\n@click.option(\n    \"--queue_maxsize\",\n    type=int,\n    default=8,\n    help=\"Maximum size of the frame buffer queue.\",\n)\n@click.option(\n    \"--crop_size\",\n    type=int,\n    default=None,\n    help=\"Crop size. If not provided, the crop size from training_config.yaml is used.\",\n)\n@click.option(\n    \"--peak_threshold\",\n    type=float,\n    default=0.2,\n    help=\"Minimum confidence map value to consider a peak as valid.\",\n)\n@click.option(\n    \"--integral_refinement\",\n    type=str,\n    default=\"integral\",\n    help=\"If `None`, returns the grid-aligned peaks with no refinement. If `'integral'`, peaks will be refined with integral regression. Default: 'integral'.\",\n)\n@click.option(\n    \"-o\",\n    \"--output_path\",\n    type=str,\n    default=None,\n    help=\"The output filename to use for the predicted data. If not provided, defaults to '[data_path].slp'.\",\n)\n@click.option(\n    \"--device\",\n    type=str,\n    default=\"auto\",\n    help=\"Device on which torch.Tensor will be allocated. One of the ('cpu', 'cuda', 'mps', 'auto', 'opencl', 'ideep', 'hip', 'msnpu'). Default: 'auto' (based on available backend either cuda, mps or cpu is chosen).\",\n)\n@click.option(\n    \"--tracking\",\n    is_flag=True,\n    default=False,\n    help=\"If True, runs tracking on the predicted instances.\",\n)\n@click.option(\n    \"--tracking_window_size\",\n    type=int,\n    default=5,\n    help=\"Number of frames to look for in the candidate instances to match with the current detections.\",\n)\n@click.option(\n    \"--min_new_track_points\",\n    type=int,\n    default=0,\n    help=\"We won't spawn a new track for an instance with fewer than this many points.\",\n)\n@click.option(\n    \"--candidates_method\",\n    type=str,\n    default=\"fixed_window\",\n    help=\"Either of `fixed_window` or `local_queues`. In fixed window method, candidates from the last `window_size` frames. In local queues, last `window_size` instances for each track ID is considered for matching against the current detection.\",\n)\n@click.option(\n    \"--min_match_points\",\n    type=int,\n    default=0,\n    help=\"Minimum non-NaN points for match candidates.\",\n)\n@click.option(\n    \"--features\",\n    type=str,\n    default=\"keypoints\",\n    help=\"Feature representation for the candidates to update current detections. One of [`keypoints`, `centroids`, `bboxes`, `image`].\",\n)\n@click.option(\n    \"--scoring_method\",\n    type=str,\n    default=\"oks\",\n    help=\"Method to compute association score between features from the current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`, `euclidean_dist`].\",\n)\n@click.option(\n    \"--scoring_reduction\",\n    type=str,\n    default=\"mean\",\n    help=\"Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [`mean`, `max`, `robust_quantile`].\",\n)\n@click.option(\n    \"--robust_best_instance\",\n    type=float,\n    default=1.0,\n    help=\"If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.\",\n)\n@click.option(\n    \"--track_matching_method\",\n    type=str,\n    default=\"hungarian\",\n    help=\"Track matching algorithm. One of `hungarian`, `greedy`.\",\n)\n@click.option(\n    \"--max_tracks\",\n    type=int,\n    default=None,\n    help=\"Maximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate)\",\n)\n@click.option(\n    \"--use_flow\",\n    is_flag=True,\n    default=False,\n    help=\"If True, `FlowShiftTracker` is used, where the poses are matched using optical flow shifts.\",\n)\n@click.option(\n    \"--of_img_scale\",\n    type=float,\n    default=1.0,\n    help=\"Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements.\",\n)\n@click.option(\n    \"--of_window_size\",\n    type=int,\n    default=21,\n    help=\"Optical flow window size to consider at each pyramid scale level.\",\n)\n@click.option(\n    \"--of_max_levels\",\n    type=int,\n    default=3,\n    help=\"Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling.\",\n)\n@click.option(\n    \"--post_connect_single_breaks\",\n    is_flag=True,\n    default=False,\n    help=\"If True and `max_tracks` is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.\",\n)\ndef main(**kwargs):\n    \"\"\"CLI command that handles argument conversion and calls run_inference.\"\"\"\n    # Convert model_paths from tuple to list\n    if \"model_paths\" in kwargs and kwargs[\"model_paths\"]:\n        kwargs[\"model_paths\"] = list(kwargs[\"model_paths\"])\n    else:\n        kwargs[\"model_paths\"] = None\n\n    # Convert frames string to list\n    if \"frames\" in kwargs and kwargs[\"frames\"]:\n        kwargs[\"frames\"] = frame_list(kwargs[\"frames\"])\n    else:\n        kwargs[\"frames\"] = None\n\n    # Call the original function\n    return run_inference(**kwargs)\n</code></pre>"},{"location":"api/predict/#sleap_nn.predict.run_inference","title":"<code>run_inference(data_path, model_paths=None, backbone_ckpt_path=None, head_ckpt_path=None, max_instances=None, max_width=None, max_height=None, ensure_rgb=None, input_scale=None, ensure_grayscale=None, anchor_part=None, only_labeled_frames=False, only_suggested_frames=False, batch_size=4, queue_maxsize=8, video_index=None, video_dataset=None, video_input_format='channels_last', frames=None, crop_size=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, return_confmaps=False, return_pafs=False, return_paf_graph=False, max_edge_length_ratio=0.25, dist_penalty_weight=1.0, n_points=10, min_instance_peaks=0, min_line_scores=0.25, return_class_maps=False, return_class_vectors=False, make_labels=True, output_path=None, device='auto', tracking=False, tracking_window_size=5, min_new_track_points=0, candidates_method='fixed_window', min_match_points=0, features='keypoints', scoring_method='oks', scoring_reduction='mean', robust_best_instance=1.0, track_matching_method='hungarian', max_tracks=None, use_flow=False, of_img_scale=1.0, of_window_size=21, of_max_levels=3, post_connect_single_breaks=False)</code>","text":"<p>Entry point to run inference on trained SLEAP-NN models.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> to run inference on.</p> required <code>model_paths</code> <code>Optional[List[str]]</code> <p>(List[str]) List of paths to the directory where the best.ckpt     and training_config.yaml are saved.</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code>     from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights     are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt     from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>(int) Maximum width the image should be padded to. If not provided, the     values from the training config are used. Default: None.</p> <code>None</code> <code>max_height</code> <code>Optional[int]</code> <p>(int) Maximum height the image should be padded to. If not provided, the     values from the training config are used. Default: None.</p> <code>None</code> <code>input_scale</code> <code>Optional[float]</code> <p>(float) Scale factor to apply to the input image. If not provided, the     values from the training config are used. Default: None.</p> <code>None</code> <code>ensure_rgb</code> <code>Optional[bool]</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one     channel when this is set to <code>True</code>, then the images from single-channel     is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the     values from the training config are used. Default: <code>None</code>.</p> <code>None</code> <code>ensure_grayscale</code> <code>Optional[bool]</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this     is set to True, then we convert the image to grayscale (single-channel)     image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the     values from the training config are used. Default: <code>None</code>.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The node name to use as the anchor for the centroid. If not     provided, the anchor part in the <code>training_config.yaml</code> is used. Default: <code>None</code>.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 8.</p> <code>8</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with     an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <code>frames</code> <code>Optional[list]</code> <p>(list) List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>crop_size</code> <code>Optional[int]</code> <p>(int) Crop size. If not provided, the crop size from training_config.yaml is used.     Default: None.</p> <code>None</code> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below     this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown     centroid and centered-instance model, where the first element corresponds     to centroid model peak finding threshold and the second element is for     centered-instance model peak finding.</p> <code>0.2</code> <code>integral_refinement</code> <code>Optional[str]</code> <p>(str) If <code>None</code>, returns the grid-aligned peaks with no refinement.     If <code>\"integral\"</code>, peaks will be refined with integral regression.     Default: <code>\"integral\"</code>.</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an     integer scalar. Default: 5.</p> <code>5</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned     along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>return_pafs</code> <code>bool</code> <p>(bool) If <code>True</code>, the part affinity fields will be returned together with     the predicted instances. This will result in slower inference times since     the data must be copied off of the GPU, but is useful for visualizing the     raw output of the model. Default: False.</p> <code>False</code> <code>return_class_vectors</code> <code>bool</code> <p>If <code>True</code>, the classification probabilities will be     returned together with the predicted peaks. This will not line up with the     grouped instances, for which the associtated class probabilities will always     be returned in <code>\"instance_scores\"</code>.</p> <code>False</code> <code>return_paf_graph</code> <code>bool</code> <p>(bool) If <code>True</code>, the part affinity field graph will be returned     together with the predicted instances. The graph is obtained by parsing the     part affinity fields with the <code>paf_scorer</code> instance and is an intermediate     representation used during instance grouping. Default: False.</p> <code>False</code> <code>max_edge_length_ratio</code> <code>float</code> <p>(float) The maximum expected length of a connected pair of points     as a fraction of the image size. Candidate connections longer than this     length will be penalized during matching. Default: 0.25.</p> <code>0.25</code> <code>dist_penalty_weight</code> <code>float</code> <p>(float) A coefficient to scale weight of the distance penalty as     a scalar float. Set to values greater than 1.0 to enforce the distance     penalty more strictly.Default: 1.0.</p> <code>1.0</code> <code>n_points</code> <code>int</code> <p>(int) Number of points to sample along the line integral. Default: 10.</p> <code>10</code> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Union[int, float] Minimum number of peaks the instance should     have to be considered a real instance. Instances with fewer peaks than     this will be discarded (useful for filtering spurious detections).     Default: 0.</p> <code>0</code> <code>min_line_scores</code> <code>float</code> <p>(float) Minimum line score (between -1 and 1) required to form a match     between candidate point pairs. Useful for rejecting spurious detections when     there are no better ones. Default: 0.25.</p> <code>0.25</code> <code>return_class_maps</code> <code>bool</code> <p>If <code>True</code>, the class maps will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>False</code> <code>make_labels</code> <code>bool</code> <p>(bool) If <code>True</code> (the default), returns a <code>sio.Labels</code> instance with     <code>sio.PredictedInstance</code>s. If <code>False</code>, just return a list of     dictionaries containing the raw arrays returned by the inference model.     Default: True.</p> <code>True</code> <code>output_path</code> <code>Optional[str]</code> <p>(str) Path to save the labels file if <code>make_labels</code> is True.     Default is current working directory.</p> <code>None</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the     ('cpu', 'cuda', 'mps', 'auto', 'opencl', 'ideep', 'hip', 'msnpu').     Default: \"auto\" (based on available backend either cuda, mps or cpu is chosen).</p> <code>'auto'</code> <code>tracking</code> <code>bool</code> <p>(bool) If True, runs tracking on the predicted instances.</p> <code>False</code> <code>tracking_window_size</code> <code>int</code> <p>Number of frames to look for in the candidate instances to match     with the current detections. Default: 5.</p> <code>5</code> <code>min_new_track_points</code> <code>int</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>0</code> <code>candidates_method</code> <code>str</code> <p>Either of <code>fixed_window</code> or <code>local_queues</code>. In fixed window method, candidates from the last <code>window_size</code> frames. In local queues, last <code>window_size</code> instances for each track ID is considered for matching against the current detection. Default: <code>fixed_window</code>.</p> <code>'fixed_window'</code> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>0</code> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>'keypoints'</code> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>'oks'</code> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>'mean'</code> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>1.0</code> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>'hungarian'</code> <code>max_tracks</code> <code>Optional[int]</code> <p>Meaximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate) Default: None.</p> <code>None</code> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using</p> <code>False</code> <code>optical</code> <code>flow shifts. Default</code> <p><code>False</code>.</p> required <code>of_img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0. (only if <code>use_flow</code> is True)</p> <code>1.0</code> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21. (only if <code>use_flow</code> is True)</p> <code>21</code> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling. Default: 3. (only if <code>use_flow</code> is True).</p> <code>3</code> <code>post_connect_single_breaks</code> <code>bool</code> <p>If True and <code>max_tracks</code> is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns <code>sio.Labels</code> object if <code>make_labels</code> is True. Else this function returns     a list of Dictionaries with the predictions.</p> Source code in <code>sleap_nn/predict.py</code> <pre><code>def run_inference(\n    data_path: str,\n    model_paths: Optional[List[str]] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    max_instances: Optional[int] = None,\n    max_width: Optional[int] = None,\n    max_height: Optional[int] = None,\n    ensure_rgb: Optional[bool] = None,\n    input_scale: Optional[float] = None,\n    ensure_grayscale: Optional[bool] = None,\n    anchor_part: Optional[str] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    batch_size: int = 4,\n    queue_maxsize: int = 8,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n    frames: Optional[list] = None,\n    crop_size: Optional[int] = None,\n    peak_threshold: Union[float, List[float]] = 0.2,\n    integral_refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: bool = False,\n    return_pafs: bool = False,\n    return_paf_graph: bool = False,\n    max_edge_length_ratio: float = 0.25,\n    dist_penalty_weight: float = 1.0,\n    n_points: int = 10,\n    min_instance_peaks: Union[int, float] = 0,\n    min_line_scores: float = 0.25,\n    return_class_maps: bool = False,\n    return_class_vectors: bool = False,\n    make_labels: bool = True,\n    output_path: Optional[str] = None,\n    device: str = \"auto\",\n    tracking: bool = False,\n    tracking_window_size: int = 5,\n    min_new_track_points: int = 0,\n    candidates_method: str = \"fixed_window\",\n    min_match_points: int = 0,\n    features: str = \"keypoints\",\n    scoring_method: str = \"oks\",\n    scoring_reduction: str = \"mean\",\n    robust_best_instance: float = 1.0,\n    track_matching_method: str = \"hungarian\",\n    max_tracks: Optional[int] = None,\n    use_flow: bool = False,\n    of_img_scale: float = 1.0,\n    of_window_size: int = 21,\n    of_max_levels: int = 3,\n    post_connect_single_breaks: bool = False,\n):\n    \"\"\"Entry point to run inference on trained SLEAP-NN models.\n\n    Args:\n        data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n        model_paths: (List[str]) List of paths to the directory where the best.ckpt\n                and training_config.yaml are saved.\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n        max_instances: (int) Max number of instances to consider from the predictions.\n        max_width: (int) Maximum width the image should be padded to. If not provided, the\n                values from the training config are used. Default: None.\n        max_height: (int) Maximum height the image should be padded to. If not provided, the\n                values from the training config are used. Default: None.\n        input_scale: (float) Scale factor to apply to the input image. If not provided, the\n                values from the training config are used. Default: None.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n                channel when this is set to `True`, then the images from single-channel\n                is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the\n                values from the training config are used. Default: `None`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n                is set to True, then we convert the image to grayscale (single-channel)\n                image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the\n                values from the training config are used. Default: `None`.\n        anchor_part: (str) The node name to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used. Default: `None`.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used with\n                an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n        frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n        crop_size: (int) Crop size. If not provided, the crop size from training_config.yaml is used.\n                Default: None.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n        integral_refinement: (str) If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: `\"integral\"`.\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n        return_pafs: (bool) If `True`, the part affinity fields will be returned together with\n                the predicted instances. This will result in slower inference times since\n                the data must be copied off of the GPU, but is useful for visualizing the\n                raw output of the model. Default: False.\n        return_class_vectors: If `True`, the classification probabilities will be\n                returned together with the predicted peaks. This will not line up with the\n                grouped instances, for which the associtated class probabilities will always\n                be returned in `\"instance_scores\"`.\n        return_paf_graph: (bool) If `True`, the part affinity field graph will be returned\n                together with the predicted instances. The graph is obtained by parsing the\n                part affinity fields with the `paf_scorer` instance and is an intermediate\n                representation used during instance grouping. Default: False.\n        max_edge_length_ratio: (float) The maximum expected length of a connected pair of points\n                as a fraction of the image size. Candidate connections longer than this\n                length will be penalized during matching. Default: 0.25.\n        dist_penalty_weight: (float) A coefficient to scale weight of the distance penalty as\n                a scalar float. Set to values greater than 1.0 to enforce the distance\n                penalty more strictly.Default: 1.0.\n        n_points: (int) Number of points to sample along the line integral. Default: 10.\n        min_instance_peaks: Union[int, float] Minimum number of peaks the instance should\n                have to be considered a real instance. Instances with fewer peaks than\n                this will be discarded (useful for filtering spurious detections).\n                Default: 0.\n        min_line_scores: (float) Minimum line score (between -1 and 1) required to form a match\n                between candidate point pairs. Useful for rejecting spurious detections when\n                there are no better ones. Default: 0.25.\n        return_class_maps: If `True`, the class maps will be returned together with\n            the predicted instances. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        make_labels: (bool) If `True` (the default), returns a `sio.Labels` instance with\n                `sio.PredictedInstance`s. If `False`, just return a list of\n                dictionaries containing the raw arrays returned by the inference model.\n                Default: True.\n        output_path: (str) Path to save the labels file if `make_labels` is True.\n                Default is current working directory.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n                ('cpu', 'cuda', 'mps', 'auto', 'opencl', 'ideep', 'hip', 'msnpu').\n                Default: \"auto\" (based on available backend either cuda, mps or cpu is chosen).\n        tracking: (bool) If True, runs tracking on the predicted instances.\n        tracking_window_size: Number of frames to look for in the candidate instances to match\n                with the current detections. Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n            method, candidates from the last `window_size` frames. In local queues,\n            last `window_size` instances for each track ID is considered for matching\n            against the current detection. Default: `fixed_window`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n            (only for local queues candidate) Default: None.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n        optical flow shifts. Default: `False`.\n        of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0. (only if `use_flow` is True)\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21. (only if `use_flow` is True)\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n            Default: 3. (only if `use_flow` is True).\n        post_connect_single_breaks: If True and `max_tracks` is not None with local queues candidate method,\n            connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.\n\n    Returns:\n        Returns `sio.Labels` object if `make_labels` is True. Else this function returns\n            a list of Dictionaries with the predictions.\n\n    \"\"\"\n    preprocess_config = {  # if not given, then use from training config\n        \"ensure_rgb\": ensure_rgb,\n        \"ensure_grayscale\": ensure_grayscale,\n        \"crop_hw\": (crop_size, crop_size) if crop_size is not None else None,\n        \"max_width\": max_width,\n        \"max_height\": max_height,\n        \"scale\": input_scale,\n    }\n\n    if model_paths is None or not len(\n        model_paths\n    ):  # if model paths is not provided, run tracking-only pipeline.\n        if not tracking:\n            message = \"\"\"Neither tracker nor path to trained models specified. Use `model_paths` to specify models to use. To retrack on predictions, set `tracking` to True.\"\"\"\n            logger.error(message)\n            raise ValueError(message)\n\n        else:\n            start_inf_time = time()\n            start_timestamp = str(datetime.now())\n            logger.info(f\"Started tracking at: {start_timestamp}\")\n\n            labels = sio.load_slp(data_path)\n            frames = sorted(labels.labeled_frames, key=lambda lf: lf.frame_idx)\n\n            if post_connect_single_breaks:\n                if max_tracks is None:\n                    max_tracks = max_instances\n\n            tracked_frames = run_tracker(\n                untracked_frames=frames,\n                window_size=tracking_window_size,\n                min_new_track_points=min_new_track_points,\n                candidates_method=candidates_method,\n                min_match_points=min_match_points,\n                features=features,\n                scoring_method=scoring_method,\n                scoring_reduction=scoring_reduction,\n                robust_best_instance=robust_best_instance,\n                track_matching_method=track_matching_method,\n                max_tracks=max_tracks,\n                use_flow=use_flow,\n                of_img_scale=of_img_scale,\n                of_window_size=of_window_size,\n                of_max_levels=of_max_levels,\n                post_connect_single_breaks=post_connect_single_breaks,\n            )\n\n            finish_timestamp = str(datetime.now())\n            total_elapsed = time() - start_inf_time\n            logger.info(f\"Finished tracking at: {finish_timestamp}\")\n            logger.info(f\"Total runtime: {total_elapsed} secs\")\n\n            output = sio.Labels(\n                labeled_frames=tracked_frames,\n                videos=labels.videos,\n                skeletons=labels.skeletons,\n            )\n\n    else:\n        start_inf_time = time()\n        start_timestamp = str(datetime.now())\n        logger.info(f\"Started inference at: {start_timestamp}\")\n\n        if device == \"auto\":\n            device = (\n                \"cuda\"\n                if torch.cuda.is_available()\n                else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n            )\n\n        if integral_refinement is not None and device == \"mps\":  # TODO\n            # kornia/geometry/transform/imgwarp.py:382: in get_perspective_transform. NotImplementedError: The operator 'aten::_linalg_solve_ex.result' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n            logger.info(\n                \"Integral refinement is not supported with MPS device. Using CPU.\"\n            )\n            device = \"cpu\"  # not supported with mps\n\n        logger.info(f\"Using device: {device}\")\n\n        # initializes the inference model\n        predictor = Predictor.from_model_paths(\n            model_paths,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=OmegaConf.create(preprocess_config),\n            anchor_part=anchor_part,\n        )\n\n        if (\n            tracking\n            and not isinstance(predictor, BottomUpMultiClassPredictor)\n            and not isinstance(predictor, TopDownMultiClassPredictor)\n        ):\n            predictor.tracker = Tracker.from_config(\n                candidates_method=candidates_method,\n                min_match_points=min_match_points,\n                window_size=tracking_window_size,\n                min_new_track_points=min_new_track_points,\n                features=features,\n                scoring_method=scoring_method,\n                scoring_reduction=scoring_reduction,\n                robust_best_instance=robust_best_instance,\n                track_matching_method=track_matching_method,\n                max_tracks=max_tracks,\n                use_flow=use_flow,\n                of_img_scale=of_img_scale,\n                of_window_size=of_window_size,\n                of_max_levels=of_max_levels,\n            )\n\n        if isinstance(predictor, BottomUpPredictor):\n            predictor.inference_model.paf_scorer.max_edge_length_ratio = (\n                max_edge_length_ratio\n            )\n            predictor.inference_model.paf_scorer.dist_penalty_weight = (\n                dist_penalty_weight\n            )\n            predictor.inference_model.return_pafs = return_pafs\n            predictor.inference_model.return_paf_graph = return_paf_graph\n            predictor.inference_model.paf_scorer.max_edge_length_ratio = (\n                max_edge_length_ratio\n            )\n            predictor.inference_model.paf_scorer.min_line_scores = min_line_scores\n            predictor.inference_model.paf_scorer.min_instance_peaks = min_instance_peaks\n            predictor.inference_model.paf_scorer.n_points = n_points\n\n        if isinstance(predictor, BottomUpMultiClassPredictor):\n            predictor.inference_model.return_class_maps = return_class_maps\n\n        if isinstance(predictor, TopDownMultiClassPredictor):\n            predictor.inference_model.instance_peaks.return_class_vectors = (\n                return_class_vectors\n            )\n\n        # initialize make_pipeline function\n\n        predictor.make_pipeline(\n            data_path,\n            queue_maxsize,\n            frames,\n            only_labeled_frames,\n            only_suggested_frames,\n            video_index=video_index,\n            video_dataset=video_dataset,\n            video_input_format=video_input_format,\n        )\n\n        # run predict\n        output = predictor.predict(\n            make_labels=make_labels,\n        )\n\n        if tracking and post_connect_single_breaks:\n            if max_tracks is None:\n                max_tracks = max_instances\n\n            if max_tracks is None:\n                message = \"Max_tracks (and max instances) is None. To connect single breaks, max_tracks should be set to an integer.\"\n                logger.error(message)\n                raise ValueError(message)\n\n            start_final_pass_time = time()\n            start_fp_timestamp = str(datetime.now())\n            logger.info(\n                f\"Started final-pass (connecting single breaks) at: {start_fp_timestamp}\"\n            )\n            corrected_lfs = connect_single_breaks(\n                lfs=[x for x in output], max_instances=max_tracks\n            )\n            finish_fp_timestamp = str(datetime.now())\n            total_fp_elapsed = time() - start_final_pass_time\n            logger.info(\n                f\"Finished final-pass (connecting single breaks) at: {finish_fp_timestamp}\"\n            )\n            logger.info(f\"Total runtime: {total_fp_elapsed} secs\")\n\n            output = sio.Labels(\n                labeled_frames=corrected_lfs,\n                videos=output.videos,\n                skeletons=output.skeletons,\n            )\n\n        finish_timestamp = str(datetime.now())\n        total_elapsed = time() - start_inf_time\n        logger.info(f\"Finished inference at: {finish_timestamp}\")\n        logger.info(\n            f\"Total runtime: {total_elapsed} secs\"\n        )  # TODO: add number of predicted frames\n\n    if make_labels:\n        if output_path is None:\n            output_path = Path(data_path).with_suffix(\".predictions.slp\")\n        output.save(Path(output_path).as_posix(), restore_original_videos=False)\n    finish_timestamp = str(datetime.now())\n    logger.info(f\"Predictions output path: {output_path}\")\n    logger.info(f\"Saved file at: {finish_timestamp}\")\n\n    return output\n</code></pre>"},{"location":"api/train/","title":"train","text":""},{"location":"api/train/#sleap_nn.train","title":"<code>sleap_nn.train</code>","text":"<p>Entry point for sleap_nn training.</p> <p>Functions:</p> Name Description <code>add_help</code> <p>Decorator to inject and override hydra/help config. Provide readable help messages for <code>sleap-nn-train</code> CLI.</p> <code>main</code> <p>Train SLEAP-NN model using CLI.</p> <code>run_training</code> <p>Create ModelTrainer instance and start training.</p> <code>train</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p>"},{"location":"api/train/#sleap_nn.train.add_help","title":"<code>add_help(name='sleap_help')</code>","text":"<p>Decorator to inject and override hydra/help config. Provide readable help messages for <code>sleap-nn-train</code> CLI.</p> Source code in <code>sleap_nn/train.py</code> <pre><code>def add_help(name: str = \"sleap_help\"):\n    \"\"\"Decorator to inject and override hydra/help config. Provide readable help messages for `sleap-nn-train` CLI.\"\"\"\n    template = (\n        \"${hydra.help.header}\\n\\n\"\n        \"Usage:\\n\"\n        \"  ${hydra.help.app_name} --config-dir &lt;dir&gt; --config-name &lt;name&gt; [overrides]\\n\\n\"\n        \"Common overrides:\\n\"\n        \"  trainer_config.max_epochs=100\\n\"\n        \"  trainer_config.batch_size=32\\n\\n\"\n        \"Examples:\\n\"\n        \"  Start new run:\\n\"\n        \"    ${hydra.help.app_name} --config-dir . --config-name myrun\\n\"\n        \"  Resume 20 more epochs:\\n\"\n        \"    ${hydra.help.app_name} --config-dir . --config-name myrun \\\\\\n\"\n        \"      trainer_config.resume_ckpt_path=&lt;path/to/ckpt&gt; \\\\\\n\"\n        \"      trainer_config.max_epochs=20\\n\\n\"\n        \"Tips:\\n\"\n        \"  - Use -m/--multirun for sweeps; outputs go under hydra.sweep.dir.\\n\"\n        \"  - For Hydra flags and completion, use --hydra-help.\\n\\n\"\n        \"${hydra.help.footer}\\n\"\n    )\n\n    cs = ConfigStore.instance()\n    cs.store(\n        group=\"hydra/help\",\n        name=name,\n        node=HelpConf(  # \u2190 correct type\n            app_name=\"sleap-nn-train\",\n            header=\"sleap-nn-train \u2014 Train SLEAP models from a config YAML file.\",\n            footer=\"For a detailed list of all available config options, please refer to https://nn.sleap.ai/dev/config/.\",\n            template=template,\n        ),\n    )\n\n    def outer(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            joined = \" \".join(sys.argv[1:])\n            if \"hydra/help=\" not in joined and \"hydra.help.\" not in joined:\n                sys.argv.insert(1, f\"hydra/help={name}\")\n            return fn(*args, **kwargs)\n\n        return wrapper\n\n    return outer\n</code></pre>"},{"location":"api/train/#sleap_nn.train.main","title":"<code>main(cfg)</code>","text":"<p>Train SLEAP-NN model using CLI.</p> Source code in <code>sleap_nn/train.py</code> <pre><code>@add_help()\n@hydra.main(version_base=None, config_path=None, config_name=None)\ndef main(cfg: DictConfig):\n    \"\"\"Train SLEAP-NN model using CLI.\"\"\"\n    # Inject help message (override hydra/help here)\n\n    if not hasattr(cfg, \"model_config\") or not cfg.model_config:\n        # set save_ckpt_path to current working dir if not specified\n        print(\n            \"No model config found! Use `sleap-nn-train --help` for more information.\"\n        )\n        raise SystemExit(2)\n    logger.info(\"Input config:\")\n    logger.info(\"\\n\" + OmegaConf.to_yaml(cfg))\n    run_training(cfg)\n</code></pre>"},{"location":"api/train/#sleap_nn.train.run_training","title":"<code>run_training(config)</code>","text":"<p>Create ModelTrainer instance and start training.</p> Source code in <code>sleap_nn/train.py</code> <pre><code>def run_training(config: DictConfig):\n    \"\"\"Create ModelTrainer instance and start training.\"\"\"\n    start_train_time = time()\n    start_timestamp = str(datetime.now())\n    logger.info(f\"Started training at: {start_timestamp}\")\n\n    trainer = ModelTrainer.get_model_trainer_from_config(config)\n    trainer.train()\n\n    finish_timestamp = str(datetime.now())\n    total_elapsed = time() - start_train_time\n    logger.info(f\"Finished training at: {finish_timestamp}\")\n    logger.info(f\"Total training time: {total_elapsed} secs\")\n\n    rank = trainer.trainer.global_rank if trainer.trainer is not None else -1\n\n    logger.info(f\"Training Config: {OmegaConf.to_yaml(trainer.config)}\")\n\n    if rank in [0, -1]:\n        # run inference on val dataset\n        if trainer.config.trainer_config.save_ckpt:\n            data_paths = {}\n            for index, path in enumerate(trainer.config.data_config.train_labels_path):\n                logger.info(\n                    f\"Training labels path for index {index}: {trainer.config.trainer_config.save_ckpt_path}\"\n                )\n                data_paths[f\"train_{index}\"] = (\n                    Path(trainer.config.trainer_config.save_ckpt_path)\n                    / f\"labels_train_gt_{index}.slp\"\n                ).as_posix()\n                data_paths[f\"val_{index}\"] = (\n                    Path(trainer.config.trainer_config.save_ckpt_path)\n                    / f\"labels_val_gt_{index}.slp\"\n                ).as_posix()\n\n            if (\n                OmegaConf.select(config, \"data_config.test_file_path\", default=None)\n                is not None\n            ):\n                data_paths[\"test\"] = config.data_config.test_file_path\n\n            for d_name, path in data_paths.items():\n                labels = sio.load_slp(path)\n\n                pred_labels = predict(\n                    data_path=path,\n                    model_paths=[trainer.config.trainer_config.save_ckpt_path],\n                    peak_threshold=0.2,\n                    make_labels=True,\n                    device=trainer.trainer.strategy.root_device,\n                    output_path=Path(trainer.config.trainer_config.save_ckpt_path)\n                    / f\"pred_{d_name}.slp\",\n                    ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n                    ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n                )\n\n                if not len(pred_labels):\n                    logger.info(\n                        f\"Skipping eval on `{d_name}` dataset as there are no labeled frames...\"\n                    )\n                    continue  # skip if there are no labeled frames\n\n                evaluator = Evaluator(\n                    ground_truth_instances=labels, predicted_instances=pred_labels\n                )\n                metrics = evaluator.evaluate()\n                np.savez(\n                    (\n                        Path(trainer.config.trainer_config.save_ckpt_path)\n                        / f\"{d_name}_pred_metrics.npz\"\n                    ).as_posix(),\n                    **metrics,\n                )\n\n                logger.info(f\"---------Evaluation on `{d_name}` dataset---------\")\n                logger.info(f\"OKS mAP: {metrics['voc_metrics']['oks_voc.mAP']}\")\n                logger.info(f\"Average distance: {metrics['distance_metrics']['avg']}\")\n                logger.info(f\"p90 dist: {metrics['distance_metrics']['p90']}\")\n                logger.info(f\"p50 dist: {metrics['distance_metrics']['p50']}\")\n</code></pre>"},{"location":"api/train/#sleap_nn.train.train","title":"<code>train(train_labels_path=None, val_labels_path=None, validation_fraction=0.1, test_file_path=None, provider='LabelsReader', user_instances_only=True, data_pipeline_fw='torch_dataset', cache_img_path=None, use_existing_imgs=False, delete_cache_imgs_after_training=True, ensure_rgb=False, ensure_grayscale=False, scale=1.0, max_height=None, max_width=None, crop_hw=None, min_crop_size=100, use_augmentations_train=False, intensity_aug=None, geometry_aug=None, init_weight='default', pretrained_backbone_weights=None, pretrained_head_weights=None, backbone_config='unet', head_configs=None, batch_size=1, shuffle_train=False, num_workers=0, ckpt_save_top_k=1, ckpt_save_last=None, trainer_num_devices='auto', trainer_accelerator='auto', enable_progress_bar=True, min_train_steps_per_epoch=200, train_steps_per_epoch=None, visualize_preds_during_training=False, keep_viz=False, max_epochs=10, seed=0, use_wandb=False, save_ckpt=False, save_ckpt_path=None, resume_ckpt_path=None, wandb_entity=None, wandb_project=None, wandb_name=None, wandb_api_key=None, wandb_mode=None, wandb_resume_prv_runid=None, wandb_group_name=None, optimizer='Adam', learning_rate=0.001, amsgrad=False, lr_scheduler=None, early_stopping=False, early_stopping_min_delta=0.0, early_stopping_patience=1, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, zmq_publish_address=None, zmq_controller_address=None, zmq_controller_timeout=10)</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to training data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>val_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to validation data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>validation_fraction</code> <code>float</code> <p>Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the <code>validation_labels</code> are already specified, this has no effect. Default: 0.1.</p> <code>0.1</code> <code>test_file_path</code> <code>Optional[str]</code> <p>Path to test dataset (<code>.slp</code> file or <code>.mp4</code> file). Note: This is used to get evaluation on test set after training is completed.</p> <code>None</code> <code>provider</code> <code>str</code> <p>Provider class to read the input sleap files. Only \"LabelsReader\" supported for the training pipeline. Default: \"LabelsReader\".</p> <code>'LabelsReader'</code> <code>user_instances_only</code> <code>bool</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used. Default: <code>True</code>.</p> <code>True</code> <code>data_pipeline_fw</code> <code>str</code> <p>Framework to create the data loaders. One of [<code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code>]. Default: \"torch_dataset\".</p> <code>'torch_dataset'</code> <code>cache_img_path</code> <code>Optional[str]</code> <p>Path to save <code>.jpg</code> images created with <code>torch_dataset_cache_img_disk</code> data pipeline framework. If <code>None</code>, the path provided in <code>trainer_config.save_ckpt</code> is used (else working dir is used). The <code>train_imgs</code> and <code>val_imgs</code> dirs are created inside this path. Default: None.</p> <code>None</code> <code>use_existing_imgs</code> <code>bool</code> <p>Use existing train and val images/ chunks in the <code>cache_img_path</code> for <code>torch_dataset_cache_img_disk</code> frameworks. If <code>True</code>, the <code>cache_img_path</code> should have <code>train_imgs</code> and <code>val_imgs</code> dirs. Default: False.</p> <code>False</code> <code>delete_cache_imgs_after_training</code> <code>bool</code> <p>If <code>False</code>, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. Default: True.</p> <code>True</code> <code>ensure_rgb</code> <code>bool</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to <code>True</code>, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: <code>False</code>.</p> <code>False</code> <code>ensure_grayscale</code> <code>bool</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: <code>False</code>.</p> <code>False</code> <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>1.0</code> <code>max_height</code> <code>Optional[int]</code> <p>Maximum height the image should be padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>Maximum width the image should be padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>crop_hw</code> <code>Optional[Tuple[int, int]]</code> <p>Crop height and width of each instance (h, w) for centered-instance model. If <code>None</code>, this would be automatically computed based on the largest instance in the <code>sio.Labels</code> file. Default: None.</p> <code>None</code> <code>min_crop_size</code> <code>Optional[int]</code> <p>Minimum crop size to be used if <code>crop_hw</code> is <code>None</code>. Default: 100.</p> <code>100</code> <code>use_augmentations_train</code> <code>bool</code> <p>True if the data augmentation should be applied to the training data, else False. Default: False.</p> <code>False</code> <code>intensity_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"] or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.IntensityConfig</code>. For eg: {             \"uniform_noise_min\": 1.0,             \"uniform_noise_p\": 1.0         }</p> <code>None</code> <code>geometry_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"]. or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.GeometryConfig</code>. For eg: {             \"rotation\": 45,             \"affine_p\": 1.0         }</p> <code>None</code> <code>init_weight</code> <code>str</code> <p>model weights initialization method. \"default\" uses kaiming uniform initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".</p> <code>'default'</code> <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the backbone is initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the head layers are initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>backbone_config</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set, then pass a dictionary with the structure: {     \"unet((or) convnext (or)swint)\":         {(params in the corresponding architecture given in <code>sleap_nn.config.model_config.backbone_config</code>)         } }. For eg: {             \"unet\":                 {                     \"in_channels\": 3,                     \"filters\": 64,                     \"max_stride\": 32,                     \"output_stride\": 2                 }         }</p> <code>'unet'</code> <code>head_configs</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"]. The default <code>sigma</code> and <code>output_strides</code> are used if a string is passed. To set custom parameters, pass in a dictionary with the structure: {     \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):         {             \"confmaps\":                 {                     # params in the corresponding head type given in <code>sleap_nn.config.model_config.head_configs</code>                 },             \"pafs\":                 {                     # only for bottomup                 }         } }. For eg: {             \"single_instance\":                 {                     \"confmaps\":                         {                             \"part_names\": None,                             \"sigma\": 2.5,                             \"output_stride\": 2                         }                 }         }</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of samples per batch or batch size for training data. Default: 1.</p> <code>1</code> <code>shuffle_train</code> <code>bool</code> <p>True to have the train data reshuffled at every epoch. Default: False.</p> <code>False</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: 0.</p> <code>0</code> <code>ckpt_save_top_k</code> <code>int</code> <p>If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. Default: 1.</p> <code>1</code> <code>ckpt_save_last</code> <code>Optional[bool]</code> <p>When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. Default: None.</p> <code>None</code> <code>trainer_num_devices</code> <code>Union[str, int]</code> <p>Number of devices to train on (int), which devices to train on (list or str), or \"auto\" to select automatically. Default: \"auto\".</p> <code>'auto'</code> <code>trainer_accelerator</code> <code>str</code> <p>One of the (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the <code>Trainer</code> to be connected to. Default: \"auto\".</p> <code>'auto'</code> <code>enable_progress_bar</code> <code>bool</code> <p>When True, enables printing the logs during training. Default: True.</p> <code>True</code> <code>min_train_steps_per_epoch</code> <code>int</code> <p>Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer <code>limit_train_batches</code> parameter of Torch <code>Trainer</code>. Default: 200.</p> <code>200</code> <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data or <code>min_train_steps_per_epoch</code>, whichever is largest. Default: <code>None</code>. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>None</code> <code>visualize_preds_during_training</code> <code>bool</code> <p>If set to <code>True</code>, sample predictions (keypoints  + confidence maps) are saved to <code>viz</code> folder in the ckpt dir and in wandb table.</p> <code>False</code> <code>keep_viz</code> <code>bool</code> <p>If set to <code>True</code>, the <code>viz</code> folder containing training visualizations will be kept after training completes. If <code>False</code>, the folder will be deleted. This parameter only has an effect when <code>visualize_preds_during_training</code> is <code>True</code>. Default: <code>False</code>.</p> <code>False</code> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs to run. Default: 10.</p> <code>10</code> <code>seed</code> <code>int</code> <p>Seed value for the current experiment. default: 0.</p> <code>0</code> <code>save_ckpt</code> <code>bool</code> <p>True to enable checkpointing. Default: False.</p> <code>False</code> <code>save_ckpt_path</code> <code>Optional[str]</code> <p>Directory path to save the training config and checkpoint files. If <code>None</code> and <code>save_ckpt</code> is <code>True</code>, then the current working dir is used as the ckpt path. Default: None</p> <code>None</code> <code>resume_ckpt_path</code> <code>Optional[str]</code> <p>Path to <code>.ckpt</code> file from which training is resumed. Default: None.</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>True to enable wandb logging. Default: False.</p> <code>False</code> <code>wandb_entity</code> <code>Optional[str]</code> <p>Entity of wandb project. Default: None. (The default entity in the user profile settings is used)</p> <code>None</code> <code>wandb_project</code> <code>Optional[str]</code> <p>Project name for the current wandb run. Default: None.</p> <code>None</code> <code>wandb_name</code> <code>Optional[str]</code> <p>Name of the current wandb run. Default: None.</p> <code>None</code> <code>wandb_api_key</code> <code>Optional[str]</code> <p>API key. The API key is masked when saved to config files. Default: None.</p> <code>None</code> <code>wandb_mode</code> <code>Optional[str]</code> <p>\"offline\" if only local logging is required. Default: None.</p> <code>None</code> <code>wandb_resume_prv_runid</code> <code>Optional[str]</code> <p>Previous run ID if training should be resumed from a previous ckpt. Default: None</p> <code>None</code> <code>wandb_group_name</code> <code>Optional[str]</code> <p>Group name for the wandb run. Default: None.</p> <code>None</code> <code>optimizer</code> <code>str</code> <p>Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".</p> <code>'Adam'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of type float. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>bool</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <code>lr_scheduler</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in <code>sleap_nn.config.trainer_config</code> are used). To use custom values, pass a dictionary with the structure in <code>sleap_nn.config.trainer_config.LRSchedulerConfig</code>. For eg, {             \"step_lr\":                 {                     (params in <code>sleap_nn.config.trainer_config.StepLRConfig</code>)                 }         }</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>True if early stopping should be enabled. Default: False.</p> <code>False</code> <code>early_stopping_min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. Default: 0.0.</p> <code>0.0</code> <code>early_stopping_patience</code> <code>int</code> <p>Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. Default: 1.</p> <code>1</code> <code>online_mining</code> <code>bool</code> <p>If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss.</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>float</code> <p>The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>int</code> <p>The minimum number of keypoints that will be considered as \"hard\", even if they are not below the <code>hard_to_easy_ratio</code>.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether.</p> <code>None</code> <code>loss_scale</code> <code>float</code> <p>Factor to scale the hard keypoint losses by for oks.</p> <code>5.0</code> <code>zmq_publish_address</code> <code>Optional[str]</code> <p>(str) Specifies the address and port to which the training logs (loss values) should be sent to.</p> <code>None</code> <code>zmq_controller_address</code> <code>Optional[str]</code> <p>(str) Specifies the address and port to listen to to stop the training (specific to SLEAP GUI).</p> <code>None</code> <code>zmq_controller_timeout</code> <code>int</code> <p>(int) Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed.</p> <code>10</code> Source code in <code>sleap_nn/train.py</code> <pre><code>def train(\n    train_labels_path: Optional[List[str]] = None,\n    val_labels_path: Optional[List[str]] = None,\n    validation_fraction: float = 0.1,\n    test_file_path: Optional[str] = None,\n    provider: str = \"LabelsReader\",\n    user_instances_only: bool = True,\n    data_pipeline_fw: str = \"torch_dataset\",\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    delete_cache_imgs_after_training: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    scale: float = 1.0,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n    crop_hw: Optional[Tuple[int, int]] = None,\n    min_crop_size: Optional[int] = 100,\n    use_augmentations_train: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometry_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    init_weight: str = \"default\",\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    backbone_config: Union[str, Dict[str, Any]] = \"unet\",\n    head_configs: Union[str, Dict[str, Any]] = None,\n    batch_size: int = 1,\n    shuffle_train: bool = False,\n    num_workers: int = 0,\n    ckpt_save_top_k: int = 1,\n    ckpt_save_last: Optional[bool] = None,\n    trainer_num_devices: Union[str, int] = \"auto\",\n    trainer_accelerator: str = \"auto\",\n    enable_progress_bar: bool = True,\n    min_train_steps_per_epoch: int = 200,\n    train_steps_per_epoch: Optional[int] = None,\n    visualize_preds_during_training: bool = False,\n    keep_viz: bool = False,\n    max_epochs: int = 10,\n    seed: int = 0,\n    use_wandb: bool = False,\n    save_ckpt: bool = False,\n    save_ckpt_path: Optional[str] = None,\n    resume_ckpt_path: Optional[str] = None,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_name: Optional[str] = None,\n    wandb_api_key: Optional[str] = None,\n    wandb_mode: Optional[str] = None,\n    wandb_resume_prv_runid: Optional[str] = None,\n    wandb_group_name: Optional[str] = None,\n    optimizer: str = \"Adam\",\n    learning_rate: float = 1e-3,\n    amsgrad: bool = False,\n    lr_scheduler: Optional[Union[str, Dict[str, Any]]] = None,\n    early_stopping: bool = False,\n    early_stopping_min_delta: float = 0.0,\n    early_stopping_patience: int = 1,\n    online_mining: bool = False,\n    hard_to_easy_ratio: float = 2.0,\n    min_hard_keypoints: int = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: float = 5.0,\n    zmq_publish_address: Optional[str] = None,\n    zmq_controller_address: Optional[str] = None,\n    zmq_controller_timeout: int = 10,\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        train_labels_path: List of paths to training data (`.slp` file). Default: `None`\n        val_labels_path: List of paths to validation data (`.slp` file). Default: `None`\n        validation_fraction: Float between 0 and 1 specifying the fraction of the\n            training set to sample for generating the validation set. The remaining\n            labeled frames will be left in the training set. If the `validation_labels`\n            are already specified, this has no effect. Default: 0.1.\n        test_file_path: Path to test dataset (`.slp` file or `.mp4` file).\n            Note: This is used to get evaluation on test set after training is completed.\n        provider: Provider class to read the input sleap files. Only \"LabelsReader\"\n            supported for the training pipeline. Default: \"LabelsReader\".\n        user_instances_only: `True` if only user labeled instances should be used for\n            training. If `False`, both user labeled and predicted instances would be used.\n            Default: `True`.\n        data_pipeline_fw: Framework to create the data loaders. One of [`torch_dataset`,\n            `torch_dataset_cache_img_memory`, `torch_dataset_cache_img_disk`]. Default: \"torch_dataset\".\n        cache_img_path: Path to save `.jpg` images created with `torch_dataset_cache_img_disk` data pipeline\n            framework. If `None`, the path provided in `trainer_config.save_ckpt` is used (else working dir is used). The `train_imgs` and `val_imgs` dirs are created inside this path. Default: None.\n        use_existing_imgs: Use existing train and val images/ chunks in the `cache_img_path` for `torch_dataset_cache_img_disk` frameworks. If `True`, the `cache_img_path` should have `train_imgs` and `val_imgs` dirs.\n            Default: False.\n        delete_cache_imgs_after_training: If `False`, the images (torch_dataset_cache_img_disk) are\n            retained after training. Else, the files are deleted. Default: True.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n            channel when this is set to `True`, then the images from single-channel\n            is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n            is set to True, then we convert the image to grayscale (single-channel)\n            image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        max_height: Maximum height the image should be padded to. If not provided, the\n            original image size will be retained. Default: None.\n        max_width: Maximum width the image should be padded to. If not provided, the\n            original image size will be retained. Default: None.\n        crop_hw: Crop height and width of each instance (h, w) for centered-instance model.\n            If `None`, this would be automatically computed based on the largest instance\n            in the `sio.Labels` file. Default: None.\n        min_crop_size: Minimum crop size to be used if `crop_hw` is `None`. Default: 100.\n        use_augmentations_train: True if the data augmentation should be applied to the\n            training data, else False. Default: False.\n        intensity_aug: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"]\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.IntensityConfig`.\n            For eg: {\n                        \"uniform_noise_min\": 1.0,\n                        \"uniform_noise_p\": 1.0\n                    }\n        geometry_aug: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"].\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.GeometryConfig`.\n            For eg: {\n                        \"rotation\": 45,\n                        \"affine_p\": 1.0\n                    }\n        init_weight: model weights initialization method. \"default\" uses kaiming uniform\n            initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".\n        pretrained_backbone_weights: Path of the `ckpt` (or `.h5` file from SLEAP) file with which the backbone is\n            initialized. If `None`, random init is used. Default: None.\n        pretrained_head_weights: Path of the `ckpt` (or `.h5` file from SLEAP) file with which the head layers are\n            initialized. If `None`, random init is used. Default: None.\n        backbone_config: One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\",\n            \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\",\n            \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set,\n            then pass a dictionary with the structure:\n            {\n                \"unet((or) convnext (or)swint)\":\n                    {(params in the corresponding architecture given in `sleap_nn.config.model_config.backbone_config`)\n                    }\n            }.\n            For eg: {\n                        \"unet\":\n                            {\n                                \"in_channels\": 3,\n                                \"filters\": 64,\n                                \"max_stride\": 32,\n                                \"output_stride\": 2\n                            }\n                    }\n        head_configs: One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"].\n            The default `sigma` and `output_strides` are used if a string is passed. To\n            set custom parameters, pass in a dictionary with the structure:\n            {\n                \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):\n                    {\n                        \"confmaps\":\n                            {\n                                # params in the corresponding head type given in `sleap_nn.config.model_config.head_configs`\n                            },\n                        \"pafs\":\n                            {\n                                # only for bottomup\n                            }\n                    }\n            }.\n            For eg: {\n                        \"single_instance\":\n                            {\n                                \"confmaps\":\n                                    {\n                                        \"part_names\": None,\n                                        \"sigma\": 2.5,\n                                        \"output_stride\": 2\n                                    }\n                            }\n                    }\n        batch_size: Number of samples per batch or batch size for training data. Default: 1.\n        shuffle_train: True to have the train data reshuffled at every epoch. Default: False.\n        num_workers: Number of subprocesses to use for data loading. 0 means that the data\n            will be loaded in the main process. Default: 0.\n        ckpt_save_top_k: If save_top_k == k, the best k models according to the quantity\n            monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1,\n            all models are saved. Please note that the monitors are checked every every_n_epochs\n            epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an\n            epoch, the name of the saved file will be appended with a version count starting\n            with v1 unless enable_version_counter is set to False. Default: 1.\n        ckpt_save_last: When True, saves a last.ckpt whenever a checkpoint file gets saved.\n            On a local filesystem, this will be a symbolic link, and otherwise a copy of\n            the checkpoint file. This allows accessing the latest checkpoint in a deterministic\n            manner. Default: None.\n        trainer_num_devices: Number of devices to train on (int), which devices to train\n            on (list or str), or \"auto\" to select automatically. Default: \"auto\".\n        trainer_accelerator: One of the (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\"). \"auto\" recognises\n            the machine the model is running on and chooses the appropriate accelerator for\n            the `Trainer` to be connected to. Default: \"auto\".\n        enable_progress_bar: When True, enables printing the logs during training. Default: True.\n        min_train_steps_per_epoch: Minimum number of iterations in a single epoch. (Useful if model\n            is trained with very few data points). Refer `limit_train_batches` parameter\n            of Torch `Trainer`. Default: 200.\n        train_steps_per_epoch: Number of minibatches (steps) to train for in an epoch. If set to `None`,\n            this is set to the number of batches in the training data or `min_train_steps_per_epoch`,\n            whichever is largest. Default: `None`. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        visualize_preds_during_training: If set to `True`, sample predictions (keypoints  + confidence maps)\n            are saved to `viz` folder in the ckpt dir and in wandb table.\n        keep_viz: If set to `True`, the `viz` folder containing training visualizations will be kept after training completes. If `False`, the folder will be deleted. This parameter only has an effect when `visualize_preds_during_training` is `True`. Default: `False`.\n        max_epochs: Maximum number of epochs to run. Default: 10.\n        seed: Seed value for the current experiment. default: 0.\n        save_ckpt: True to enable checkpointing. Default: False.\n        save_ckpt_path: Directory path to save the training config and checkpoint files.\n            If `None` and `save_ckpt` is `True`, then the current working dir is used as\n            the ckpt path. Default: None\n        resume_ckpt_path: Path to `.ckpt` file from which training is resumed. Default: None.\n        use_wandb: True to enable wandb logging. Default: False.\n        wandb_entity: Entity of wandb project. Default: None.\n            (The default entity in the user profile settings is used)\n        wandb_project: Project name for the current wandb run. Default: None.\n        wandb_name: Name of the current wandb run. Default: None.\n        wandb_api_key: API key. The API key is masked when saved to config files. Default: None.\n        wandb_mode: \"offline\" if only local logging is required. Default: None.\n        wandb_resume_prv_runid: Previous run ID if training should be resumed from a previous\n            ckpt. Default: None\n        wandb_group_name: Group name for the wandb run. Default: None.\n        optimizer: Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".\n        learning_rate: Learning rate of type float. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n        lr_scheduler: One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in\n            `sleap_nn.config.trainer_config` are used). To use custom values, pass a\n            dictionary with the structure in `sleap_nn.config.trainer_config.LRSchedulerConfig`.\n            For eg, {\n                        \"step_lr\":\n                            {\n                                (params in `sleap_nn.config.trainer_config.StepLRConfig`)\n                            }\n                    }\n        early_stopping: True if early stopping should be enabled. Default: False.\n        early_stopping_min_delta: Minimum change in the monitored quantity to qualify as\n            an improvement, i.e. an absolute change of less than or equal to min_delta,\n            will count as no improvement. Default: 0.0.\n        early_stopping_patience: Number of checks with no improvement after which training\n            will be stopped. Under the default configuration, one check happens after every\n            training epoch. Default: 1.\n        online_mining: If True, online hard keypoint mining (OHKM) will be enabled. When\n            this is enabled, the loss is computed per keypoint (or edge for PAFs) and\n            sorted from lowest (easy) to highest (hard). The hard keypoint loss will be\n            scaled to have a higher weight in the total loss, encouraging the training\n            to focus on tricky body parts that are more difficult to learn.\n            If False, no mining will be performed and all keypoints will be weighted\n            equally in the loss.\n        hard_to_easy_ratio: The minimum ratio of the individual keypoint loss with\n            respect to the lowest keypoint loss in order to be considered as \"hard\".\n            This helps to switch focus on across groups of keypoints during training.\n        min_hard_keypoints: The minimum number of keypoints that will be considered as\n            \"hard\", even if they are not below the `hard_to_easy_ratio`.\n        max_hard_keypoints: The maximum number of hard keypoints to apply scaling to.\n            This can help when there are few very easy keypoints which may skew the\n            ratio and result in loss scaling being applied to most keypoints, which can\n            reduce the impact of hard mining altogether.\n        loss_scale: Factor to scale the hard keypoint losses by for oks.\n        zmq_publish_address: (str) Specifies the address and port to which the training logs (loss values) should be sent to.\n        zmq_controller_address: (str) Specifies the address and port to listen to to stop the training (specific to SLEAP GUI).\n        zmq_controller_timeout: (int) Polling timeout in microseconds specified as an integer. This controls how long the poller\n            should wait to receive a response and should be set to a small value to minimize the impact on training speed.\n    \"\"\"\n    data_config = get_data_config(\n        train_labels_path=train_labels_path,\n        val_labels_path=val_labels_path,\n        validation_fraction=validation_fraction,\n        test_file_path=test_file_path,\n        provider=provider,\n        user_instances_only=user_instances_only,\n        data_pipeline_fw=data_pipeline_fw,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        delete_cache_imgs_after_training=delete_cache_imgs_after_training,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        scale=scale,\n        max_height=max_height,\n        max_width=max_width,\n        crop_hw=crop_hw,\n        min_crop_size=min_crop_size,\n        use_augmentations_train=use_augmentations_train,\n        intensity_aug=intensity_aug,\n        geometry_aug=geometry_aug,\n    )\n\n    model_config = get_model_config(\n        init_weight=init_weight,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n    )\n\n    trainer_config = get_trainer_config(\n        batch_size=batch_size,\n        shuffle_train=shuffle_train,\n        num_workers=num_workers,\n        ckpt_save_top_k=ckpt_save_top_k,\n        ckpt_save_last=ckpt_save_last,\n        trainer_num_devices=trainer_num_devices,\n        trainer_accelerator=trainer_accelerator,\n        enable_progress_bar=enable_progress_bar,\n        min_train_steps_per_epoch=min_train_steps_per_epoch,\n        train_steps_per_epoch=train_steps_per_epoch,\n        visualize_preds_during_training=visualize_preds_during_training,\n        keep_viz=keep_viz,\n        max_epochs=max_epochs,\n        seed=seed,\n        use_wandb=use_wandb,\n        save_ckpt=save_ckpt,\n        save_ckpt_path=save_ckpt_path,\n        resume_ckpt_path=resume_ckpt_path,\n        wandb_entity=wandb_entity,\n        wandb_project=wandb_project,\n        wandb_name=wandb_name,\n        wandb_api_key=wandb_api_key,\n        wandb_mode=wandb_mode,\n        wandb_resume_prv_runid=wandb_resume_prv_runid,\n        wandb_group_name=wandb_group_name,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n        lr_scheduler=lr_scheduler,\n        early_stopping=early_stopping,\n        early_stopping_min_delta=early_stopping_min_delta,\n        early_stopping_patience=early_stopping_patience,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        zmq_publish_address=zmq_publish_address,\n        zmq_controller_address=zmq_controller_address,\n        zmq_controller_timeout=zmq_controller_timeout,\n    )\n\n    # create omegaconf object\n    training_job_config = TrainingJobConfig(\n        data_config=data_config,\n        model_config=model_config,\n        trainer_config=trainer_config,\n    )\n    omegaconf_config = training_job_config.to_sleap_nn_cfg()\n\n    # run training\n    run_training(omegaconf_config.copy())\n</code></pre>"},{"location":"api/architectures/","title":"architectures","text":""},{"location":"api/architectures/#sleap_nn.architectures","title":"<code>sleap_nn.architectures</code>","text":"<p>Modules related to model architectures.</p> <p>Modules:</p> Name Description <code>common</code> <p>Common utilities for architecture and model building.</p> <code>convnext</code> <p>This module provides a generalized implementation of ConvNext.</p> <code>encoder_decoder</code> <p>Generic encoder-decoder fully convolutional backbones.</p> <code>heads</code> <p>Model head definitions for defining model output types.</p> <code>model</code> <p>This module defines the main SLEAP model class for defining a trainable model.</p> <code>swint</code> <p>This module provides a generalized implementation of SwinT.</p> <code>unet</code> <p>This module provides a generalized implementation of UNet.</p> <code>utils</code> <p>Miscellaneous utility functions for architectures and modeling.</p>"},{"location":"api/architectures/common/","title":"common","text":""},{"location":"api/architectures/common/#sleap_nn.architectures.common","title":"<code>sleap_nn.architectures.common</code>","text":"<p>Common utilities for architecture and model building.</p> <p>Classes:</p> Name Description <code>MaxPool2dWithSamePadding</code> <p>A MaxPool2d module with support for same padding.</p>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding","title":"<code>MaxPool2dWithSamePadding</code>","text":"<p>               Bases: <code>MaxPool2d</code></p> <p>A MaxPool2d module with support for same padding.</p> <p>This class extends the torch.nn.MaxPool2d module and adds the ability to perform 'same' padding, similar to 'same' padding in convolutional layers. When 'same' padding is specified, the input tensor is padded with zeros to ensure that the output spatial dimensions match the input spatial dimensions as closely as possible.</p> <p>Parameters:</p> Name Type Description Default <code>nn.MaxPool2d</code> <code>arguments</code> <p>Arguments that are passed to the parent torch.nn.MaxPool2d class.</p> required <p>Methods:</p> Name Description <code>forward</code> <p>torch.Tensor) -&gt; torch.Tensor: Forward pass through the MaxPool2dWithSamePadding module.</p> Note <p>The 'same' padding is applied only when self.padding is set to \"same\".</p> Example Source code in <code>sleap_nn/architectures/common.py</code> <pre><code>class MaxPool2dWithSamePadding(nn.MaxPool2d):\n    \"\"\"A MaxPool2d module with support for same padding.\n\n    This class extends the torch.nn.MaxPool2d module and adds the ability\n    to perform 'same' padding, similar to 'same' padding in convolutional\n    layers. When 'same' padding is specified, the input tensor is padded\n    with zeros to ensure that the output spatial dimensions match the input\n    spatial dimensions as closely as possible.\n\n    Args:\n        nn.MaxPool2d arguments: Arguments that are passed to the parent\n            torch.nn.MaxPool2d class.\n\n    Methods:\n        forward(x: torch.Tensor) -&gt; torch.Tensor:\n            Forward pass through the MaxPool2dWithSamePadding module.\n\n    Note:\n        The 'same' padding is applied only when self.padding is set to \"same\".\n\n    Example:\n        # Create an instance of MaxPool2dWithSamePadding\n        maxpool_layer = MaxPool2dWithSamePadding(kernel_size=3, stride=2, padding=\"same\")\n\n        # Perform a forward pass on an input tensor\n        input_tensor = torch.rand(1, 3, 32, 32)  # Example input tensor\n        output = maxpool_layer(input_tensor)  # Apply the MaxPool2d operation with same padding.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the MaxPool2dWithSamePadding module.\"\"\"\n        super().__init__(*args, **kwargs)\n\n    def _calc_same_pad(self, i: int, k: int, s: int, d: int) -&gt; int:\n        \"\"\"Calculate the required padding to achieve 'same' padding.\n\n        Args:\n            i (int): Input dimension (height or width).\n            k (int): Kernel size.\n            s (int): Stride.\n            d (int): Dilation.\n\n        Returns:\n            int: The calculated padding value.\n        \"\"\"\n        return int(\n            max(\n                (torch.ceil(torch.tensor(i / s)).item() - 1) * s + (k - 1) * d + 1 - i,\n                0,\n            )\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the MaxPool2dWithSamePadding module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the MaxPool2d operation.\n        \"\"\"\n        if self.padding == \"same\":\n            ih, iw = x.size()[-2:]\n\n            pad_h = self._calc_same_pad(\n                i=ih,\n                k=(\n                    self.kernel_size\n                    if type(self.kernel_size) is int\n                    else self.kernel_size[0]\n                ),\n                s=self.stride if type(self.stride) is int else self.stride[0],\n                d=self.dilation if type(self.dilation) is int else self.dilation[0],\n            )\n            pad_w = self._calc_same_pad(\n                i=iw,\n                k=(\n                    self.kernel_size\n                    if type(self.kernel_size) is int\n                    else self.kernel_size[1]\n                ),\n                s=self.stride if type(self.stride) is int else self.stride[1],\n                d=self.dilation if type(self.dilation) is int else self.dilation[1],\n            )\n\n            if pad_h &gt; 0 or pad_w &gt; 0:\n                x = F.pad(\n                    x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n                )\n            self.padding = 0\n\n        return F.max_pool2d(\n            x,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            self.dilation,\n            ceil_mode=self.ceil_mode,\n            return_indices=self.return_indices,\n        )\n</code></pre>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding--create-an-instance-of-maxpool2dwithsamepadding","title":"Create an instance of MaxPool2dWithSamePadding","text":"<p>maxpool_layer = MaxPool2dWithSamePadding(kernel_size=3, stride=2, padding=\"same\")</p>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding--perform-a-forward-pass-on-an-input-tensor","title":"Perform a forward pass on an input tensor","text":"<p>input_tensor = torch.rand(1, 3, 32, 32)  # Example input tensor output = maxpool_layer(input_tensor)  # Apply the MaxPool2d operation with same padding.</p>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the MaxPool2dWithSamePadding module.</p> Source code in <code>sleap_nn/architectures/common.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize the MaxPool2dWithSamePadding module.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the MaxPool2dWithSamePadding module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the MaxPool2d operation.</p> Source code in <code>sleap_nn/architectures/common.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the MaxPool2dWithSamePadding module.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the MaxPool2d operation.\n    \"\"\"\n    if self.padding == \"same\":\n        ih, iw = x.size()[-2:]\n\n        pad_h = self._calc_same_pad(\n            i=ih,\n            k=(\n                self.kernel_size\n                if type(self.kernel_size) is int\n                else self.kernel_size[0]\n            ),\n            s=self.stride if type(self.stride) is int else self.stride[0],\n            d=self.dilation if type(self.dilation) is int else self.dilation[0],\n        )\n        pad_w = self._calc_same_pad(\n            i=iw,\n            k=(\n                self.kernel_size\n                if type(self.kernel_size) is int\n                else self.kernel_size[1]\n            ),\n            s=self.stride if type(self.stride) is int else self.stride[1],\n            d=self.dilation if type(self.dilation) is int else self.dilation[1],\n        )\n\n        if pad_h &gt; 0 or pad_w &gt; 0:\n            x = F.pad(\n                x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n            )\n        self.padding = 0\n\n    return F.max_pool2d(\n        x,\n        self.kernel_size,\n        self.stride,\n        self.padding,\n        self.dilation,\n        ceil_mode=self.ceil_mode,\n        return_indices=self.return_indices,\n    )\n</code></pre>"},{"location":"api/architectures/convnext/","title":"convnext","text":""},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext","title":"<code>sleap_nn.architectures.convnext</code>","text":"<p>This module provides a generalized implementation of ConvNext.</p> <p>See the <code>ConvNextWrapper</code> class docstring for more information.</p> <p>Classes:</p> Name Description <code>ConvNeXtEncoder</code> <p>ConvNext backbone for pose estimation.</p> <code>ConvNextWrapper</code> <p>ConvNext architecture for pose estimation.</p>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNeXtEncoder","title":"<code>ConvNeXtEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>ConvNext backbone for pose estimation.</p> <p>This class implements ConvNext from the <code>A ConvNet for the 2020s &lt;https://arxiv.org/abs/2201.03545&gt;</code> paper. Source: torchvision.models. This module serves as the backbone/ encoder architecture to extract features from the input image.</p> <p>Parameters:</p> Name Type Description Default <code>blocks</code> <code>dict) </code> <p>Dictionary of depths and channels. Default is \"Tiny architecture\"</p> <code>{'depths': [3, 3, 9, 3], 'channels': [96, 192, 384, 768]}</code> <code>in_channels</code> <code>int</code> <p>Input number of channels. Default: 1.</p> <code>1</code> <code>stem_kernel</code> <code>int</code> <p>Size of the convolutional kernels in the stem layer.             Default is 4.</p> <code>4</code> <code>stem_stride</code> <code>int</code> <p>Convolutional stride in the stem layer. Default is 2.</p> <code>2</code> <code>stochastic_depth_prob</code> <code>float</code> <p>Stochastic depth rate. Default: 0.1.</p> <code>0.0</code> <code>layer_scale</code> <code>float</code> <p>Scale for Layer normalization layer. Default: 1e-6.</p> <code>1e-06</code> <code>block</code> <code>Module</code> <p>SwinTransformer Block. Default: None.</p> <code>None</code> <code>norm_layer</code> <code>Module</code> <p>Normalization layer. Default: None.</p> <code>None</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the ConvNext Encoder.</p> <code>forward</code> <p>Forward pass through the ConvNext encoder.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>class ConvNeXtEncoder(nn.Module):\n    \"\"\"ConvNext backbone for pose estimation.\n\n    This class implements ConvNext from the `A ConvNet for the 2020s &lt;https://arxiv.org/abs/2201.03545&gt;`\n    paper. Source: torchvision.models. This module serves as the backbone/ encoder\n    architecture to extract features from the input image.\n\n    Args:\n        blocks (dict) : Dictionary of depths and channels. Default is \"Tiny architecture\"\n                        {'depths': [3,3,9,3], 'channels':[96, 192, 384, 768]}\n        in_channels (int): Input number of channels. Default: 1.\n        stem_kernel (int): Size of the convolutional kernels in the stem layer.\n                        Default is 4.\n        stem_stride (int): Convolutional stride in the stem layer. Default is 2.\n        stochastic_depth_prob (float): Stochastic depth rate. Default: 0.1.\n        layer_scale (float): Scale for Layer normalization layer. Default: 1e-6.\n        block (nn.Module, optional): SwinTransformer Block. Default: None.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        blocks: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n        in_channels: int = 1,\n        stem_kernel: int = 4,\n        stem_stride: int = 2,\n        stochastic_depth_prob: float = 0.0,\n        layer_scale: float = 1e-6,\n        block: Optional[Callable[..., nn.Module]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the ConvNext Encoder.\"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        depths, channels = blocks[\"depths\"], blocks[\"channels\"]\n        block_setting = [0] * len(depths)\n        for idx in range(len(depths)):\n            if idx == len(depths) - 1:\n                last = None\n            else:\n                last = channels[idx + 1]\n            block_setting[idx] = CNBlockConfig(channels[idx], last, depths[idx])\n        if block is None:\n            block = CNBlock\n\n        if norm_layer is None:\n            norm_layer = partial(LayerNorm2d, eps=1e-6)\n\n        layers: List[nn.Module] = []\n\n        # Stem\n        firstconv_output_channels = block_setting[0].input_channels\n        layers.append(\n            Conv2dNormActivation(\n                in_channels,\n                firstconv_output_channels,\n                kernel_size=stem_kernel,\n                stride=stem_stride,\n                padding=1,  ## 0 -&gt; 1\n                norm_layer=norm_layer,\n                activation_layer=None,\n                bias=True,\n            )\n        )\n\n        total_stage_blocks = sum(cnf.num_layers for cnf in block_setting)\n        stage_block_id = 0\n        for cnf in block_setting:\n            # Bottlenecks\n            stage: List[nn.Module] = []\n            for _ in range(cnf.num_layers):\n                # adjust stochastic depth probability based on the depth of the stage block\n                sd_prob = (\n                    stochastic_depth_prob * stage_block_id / (total_stage_blocks - 1.0)\n                )\n                stage.append(block(cnf.input_channels, layer_scale, sd_prob))\n                stage_block_id += 1\n            layers.append(nn.Sequential(*stage))\n            if cnf.out_channels is not None:\n                # Downsampling\n                layers.append(\n                    nn.Sequential(\n                        norm_layer(cnf.input_channels),\n                        nn.Conv2d(\n                            cnf.input_channels,\n                            cnf.out_channels,\n                            kernel_size=2,\n                            stride=2,\n                        ),\n                    )\n                )\n        self.features = nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -&gt; Tensor:\n        features_list = []\n        for l in self.features:\n            x = l(x)\n            features_list.append(x)\n        return features_list\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass through the ConvNext encoder.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Outputs a list of tensors from each stage after applying the ConvNext backbone.\n        \"\"\"\n        return self._forward_impl(x)\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNeXtEncoder.__init__","title":"<code>__init__(blocks={'depths': [3, 3, 9, 3], 'channels': [96, 192, 384, 768]}, in_channels=1, stem_kernel=4, stem_stride=2, stochastic_depth_prob=0.0, layer_scale=1e-06, block=None, norm_layer=None, **kwargs)</code>","text":"<p>Initialize the ConvNext Encoder.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def __init__(\n    self,\n    blocks: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n    in_channels: int = 1,\n    stem_kernel: int = 4,\n    stem_stride: int = 2,\n    stochastic_depth_prob: float = 0.0,\n    layer_scale: float = 1e-6,\n    block: Optional[Callable[..., nn.Module]] = None,\n    norm_layer: Optional[Callable[..., nn.Module]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the ConvNext Encoder.\"\"\"\n    super().__init__()\n    _log_api_usage_once(self)\n\n    depths, channels = blocks[\"depths\"], blocks[\"channels\"]\n    block_setting = [0] * len(depths)\n    for idx in range(len(depths)):\n        if idx == len(depths) - 1:\n            last = None\n        else:\n            last = channels[idx + 1]\n        block_setting[idx] = CNBlockConfig(channels[idx], last, depths[idx])\n    if block is None:\n        block = CNBlock\n\n    if norm_layer is None:\n        norm_layer = partial(LayerNorm2d, eps=1e-6)\n\n    layers: List[nn.Module] = []\n\n    # Stem\n    firstconv_output_channels = block_setting[0].input_channels\n    layers.append(\n        Conv2dNormActivation(\n            in_channels,\n            firstconv_output_channels,\n            kernel_size=stem_kernel,\n            stride=stem_stride,\n            padding=1,  ## 0 -&gt; 1\n            norm_layer=norm_layer,\n            activation_layer=None,\n            bias=True,\n        )\n    )\n\n    total_stage_blocks = sum(cnf.num_layers for cnf in block_setting)\n    stage_block_id = 0\n    for cnf in block_setting:\n        # Bottlenecks\n        stage: List[nn.Module] = []\n        for _ in range(cnf.num_layers):\n            # adjust stochastic depth probability based on the depth of the stage block\n            sd_prob = (\n                stochastic_depth_prob * stage_block_id / (total_stage_blocks - 1.0)\n            )\n            stage.append(block(cnf.input_channels, layer_scale, sd_prob))\n            stage_block_id += 1\n        layers.append(nn.Sequential(*stage))\n        if cnf.out_channels is not None:\n            # Downsampling\n            layers.append(\n                nn.Sequential(\n                    norm_layer(cnf.input_channels),\n                    nn.Conv2d(\n                        cnf.input_channels,\n                        cnf.out_channels,\n                        kernel_size=2,\n                        stride=2,\n                    ),\n                )\n            )\n    self.features = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNeXtEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the ConvNext encoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Outputs a list of tensors from each stage after applying the ConvNext backbone.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass through the ConvNext encoder.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Outputs a list of tensors from each stage after applying the ConvNext backbone.\n    \"\"\"\n    return self._forward_impl(x)\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper","title":"<code>ConvNextWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>ConvNext architecture for pose estimation.</p> <p>This class defines the ConvNext architecture for pose estimation, combining an ConvNext as the encoder and a decoder. The encoder extracts features from the input, while the decoder generates confidence maps based on the features.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"].</p> required <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>arch</code> <code>dict</code> <p>Dictionary of depths and channels. Default is \"Tiny architecture\"</p> <code>{'depths': [3, 3, 9, 3], 'channels': [96, 192, 384, 768]}</code> <code>{'depths'</code> <p>[3,3,9,3], 'channels':[96, 192, 384, 768]}</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>stem_patch_kernel</code> <code>int</code> <p>Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>4</code> <code>stem_patch_stride</code> <code>int</code> <p>Convolutional stride in the stem layer. Default is 2.</p> <code>2</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>16</code> for all convnext architectures.</p> <code>32</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the ConvNeXt architecture.</p> <code>from_config</code> <p>Create ConvNextWrapper from a config.</p> <p>Attributes:</p> Name Type Description <code>max_channels</code> <p>Returns the maximum channels of the ConvNext (last layer of the encoder).</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>class ConvNextWrapper(nn.Module):\n    \"\"\"ConvNext architecture for pose estimation.\n\n    This class defines the ConvNext architecture for pose estimation, combining an\n    ConvNext as the encoder and a decoder. The encoder extracts features from the input,\n    while the decoder generates confidence maps based on the features.\n\n    Args:\n        model_type: One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"].\n        output_stride: Minimum of the strides of the output heads. The input confidence map.\n        tensor is expected to be at the same stride.\n        in_channels: Number of input channels. Default is 1.\n        arch: Dictionary of depths and channels. Default is \"Tiny architecture\"\n        {'depths': [3,3,9,3], 'channels':[96, 192, 384, 768]}\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        stem_patch_kernel: Size of the convolutional kernels in the stem layer. Default is 4.\n        stem_patch_stride: Convolutional stride in the stem layer. Default is 2.\n        filters_rate: Factor to adjust the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n        max_stride: Factor by which input image size is reduced through the layers. This is always `16` for all convnext architectures.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        output_stride: int,\n        arch: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n        in_channels: int = 1,\n        kernel_size: int = 3,\n        stem_patch_kernel: int = 4,\n        stem_patch_stride: int = 2,\n        filters_rate: int = 2,\n        convs_per_block: int = 2,\n        up_interpolate: bool = True,\n        max_stride: int = 32,\n        block_contraction: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.filters_rate = filters_rate\n        self.block_contraction = block_contraction\n        arch_types = {\n            \"tiny\": {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n            \"small\": {\"depths\": [3, 3, 27, 3], \"channels\": [96, 192, 384, 768]},\n            \"base\": {\"depths\": [3, 3, 27, 3], \"channels\": [128, 256, 512, 1024]},\n            \"large\": {\"depths\": [3, 3, 27, 3], \"channels\": [192, 384, 768, 1536]},\n        }\n        if model_type in arch_types:\n            self.arch = arch_types[model_type]\n        elif arch is not None:\n            self.arch = arch\n        else:\n            self.arch = arch_types[\"tiny\"]\n\n        self.max_stride = (\n            stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * final_max_pool_stride\n        self.stem_blocks = 1  # 1 stem block + 3 down blocks in convnext\n\n        self.up_blocks = np.log2(\n            self.max_stride / (stem_patch_stride * output_stride)\n        ).astype(int) + np.log2(stem_patch_stride).astype(int)\n        self.convs_per_block = convs_per_block\n        self.stem_patch_kernel = stem_patch_kernel\n        self.stem_patch_stride = stem_patch_stride\n        self.output_stride = output_stride\n        self.up_interpolate = up_interpolate\n        self.down_blocks = len(self.arch[\"channels\"]) - 1\n\n        self.enc = ConvNeXtEncoder(\n            blocks=self.arch,\n            in_channels=in_channels,\n            stem_stride=stem_patch_stride,\n            stem_kernel=stem_patch_kernel,\n        )\n\n        # Add additional pooling layer after encoder\n        self.additional_pool = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=2, padding=\"same\"\n        )\n\n        # Create middle blocks\n        self.middle_blocks = nn.ModuleList()\n        # Get the last block filters from encoder\n        last_block_filters = self.arch[\"channels\"][-1]\n\n        if convs_per_block &gt; 1:\n            # Middle expansion block\n            middle_expand = SimpleConvBlock(\n                in_channels=last_block_filters,\n                pool=False,\n                pool_before_convs=False,\n                pooling_stride=2,\n                num_convs=convs_per_block - 1,\n                filters=int(last_block_filters * filters_rate),\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=\"convnext_middle_expand\",\n            )\n            self.middle_blocks.append(middle_expand)\n\n        # Middle contraction block\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            block_filters = int(last_block_filters)\n        else:\n            # Keep the block output filters the same\n            block_filters = int(last_block_filters * filters_rate)\n\n        middle_contract = SimpleConvBlock(\n            in_channels=int(last_block_filters * filters_rate),\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=1,\n            filters=block_filters,\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_contract\",\n        )\n        self.middle_blocks.append(middle_contract)\n\n        self.current_stride = (\n            self.stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * pool\n\n        # Calculate x_in_shape based on whether we have block contraction\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            x_in_shape = int(self.arch[\"channels\"][-1])\n        else:\n            # Keep the block output filters the same\n            x_in_shape = int(self.arch[\"channels\"][-1] * filters_rate)\n\n        self.dec = Decoder(\n            x_in_shape=x_in_shape,\n            current_stride=self.current_stride,\n            filters=self.arch[\"channels\"][0],\n            up_blocks=self.up_blocks,\n            down_blocks=self.down_blocks,\n            filters_rate=filters_rate,\n            kernel_size=self.kernel_size,\n            stem_blocks=1,\n            block_contraction=self.block_contraction,\n            output_stride=self.output_stride,\n            up_interpolate=up_interpolate,\n        )\n\n        if len(self.dec.decoder_stack):\n            self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n        else:\n            self.final_dec_channels = block_filters\n\n        self.decoder_stride_to_filters = self.dec.stride_to_filters\n\n    @property\n    def max_channels(self):\n        \"\"\"Returns the maximum channels of the ConvNext (last layer of the encoder).\"\"\"\n        return self.dec.x_in_shape\n\n    @classmethod\n    def from_config(cls, config: OmegaConf):\n        \"\"\"Create ConvNextWrapper from a config.\"\"\"\n        return cls(\n            in_channels=config.in_channels,\n            model_type=config.model_type,\n            arch=config.arch,\n            kernel_size=config.kernel_size,\n            filters_rate=config.filters_rate,\n            convs_per_block=config.convs_per_block,\n            up_interpolate=config.up_interpolate,\n            output_stride=config.output_stride,\n            stem_patch_kernel=config.stem_patch_kernel,\n            stem_patch_stride=config.stem_patch_stride,\n            max_stride=config.max_stride,\n            block_contraction=(\n                config.block_contraction\n                if hasattr(config, \"block_contraction\")\n                else False\n            ),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the ConvNeXt architecture.\n\n        Args:\n            x: Input tensor (Batch, Channels, Height, Width).\n\n        Returns:\n            x: Outputs a dictionary with `outputs` and `strides` containing the output\n            at different strides.\n        \"\"\"\n        enc_output = self.enc(x)\n        x, features = enc_output[-1], enc_output[::2]\n        features = features[::-1]\n\n        # Apply additional pooling layer\n        x = self.additional_pool(x)\n\n        # Process through middle blocks\n        middle_output = x\n        for middle_block in self.middle_blocks:\n            middle_output = middle_block(middle_output)\n\n        x = self.dec(middle_output, features)\n        x[\"middle_output\"] = middle_output\n        return x\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.max_channels","title":"<code>max_channels</code>  <code>property</code>","text":"<p>Returns the maximum channels of the ConvNext (last layer of the encoder).</p>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.__init__","title":"<code>__init__(model_type, output_stride, arch={'depths': [3, 3, 9, 3], 'channels': [96, 192, 384, 768]}, in_channels=1, kernel_size=3, stem_patch_kernel=4, stem_patch_stride=2, filters_rate=2, convs_per_block=2, up_interpolate=True, max_stride=32, block_contraction=False)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    output_stride: int,\n    arch: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n    in_channels: int = 1,\n    kernel_size: int = 3,\n    stem_patch_kernel: int = 4,\n    stem_patch_stride: int = 2,\n    filters_rate: int = 2,\n    convs_per_block: int = 2,\n    up_interpolate: bool = True,\n    max_stride: int = 32,\n    block_contraction: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.kernel_size = kernel_size\n    self.filters_rate = filters_rate\n    self.block_contraction = block_contraction\n    arch_types = {\n        \"tiny\": {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n        \"small\": {\"depths\": [3, 3, 27, 3], \"channels\": [96, 192, 384, 768]},\n        \"base\": {\"depths\": [3, 3, 27, 3], \"channels\": [128, 256, 512, 1024]},\n        \"large\": {\"depths\": [3, 3, 27, 3], \"channels\": [192, 384, 768, 1536]},\n    }\n    if model_type in arch_types:\n        self.arch = arch_types[model_type]\n    elif arch is not None:\n        self.arch = arch\n    else:\n        self.arch = arch_types[\"tiny\"]\n\n    self.max_stride = (\n        stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * final_max_pool_stride\n    self.stem_blocks = 1  # 1 stem block + 3 down blocks in convnext\n\n    self.up_blocks = np.log2(\n        self.max_stride / (stem_patch_stride * output_stride)\n    ).astype(int) + np.log2(stem_patch_stride).astype(int)\n    self.convs_per_block = convs_per_block\n    self.stem_patch_kernel = stem_patch_kernel\n    self.stem_patch_stride = stem_patch_stride\n    self.output_stride = output_stride\n    self.up_interpolate = up_interpolate\n    self.down_blocks = len(self.arch[\"channels\"]) - 1\n\n    self.enc = ConvNeXtEncoder(\n        blocks=self.arch,\n        in_channels=in_channels,\n        stem_stride=stem_patch_stride,\n        stem_kernel=stem_patch_kernel,\n    )\n\n    # Add additional pooling layer after encoder\n    self.additional_pool = MaxPool2dWithSamePadding(\n        kernel_size=2, stride=2, padding=\"same\"\n    )\n\n    # Create middle blocks\n    self.middle_blocks = nn.ModuleList()\n    # Get the last block filters from encoder\n    last_block_filters = self.arch[\"channels\"][-1]\n\n    if convs_per_block &gt; 1:\n        # Middle expansion block\n        middle_expand = SimpleConvBlock(\n            in_channels=last_block_filters,\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=convs_per_block - 1,\n            filters=int(last_block_filters * filters_rate),\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_expand\",\n        )\n        self.middle_blocks.append(middle_expand)\n\n    # Middle contraction block\n    if self.block_contraction:\n        # Contract the channels with an exponent lower than the last encoder block\n        block_filters = int(last_block_filters)\n    else:\n        # Keep the block output filters the same\n        block_filters = int(last_block_filters * filters_rate)\n\n    middle_contract = SimpleConvBlock(\n        in_channels=int(last_block_filters * filters_rate),\n        pool=False,\n        pool_before_convs=False,\n        pooling_stride=2,\n        num_convs=1,\n        filters=block_filters,\n        kernel_size=kernel_size,\n        use_bias=True,\n        batch_norm=False,\n        activation=\"relu\",\n        prefix=\"convnext_middle_contract\",\n    )\n    self.middle_blocks.append(middle_contract)\n\n    self.current_stride = (\n        self.stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * pool\n\n    # Calculate x_in_shape based on whether we have block contraction\n    if self.block_contraction:\n        # Contract the channels with an exponent lower than the last encoder block\n        x_in_shape = int(self.arch[\"channels\"][-1])\n    else:\n        # Keep the block output filters the same\n        x_in_shape = int(self.arch[\"channels\"][-1] * filters_rate)\n\n    self.dec = Decoder(\n        x_in_shape=x_in_shape,\n        current_stride=self.current_stride,\n        filters=self.arch[\"channels\"][0],\n        up_blocks=self.up_blocks,\n        down_blocks=self.down_blocks,\n        filters_rate=filters_rate,\n        kernel_size=self.kernel_size,\n        stem_blocks=1,\n        block_contraction=self.block_contraction,\n        output_stride=self.output_stride,\n        up_interpolate=up_interpolate,\n    )\n\n    if len(self.dec.decoder_stack):\n        self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n    else:\n        self.final_dec_channels = block_filters\n\n    self.decoder_stride_to_filters = self.dec.stride_to_filters\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the ConvNeXt architecture.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor (Batch, Channels, Height, Width).</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tuple[List[Tensor], List]</code> <p>Outputs a dictionary with <code>outputs</code> and <code>strides</code> containing the output at different strides.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the ConvNeXt architecture.\n\n    Args:\n        x: Input tensor (Batch, Channels, Height, Width).\n\n    Returns:\n        x: Outputs a dictionary with `outputs` and `strides` containing the output\n        at different strides.\n    \"\"\"\n    enc_output = self.enc(x)\n    x, features = enc_output[-1], enc_output[::2]\n    features = features[::-1]\n\n    # Apply additional pooling layer\n    x = self.additional_pool(x)\n\n    # Process through middle blocks\n    middle_output = x\n    for middle_block in self.middle_blocks:\n        middle_output = middle_block(middle_output)\n\n    x = self.dec(middle_output, features)\n    x[\"middle_output\"] = middle_output\n    return x\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create ConvNextWrapper from a config.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>@classmethod\ndef from_config(cls, config: OmegaConf):\n    \"\"\"Create ConvNextWrapper from a config.\"\"\"\n    return cls(\n        in_channels=config.in_channels,\n        model_type=config.model_type,\n        arch=config.arch,\n        kernel_size=config.kernel_size,\n        filters_rate=config.filters_rate,\n        convs_per_block=config.convs_per_block,\n        up_interpolate=config.up_interpolate,\n        output_stride=config.output_stride,\n        stem_patch_kernel=config.stem_patch_kernel,\n        stem_patch_stride=config.stem_patch_stride,\n        max_stride=config.max_stride,\n        block_contraction=(\n            config.block_contraction\n            if hasattr(config, \"block_contraction\")\n            else False\n        ),\n    )\n</code></pre>"},{"location":"api/architectures/encoder_decoder/","title":"encoder_decoder","text":""},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder","title":"<code>sleap_nn.architectures.encoder_decoder</code>","text":"<p>Generic encoder-decoder fully convolutional backbones.</p> <p>This module contains building blocks for creating encoder-decoder architectures of general form.</p> <p>The encoder branch of the network forms the initial multi-scale feature extraction via repeated blocks of convolutions and pooling steps.</p> <p>The decoder branch is then responsible for upsampling the low resolution feature maps to achieve the target output stride.</p> <p>This pattern is generalizable and describes most fully convolutional architectures. For example:     - simple convolutions with pooling form the structure in <code>LEAP CNN &lt;https://www.nature.com/articles/s41592-018-0234-5&gt;</code>;     - adding skip connections forms <code>U-Net &lt;https://arxiv.org/pdf/1505.04597.pdf&gt;</code>;     - using residual blocks with skip connections forms the base module in <code>stacked     hourglass &lt;https://arxiv.org/pdf/1603.06937.pdf&gt;</code>;     - using dense blocks with skip connections forms <code>FC-DenseNet &lt;https://arxiv.org/pdf/1611.09326.pdf&gt;</code>.</p> <p>This module implements blocks used in all of these variants on top of a generic base classes.</p> <p>See the <code>EncoderDecoder</code> base class for requirements for creating new architectures.</p> <p>Classes:</p> Name Description <code>Decoder</code> <p>Decoder module for the UNet architecture.</p> <code>Encoder</code> <p>Encoder module for a neural network architecture.</p> <code>SimpleConvBlock</code> <p>A simple convolutional block module.</p> <code>SimpleUpsamplingBlock</code> <p>A simple upsampling and refining block module.</p> <code>StemBlock</code> <p>Stem block module for initial feature extraction.</p>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Decoder","title":"<code>Decoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Decoder module for the UNet architecture.</p> <p>This class defines the decoder part of the UNet, which consists of a stack of upsampling and refining blocks for feature reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>x_in_shape</code> <code>int</code> <p>Number of input channels for the decoder's input.</p> required <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map</p> required <code>current_stride</code> <code>int</code> <p>Current stride value to adjust during upsampling.</p> required <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 64.</p> <code>64</code> <code>up_blocks</code> <code>int</code> <p>Number of upsampling blocks. Default is 4.</p> <code>4</code> <code>down_blocks</code> <code>int</code> <p>Number of downsampling blocks. Default is 3.</p> <code>3</code> <code>stem_blocks</code> <code>int</code> <p>If &gt;0, will create additional \"down\" blocks for initial downsampling. These will be configured identically to the down blocks below.</p> <code>0</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step. The original implementation does not do this, but the CARE implementation does.</p> <code>False</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the Decoder module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class Decoder(nn.Module):\n    \"\"\"Decoder module for the UNet architecture.\n\n    This class defines the decoder part of the UNet,\n    which consists of a stack of upsampling and refining blocks for feature reconstruction.\n\n    Args:\n        x_in_shape: Number of input channels for the decoder's input.\n        output_stride: Minimum of the strides of the output heads. The input confidence map\n        tensor is expected to be at the same stride.\n        current_stride: Current stride value to adjust during upsampling.\n        filters: Number of filters for the initial block. Default is 64.\n        up_blocks: Number of upsampling blocks. Default is 4.\n        down_blocks: Number of downsampling blocks. Default is 3.\n        stem_blocks: If &gt;0, will create additional \"down\" blocks for initial\n            downsampling. These will be configured identically to the down blocks below.\n        filters_rate: Factor to adjust the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step. The original implementation does not\n            do this, but the CARE implementation does.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        x_in_shape: int,\n        output_stride: int,\n        current_stride: int,\n        filters: int = 64,\n        up_blocks: int = 4,\n        down_blocks: int = 3,\n        stem_blocks: int = 0,\n        filters_rate: int = 2,\n        convs_per_block: int = 2,\n        kernel_size: int = 3,\n        block_contraction: bool = False,\n        up_interpolate: bool = True,\n        prefix: str = \"dec\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.x_in_shape = x_in_shape\n        self.current_stride = current_stride\n        self.filters = filters\n        self.up_blocks = up_blocks\n        self.down_blocks = down_blocks\n        self.stem_blocks = stem_blocks\n        self.filters_rate = filters_rate\n        self.convs_per_block = convs_per_block\n        self.kernel_size = kernel_size\n        self.block_contraction = block_contraction\n        self.prefix = prefix\n        self.stride_to_filters = {}\n\n        self.current_strides = []\n        self.residuals = 0\n\n        self.decoder_stack = nn.ModuleList([])\n\n        self.stride_to_filters[current_stride] = x_in_shape\n\n        for block in range(up_blocks):\n            prev_block_filters = -1 if block == 0 else block_filters_out\n            block_filters_out = int(\n                filters\n                * (filters_rate ** max(0, down_blocks + self.stem_blocks - 1 - block))\n            )\n\n            if self.block_contraction:\n                block_filters_out = int(\n                    self.filters\n                    * (\n                        self.filters_rate\n                        ** (self.down_blocks + self.stem_blocks - 2 - block)\n                    )\n                )\n\n            next_stride = current_stride // 2\n\n            if self.stem_blocks &gt; 0 and block &gt;= down_blocks + self.stem_blocks:\n                # This accounts for the case where we dont have any more down block features to concatenate with.\n                # In this case, add a simple upsampling block with a conv layer and with no concatenation\n                self.decoder_stack.append(\n                    SimpleUpsamplingBlock(\n                        x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                        current_stride=current_stride,\n                        upsampling_stride=2,\n                        interp_method=\"bilinear\",\n                        refine_convs=1,\n                        refine_convs_filters=block_filters_out,\n                        refine_convs_kernel_size=self.kernel_size,\n                        refine_convs_batch_norm=False,\n                        up_interpolate=up_interpolate,\n                        transpose_convs_filters=block_filters_out,\n                        transpose_convs_batch_norm=False,\n                        feat_concat=False,\n                        prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                    )\n                )\n            else:\n                self.decoder_stack.append(\n                    SimpleUpsamplingBlock(\n                        x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                        current_stride=current_stride,\n                        upsampling_stride=2,\n                        interp_method=\"bilinear\",\n                        refine_convs=self.convs_per_block,\n                        refine_convs_filters=block_filters_out,\n                        refine_convs_kernel_size=self.kernel_size,\n                        refine_convs_batch_norm=False,\n                        up_interpolate=up_interpolate,\n                        transpose_convs_filters=block_filters_out,\n                        transpose_convs_batch_norm=False,\n                        prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                    )\n                )\n\n            self.stride_to_filters[next_stride] = block_filters_out\n\n            self.current_strides.append(next_stride)\n            current_stride = next_stride\n            self.residuals += 1\n\n    def forward(\n        self, x: torch.Tensor, features: List[torch.Tensor]\n    ) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the Decoder module.\n\n        Args:\n            x: Input tensor for the decoder.\n            features: List of feature tensors from different encoder levels.\n\n        Returns:\n            outputs: List of output tensors after applying the decoder operations.\n            current_strides: the current strides from the decoder blocks.\n        \"\"\"\n        outputs = {\n            \"outputs\": [],\n        }\n        outputs[\"intermediate_feat\"] = x\n        for i in range(len(self.decoder_stack)):\n            if i &lt; len(features):\n                x = self.decoder_stack[i](x, features[i])\n            else:\n                x = self.decoder_stack[i](x, None)\n            outputs[\"outputs\"].append(x)\n        outputs[\"strides\"] = self.current_strides\n\n        return outputs\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Decoder.__init__","title":"<code>__init__(x_in_shape, output_stride, current_stride, filters=64, up_blocks=4, down_blocks=3, stem_blocks=0, filters_rate=2, convs_per_block=2, kernel_size=3, block_contraction=False, up_interpolate=True, prefix='dec')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    x_in_shape: int,\n    output_stride: int,\n    current_stride: int,\n    filters: int = 64,\n    up_blocks: int = 4,\n    down_blocks: int = 3,\n    stem_blocks: int = 0,\n    filters_rate: int = 2,\n    convs_per_block: int = 2,\n    kernel_size: int = 3,\n    block_contraction: bool = False,\n    up_interpolate: bool = True,\n    prefix: str = \"dec\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.x_in_shape = x_in_shape\n    self.current_stride = current_stride\n    self.filters = filters\n    self.up_blocks = up_blocks\n    self.down_blocks = down_blocks\n    self.stem_blocks = stem_blocks\n    self.filters_rate = filters_rate\n    self.convs_per_block = convs_per_block\n    self.kernel_size = kernel_size\n    self.block_contraction = block_contraction\n    self.prefix = prefix\n    self.stride_to_filters = {}\n\n    self.current_strides = []\n    self.residuals = 0\n\n    self.decoder_stack = nn.ModuleList([])\n\n    self.stride_to_filters[current_stride] = x_in_shape\n\n    for block in range(up_blocks):\n        prev_block_filters = -1 if block == 0 else block_filters_out\n        block_filters_out = int(\n            filters\n            * (filters_rate ** max(0, down_blocks + self.stem_blocks - 1 - block))\n        )\n\n        if self.block_contraction:\n            block_filters_out = int(\n                self.filters\n                * (\n                    self.filters_rate\n                    ** (self.down_blocks + self.stem_blocks - 2 - block)\n                )\n            )\n\n        next_stride = current_stride // 2\n\n        if self.stem_blocks &gt; 0 and block &gt;= down_blocks + self.stem_blocks:\n            # This accounts for the case where we dont have any more down block features to concatenate with.\n            # In this case, add a simple upsampling block with a conv layer and with no concatenation\n            self.decoder_stack.append(\n                SimpleUpsamplingBlock(\n                    x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                    current_stride=current_stride,\n                    upsampling_stride=2,\n                    interp_method=\"bilinear\",\n                    refine_convs=1,\n                    refine_convs_filters=block_filters_out,\n                    refine_convs_kernel_size=self.kernel_size,\n                    refine_convs_batch_norm=False,\n                    up_interpolate=up_interpolate,\n                    transpose_convs_filters=block_filters_out,\n                    transpose_convs_batch_norm=False,\n                    feat_concat=False,\n                    prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                )\n            )\n        else:\n            self.decoder_stack.append(\n                SimpleUpsamplingBlock(\n                    x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                    current_stride=current_stride,\n                    upsampling_stride=2,\n                    interp_method=\"bilinear\",\n                    refine_convs=self.convs_per_block,\n                    refine_convs_filters=block_filters_out,\n                    refine_convs_kernel_size=self.kernel_size,\n                    refine_convs_batch_norm=False,\n                    up_interpolate=up_interpolate,\n                    transpose_convs_filters=block_filters_out,\n                    transpose_convs_batch_norm=False,\n                    prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                )\n            )\n\n        self.stride_to_filters[next_stride] = block_filters_out\n\n        self.current_strides.append(next_stride)\n        current_stride = next_stride\n        self.residuals += 1\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Decoder.forward","title":"<code>forward(x, features)</code>","text":"<p>Forward pass through the Decoder module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor for the decoder.</p> required <code>features</code> <code>List[Tensor]</code> <p>List of feature tensors from different encoder levels.</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>Tuple[List[Tensor], List]</code> <p>List of output tensors after applying the decoder operations. current_strides: the current strides from the decoder blocks.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, features: List[torch.Tensor]\n) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the Decoder module.\n\n    Args:\n        x: Input tensor for the decoder.\n        features: List of feature tensors from different encoder levels.\n\n    Returns:\n        outputs: List of output tensors after applying the decoder operations.\n        current_strides: the current strides from the decoder blocks.\n    \"\"\"\n    outputs = {\n        \"outputs\": [],\n    }\n    outputs[\"intermediate_feat\"] = x\n    for i in range(len(self.decoder_stack)):\n        if i &lt; len(features):\n            x = self.decoder_stack[i](x, features[i])\n        else:\n            x = self.decoder_stack[i](x, None)\n        outputs[\"outputs\"].append(x)\n    outputs[\"strides\"] = self.current_strides\n\n    return outputs\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Encoder","title":"<code>Encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Encoder module for a neural network architecture.</p> <p>This class defines the encoder part of a neural network architecture, which consists of a stack of convolutional blocks for feature extraction.</p> <p>The Encoder consists of a stack of SimpleConvBlocks designed for feature extraction.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 3.</p> <code>3</code> <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 64.</p> <code>64</code> <code>down_blocks</code> <code>int</code> <p>Number of downsampling blocks. Default is 4.</p> <code>4</code> <code>filters_rate</code> <code>Union[float, int]</code> <p>Factor to increase the number of filters per block. Default is 2.</p> <code>2</code> <code>current_stride</code> <code>int</code> <p>Initial stride for pooling operations. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>Union[int, Tuple[int, int]]</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the Encoder module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class Encoder(nn.Module):\n    \"\"\"Encoder module for a neural network architecture.\n\n    This class defines the encoder part of a neural network architecture,\n    which consists of a stack of convolutional blocks for feature extraction.\n\n    The Encoder consists of a stack of SimpleConvBlocks designed for feature extraction.\n\n    Args:\n        in_channels: Number of input channels. Default is 3.\n        filters: Number of filters for the initial block. Default is 64.\n        down_blocks: Number of downsampling blocks. Default is 4.\n        filters_rate: Factor to increase the number of filters per block. Default is 2.\n        current_stride: Initial stride for pooling operations. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        filters: int = 64,\n        down_blocks: int = 4,\n        filters_rate: Union[float, int] = 2,\n        current_stride: int = 2,\n        convs_per_block: int = 2,\n        kernel_size: Union[int, Tuple[int, int]] = 3,\n        stem_blocks: int = 0,\n        prefix: str = \"enc\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.filters = filters\n        self.down_blocks = down_blocks\n        self.filters_rate = filters_rate\n        self.current_stride = current_stride\n        self.convs_per_block = convs_per_block\n        self.kernel_size = kernel_size\n        self.stem_blocks = stem_blocks\n        self.prefix = prefix\n\n        self.encoder_stack = nn.ModuleList([])\n        block_filters = int(filters * (filters_rate ** (stem_blocks - 1)))\n        for block in range(down_blocks):\n            prev_block_filters = -1 if block + self.stem_blocks == 0 else block_filters\n            block_filters = int(filters * (filters_rate ** (block + self.stem_blocks)))\n\n            self.encoder_stack.append(\n                SimpleConvBlock(\n                    in_channels=(\n                        in_channels\n                        if block + self.stem_blocks == 0\n                        else prev_block_filters\n                    ),\n                    pool=(block + self.stem_blocks &gt; 0),\n                    pool_before_convs=True,\n                    pooling_stride=2,\n                    num_convs=convs_per_block,\n                    filters=block_filters,\n                    kernel_size=self.kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"{self.prefix}{block}\",\n                    name=f\"{self.prefix}{block}\",\n                )\n            )\n        after_block_filters = block_filters\n\n        # Add final pooling layer with proper naming\n        block += 1\n        final_pool_dict = OrderedDict()\n        final_pool_dict[f\"{self.prefix}{block}_last_pool\"] = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=2, padding=\"same\"\n        )\n        self.encoder_stack.append(nn.Sequential(final_pool_dict))\n\n        self.intermediate_features = {}\n        for i, block in enumerate(self.encoder_stack):\n            if isinstance(block, SimpleConvBlock) and block.pool:\n                current_stride *= block.pooling_stride\n\n            if current_stride not in self.intermediate_features.values():\n                self.intermediate_features[i] = current_stride\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the Encoder module.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the encoder operations.\n            list: List of intermediate feature tensors from different levels of the encoder.\n        \"\"\"\n        features = []\n\n        for i in range(len(self.encoder_stack)):\n            x = self.encoder_stack[i](x)\n\n            if i in self.intermediate_features.keys():\n                features.append(x)\n\n        return x, features[::-1]\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Encoder.__init__","title":"<code>__init__(in_channels=3, filters=64, down_blocks=4, filters_rate=2, current_stride=2, convs_per_block=2, kernel_size=3, stem_blocks=0, prefix='enc')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int = 3,\n    filters: int = 64,\n    down_blocks: int = 4,\n    filters_rate: Union[float, int] = 2,\n    current_stride: int = 2,\n    convs_per_block: int = 2,\n    kernel_size: Union[int, Tuple[int, int]] = 3,\n    stem_blocks: int = 0,\n    prefix: str = \"enc\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filters = filters\n    self.down_blocks = down_blocks\n    self.filters_rate = filters_rate\n    self.current_stride = current_stride\n    self.convs_per_block = convs_per_block\n    self.kernel_size = kernel_size\n    self.stem_blocks = stem_blocks\n    self.prefix = prefix\n\n    self.encoder_stack = nn.ModuleList([])\n    block_filters = int(filters * (filters_rate ** (stem_blocks - 1)))\n    for block in range(down_blocks):\n        prev_block_filters = -1 if block + self.stem_blocks == 0 else block_filters\n        block_filters = int(filters * (filters_rate ** (block + self.stem_blocks)))\n\n        self.encoder_stack.append(\n            SimpleConvBlock(\n                in_channels=(\n                    in_channels\n                    if block + self.stem_blocks == 0\n                    else prev_block_filters\n                ),\n                pool=(block + self.stem_blocks &gt; 0),\n                pool_before_convs=True,\n                pooling_stride=2,\n                num_convs=convs_per_block,\n                filters=block_filters,\n                kernel_size=self.kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=f\"{self.prefix}{block}\",\n                name=f\"{self.prefix}{block}\",\n            )\n        )\n    after_block_filters = block_filters\n\n    # Add final pooling layer with proper naming\n    block += 1\n    final_pool_dict = OrderedDict()\n    final_pool_dict[f\"{self.prefix}{block}_last_pool\"] = MaxPool2dWithSamePadding(\n        kernel_size=2, stride=2, padding=\"same\"\n    )\n    self.encoder_stack.append(nn.Sequential(final_pool_dict))\n\n    self.intermediate_features = {}\n    for i, block in enumerate(self.encoder_stack):\n        if isinstance(block, SimpleConvBlock) and block.pool:\n            current_stride *= block.pooling_stride\n\n        if current_stride not in self.intermediate_features.values():\n            self.intermediate_features[i] = current_stride\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Encoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the Encoder module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the encoder operations. list: List of intermediate feature tensors from different levels of the encoder.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the Encoder module.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the encoder operations.\n        list: List of intermediate feature tensors from different levels of the encoder.\n    \"\"\"\n    features = []\n\n    for i in range(len(self.encoder_stack)):\n        x = self.encoder_stack[i](x)\n\n        if i in self.intermediate_features.keys():\n            features.append(x)\n\n    return x, features[::-1]\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleConvBlock","title":"<code>SimpleConvBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple convolutional block module.</p> <p>This class defines a convolutional block that consists of convolutional layers, optional pooling layers, batch normalization, and activation functions.</p> <p>The layers within the SimpleConvBlock are organized as follows:</p> <ol> <li>Optional max pooling (with same padding) layer (before convolutional layers).</li> <li>Convolutional layers with specified number of filters, kernel size, and activation.</li> <li>Optional batch normalization layer after each convolutional layer (if batch_norm is True).</li> <li>Activation function after each convolutional layer (ReLU, Sigmoid, Tanh, etc.).</li> <li>Optional max pooling (with same padding) layer (after convolutional layers).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>pool</code> <code>bool</code> <p>Whether to include pooling layers. Default is True.</p> <code>True</code> <code>pooling_stride</code> <code>int</code> <p>Stride for pooling layers. Default is 2.</p> <code>2</code> <code>pool_before_convs</code> <code>bool</code> <p>Whether to apply pooling before convolutional layers. Default is False.</p> <code>False</code> <code>num_convs</code> <code>int</code> <p>Number of convolutional layers. Default is 2.</p> <code>2</code> <code>filters</code> <code>int</code> <p>Number of filters for convolutional layers. Default is 32.</p> <code>32</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias in convolutional layers. Default is True.</p> <code>True</code> <code>batch_norm</code> <code>bool</code> <p>Whether to apply batch normalization. Default is False.</p> <code>False</code> <code>activation</code> <code>Text</code> <p>Activation function name. Default is \"relu\".</p> <code>'relu'</code> Note <p>The 'same' padding is applied using custom MaxPool2dWithSamePadding layers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SimpleConvBlock module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class SimpleConvBlock(nn.Module):\n    \"\"\"A simple convolutional block module.\n\n    This class defines a convolutional block that consists of convolutional layers,\n    optional pooling layers, batch normalization, and activation functions.\n\n    The layers within the SimpleConvBlock are organized as follows:\n\n    1. Optional max pooling (with same padding) layer (before convolutional layers).\n    2. Convolutional layers with specified number of filters, kernel size, and activation.\n    3. Optional batch normalization layer after each convolutional layer (if batch_norm is True).\n    4. Activation function after each convolutional layer (ReLU, Sigmoid, Tanh, etc.).\n    5. Optional max pooling (with same padding) layer (after convolutional layers).\n\n    Args:\n        in_channels: Number of input channels.\n        pool: Whether to include pooling layers. Default is True.\n        pooling_stride: Stride for pooling layers. Default is 2.\n        pool_before_convs: Whether to apply pooling before convolutional layers. Default is False.\n        num_convs: Number of convolutional layers. Default is 2.\n        filters: Number of filters for convolutional layers. Default is 32.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        use_bias: Whether to use bias in convolutional layers. Default is True.\n        batch_norm: Whether to apply batch normalization. Default is False.\n        activation: Activation function name. Default is \"relu\".\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n\n    Note:\n        The 'same' padding is applied using custom MaxPool2dWithSamePadding layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        pool: bool = True,\n        pooling_stride: int = 2,\n        pool_before_convs: bool = False,\n        num_convs: int = 2,\n        filters: int = 32,\n        kernel_size: int = 3,\n        use_bias: bool = True,\n        batch_norm: bool = False,\n        activation: Text = \"relu\",\n        prefix: Text = \"\",\n        name: Text = \"\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.pool = pool\n        self.pooling_stride = pooling_stride\n        self.pool_before_convs = pool_before_convs\n        self.num_convs = num_convs\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.use_bias = use_bias\n        self.batch_norm = batch_norm\n        self.activation = activation\n        self.prefix = prefix\n        self.name = name\n\n        self.blocks = OrderedDict()\n        if pool and pool_before_convs:\n            self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n                kernel_size=2, stride=pooling_stride, padding=\"same\"\n            )\n\n        for i in range(num_convs):\n            self.blocks[f\"{prefix}_conv{i}\"] = nn.Conv2d(\n                in_channels=in_channels if i == 0 else filters,\n                out_channels=filters,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=\"same\",\n                bias=use_bias,\n            )\n\n            if batch_norm:\n                self.blocks[f\"{prefix}_bn{i}\"] = nn.BatchNorm2d(filters)\n\n            self.blocks[f\"{prefix}_act{i}_{activation}\"] = get_act_fn(activation)\n\n        if pool and not pool_before_convs:\n            self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n                kernel_size=2, stride=pooling_stride, padding=\"same\"\n            )\n\n        self.blocks = nn.Sequential(self.blocks)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the SimpleConvBlock module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the convolutional block operations.\n        \"\"\"\n        for block in self.blocks:\n            x = block(x)\n        return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleConvBlock.__init__","title":"<code>__init__(in_channels, pool=True, pooling_stride=2, pool_before_convs=False, num_convs=2, filters=32, kernel_size=3, use_bias=True, batch_norm=False, activation='relu', prefix='', name='')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    pool: bool = True,\n    pooling_stride: int = 2,\n    pool_before_convs: bool = False,\n    num_convs: int = 2,\n    filters: int = 32,\n    kernel_size: int = 3,\n    use_bias: bool = True,\n    batch_norm: bool = False,\n    activation: Text = \"relu\",\n    prefix: Text = \"\",\n    name: Text = \"\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.pool = pool\n    self.pooling_stride = pooling_stride\n    self.pool_before_convs = pool_before_convs\n    self.num_convs = num_convs\n    self.filters = filters\n    self.kernel_size = kernel_size\n    self.use_bias = use_bias\n    self.batch_norm = batch_norm\n    self.activation = activation\n    self.prefix = prefix\n    self.name = name\n\n    self.blocks = OrderedDict()\n    if pool and pool_before_convs:\n        self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=pooling_stride, padding=\"same\"\n        )\n\n    for i in range(num_convs):\n        self.blocks[f\"{prefix}_conv{i}\"] = nn.Conv2d(\n            in_channels=in_channels if i == 0 else filters,\n            out_channels=filters,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=\"same\",\n            bias=use_bias,\n        )\n\n        if batch_norm:\n            self.blocks[f\"{prefix}_bn{i}\"] = nn.BatchNorm2d(filters)\n\n        self.blocks[f\"{prefix}_act{i}_{activation}\"] = get_act_fn(activation)\n\n    if pool and not pool_before_convs:\n        self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=pooling_stride, padding=\"same\"\n        )\n\n    self.blocks = nn.Sequential(self.blocks)\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleConvBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the SimpleConvBlock module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the convolutional block operations.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the SimpleConvBlock module.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the convolutional block operations.\n    \"\"\"\n    for block in self.blocks:\n        x = block(x)\n    return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleUpsamplingBlock","title":"<code>SimpleUpsamplingBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple upsampling and refining block module.</p> <p>This class defines an upsampling and refining block that consists of upsampling layers, convolutional layers for refinement, batch normalization, and activation functions.</p> <p>The block includes: 1. Upsampling layers with adjustable stride and interpolation method. 2. Refinement convolutional layers with customizable parameters. 3. BatchNormalization layers (if specified; can be before or after activation function). 4. Activation functions (default is ReLU) applied before or after BatchNormalization.</p> <p>Parameters:</p> Name Type Description Default <code>x_in_shape</code> <code>int</code> <p>Number of input channels for the feature map.</p> required <code>current_stride</code> <code>int</code> <p>Current stride value to adjust during upsampling.</p> required <code>upsampling_stride</code> <code>int</code> <p>Stride for upsampling. Default is 2.</p> <code>2</code> <code>interp_method</code> <code>Text</code> <p>Interpolation method for upsampling. Default is \"bilinear\".</p> <code>'bilinear'</code> <code>refine_convs</code> <code>int</code> <p>Number of convolutional layers for refinement. Default is 2.</p> <code>2</code> <code>refine_convs_filters</code> <code>int</code> <p>Number of filters for refinement convolutional layers. Default is 64.</p> <code>64</code> <code>refine_convs_kernel_size</code> <code>int</code> <p>Size of the refinement convolutional kernels. Default is 3.</p> <code>3</code> <code>refine_convs_use_bias</code> <code>bool</code> <p>Whether to use bias in refinement convolutional layers. Default is True.</p> <code>True</code> <code>refine_convs_batch_norm</code> <code>bool</code> <p>Whether to apply batch normalization. Default is True.</p> <code>False</code> <code>refine_convs_batch_norm_before_activation</code> <code>bool</code> <p>Whether to apply batch normalization before activation.</p> <code>True</code> <code>refine_convs_activation</code> <code>Text</code> <p>Activation function name. Default is \"relu\".</p> <code>'relu'</code> <code>transpose_convs_filters</code> <code>int</code> <p>Number of filters for Transpose convolutional layers. Default is 64.</p> <code>64</code> <code>transpose_convs_use_bias</code> <code>bool</code> <p>Whether to use bias in Transpose convolutional layers. Default is True.</p> <code>True</code> <code>transpose_convs_batch_norm</code> <code>bool</code> <p>Whether to apply batch normalization for Transpose Conv layers. Default is True.</p> <code>True</code> <code>transpose_convs_batch_norm_before_activation</code> <code>bool</code> <p>Whether to apply batch normalization before activation.</p> <code>True</code> <code>transpose_convs_activation</code> <code>Text</code> <p>Activation function name for Transpose Conv layers. Default is \"relu\".</p> <code>'relu'</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SimpleUpsamplingBlock module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class SimpleUpsamplingBlock(nn.Module):\n    \"\"\"A simple upsampling and refining block module.\n\n    This class defines an upsampling and refining block that consists of upsampling layers,\n    convolutional layers for refinement, batch normalization, and activation functions.\n\n    The block includes:\n    1. Upsampling layers with adjustable stride and interpolation method.\n    2. Refinement convolutional layers with customizable parameters.\n    3. BatchNormalization layers (if specified; can be before or after activation function).\n    4. Activation functions (default is ReLU) applied before or after BatchNormalization.\n\n    Args:\n        x_in_shape: Number of input channels for the feature map.\n        current_stride: Current stride value to adjust during upsampling.\n        upsampling_stride: Stride for upsampling. Default is 2.\n        interp_method: Interpolation method for upsampling. Default is \"bilinear\".\n        refine_convs: Number of convolutional layers for refinement. Default is 2.\n        refine_convs_filters: Number of filters for refinement convolutional layers. Default is 64.\n        refine_convs_kernel_size: Size of the refinement convolutional kernels. Default is 3.\n        refine_convs_use_bias: Whether to use bias in refinement convolutional layers. Default is True.\n        refine_convs_batch_norm: Whether to apply batch normalization. Default is True.\n        refine_convs_batch_norm_before_activation: Whether to apply batch normalization before activation.\n        refine_convs_activation: Activation function name. Default is \"relu\".\n        transpose_convs_filters: Number of filters for Transpose convolutional layers. Default is 64.\n        transpose_convs_use_bias: Whether to use bias in Transpose convolutional layers. Default is True.\n        transpose_convs_batch_norm: Whether to apply batch normalization for Transpose Conv layers. Default is True.\n        transpose_convs_batch_norm_before_activation: Whether to apply batch normalization before activation.\n        transpose_convs_activation: Activation function name for Transpose Conv layers. Default is \"relu\".\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        x_in_shape: int,\n        current_stride: int,\n        upsampling_stride: int = 2,\n        up_interpolate: bool = True,\n        interp_method: Text = \"bilinear\",\n        refine_convs: int = 2,\n        refine_convs_filters: int = 64,\n        refine_convs_kernel_size: int = 3,\n        refine_convs_use_bias: bool = True,\n        refine_convs_batch_norm: bool = False,\n        refine_convs_batch_norm_before_activation: bool = True,\n        refine_convs_activation: Text = \"relu\",\n        transpose_convs_filters: int = 64,\n        transpose_convs_kernel_size: int = 3,\n        transpose_convs_use_bias: bool = True,\n        transpose_convs_batch_norm: bool = True,\n        transpose_convs_batch_norm_before_activation: bool = True,\n        transpose_convs_activation: Text = \"relu\",\n        feat_concat: bool = True,\n        prefix: Text = \"\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.x_in_shape = x_in_shape\n        self.current_stride = current_stride\n        self.upsampling_stride = upsampling_stride\n        self.interp_method = interp_method\n        self.refine_convs = refine_convs\n        self.refine_convs_filters = refine_convs_filters\n        self.refine_convs_kernel_size = refine_convs_kernel_size\n        self.refine_convs_use_bias = refine_convs_use_bias\n        self.refine_convs_batch_norm = refine_convs_batch_norm\n        self.refine_convs_batch_norm_before_activation = (\n            refine_convs_batch_norm_before_activation\n        )\n        self.refine_convs_activation = refine_convs_activation\n        self.up_interpolate = up_interpolate\n        self.feat_concat = feat_concat\n        self.prefix = prefix\n\n        self.blocks = OrderedDict()\n        if current_stride is not None:\n            # Append the strides to the block prefix.\n            new_stride = current_stride // upsampling_stride\n\n        # Upsample via interpolation.\n        if self.up_interpolate:\n            self.blocks[f\"{prefix}_interp_{interp_method}\"] = nn.Upsample(\n                scale_factor=upsampling_stride,\n                mode=interp_method,\n                align_corners=False,\n            )\n        else:\n            # Upsample via strided transposed convolution.\n            # The transpose conv should output the target number of filters\n            self.blocks[f\"{prefix}_trans_conv\"] = nn.ConvTranspose2d(\n                in_channels=x_in_shape,  # Input channels from the input tensor\n                out_channels=transpose_convs_filters,  # Output channels for the upsampled tensor\n                kernel_size=transpose_convs_kernel_size,\n                stride=upsampling_stride,\n                output_padding=1,\n                padding=1,\n                bias=transpose_convs_use_bias,\n            )\n            self.norm_act_layers = 1\n            if (\n                transpose_convs_batch_norm\n                and transpose_convs_batch_norm_before_activation\n            ):\n                self.blocks[f\"{prefix}_trans_conv_bn\"] = nn.BatchNorm2d(\n                    num_features=transpose_convs_filters\n                )\n                self.norm_act_layers += 1\n\n            self.blocks[f\"{prefix}_trans_conv_act_{transpose_convs_activation}\"] = (\n                get_act_fn(transpose_convs_activation)\n            )\n            self.norm_act_layers += 1\n\n            if (\n                transpose_convs_batch_norm\n                and not transpose_convs_batch_norm_before_activation\n            ):\n                self.blocks[f\"{prefix}_trans_conv_bn_after\"] = nn.BatchNorm2d(\n                    num_features=transpose_convs_filters\n                )\n                self.norm_act_layers += 1\n\n        # Add further convolutions to refine after upsampling and/or skip.\n        for i in range(refine_convs):\n            filters = refine_convs_filters\n            # For the first conv, calculate the actual input channels after concatenation\n            if i == 0:\n                if not self.feat_concat:\n                    first_conv_in_channels = refine_convs_filters\n                else:\n                    if self.up_interpolate:\n                        # With interpolation, input is x_in_shape + feature channels\n                        # The feature channels are the same as x_in_shape since they come from the same level\n                        first_conv_in_channels = x_in_shape + refine_convs_filters\n                    else:\n                        # With transpose conv, input is transpose_conv_output + feature channels\n                        first_conv_in_channels = (\n                            refine_convs_filters + transpose_convs_filters\n                        )\n            else:\n                if not self.feat_concat:\n                    first_conv_in_channels = refine_convs_filters\n                first_conv_in_channels = filters\n\n            self.blocks[f\"{prefix}_refine_conv{i}\"] = nn.Conv2d(\n                in_channels=int(first_conv_in_channels),\n                out_channels=int(filters),\n                kernel_size=refine_convs_kernel_size,\n                stride=1,\n                padding=\"same\",\n                bias=refine_convs_use_bias,\n            )\n\n            if refine_convs_batch_norm and refine_convs_batch_norm_before_activation:\n                self.blocks[f\"{prefix}_refine_conv{i}_bn\"] = nn.BatchNorm2d(\n                    num_features=refine_convs_filters\n                )\n\n            self.blocks[f\"{prefix}_refine_conv{i}_act_{refine_convs_activation}\"] = (\n                get_act_fn(refine_convs_activation)\n            )\n\n            if (\n                refine_convs_batch_norm\n                and not refine_convs_batch_norm_before_activation\n            ):\n                self.blocks[f\"{prefix}_refine_conv_bn_after{i}\"] = nn.BatchNorm2d(\n                    num_features=refine_convs_filters\n                )\n\n        self.blocks = nn.Sequential(self.blocks)\n\n    def forward(self, x: torch.Tensor, feature: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the SimpleUpsamplingBlock module.\n\n        Args:\n            x: Input tensor.\n            feature: Feature tensor to be concatenated with the upsampled tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the upsampling and refining operations.\n        \"\"\"\n        for idx, b in enumerate(self.blocks):\n            if (\n                not self.up_interpolate\n                and idx == self.norm_act_layers\n                and feature is not None\n            ):\n                x = torch.concat((feature, x), dim=1)\n            elif (\n                self.up_interpolate and idx == 1 and feature is not None\n            ):  # Right after upsampling or convtranspose2d.\n                x = torch.concat((feature, x), dim=1)\n            x = b(x)\n        return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleUpsamplingBlock.__init__","title":"<code>__init__(x_in_shape, current_stride, upsampling_stride=2, up_interpolate=True, interp_method='bilinear', refine_convs=2, refine_convs_filters=64, refine_convs_kernel_size=3, refine_convs_use_bias=True, refine_convs_batch_norm=False, refine_convs_batch_norm_before_activation=True, refine_convs_activation='relu', transpose_convs_filters=64, transpose_convs_kernel_size=3, transpose_convs_use_bias=True, transpose_convs_batch_norm=True, transpose_convs_batch_norm_before_activation=True, transpose_convs_activation='relu', feat_concat=True, prefix='')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    x_in_shape: int,\n    current_stride: int,\n    upsampling_stride: int = 2,\n    up_interpolate: bool = True,\n    interp_method: Text = \"bilinear\",\n    refine_convs: int = 2,\n    refine_convs_filters: int = 64,\n    refine_convs_kernel_size: int = 3,\n    refine_convs_use_bias: bool = True,\n    refine_convs_batch_norm: bool = False,\n    refine_convs_batch_norm_before_activation: bool = True,\n    refine_convs_activation: Text = \"relu\",\n    transpose_convs_filters: int = 64,\n    transpose_convs_kernel_size: int = 3,\n    transpose_convs_use_bias: bool = True,\n    transpose_convs_batch_norm: bool = True,\n    transpose_convs_batch_norm_before_activation: bool = True,\n    transpose_convs_activation: Text = \"relu\",\n    feat_concat: bool = True,\n    prefix: Text = \"\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.x_in_shape = x_in_shape\n    self.current_stride = current_stride\n    self.upsampling_stride = upsampling_stride\n    self.interp_method = interp_method\n    self.refine_convs = refine_convs\n    self.refine_convs_filters = refine_convs_filters\n    self.refine_convs_kernel_size = refine_convs_kernel_size\n    self.refine_convs_use_bias = refine_convs_use_bias\n    self.refine_convs_batch_norm = refine_convs_batch_norm\n    self.refine_convs_batch_norm_before_activation = (\n        refine_convs_batch_norm_before_activation\n    )\n    self.refine_convs_activation = refine_convs_activation\n    self.up_interpolate = up_interpolate\n    self.feat_concat = feat_concat\n    self.prefix = prefix\n\n    self.blocks = OrderedDict()\n    if current_stride is not None:\n        # Append the strides to the block prefix.\n        new_stride = current_stride // upsampling_stride\n\n    # Upsample via interpolation.\n    if self.up_interpolate:\n        self.blocks[f\"{prefix}_interp_{interp_method}\"] = nn.Upsample(\n            scale_factor=upsampling_stride,\n            mode=interp_method,\n            align_corners=False,\n        )\n    else:\n        # Upsample via strided transposed convolution.\n        # The transpose conv should output the target number of filters\n        self.blocks[f\"{prefix}_trans_conv\"] = nn.ConvTranspose2d(\n            in_channels=x_in_shape,  # Input channels from the input tensor\n            out_channels=transpose_convs_filters,  # Output channels for the upsampled tensor\n            kernel_size=transpose_convs_kernel_size,\n            stride=upsampling_stride,\n            output_padding=1,\n            padding=1,\n            bias=transpose_convs_use_bias,\n        )\n        self.norm_act_layers = 1\n        if (\n            transpose_convs_batch_norm\n            and transpose_convs_batch_norm_before_activation\n        ):\n            self.blocks[f\"{prefix}_trans_conv_bn\"] = nn.BatchNorm2d(\n                num_features=transpose_convs_filters\n            )\n            self.norm_act_layers += 1\n\n        self.blocks[f\"{prefix}_trans_conv_act_{transpose_convs_activation}\"] = (\n            get_act_fn(transpose_convs_activation)\n        )\n        self.norm_act_layers += 1\n\n        if (\n            transpose_convs_batch_norm\n            and not transpose_convs_batch_norm_before_activation\n        ):\n            self.blocks[f\"{prefix}_trans_conv_bn_after\"] = nn.BatchNorm2d(\n                num_features=transpose_convs_filters\n            )\n            self.norm_act_layers += 1\n\n    # Add further convolutions to refine after upsampling and/or skip.\n    for i in range(refine_convs):\n        filters = refine_convs_filters\n        # For the first conv, calculate the actual input channels after concatenation\n        if i == 0:\n            if not self.feat_concat:\n                first_conv_in_channels = refine_convs_filters\n            else:\n                if self.up_interpolate:\n                    # With interpolation, input is x_in_shape + feature channels\n                    # The feature channels are the same as x_in_shape since they come from the same level\n                    first_conv_in_channels = x_in_shape + refine_convs_filters\n                else:\n                    # With transpose conv, input is transpose_conv_output + feature channels\n                    first_conv_in_channels = (\n                        refine_convs_filters + transpose_convs_filters\n                    )\n        else:\n            if not self.feat_concat:\n                first_conv_in_channels = refine_convs_filters\n            first_conv_in_channels = filters\n\n        self.blocks[f\"{prefix}_refine_conv{i}\"] = nn.Conv2d(\n            in_channels=int(first_conv_in_channels),\n            out_channels=int(filters),\n            kernel_size=refine_convs_kernel_size,\n            stride=1,\n            padding=\"same\",\n            bias=refine_convs_use_bias,\n        )\n\n        if refine_convs_batch_norm and refine_convs_batch_norm_before_activation:\n            self.blocks[f\"{prefix}_refine_conv{i}_bn\"] = nn.BatchNorm2d(\n                num_features=refine_convs_filters\n            )\n\n        self.blocks[f\"{prefix}_refine_conv{i}_act_{refine_convs_activation}\"] = (\n            get_act_fn(refine_convs_activation)\n        )\n\n        if (\n            refine_convs_batch_norm\n            and not refine_convs_batch_norm_before_activation\n        ):\n            self.blocks[f\"{prefix}_refine_conv_bn_after{i}\"] = nn.BatchNorm2d(\n                num_features=refine_convs_filters\n            )\n\n    self.blocks = nn.Sequential(self.blocks)\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleUpsamplingBlock.forward","title":"<code>forward(x, feature)</code>","text":"<p>Forward pass through the SimpleUpsamplingBlock module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>feature</code> <code>Tensor</code> <p>Feature tensor to be concatenated with the upsampled tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the upsampling and refining operations.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor, feature: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the SimpleUpsamplingBlock module.\n\n    Args:\n        x: Input tensor.\n        feature: Feature tensor to be concatenated with the upsampled tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the upsampling and refining operations.\n    \"\"\"\n    for idx, b in enumerate(self.blocks):\n        if (\n            not self.up_interpolate\n            and idx == self.norm_act_layers\n            and feature is not None\n        ):\n            x = torch.concat((feature, x), dim=1)\n        elif (\n            self.up_interpolate and idx == 1 and feature is not None\n        ):  # Right after upsampling or convtranspose2d.\n            x = torch.concat((feature, x), dim=1)\n        x = b(x)\n    return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.StemBlock","title":"<code>StemBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Stem block module for initial feature extraction.</p> <p>This class defines a stem block that consists of a stack of convolutional blocks for initial feature extraction before the main encoder. The stem blocks are typically used for initial downsampling and feature extraction.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 3.</p> <code>3</code> <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 64.</p> <code>64</code> <code>stem_blocks</code> <code>int</code> <p>Number of stem blocks. Default is 0.</p> <code>0</code> <code>filters_rate</code> <code>Union[float, int]</code> <p>Factor to increase the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 7.</p> <code>7</code> <code>prefix</code> <code>str</code> <p>Prefix for layer naming. Default is \"stem\".</p> <code>'stem'</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the StemBlock module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class StemBlock(nn.Module):\n    \"\"\"Stem block module for initial feature extraction.\n\n    This class defines a stem block that consists of a stack of convolutional blocks\n    for initial feature extraction before the main encoder. The stem blocks are typically\n    used for initial downsampling and feature extraction.\n\n    Args:\n        in_channels: Number of input channels. Default is 3.\n        filters: Number of filters for the initial block. Default is 64.\n        stem_blocks: Number of stem blocks. Default is 0.\n        filters_rate: Factor to increase the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 7.\n        prefix: Prefix for layer naming. Default is \"stem\".\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        filters: int = 64,\n        stem_blocks: int = 0,\n        filters_rate: Union[float, int] = 2,\n        convs_per_block: int = 2,\n        kernel_size: int = 7,\n        prefix: str = \"stem\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.filters = filters\n        self.stem_blocks = stem_blocks\n        self.filters_rate = filters_rate\n        self.convs_per_block = convs_per_block\n        self.kernel_size = kernel_size\n        self.prefix = prefix\n\n        self.stem_stack = nn.ModuleList([])\n\n        for block in range(self.stem_blocks):\n            prev_block_filters = in_channels if block == 0 else block_filters\n            block_filters = int(self.filters * (self.filters_rate**block))\n\n            self.stem_stack.append(\n                SimpleConvBlock(\n                    in_channels=prev_block_filters,\n                    pool=(block &gt; 0),\n                    pool_before_convs=True,\n                    pooling_stride=2,\n                    num_convs=convs_per_block,\n                    filters=block_filters,\n                    kernel_size=kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"{prefix}{block}\",\n                )\n            )\n\n        # Always finish with a pooling block to account for pooling before convs.\n        final_pool_dict = OrderedDict()\n        final_pool_dict[f\"{self.prefix}{block+1}_last_pool\"] = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=2, padding=\"same\"\n        )\n        self.stem_stack.append(nn.Sequential(final_pool_dict))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the StemBlock module.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the stem operations.\n        \"\"\"\n        for block in self.stem_stack:\n            x = block(x)\n        return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.StemBlock.__init__","title":"<code>__init__(in_channels=3, filters=64, stem_blocks=0, filters_rate=2, convs_per_block=2, kernel_size=7, prefix='stem')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int = 3,\n    filters: int = 64,\n    stem_blocks: int = 0,\n    filters_rate: Union[float, int] = 2,\n    convs_per_block: int = 2,\n    kernel_size: int = 7,\n    prefix: str = \"stem\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filters = filters\n    self.stem_blocks = stem_blocks\n    self.filters_rate = filters_rate\n    self.convs_per_block = convs_per_block\n    self.kernel_size = kernel_size\n    self.prefix = prefix\n\n    self.stem_stack = nn.ModuleList([])\n\n    for block in range(self.stem_blocks):\n        prev_block_filters = in_channels if block == 0 else block_filters\n        block_filters = int(self.filters * (self.filters_rate**block))\n\n        self.stem_stack.append(\n            SimpleConvBlock(\n                in_channels=prev_block_filters,\n                pool=(block &gt; 0),\n                pool_before_convs=True,\n                pooling_stride=2,\n                num_convs=convs_per_block,\n                filters=block_filters,\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=f\"{prefix}{block}\",\n            )\n        )\n\n    # Always finish with a pooling block to account for pooling before convs.\n    final_pool_dict = OrderedDict()\n    final_pool_dict[f\"{self.prefix}{block+1}_last_pool\"] = MaxPool2dWithSamePadding(\n        kernel_size=2, stride=2, padding=\"same\"\n    )\n    self.stem_stack.append(nn.Sequential(final_pool_dict))\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.StemBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the StemBlock module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the stem operations.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the StemBlock module.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the stem operations.\n    \"\"\"\n    for block in self.stem_stack:\n        x = block(x)\n    return x\n</code></pre>"},{"location":"api/architectures/heads/","title":"heads","text":""},{"location":"api/architectures/heads/#sleap_nn.architectures.heads","title":"<code>sleap_nn.architectures.heads</code>","text":"<p>Model head definitions for defining model output types.</p> <p>Classes:</p> Name Description <code>CenteredInstanceConfmapsHead</code> <p>Head for specifying centered instance confidence maps.</p> <code>CentroidConfmapsHead</code> <p>Head for specifying instance centroid confidence maps.</p> <code>ClassMapsHead</code> <p>Head for specifying class identity maps.</p> <code>ClassVectorsHead</code> <p>Head for specifying classification heads.</p> <code>Head</code> <p>Base class for model output heads.</p> <code>MultiInstanceConfmapsHead</code> <p>Head for specifying multi-instance confidence maps.</p> <code>OffsetRefinementHead</code> <p>Head for specifying offset refinement maps.</p> <code>PartAffinityFieldsHead</code> <p>Head for specifying multi-instance part affinity fields.</p> <code>SingleInstanceConfmapsHead</code> <p>Head for specifying single instance confidence maps.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead","title":"<code>CenteredInstanceConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying centered instance confidence maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>anchor_part</code> <p>Name of the part to use as an anchor node. If not specified, the bounding box centroid will be used.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class CenteredInstanceConfmapsHead(Head):\n    \"\"\"Head for specifying centered instance confidence maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        anchor_part: Name of the part to use as an anchor node. If not specified, the\n            bounding box centroid will be used.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        anchor_part: Optional[Text] = None,\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.anchor_part = anchor_part\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.part_names)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n    ) -&gt; \"CenteredInstanceConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.part_names is not None:\n            part_names = config.part_names\n        elif part_names is None:\n            message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            anchor_part=config.anchor_part,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead.__init__","title":"<code>__init__(part_names, anchor_part=None, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    anchor_part: Optional[Text] = None,\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.anchor_part = anchor_part\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead.from_config","title":"<code>from_config(config, part_names=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>CenteredInstanceConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n) -&gt; \"CenteredInstanceConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.part_names is not None:\n        part_names = config.part_names\n    elif part_names is None:\n        message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        anchor_part=config.anchor_part,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead","title":"<code>CentroidConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying instance centroid confidence maps.</p> <p>Attributes:</p> Name Type Description <code>anchor_part</code> <p>Name of the part to use as an anchor node. If not specified, the bounding box centroid will be used.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class CentroidConfmapsHead(Head):\n    \"\"\"Head for specifying instance centroid confidence maps.\n\n    Attributes:\n        anchor_part: Name of the part to use as an anchor node. If not specified, the\n            bounding box centroid will be used.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor_part: Optional[Text] = None,\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.anchor_part = anchor_part\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return 1\n\n    @classmethod\n    def from_config(cls, config: DictConfig) -&gt; \"CentroidConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        return cls(\n            anchor_part=config.anchor_part,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead.__init__","title":"<code>__init__(anchor_part=None, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    anchor_part: Optional[Text] = None,\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.anchor_part = anchor_part\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <p>Returns:</p> Type Description <code>CentroidConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(cls, config: DictConfig) -&gt; \"CentroidConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    return cls(\n        anchor_part=config.anchor_part,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead","title":"<code>ClassMapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying class identity maps.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <p>List of string names of the classes.</p> <code>sigma</code> <p>Spread of the class maps around each node.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class ClassMapsHead(Head):\n    \"\"\"Head for specifying class identity maps.\n\n    Attributes:\n        classes: List of string names of the classes.\n        sigma: Spread of the class maps around each node.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        classes: List[Text],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.classes = classes\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.classes)\n\n    @property\n    def activation(self) -&gt; str:\n        \"\"\"Return the activation function of the head output layer.\"\"\"\n        return \"sigmoid\"\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        classes: Optional[List[Text]] = None,\n    ) -&gt; \"ClassMapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n            classes: List of string names of the classes that this head will predict.\n                This must be set if the `classes` attribute of the configuration is not\n                set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.classes is not None:\n            classes = config.classes\n        return cls(\n            classes=classes,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.activation","title":"<code>activation</code>  <code>property</code>","text":"<p>Return the activation function of the head output layer.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.__init__","title":"<code>__init__(classes, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    classes: List[Text],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.classes = classes\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.from_config","title":"<code>from_config(config, classes=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>classes</code> <p>List of string names of the classes that this head will predict. This must be set if the <code>classes</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>ClassMapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    classes: Optional[List[Text]] = None,\n) -&gt; \"ClassMapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n        classes: List of string names of the classes that this head will predict.\n            This must be set if the `classes` attribute of the configuration is not\n            set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.classes is not None:\n        classes = config.classes\n    return cls(\n        classes=classes,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead","title":"<code>ClassVectorsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying classification heads.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <p>List of string names of the classes.</p> <code>num_fc_layers</code> <p>Number of fully connected layers after flattening input features.</p> <code>num_fc_units</code> <p>Number of units (dimensions) in fully connected layers prior to classification output.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> <code>make_head</code> <p>Make head output tensor from input feature tensor.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class ClassVectorsHead(Head):\n    \"\"\"Head for specifying classification heads.\n\n    Attributes:\n        classes: List of string names of the classes.\n        num_fc_layers: Number of fully connected layers after flattening input features.\n        num_fc_units: Number of units (dimensions) in fully connected layers prior to\n            classification output.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        classes: List[Text],\n        num_fc_layers: int = 1,\n        num_fc_units: int = 64,\n        global_pool: bool = True,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.classes = classes\n        self.num_fc_layers = num_fc_layers\n        self.num_fc_units = num_fc_units\n        self.global_pool = global_pool\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.classes)\n\n    @property\n    def activation(self) -&gt; str:\n        \"\"\"Return the activation function of the head output layer.\"\"\"\n        return \"softmax\"\n\n    @property\n    def loss_function(self) -&gt; str:\n        \"\"\"Return the name of the loss function to use for this head.\"\"\"\n        return \"categorical_crossentropy\"\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        classes: Optional[List[Text]] = None,\n    ) -&gt; \"ClassVectorsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n            classes: List of string names of the classes that this head will predict.\n                This must be set if the `classes` attribute of the configuration is not\n                set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.classes is not None:\n            classes = config.classes\n        return cls(\n            classes=classes,\n            num_fc_layers=config.num_fc_layers,\n            num_fc_units=config.num_fc_units,\n            global_pool=config.global_pool,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n\n    def make_head(self, x_in: int) -&gt; nn.Sequential:\n        \"\"\"Make head output tensor from input feature tensor.\n\n        Args:\n            x_in: An int for the input shape after applying AdaptiveMaxPool2d on dim=1, assuming inputs of shape (B, C, H, W).\n\n        Returns:\n            A `nn.Sequential` with the correct shape for the head.\n        \"\"\"\n        from collections import OrderedDict\n\n        module_dict = OrderedDict()\n\n        if self.global_pool:\n            module_dict[f\"pre_classification_global_pool\"] = nn.AdaptiveMaxPool2d(1)\n\n        module_dict[f\"pre_classification_flatten\"] = nn.Flatten(start_dim=1)\n\n        for i in range(self.num_fc_layers):\n            if i == 0:\n                module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                    x_in, self.num_fc_units\n                )\n            else:\n                module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                    self.num_fc_units, self.num_fc_units\n                )\n            module_dict[f\"pre_classification{i}_relu\"] = get_act_fn(\"relu\")\n\n        module_dict[f\"ClassVectorsHead\"] = nn.Linear(self.num_fc_units, self.channels)\n        module_dict[f\"softmax\"] = get_act_fn(\"softmax\")\n\n        return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.activation","title":"<code>activation</code>  <code>property</code>","text":"<p>Return the activation function of the head output layer.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.loss_function","title":"<code>loss_function</code>  <code>property</code>","text":"<p>Return the name of the loss function to use for this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.__init__","title":"<code>__init__(classes, num_fc_layers=1, num_fc_units=64, global_pool=True, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    classes: List[Text],\n    num_fc_layers: int = 1,\n    num_fc_units: int = 64,\n    global_pool: bool = True,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.classes = classes\n    self.num_fc_layers = num_fc_layers\n    self.num_fc_units = num_fc_units\n    self.global_pool = global_pool\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.from_config","title":"<code>from_config(config, classes=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>classes</code> <p>List of string names of the classes that this head will predict. This must be set if the <code>classes</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>ClassVectorsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    classes: Optional[List[Text]] = None,\n) -&gt; \"ClassVectorsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n        classes: List of string names of the classes that this head will predict.\n            This must be set if the `classes` attribute of the configuration is not\n            set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.classes is not None:\n        classes = config.classes\n    return cls(\n        classes=classes,\n        num_fc_layers=config.num_fc_layers,\n        num_fc_units=config.num_fc_units,\n        global_pool=config.global_pool,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.make_head","title":"<code>make_head(x_in)</code>","text":"<p>Make head output tensor from input feature tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_in</code> <code>int</code> <p>An int for the input shape after applying AdaptiveMaxPool2d on dim=1, assuming inputs of shape (B, C, H, W).</p> required <p>Returns:</p> Type Description <code>Sequential</code> <p>A <code>nn.Sequential</code> with the correct shape for the head.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def make_head(self, x_in: int) -&gt; nn.Sequential:\n    \"\"\"Make head output tensor from input feature tensor.\n\n    Args:\n        x_in: An int for the input shape after applying AdaptiveMaxPool2d on dim=1, assuming inputs of shape (B, C, H, W).\n\n    Returns:\n        A `nn.Sequential` with the correct shape for the head.\n    \"\"\"\n    from collections import OrderedDict\n\n    module_dict = OrderedDict()\n\n    if self.global_pool:\n        module_dict[f\"pre_classification_global_pool\"] = nn.AdaptiveMaxPool2d(1)\n\n    module_dict[f\"pre_classification_flatten\"] = nn.Flatten(start_dim=1)\n\n    for i in range(self.num_fc_layers):\n        if i == 0:\n            module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                x_in, self.num_fc_units\n            )\n        else:\n            module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                self.num_fc_units, self.num_fc_units\n            )\n        module_dict[f\"pre_classification{i}_relu\"] = get_act_fn(\"relu\")\n\n    module_dict[f\"ClassVectorsHead\"] = nn.Linear(self.num_fc_units, self.channels)\n    module_dict[f\"softmax\"] = get_act_fn(\"softmax\")\n\n    return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head","title":"<code>Head</code>","text":"<p>Base class for model output heads.</p> <p>Attributes:</p> Name Type Description <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>make_head</code> <p>Make head output tensor from input feature tensor.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class Head:\n    \"\"\"Base class for model output heads.\n\n    Attributes:\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(self, output_stride: int = 1, loss_weight: float = 1.0) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        self.output_stride = output_stride\n        self.loss_weight = loss_weight\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the head.\"\"\"\n        return type(self).__name__\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        message = \"Subclasses must implement this method.\"\n        logger.error(message)\n        raise NotImplementedError(message)\n\n    @property\n    def activation(self) -&gt; str:\n        \"\"\"Return the activation function of the head output layer.\"\"\"\n        return \"identity\"\n\n    @property\n    def loss_function(self) -&gt; str:\n        \"\"\"Return the name of the loss function to use for this head.\"\"\"\n        return \"mse\"\n\n    def make_head(self, x_in: int) -&gt; nn.Sequential:\n        \"\"\"Make head output tensor from input feature tensor.\n\n        Args:\n            x_in: An int input for the input channels.\n\n        Returns:\n            A `nn.Sequential` with the correct shape for the head.\n        \"\"\"\n        module_dict = OrderedDict()\n        module_dict[self.name] = nn.Sequential(\n            nn.Conv2d(\n                in_channels=x_in,\n                out_channels=self.channels,\n                kernel_size=1,\n                stride=1,\n                padding=\"same\",\n            ),\n            get_act_fn(self.activation),\n        )\n\n        return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.activation","title":"<code>activation</code>  <code>property</code>","text":"<p>Return the activation function of the head output layer.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.loss_function","title":"<code>loss_function</code>  <code>property</code>","text":"<p>Return the name of the loss function to use for this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name of the head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.__init__","title":"<code>__init__(output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(self, output_stride: int = 1, loss_weight: float = 1.0) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    self.output_stride = output_stride\n    self.loss_weight = loss_weight\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.make_head","title":"<code>make_head(x_in)</code>","text":"<p>Make head output tensor from input feature tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_in</code> <code>int</code> <p>An int input for the input channels.</p> required <p>Returns:</p> Type Description <code>Sequential</code> <p>A <code>nn.Sequential</code> with the correct shape for the head.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def make_head(self, x_in: int) -&gt; nn.Sequential:\n    \"\"\"Make head output tensor from input feature tensor.\n\n    Args:\n        x_in: An int input for the input channels.\n\n    Returns:\n        A `nn.Sequential` with the correct shape for the head.\n    \"\"\"\n    module_dict = OrderedDict()\n    module_dict[self.name] = nn.Sequential(\n        nn.Conv2d(\n            in_channels=x_in,\n            out_channels=self.channels,\n            kernel_size=1,\n            stride=1,\n            padding=\"same\",\n        ),\n        get_act_fn(self.activation),\n    )\n\n    return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead","title":"<code>MultiInstanceConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying multi-instance confidence maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class MultiInstanceConfmapsHead(Head):\n    \"\"\"Head for specifying multi-instance confidence maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.part_names)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n    ) -&gt; \"MultiInstanceConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.part_names is not None:\n            part_names = config.part_names\n        elif part_names is None:\n            message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead.__init__","title":"<code>__init__(part_names, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead.from_config","title":"<code>from_config(config, part_names=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>MultiInstanceConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n) -&gt; \"MultiInstanceConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.part_names is not None:\n        part_names = config.part_names\n    elif part_names is None:\n        message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead","title":"<code>OffsetRefinementHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying offset refinement maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>sigma_threshold</code> <p>Threshold of confidence map values to use for defining the boundary of the offset maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class OffsetRefinementHead(Head):\n    \"\"\"Head for specifying offset refinement maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        sigma_threshold: Threshold of confidence map values to use for defining the\n            boundary of the offset maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        sigma_threshold: float = 0.2,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.sigma_threshold = sigma_threshold\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return int(len(self.part_names) * 2)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n        sigma_threshold: float = 0.2,\n        loss_weight: float = 1.0,\n    ) -&gt; \"OffsetRefinementHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n            sigma_threshold: Minimum confidence map value below which offsets will be\n                replaced with zeros.\n            loss_weight: Weight of the loss associated with this head.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if hasattr(config, \"part_names\"):\n            if config.part_names is not None:\n                part_names = config.part_names\n        elif hasattr(config, \"anchor_part\"):\n            part_names = [config.anchor_part]\n        else:\n            message = \"Required attribute 'part_names' is missing in the configuration.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            output_stride=config.output_stride,\n            sigma_threshold=sigma_threshold,\n            loss_weight=loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead.__init__","title":"<code>__init__(part_names, sigma_threshold=0.2, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    sigma_threshold: float = 0.2,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.sigma_threshold = sigma_threshold\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead.from_config","title":"<code>from_config(config, part_names=None, sigma_threshold=0.2, loss_weight=1.0)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <code>sigma_threshold</code> <p>Minimum confidence map value below which offsets will be replaced with zeros.</p> <code>loss_weight</code> <p>Weight of the loss associated with this head.</p> <p>Returns:</p> Type Description <code>OffsetRefinementHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n    sigma_threshold: float = 0.2,\n    loss_weight: float = 1.0,\n) -&gt; \"OffsetRefinementHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n        sigma_threshold: Minimum confidence map value below which offsets will be\n            replaced with zeros.\n        loss_weight: Weight of the loss associated with this head.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if hasattr(config, \"part_names\"):\n        if config.part_names is not None:\n            part_names = config.part_names\n    elif hasattr(config, \"anchor_part\"):\n        part_names = [config.anchor_part]\n    else:\n        message = \"Required attribute 'part_names' is missing in the configuration.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        output_stride=config.output_stride,\n        sigma_threshold=sigma_threshold,\n        loss_weight=loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead","title":"<code>PartAffinityFieldsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying multi-instance part affinity fields.</p> <p>Attributes:</p> Name Type Description <code>edges</code> <p>List of tuples of <code>(source, destination)</code> node names.</p> <code>sigma</code> <p>Spread of the part affinity fields.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class PartAffinityFieldsHead(Head):\n    \"\"\"Head for specifying multi-instance part affinity fields.\n\n    Attributes:\n        edges: List of tuples of `(source, destination)` node names.\n        sigma: Spread of the part affinity fields.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        edges: Sequence[Tuple[Text, Text]],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.edges = edges\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return int(len(self.edges) * 2)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        edges: Optional[Sequence[Tuple[Text, Text]]] = None,\n    ) -&gt; \"PartAffinityFieldsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            edges: List of 2-tuples of the form `(source_node, destination_node)` that\n                define pairs of text names of the directed edges of the graph. This must\n                be set if the `edges` attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.edges is not None:\n            edges = config.edges\n        return cls(\n            edges=edges,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead.__init__","title":"<code>__init__(edges, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    edges: Sequence[Tuple[Text, Text]],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.edges = edges\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead.from_config","title":"<code>from_config(config, edges=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>edges</code> <p>List of 2-tuples of the form <code>(source_node, destination_node)</code> that define pairs of text names of the directed edges of the graph. This must be set if the <code>edges</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>PartAffinityFieldsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    edges: Optional[Sequence[Tuple[Text, Text]]] = None,\n) -&gt; \"PartAffinityFieldsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        edges: List of 2-tuples of the form `(source_node, destination_node)` that\n            define pairs of text names of the directed edges of the graph. This must\n            be set if the `edges` attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.edges is not None:\n        edges = config.edges\n    return cls(\n        edges=edges,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead","title":"<code>SingleInstanceConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying single instance confidence maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class SingleInstanceConfmapsHead(Head):\n    \"\"\"Head for specifying single instance confidence maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.part_names)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n    ) -&gt; \"SingleInstanceConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.part_names is not None:\n            part_names = config.part_names\n        elif part_names is None:\n            message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead.__init__","title":"<code>__init__(part_names, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead.from_config","title":"<code>from_config(config, part_names=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>SingleInstanceConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n) -&gt; \"SingleInstanceConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.part_names is not None:\n        part_names = config.part_names\n    elif part_names is None:\n        message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/model/","title":"model","text":""},{"location":"api/architectures/model/#sleap_nn.architectures.model","title":"<code>sleap_nn.architectures.model</code>","text":"<p>This module defines the main SLEAP model class for defining a trainable model.</p> <p>This is a higher level wrapper around <code>nn.Module</code> that holds all the configuration parameters required to construct the actual model. This allows for easy querying of the model configuration without actually instantiating the model itself.</p> <p>Classes:</p> Name Description <code>Model</code> <p>Model creates a model consisting of a backbone and head.</p> <p>Functions:</p> Name Description <code>get_backbone</code> <p>Get a backbone model <code>nn.Module</code> based on the provided name.</p> <code>get_head</code> <p>Get a head <code>nn.Module</code> based on the provided name.</p>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Module</code></p> <p>Model creates a model consisting of a backbone and head.</p> <p>Attributes:</p> Name Type Description <code>backbone_type</code> <p>Backbone type. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>backbone_config</code> <p>An <code>DictConfig</code> configuration dictionary for the model backbone.</p> <code>head_configs</code> <p>An <code>DictConfig</code> configuration dictionary for the model heads.</p> <code>model_type</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the backbone and head based on the backbone_config.</p> <code>forward</code> <p>Forward pass through the model.</p> <code>from_config</code> <p>Create the model from a config dictionary.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>class Model(nn.Module):\n    \"\"\"Model creates a model consisting of a backbone and head.\n\n    Attributes:\n        backbone_type: Backbone type. One of `unet`, `convnext` and `swint`.\n        backbone_config: An `DictConfig` configuration dictionary for the model backbone.\n        head_configs: An `DictConfig` configuration dictionary for the model heads.\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone_type: str,\n        backbone_config: DictConfig,\n        head_configs: DictConfig,\n        model_type: str,\n    ) -&gt; None:\n        \"\"\"Initialize the backbone and head based on the backbone_config.\"\"\"\n        super().__init__()\n        self.backbone_type = backbone_type\n        self.backbone_config = backbone_config\n        self.head_configs = head_configs\n\n        self.heads = get_head(model_type, self.head_configs)\n\n        output_strides = []\n        for head_type in head_configs:\n            head_config = head_configs[head_type]\n            output_strides.append(head_config.output_stride)\n\n        min_output_stride = min(output_strides)\n        min_output_stride = min(min_output_stride, self.backbone_config.output_stride)\n\n        self.backbone = get_backbone(\n            self.backbone_type,\n            backbone_config,\n        )\n\n        self.head_layers = nn.ModuleList([])\n        for head in self.heads:\n            if isinstance(head, ClassVectorsHead):\n                in_channels = int(self.backbone.middle_blocks[-1].filters)\n            else:\n                in_channels = self.backbone.decoder_stride_to_filters[\n                    head.output_stride\n                ]\n            self.head_layers.append(head.make_head(x_in=in_channels))\n\n    @classmethod\n    def from_config(\n        cls,\n        backbone_type: str,\n        backbone_config: DictConfig,\n        head_configs: DictConfig,\n        model_type: str,\n    ) -&gt; \"Model\":\n        \"\"\"Create the model from a config dictionary.\"\"\"\n        return cls(\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            model_type=model_type,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the model.\"\"\"\n        if x.shape[-3] != self.backbone_config.in_channels:\n            if x.shape[-3] == 1:\n                # convert grayscale to rgb\n                x = x.repeat(1, 3, 1, 1)\n            elif x.shape[-3] == 3:\n                # convert rgb to grayscale\n                x = F.rgb_to_grayscale(x, num_output_channels=1)\n\n        backbone_outputs = self.backbone(x)\n\n        outputs = {}\n        for head, head_layer in zip(self.heads, self.head_layers):\n            if not len(backbone_outputs[\"outputs\"]):\n                outputs[head.name] = head_layer(backbone_outputs[\"middle_output\"])\n            else:\n                if isinstance(head, ClassVectorsHead):\n                    backbone_out = backbone_outputs[\"intermediate_feat\"]\n                    outputs[head.name] = head_layer(backbone_out)\n                else:\n                    idx = backbone_outputs[\"strides\"].index(head.output_stride)\n                    outputs[head.name] = head_layer(backbone_outputs[\"outputs\"][idx])\n\n        return outputs\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model.__init__","title":"<code>__init__(backbone_type, backbone_config, head_configs, model_type)</code>","text":"<p>Initialize the backbone and head based on the backbone_config.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def __init__(\n    self,\n    backbone_type: str,\n    backbone_config: DictConfig,\n    head_configs: DictConfig,\n    model_type: str,\n) -&gt; None:\n    \"\"\"Initialize the backbone and head based on the backbone_config.\"\"\"\n    super().__init__()\n    self.backbone_type = backbone_type\n    self.backbone_config = backbone_config\n    self.head_configs = head_configs\n\n    self.heads = get_head(model_type, self.head_configs)\n\n    output_strides = []\n    for head_type in head_configs:\n        head_config = head_configs[head_type]\n        output_strides.append(head_config.output_stride)\n\n    min_output_stride = min(output_strides)\n    min_output_stride = min(min_output_stride, self.backbone_config.output_stride)\n\n    self.backbone = get_backbone(\n        self.backbone_type,\n        backbone_config,\n    )\n\n    self.head_layers = nn.ModuleList([])\n    for head in self.heads:\n        if isinstance(head, ClassVectorsHead):\n            in_channels = int(self.backbone.middle_blocks[-1].filters)\n        else:\n            in_channels = self.backbone.decoder_stride_to_filters[\n                head.output_stride\n            ]\n        self.head_layers.append(head.make_head(x_in=in_channels))\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the model.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the model.\"\"\"\n    if x.shape[-3] != self.backbone_config.in_channels:\n        if x.shape[-3] == 1:\n            # convert grayscale to rgb\n            x = x.repeat(1, 3, 1, 1)\n        elif x.shape[-3] == 3:\n            # convert rgb to grayscale\n            x = F.rgb_to_grayscale(x, num_output_channels=1)\n\n    backbone_outputs = self.backbone(x)\n\n    outputs = {}\n    for head, head_layer in zip(self.heads, self.head_layers):\n        if not len(backbone_outputs[\"outputs\"]):\n            outputs[head.name] = head_layer(backbone_outputs[\"middle_output\"])\n        else:\n            if isinstance(head, ClassVectorsHead):\n                backbone_out = backbone_outputs[\"intermediate_feat\"]\n                outputs[head.name] = head_layer(backbone_out)\n            else:\n                idx = backbone_outputs[\"strides\"].index(head.output_stride)\n                outputs[head.name] = head_layer(backbone_outputs[\"outputs\"][idx])\n\n    return outputs\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model.from_config","title":"<code>from_config(backbone_type, backbone_config, head_configs, model_type)</code>  <code>classmethod</code>","text":"<p>Create the model from a config dictionary.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    backbone_type: str,\n    backbone_config: DictConfig,\n    head_configs: DictConfig,\n    model_type: str,\n) -&gt; \"Model\":\n    \"\"\"Create the model from a config dictionary.\"\"\"\n    return cls(\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        model_type=model_type,\n    )\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.get_backbone","title":"<code>get_backbone(backbone, backbone_config)</code>","text":"<p>Get a backbone model <code>nn.Module</code> based on the provided name.</p> <p>This function returns an instance of a PyTorch <code>nn.Module</code> corresponding to the given backbone name.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>str</code> <p>Name of the backbone. Supported values are 'unet'.</p> required <code>backbone_config</code> <code>DictConfig</code> <p>A config for the backbone.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: An instance of the requested backbone model.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the provided backbone name is not one of the supported values.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def get_backbone(backbone: str, backbone_config: DictConfig) -&gt; nn.Module:\n    \"\"\"Get a backbone model `nn.Module` based on the provided name.\n\n    This function returns an instance of a PyTorch `nn.Module`\n    corresponding to the given backbone name.\n\n    Args:\n        backbone (str): Name of the backbone. Supported values are 'unet'.\n        backbone_config (DictConfig): A config for the backbone.\n\n    Returns:\n        nn.Module: An instance of the requested backbone model.\n\n    Raises:\n        KeyError: If the provided backbone name is not one of the supported values.\n    \"\"\"\n    backbones = {\"unet\": UNet, \"convnext\": ConvNextWrapper, \"swint\": SwinTWrapper}\n\n    if backbone not in backbones:\n        message = f\"Unsupported backbone: {backbone}. Supported backbones are: {', '.join(backbones.keys())}\"\n        logger.error(message)\n        raise KeyError(message)\n\n    backbone = backbones[backbone].from_config(backbone_config)\n\n    return backbone\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.get_head","title":"<code>get_head(model_type, head_config)</code>","text":"<p>Get a head <code>nn.Module</code> based on the provided name.</p> <p>This function returns an instance of a PyTorch <code>nn.Module</code> corresponding to the given head name.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Name of the head. Supported values are - 'single_instance' - 'centroid' - 'centered_instance' - 'bottomup' - 'multi_class_bottomup' - 'multi_class_topdown'</p> required <code>head_config</code> <code>DictConfig</code> <p>A config for the head.</p> required <p>Returns:</p> Type Description <code>Head</code> <p>nn.Module: An instance of the requested head.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def get_head(model_type: str, head_config: DictConfig) -&gt; Head:\n    \"\"\"Get a head `nn.Module` based on the provided name.\n\n    This function returns an instance of a PyTorch `nn.Module`\n    corresponding to the given head name.\n\n    Args:\n        model_type (str): Name of the head. Supported values are\n            - 'single_instance'\n            - 'centroid'\n            - 'centered_instance'\n            - 'bottomup'\n            - 'multi_class_bottomup'\n            - 'multi_class_topdown'\n        head_config (DictConfig): A config for the head.\n\n    Returns:\n        nn.Module: An instance of the requested head.\n    \"\"\"\n    heads = []\n    if model_type == \"single_instance\":\n        heads.append(SingleInstanceConfmapsHead(**head_config.confmaps))\n\n    elif model_type == \"centered_instance\":\n        heads.append(CenteredInstanceConfmapsHead(**head_config.confmaps))\n\n    elif model_type == \"centroid\":\n        heads.append(CentroidConfmapsHead(**head_config.confmaps))\n\n    elif model_type == \"bottomup\":\n        heads.append(MultiInstanceConfmapsHead(**head_config.confmaps))\n        heads.append(PartAffinityFieldsHead(**head_config.pafs))\n\n    elif model_type == \"multi_class_bottomup\":\n        heads.append(MultiInstanceConfmapsHead(**head_config.confmaps))\n        heads.append(ClassMapsHead(**head_config.class_maps))\n\n    elif model_type == \"multi_class_topdown\":\n        heads.append(CenteredInstanceConfmapsHead(**head_config.confmaps))\n        heads.append(ClassVectorsHead(**head_config.class_vectors))\n\n    else:\n        message = f\"{model_type} is not a defined model type. Please choose one of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\"\n        logger.error(message)\n        raise Exception(message)\n\n    return heads\n</code></pre>"},{"location":"api/architectures/swint/","title":"swint","text":""},{"location":"api/architectures/swint/#sleap_nn.architectures.swint","title":"<code>sleap_nn.architectures.swint</code>","text":"<p>This module provides a generalized implementation of SwinT.</p> <p>See the <code>SwinTWrapper</code> class docstring for more information.</p> <p>Classes:</p> Name Description <code>SwinTWrapper</code> <p>SwinT architecture for pose estimation.</p> <code>SwinTransformerEncoder</code> <p>SwinT backbone for pose estimation.</p>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper","title":"<code>SwinTWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>SwinT architecture for pose estimation.</p> <p>This class defines the SwinT architecture for pose estimation, combining an SwinT as the encoder and a decoder. The encoder extracts features from the input, while the decoder generates confidence maps based on the features.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\"].</p> required <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map.</p> required <code>patch_size</code> <code>List[int]</code> <p>Patch size. Default: [4,4]</p> <code>[4, 4]</code> <code>arch</code> <code>dict</code> <p>Dictionary of embed dimension, depths and number of heads in each layer.</p> <code>{'embed': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24]}</code> <code>{'embed'</code> <p>96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}</p> required <code>window_size</code> <code>List[int]</code> <p>Window size. Default: [7,7].</p> <code>[7, 7]</code> <code>stem_patch_stride</code> <code>int</code> <p>Stride for the patch. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>16</code> for all swint architectures.</p> <code>32</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SwinT architecture.</p> <code>from_config</code> <p>Create SwinTWrapper from a config.</p> <p>Attributes:</p> Name Type Description <code>max_channels</code> <p>Returns the maximum channels of the SwinT (last layer of the encoder).</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>class SwinTWrapper(nn.Module):\n    \"\"\"SwinT architecture for pose estimation.\n\n    This class defines the SwinT architecture for pose estimation, combining an\n    SwinT as the encoder and a decoder. The encoder extracts features from the input,\n    while the decoder generates confidence maps based on the features.\n\n    Args:\n        in_channels: Number of input channels. Default is 1.\n        model_type: One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\"].\n        output_stride: Minimum of the strides of the output heads. The input confidence map.\n        patch_size: Patch size. Default: [4,4]\n        arch: Dictionary of embed dimension, depths and number of heads in each layer.\n        Default is \"Tiny architecture\".\n        {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}\n        window_size: Window size. Default: [7,7].\n        stem_patch_stride: Stride for the patch. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        filters_rate: Factor to adjust the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `16` for all swint architectures.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step.\n\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        output_stride: int,\n        in_channels: int = 1,\n        patch_size: List[int] = [4, 4],\n        arch: dict = {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n        window_size: List[int] = [7, 7],\n        stem_patch_stride: int = 2,\n        kernel_size: int = 3,\n        filters_rate: int = 2,\n        convs_per_block: int = 2,\n        up_interpolate: bool = True,\n        max_stride: int = 32,\n        block_contraction: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.patch_size = patch_size\n        self.kernel_size = kernel_size\n        self.filters_rate = filters_rate\n        self.block_contraction = block_contraction\n        arch_types = {\n            \"tiny\": {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n            \"small\": {\n                \"embed\": 96,\n                \"depths\": [2, 2, 18, 2],\n                \"num_heads\": [3, 6, 12, 24],\n            },\n            \"base\": {\n                \"embed\": 128,\n                \"depths\": [2, 2, 18, 2],\n                \"num_heads\": [4, 8, 16, 32],\n            },\n        }\n        if model_type in arch_types:\n            self.arch = arch_types[model_type]\n        elif arch is not None:\n            self.arch = arch\n        else:\n            self.arch = arch_types[\"tiny\"]\n\n        self.max_stride = (\n            stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * final_max_pool_stride\n        self.stem_blocks = 1  # 1 stem block + 3 down blocks in swint\n\n        self.up_blocks = np.log2(\n            self.max_stride / (stem_patch_stride * output_stride)\n        ).astype(int) + np.log2(stem_patch_stride).astype(int)\n        self.convs_per_block = convs_per_block\n        self.stem_patch_stride = stem_patch_stride\n        self.down_blocks = len(self.arch[\"depths\"]) - 1\n        self.enc = SwinTransformerEncoder(\n            in_channels=in_channels,\n            patch_size=patch_size,\n            embed_dim=self.arch[\"embed\"],\n            depths=self.arch[\"depths\"],\n            num_heads=self.arch[\"num_heads\"],\n            window_size=window_size,\n            stem_stride=stem_patch_stride,\n        )\n\n        self.additional_pool = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=2, padding=\"same\"\n        )\n\n        # Create middle blocks\n        self.middle_blocks = nn.ModuleList()\n        # Get the last block filters from encoder\n        last_block_filters = self.arch[\"embed\"] * (2 ** (self.down_blocks))\n\n        if convs_per_block &gt; 1:\n            # Middle expansion block\n            middle_expand = SimpleConvBlock(\n                in_channels=last_block_filters,\n                pool=False,\n                pool_before_convs=False,\n                pooling_stride=2,\n                num_convs=convs_per_block - 1,\n                filters=int(last_block_filters * filters_rate),\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=\"convnext_middle_expand\",\n            )\n            self.middle_blocks.append(middle_expand)\n\n        # Middle contraction block\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            block_filters = int(last_block_filters)\n        else:\n            # Keep the block output filters the same\n            block_filters = int(last_block_filters * filters_rate)\n\n        middle_contract = SimpleConvBlock(\n            in_channels=int(last_block_filters * filters_rate),\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=1,\n            filters=block_filters,\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_contract\",\n        )\n        self.middle_blocks.append(middle_contract)\n\n        self.current_stride = (\n            self.stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * final_max_pool_stride\n\n        self.dec = Decoder(\n            x_in_shape=block_filters,\n            current_stride=self.current_stride,\n            filters=self.arch[\"embed\"],\n            up_blocks=self.up_blocks,\n            down_blocks=self.down_blocks,\n            filters_rate=filters_rate,\n            kernel_size=self.kernel_size,\n            stem_blocks=self.stem_blocks,\n            block_contraction=self.block_contraction,\n            output_stride=output_stride,\n            up_interpolate=up_interpolate,\n        )\n\n        if len(self.dec.decoder_stack):\n            self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n        else:\n            self.final_dec_channels = block_filters\n\n        self.decoder_stride_to_filters = self.dec.stride_to_filters\n\n    @property\n    def max_channels(self):\n        \"\"\"Returns the maximum channels of the SwinT (last layer of the encoder).\"\"\"\n        return self.dec.x_in_shape\n\n    @classmethod\n    def from_config(cls, config: OmegaConf):\n        \"\"\"Create SwinTWrapper from a config.\"\"\"\n        return cls(\n            in_channels=config.in_channels,\n            model_type=config.model_type,\n            arch=config.arch,\n            patch_size=(config.patch_size, config.patch_size),\n            window_size=(config.window_size, config.window_size),\n            kernel_size=config.kernel_size,\n            filters_rate=config.filters_rate,\n            convs_per_block=config.convs_per_block,\n            up_interpolate=config.up_interpolate,\n            output_stride=config.output_stride,\n            stem_patch_stride=config.stem_patch_stride,\n            max_stride=config.max_stride,\n            block_contraction=(\n                config.block_contraction\n                if hasattr(config, \"block_contraction\")\n                else False\n            ),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the SwinT architecture.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            x: Outputs a dictionary with `outputs` and `strides` containing the output\n            at different strides.\n        \"\"\"\n        enc_output = self.enc(x)\n        x, features = enc_output[-1], enc_output[::2]\n        features = features[::-1]\n\n        # Apply additional pooling layer\n        x = self.additional_pool(x)\n\n        # Process through middle blocks\n        middle_output = x\n        for middle_block in self.middle_blocks:\n            middle_output = middle_block(middle_output)\n\n        x = self.dec(middle_output, features)\n        x[\"middle_output\"] = middle_output\n        return x\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.max_channels","title":"<code>max_channels</code>  <code>property</code>","text":"<p>Returns the maximum channels of the SwinT (last layer of the encoder).</p>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.__init__","title":"<code>__init__(model_type, output_stride, in_channels=1, patch_size=[4, 4], arch={'embed': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24]}, window_size=[7, 7], stem_patch_stride=2, kernel_size=3, filters_rate=2, convs_per_block=2, up_interpolate=True, max_stride=32, block_contraction=False)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    output_stride: int,\n    in_channels: int = 1,\n    patch_size: List[int] = [4, 4],\n    arch: dict = {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n    window_size: List[int] = [7, 7],\n    stem_patch_stride: int = 2,\n    kernel_size: int = 3,\n    filters_rate: int = 2,\n    convs_per_block: int = 2,\n    up_interpolate: bool = True,\n    max_stride: int = 32,\n    block_contraction: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.patch_size = patch_size\n    self.kernel_size = kernel_size\n    self.filters_rate = filters_rate\n    self.block_contraction = block_contraction\n    arch_types = {\n        \"tiny\": {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n        \"small\": {\n            \"embed\": 96,\n            \"depths\": [2, 2, 18, 2],\n            \"num_heads\": [3, 6, 12, 24],\n        },\n        \"base\": {\n            \"embed\": 128,\n            \"depths\": [2, 2, 18, 2],\n            \"num_heads\": [4, 8, 16, 32],\n        },\n    }\n    if model_type in arch_types:\n        self.arch = arch_types[model_type]\n    elif arch is not None:\n        self.arch = arch\n    else:\n        self.arch = arch_types[\"tiny\"]\n\n    self.max_stride = (\n        stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * final_max_pool_stride\n    self.stem_blocks = 1  # 1 stem block + 3 down blocks in swint\n\n    self.up_blocks = np.log2(\n        self.max_stride / (stem_patch_stride * output_stride)\n    ).astype(int) + np.log2(stem_patch_stride).astype(int)\n    self.convs_per_block = convs_per_block\n    self.stem_patch_stride = stem_patch_stride\n    self.down_blocks = len(self.arch[\"depths\"]) - 1\n    self.enc = SwinTransformerEncoder(\n        in_channels=in_channels,\n        patch_size=patch_size,\n        embed_dim=self.arch[\"embed\"],\n        depths=self.arch[\"depths\"],\n        num_heads=self.arch[\"num_heads\"],\n        window_size=window_size,\n        stem_stride=stem_patch_stride,\n    )\n\n    self.additional_pool = MaxPool2dWithSamePadding(\n        kernel_size=2, stride=2, padding=\"same\"\n    )\n\n    # Create middle blocks\n    self.middle_blocks = nn.ModuleList()\n    # Get the last block filters from encoder\n    last_block_filters = self.arch[\"embed\"] * (2 ** (self.down_blocks))\n\n    if convs_per_block &gt; 1:\n        # Middle expansion block\n        middle_expand = SimpleConvBlock(\n            in_channels=last_block_filters,\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=convs_per_block - 1,\n            filters=int(last_block_filters * filters_rate),\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_expand\",\n        )\n        self.middle_blocks.append(middle_expand)\n\n    # Middle contraction block\n    if self.block_contraction:\n        # Contract the channels with an exponent lower than the last encoder block\n        block_filters = int(last_block_filters)\n    else:\n        # Keep the block output filters the same\n        block_filters = int(last_block_filters * filters_rate)\n\n    middle_contract = SimpleConvBlock(\n        in_channels=int(last_block_filters * filters_rate),\n        pool=False,\n        pool_before_convs=False,\n        pooling_stride=2,\n        num_convs=1,\n        filters=block_filters,\n        kernel_size=kernel_size,\n        use_bias=True,\n        batch_norm=False,\n        activation=\"relu\",\n        prefix=\"convnext_middle_contract\",\n    )\n    self.middle_blocks.append(middle_contract)\n\n    self.current_stride = (\n        self.stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * final_max_pool_stride\n\n    self.dec = Decoder(\n        x_in_shape=block_filters,\n        current_stride=self.current_stride,\n        filters=self.arch[\"embed\"],\n        up_blocks=self.up_blocks,\n        down_blocks=self.down_blocks,\n        filters_rate=filters_rate,\n        kernel_size=self.kernel_size,\n        stem_blocks=self.stem_blocks,\n        block_contraction=self.block_contraction,\n        output_stride=output_stride,\n        up_interpolate=up_interpolate,\n    )\n\n    if len(self.dec.decoder_stack):\n        self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n    else:\n        self.final_dec_channels = block_filters\n\n    self.decoder_stride_to_filters = self.dec.stride_to_filters\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the SwinT architecture.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tuple[List[Tensor], List]</code> <p>Outputs a dictionary with <code>outputs</code> and <code>strides</code> containing the output at different strides.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the SwinT architecture.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        x: Outputs a dictionary with `outputs` and `strides` containing the output\n        at different strides.\n    \"\"\"\n    enc_output = self.enc(x)\n    x, features = enc_output[-1], enc_output[::2]\n    features = features[::-1]\n\n    # Apply additional pooling layer\n    x = self.additional_pool(x)\n\n    # Process through middle blocks\n    middle_output = x\n    for middle_block in self.middle_blocks:\n        middle_output = middle_block(middle_output)\n\n    x = self.dec(middle_output, features)\n    x[\"middle_output\"] = middle_output\n    return x\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create SwinTWrapper from a config.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>@classmethod\ndef from_config(cls, config: OmegaConf):\n    \"\"\"Create SwinTWrapper from a config.\"\"\"\n    return cls(\n        in_channels=config.in_channels,\n        model_type=config.model_type,\n        arch=config.arch,\n        patch_size=(config.patch_size, config.patch_size),\n        window_size=(config.window_size, config.window_size),\n        kernel_size=config.kernel_size,\n        filters_rate=config.filters_rate,\n        convs_per_block=config.convs_per_block,\n        up_interpolate=config.up_interpolate,\n        output_stride=config.output_stride,\n        stem_patch_stride=config.stem_patch_stride,\n        max_stride=config.max_stride,\n        block_contraction=(\n            config.block_contraction\n            if hasattr(config, \"block_contraction\")\n            else False\n        ),\n    )\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTransformerEncoder","title":"<code>SwinTransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>SwinT backbone for pose estimation.</p> <p>This class implements ConvNext from the <code>\"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</code>https://arxiv.org/abs/2103.14030`paper. Source: torchvision.models. This module serves as the backbone/ encoder architecture to extract features from the input image.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>patch_size</code> <code>List[int]</code> <p>Patch size. Default: [4,4]</p> <code>[4, 4]</code> <code>embed_dim</code> <code>int</code> <p>Patch embedding dimension. Default: 96</p> <code>96</code> <code>depths</code> <code>List(int</code> <p>Depth of each Swin Transformer layer. Default: [2,2,6,2].</p> <code>[2, 2, 6, 2]</code> <code>num_heads</code> <code>List(int</code> <p>Number of attention heads in different layers.             Default: [3,6,12,24].</p> <code>[3, 6, 12, 24]</code> <code>window_size</code> <code>List[int]</code> <p>Window size. Default: [7,7].</p> <code>[7, 7]</code> <code>stem_stride</code> <code>int</code> <p>Stride for the patch. Default is 2.</p> <code>2</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4.0.</p> <code>4.0</code> <code>dropout</code> <code>float</code> <p>Dropout rate. Default: 0.0.</p> <code>0.0</code> <code>attention_dropout</code> <code>float</code> <p>Attention dropout rate. Default: 0.0.</p> <code>0.0</code> <code>stochastic_depth_prob</code> <code>float</code> <p>Stochastic depth rate. Default: 0.1.</p> <code>0.1</code> <code>num_classes</code> <code>int</code> <p>Number of classes for classification head. Default: 1000.</p> required <code>block</code> <code>Module</code> <p>SwinTransformer Block. Default: None.</p> required <code>norm_layer</code> <code>Module</code> <p>Normalization layer. Default: None.</p> <code>None</code> <code>downsample_layer</code> <code>Module</code> <p>Downsample layer (patch merging). Default: PatchMerging.</p> required <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SwinT encoder.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>class SwinTransformerEncoder(nn.Module):\n    \"\"\"SwinT backbone for pose estimation.\n\n    This class implements ConvNext from the `\"Swin Transformer: Hierarchical Vision Transformer\n    using Shifted Windows `&lt;https://arxiv.org/abs/2103.14030&gt;`paper.\n    Source: torchvision.models. This module serves as the backbone/ encoder architecture\n    to extract features from the input image.\n\n    Args:\n        in_channels (int): Number of input channels. Default is 1.\n        patch_size (List[int]): Patch size. Default: [4,4]\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (List(int)): Depth of each Swin Transformer layer. Default: [2,2,6,2].\n        num_heads (List(int)): Number of attention heads in different layers.\n                        Default: [3,6,12,24].\n        window_size (List[int]): Window size. Default: [7,7].\n        stem_stride (int): Stride for the patch. Default is 2.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.\n        dropout (float): Dropout rate. Default: 0.0.\n        attention_dropout (float): Attention dropout rate. Default: 0.0.\n        stochastic_depth_prob (float): Stochastic depth rate. Default: 0.1.\n        num_classes (int): Number of classes for classification head. Default: 1000.\n        block (nn.Module, optional): SwinTransformer Block. Default: None.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None.\n        downsample_layer (nn.Module): Downsample layer (patch merging). Default: PatchMerging.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 1,\n        patch_size: List[int] = [4, 4],\n        embed_dim: int = 96,\n        depths: List[int] = [2, 2, 6, 2],\n        num_heads: List[int] = [3, 6, 12, 24],\n        window_size: List[int] = [7, 7],\n        stem_stride: int = 2,\n        mlp_ratio: float = 4.0,\n        dropout: float = 0.0,\n        attention_dropout: float = 0.0,\n        stochastic_depth_prob: float = 0.1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ):\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        block = SwinTransformerBlock  # or v2\n        downsample_layer = PatchMerging  # or v2\n\n        if not norm_layer:\n            norm_layer = partial(nn.LayerNorm, eps=1e-5)\n\n        layers: List[nn.Module] = []\n        # split image into non-overlapping patches\n        layers.append(\n            nn.Sequential(\n                nn.Conv2d(\n                    in_channels,\n                    embed_dim,\n                    kernel_size=(patch_size[0], patch_size[1]),\n                    stride=(stem_stride, stem_stride),\n                    padding=1,\n                ),\n                Permute([0, 2, 3, 1]),\n                norm_layer(embed_dim),\n            )\n        )\n\n        total_stage_blocks = sum(depths)\n        stage_block_id = 0\n        # build SwinTransformer blocks\n        for i_stage in range(len(depths)):\n            stage: List[nn.Module] = []\n            dim = embed_dim * 2**i_stage\n            for i_layer in range(depths[i_stage]):\n                # adjust stochastic depth probability based on the depth of the stage block\n                sd_prob = (\n                    stochastic_depth_prob\n                    * float(stage_block_id)\n                    / (total_stage_blocks - 1)\n                )\n                stage.append(\n                    block(\n                        dim,\n                        num_heads[i_stage],\n                        window_size=window_size,\n                        shift_size=[\n                            0 if i_layer % 2 == 0 else w // 2 for w in window_size\n                        ],\n                        mlp_ratio=mlp_ratio,\n                        dropout=dropout,\n                        attention_dropout=attention_dropout,\n                        stochastic_depth_prob=sd_prob,\n                        norm_layer=norm_layer,\n                    )\n                )\n                stage_block_id += 1\n            layers.append(nn.Sequential(*stage))\n            # add patch merging layer\n            if i_stage &lt; (len(depths) - 1):\n                layers.append(downsample_layer(dim, norm_layer))\n        self.features = nn.Sequential(*layers)\n\n        num_features = embed_dim * 2 ** (len(depths) - 1)\n        self.norm = norm_layer(num_features)\n        self.permute = Permute([0, 3, 1, 2])  # B H W C -&gt; B C H W\n\n    def forward(self, x):\n        \"\"\"Forward pass through the SwinT encoder.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Outputs a list of tensors from each stage after applying the SwinT backbone.\n        \"\"\"\n        features_list = []\n        for idx, l in enumerate(self.features):\n            x = l(x)\n            if idx == len(self.features) - 1:\n                x = self.norm(x)\n            features_list.append(self.permute(x))\n        return features_list\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTransformerEncoder.__init__","title":"<code>__init__(in_channels=1, patch_size=[4, 4], embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=[7, 7], stem_stride=2, mlp_ratio=4.0, dropout=0.0, attention_dropout=0.0, stochastic_depth_prob=0.1, norm_layer=None)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int = 1,\n    patch_size: List[int] = [4, 4],\n    embed_dim: int = 96,\n    depths: List[int] = [2, 2, 6, 2],\n    num_heads: List[int] = [3, 6, 12, 24],\n    window_size: List[int] = [7, 7],\n    stem_stride: int = 2,\n    mlp_ratio: float = 4.0,\n    dropout: float = 0.0,\n    attention_dropout: float = 0.0,\n    stochastic_depth_prob: float = 0.1,\n    norm_layer: Optional[Callable[..., nn.Module]] = None,\n):\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n    _log_api_usage_once(self)\n\n    block = SwinTransformerBlock  # or v2\n    downsample_layer = PatchMerging  # or v2\n\n    if not norm_layer:\n        norm_layer = partial(nn.LayerNorm, eps=1e-5)\n\n    layers: List[nn.Module] = []\n    # split image into non-overlapping patches\n    layers.append(\n        nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                embed_dim,\n                kernel_size=(patch_size[0], patch_size[1]),\n                stride=(stem_stride, stem_stride),\n                padding=1,\n            ),\n            Permute([0, 2, 3, 1]),\n            norm_layer(embed_dim),\n        )\n    )\n\n    total_stage_blocks = sum(depths)\n    stage_block_id = 0\n    # build SwinTransformer blocks\n    for i_stage in range(len(depths)):\n        stage: List[nn.Module] = []\n        dim = embed_dim * 2**i_stage\n        for i_layer in range(depths[i_stage]):\n            # adjust stochastic depth probability based on the depth of the stage block\n            sd_prob = (\n                stochastic_depth_prob\n                * float(stage_block_id)\n                / (total_stage_blocks - 1)\n            )\n            stage.append(\n                block(\n                    dim,\n                    num_heads[i_stage],\n                    window_size=window_size,\n                    shift_size=[\n                        0 if i_layer % 2 == 0 else w // 2 for w in window_size\n                    ],\n                    mlp_ratio=mlp_ratio,\n                    dropout=dropout,\n                    attention_dropout=attention_dropout,\n                    stochastic_depth_prob=sd_prob,\n                    norm_layer=norm_layer,\n                )\n            )\n            stage_block_id += 1\n        layers.append(nn.Sequential(*stage))\n        # add patch merging layer\n        if i_stage &lt; (len(depths) - 1):\n            layers.append(downsample_layer(dim, norm_layer))\n    self.features = nn.Sequential(*layers)\n\n    num_features = embed_dim * 2 ** (len(depths) - 1)\n    self.norm = norm_layer(num_features)\n    self.permute = Permute([0, 3, 1, 2])  # B H W C -&gt; B C H W\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTransformerEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the SwinT encoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <p>Outputs a list of tensors from each stage after applying the SwinT backbone.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass through the SwinT encoder.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Outputs a list of tensors from each stage after applying the SwinT backbone.\n    \"\"\"\n    features_list = []\n    for idx, l in enumerate(self.features):\n        x = l(x)\n        if idx == len(self.features) - 1:\n            x = self.norm(x)\n        features_list.append(self.permute(x))\n    return features_list\n</code></pre>"},{"location":"api/architectures/unet/","title":"unet","text":""},{"location":"api/architectures/unet/#sleap_nn.architectures.unet","title":"<code>sleap_nn.architectures.unet</code>","text":"<p>This module provides a generalized implementation of UNet.</p> <p>See the <code>UNet</code> class docstring for more information.</p> <p>Classes:</p> Name Description <code>UNet</code> <p>U-Net architecture for pose estimation.</p>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet","title":"<code>UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>U-Net architecture for pose estimation.</p> <p>This class defines the U-Net architecture for pose estimation, combining an encoder and a decoder. The encoder extracts features from the input, while the decoder generates confidence maps based on the features.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>stem_kernel_size</code> <code>int</code> <p>Kernle size for the stem blocks.</p> <code>7</code> <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 32.</p> <code>32</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 1.5.</p> <code>1.5</code> <code>down_blocks</code> <code>int</code> <p>Number of downsampling blocks. Default is 4.</p> <code>4</code> <code>up_blocks</code> <code>int</code> <p>Number of upsampling blocks in the decoder. Default is 3.</p> <code>3</code> <code>stem_blocks</code> <code>int</code> <p>If &gt;0, will create additional \"down\" blocks for initial downsampling. These will be configured identically to the down blocks below.</p> <code>0</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>middle_block</code> <code>bool</code> <p>If True, add an additional block at the end of the encoder.</p> <code>True</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step. The original implementation does not do this, but the CARE implementation does.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the U-Net architecture.</p> <code>from_config</code> <p>Create UNet from a config.</p> <p>Attributes:</p> Name Type Description <code>max_channels</code> <p>Returns the maximum channels of the UNet (last layer of the encoder).</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>class UNet(nn.Module):\n    \"\"\"U-Net architecture for pose estimation.\n\n    This class defines the U-Net architecture for pose estimation, combining an\n    encoder and a decoder. The encoder extracts features from the input, while the\n    decoder generates confidence maps based on the features.\n\n    Args:\n        in_channels: Number of input channels. Default is 1.\n        output_stride: Minimum of the strides of the output heads. The input confidence map.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        stem_kernel_size: Kernle size for the stem blocks.\n        filters: Number of filters for the initial block. Default is 32.\n        filters_rate: Factor to adjust the number of filters per block. Default is 1.5.\n        down_blocks: Number of downsampling blocks. Default is 4.\n        up_blocks: Number of upsampling blocks in the decoder. Default is 3.\n        stem_blocks: If &gt;0, will create additional \"down\" blocks for initial\n            downsampling. These will be configured identically to the down blocks below.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        middle_block: If True, add an additional block at the end of the encoder.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step. The original implementation does not\n            do this, but the CARE implementation does.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_stride: int = 2,\n        in_channels: int = 1,\n        kernel_size: int = 3,\n        stem_kernel_size: int = 7,\n        filters: int = 32,\n        filters_rate: int = 1.5,\n        down_blocks: int = 4,\n        up_blocks: int = 3,\n        stem_blocks: int = 0,\n        convs_per_block: int = 2,\n        middle_block: bool = True,\n        up_interpolate: bool = True,\n        block_contraction: bool = False,\n        stacks: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.filters = filters\n        self.filters_rate = filters_rate\n        self.down_blocks = down_blocks\n        self.up_blocks = up_blocks\n        self.stem_blocks = stem_blocks\n        self.convs_per_block = convs_per_block\n        self.stem_kernel_size = stem_kernel_size\n        self.middle_block = middle_block\n        self.up_interpolate = up_interpolate\n        self.block_contraction = block_contraction\n        self.stacks = stacks\n\n        # Create stem block if stem_blocks &gt; 0\n        if self.stem_blocks &gt; 0:\n            self.stem = StemBlock(\n                in_channels=in_channels,\n                filters=filters,\n                stem_blocks=stem_blocks,\n                filters_rate=filters_rate,\n                convs_per_block=convs_per_block,\n                kernel_size=stem_kernel_size,\n                prefix=\"stem\",\n            )\n        else:\n            self.stem = None\n\n        # Initialize lists to store multiple encoders and decoders\n        self.encoders = nn.ModuleList()\n        self.decoders = nn.ModuleList()\n\n        for i in range(self.stacks):\n            # Create encoder for this stack\n            in_channels = (\n                int(self.filters * (self.filters_rate ** (self.stem_blocks)))\n                if self.stem_blocks &gt; 0\n                else in_channels\n            )\n            encoder = Encoder(\n                in_channels=in_channels,\n                filters=filters,\n                down_blocks=down_blocks,\n                filters_rate=filters_rate,\n                convs_per_block=convs_per_block,\n                kernel_size=kernel_size,\n                stem_blocks=stem_blocks,\n                prefix=f\"stack{i}_enc\",\n            )\n\n            # Create middle block separately (not part of encoder stack)\n            self.middle_blocks = nn.ModuleList()\n            # Get the last block filters from encoder\n            last_block_filters = int(\n                filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n            )\n            enc_num = len(encoder.encoder_stack)\n            if self.middle_block:\n\n                if convs_per_block &gt; 1:\n                    # Middle expansion block\n                    from sleap_nn.architectures.encoder_decoder import SimpleConvBlock\n\n                    middle_expand = SimpleConvBlock(\n                        in_channels=last_block_filters,\n                        pool=False,\n                        pool_before_convs=False,\n                        pooling_stride=2,\n                        num_convs=convs_per_block - 1,\n                        filters=int(last_block_filters * filters_rate),\n                        kernel_size=kernel_size,\n                        use_bias=True,\n                        batch_norm=False,\n                        activation=\"relu\",\n                        prefix=f\"stack{i}_enc{enc_num}_middle_expand\",\n                    )\n                    enc_num += 1\n                    self.middle_blocks.append(middle_expand)\n\n                # Middle contraction block\n                if self.block_contraction:\n                    # Contract the channels with an exponent lower than the last encoder block\n                    block_filters = int(last_block_filters)\n                else:\n                    # Keep the block output filters the same\n                    block_filters = int(last_block_filters * filters_rate)\n\n                middle_contract = SimpleConvBlock(\n                    in_channels=int(last_block_filters * filters_rate),\n                    pool=False,\n                    pool_before_convs=False,\n                    pooling_stride=2,\n                    num_convs=1,\n                    filters=block_filters,\n                    kernel_size=kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"stack{i}_enc{enc_num}_middle_contract\",\n                )\n                enc_num += 1\n                self.middle_blocks.append(middle_contract)\n\n            self.encoders.append(encoder)\n\n            # Calculate current stride for this encoder\n            # Start with stem stride if stem blocks exist\n            current_stride = 2**self.stem_blocks if self.stem_blocks &gt; 0 else 1\n\n            # Add encoder strides\n            for block in encoder.encoder_stack:\n                if hasattr(block, \"pool\") and block.pool:\n                    current_stride *= block.pooling_stride\n\n            current_stride *= (\n                2  # for last pool layer MaxPool2dWithSamePadding in encoder\n            )\n\n            # Create decoder for this stack\n            if self.block_contraction:\n                # Contract the channels with an exponent lower than the last encoder block\n                x_in_shape = int(\n                    filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n                )\n            else:\n                # Keep the block output filters the same\n                x_in_shape = int(\n                    filters * (filters_rate ** (down_blocks + stem_blocks))\n                )\n            decoder = Decoder(\n                x_in_shape=x_in_shape,\n                current_stride=current_stride,\n                filters=filters,\n                up_blocks=up_blocks,\n                down_blocks=down_blocks,\n                filters_rate=filters_rate,\n                stem_blocks=stem_blocks,\n                output_stride=output_stride,\n                kernel_size=kernel_size,\n                block_contraction=self.block_contraction,\n                up_interpolate=up_interpolate,\n                prefix=f\"stack{i}_dec\",\n            )\n            self.decoders.append(decoder)\n\n        if len(self.decoders) and len(self.decoders[-1].decoder_stack):\n            self.final_dec_channels = (\n                self.decoders[-1].decoder_stack[-1].refine_convs_filters\n            )\n        else:\n            self.final_dec_channels = (\n                last_block_filters if not self.middle_block else block_filters\n            )\n\n        self.decoder_stride_to_filters = self.decoders[-1].stride_to_filters\n\n    @classmethod\n    def from_config(cls, config: OmegaConf):\n        \"\"\"Create UNet from a config.\"\"\"\n        stem_blocks = 0\n        if config.stem_stride is not None:\n            stem_blocks = np.log2(config.stem_stride).astype(int)\n        down_blocks = np.log2(config.max_stride).astype(int) - stem_blocks\n        up_blocks = (\n            np.log2(config.max_stride / config.output_stride).astype(int) + stem_blocks\n        )\n        return cls(\n            in_channels=config.in_channels,\n            kernel_size=config.kernel_size,\n            filters=config.filters,\n            filters_rate=config.filters_rate,\n            down_blocks=down_blocks,\n            up_blocks=up_blocks,\n            stem_blocks=stem_blocks,\n            convs_per_block=config.convs_per_block,\n            middle_block=config.middle_block,\n            up_interpolate=config.up_interpolate,\n            stacks=config.stacks,\n            output_stride=config.output_stride,\n        )\n\n    @property\n    def max_channels(self):\n        \"\"\"Returns the maximum channels of the UNet (last layer of the encoder).\"\"\"\n        return self.decoders[0].x_in_shape\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the U-Net architecture.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            x: Output a tensor after applying the U-Net operations.\n            current_strides: a list of the current strides from the decoder.\n        \"\"\"\n        # Process through stem block if it exists\n        stem_output = x\n        if self.stem is not None:\n            stem_output = self.stem(x)\n\n        # Process through all stacks\n        outputs = []\n        output = stem_output\n        for i in range(self.stacks):\n            # Get encoder and decoder for this stack\n            encoder = self.encoders[i]\n            decoder = self.decoders[i]\n\n            # Forward pass through encoder\n            encoded, features = encoder(output)\n\n            # Process through middle block if it exists\n            middle_output = encoded\n            if self.middle_block and hasattr(self, \"middle_blocks\"):\n                for middle_block in self.middle_blocks:\n                    middle_output = middle_block(middle_output)\n\n            if self.stem_blocks &gt; 0:\n                features.append(stem_output)\n\n            output = decoder(middle_output, features)\n            output[\"middle_output\"] = middle_output\n            outputs.append(output)\n\n        return outputs[-1]\n</code></pre>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.max_channels","title":"<code>max_channels</code>  <code>property</code>","text":"<p>Returns the maximum channels of the UNet (last layer of the encoder).</p>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.__init__","title":"<code>__init__(output_stride=2, in_channels=1, kernel_size=3, stem_kernel_size=7, filters=32, filters_rate=1.5, down_blocks=4, up_blocks=3, stem_blocks=0, convs_per_block=2, middle_block=True, up_interpolate=True, block_contraction=False, stacks=1)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>def __init__(\n    self,\n    output_stride: int = 2,\n    in_channels: int = 1,\n    kernel_size: int = 3,\n    stem_kernel_size: int = 7,\n    filters: int = 32,\n    filters_rate: int = 1.5,\n    down_blocks: int = 4,\n    up_blocks: int = 3,\n    stem_blocks: int = 0,\n    convs_per_block: int = 2,\n    middle_block: bool = True,\n    up_interpolate: bool = True,\n    block_contraction: bool = False,\n    stacks: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.kernel_size = kernel_size\n    self.filters = filters\n    self.filters_rate = filters_rate\n    self.down_blocks = down_blocks\n    self.up_blocks = up_blocks\n    self.stem_blocks = stem_blocks\n    self.convs_per_block = convs_per_block\n    self.stem_kernel_size = stem_kernel_size\n    self.middle_block = middle_block\n    self.up_interpolate = up_interpolate\n    self.block_contraction = block_contraction\n    self.stacks = stacks\n\n    # Create stem block if stem_blocks &gt; 0\n    if self.stem_blocks &gt; 0:\n        self.stem = StemBlock(\n            in_channels=in_channels,\n            filters=filters,\n            stem_blocks=stem_blocks,\n            filters_rate=filters_rate,\n            convs_per_block=convs_per_block,\n            kernel_size=stem_kernel_size,\n            prefix=\"stem\",\n        )\n    else:\n        self.stem = None\n\n    # Initialize lists to store multiple encoders and decoders\n    self.encoders = nn.ModuleList()\n    self.decoders = nn.ModuleList()\n\n    for i in range(self.stacks):\n        # Create encoder for this stack\n        in_channels = (\n            int(self.filters * (self.filters_rate ** (self.stem_blocks)))\n            if self.stem_blocks &gt; 0\n            else in_channels\n        )\n        encoder = Encoder(\n            in_channels=in_channels,\n            filters=filters,\n            down_blocks=down_blocks,\n            filters_rate=filters_rate,\n            convs_per_block=convs_per_block,\n            kernel_size=kernel_size,\n            stem_blocks=stem_blocks,\n            prefix=f\"stack{i}_enc\",\n        )\n\n        # Create middle block separately (not part of encoder stack)\n        self.middle_blocks = nn.ModuleList()\n        # Get the last block filters from encoder\n        last_block_filters = int(\n            filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n        )\n        enc_num = len(encoder.encoder_stack)\n        if self.middle_block:\n\n            if convs_per_block &gt; 1:\n                # Middle expansion block\n                from sleap_nn.architectures.encoder_decoder import SimpleConvBlock\n\n                middle_expand = SimpleConvBlock(\n                    in_channels=last_block_filters,\n                    pool=False,\n                    pool_before_convs=False,\n                    pooling_stride=2,\n                    num_convs=convs_per_block - 1,\n                    filters=int(last_block_filters * filters_rate),\n                    kernel_size=kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"stack{i}_enc{enc_num}_middle_expand\",\n                )\n                enc_num += 1\n                self.middle_blocks.append(middle_expand)\n\n            # Middle contraction block\n            if self.block_contraction:\n                # Contract the channels with an exponent lower than the last encoder block\n                block_filters = int(last_block_filters)\n            else:\n                # Keep the block output filters the same\n                block_filters = int(last_block_filters * filters_rate)\n\n            middle_contract = SimpleConvBlock(\n                in_channels=int(last_block_filters * filters_rate),\n                pool=False,\n                pool_before_convs=False,\n                pooling_stride=2,\n                num_convs=1,\n                filters=block_filters,\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=f\"stack{i}_enc{enc_num}_middle_contract\",\n            )\n            enc_num += 1\n            self.middle_blocks.append(middle_contract)\n\n        self.encoders.append(encoder)\n\n        # Calculate current stride for this encoder\n        # Start with stem stride if stem blocks exist\n        current_stride = 2**self.stem_blocks if self.stem_blocks &gt; 0 else 1\n\n        # Add encoder strides\n        for block in encoder.encoder_stack:\n            if hasattr(block, \"pool\") and block.pool:\n                current_stride *= block.pooling_stride\n\n        current_stride *= (\n            2  # for last pool layer MaxPool2dWithSamePadding in encoder\n        )\n\n        # Create decoder for this stack\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            x_in_shape = int(\n                filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n            )\n        else:\n            # Keep the block output filters the same\n            x_in_shape = int(\n                filters * (filters_rate ** (down_blocks + stem_blocks))\n            )\n        decoder = Decoder(\n            x_in_shape=x_in_shape,\n            current_stride=current_stride,\n            filters=filters,\n            up_blocks=up_blocks,\n            down_blocks=down_blocks,\n            filters_rate=filters_rate,\n            stem_blocks=stem_blocks,\n            output_stride=output_stride,\n            kernel_size=kernel_size,\n            block_contraction=self.block_contraction,\n            up_interpolate=up_interpolate,\n            prefix=f\"stack{i}_dec\",\n        )\n        self.decoders.append(decoder)\n\n    if len(self.decoders) and len(self.decoders[-1].decoder_stack):\n        self.final_dec_channels = (\n            self.decoders[-1].decoder_stack[-1].refine_convs_filters\n        )\n    else:\n        self.final_dec_channels = (\n            last_block_filters if not self.middle_block else block_filters\n        )\n\n    self.decoder_stride_to_filters = self.decoders[-1].stride_to_filters\n</code></pre>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the U-Net architecture.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tuple[List[Tensor], List]</code> <p>Output a tensor after applying the U-Net operations. current_strides: a list of the current strides from the decoder.</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the U-Net architecture.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        x: Output a tensor after applying the U-Net operations.\n        current_strides: a list of the current strides from the decoder.\n    \"\"\"\n    # Process through stem block if it exists\n    stem_output = x\n    if self.stem is not None:\n        stem_output = self.stem(x)\n\n    # Process through all stacks\n    outputs = []\n    output = stem_output\n    for i in range(self.stacks):\n        # Get encoder and decoder for this stack\n        encoder = self.encoders[i]\n        decoder = self.decoders[i]\n\n        # Forward pass through encoder\n        encoded, features = encoder(output)\n\n        # Process through middle block if it exists\n        middle_output = encoded\n        if self.middle_block and hasattr(self, \"middle_blocks\"):\n            for middle_block in self.middle_blocks:\n                middle_output = middle_block(middle_output)\n\n        if self.stem_blocks &gt; 0:\n            features.append(stem_output)\n\n        output = decoder(middle_output, features)\n        output[\"middle_output\"] = middle_output\n        outputs.append(output)\n\n    return outputs[-1]\n</code></pre>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create UNet from a config.</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>@classmethod\ndef from_config(cls, config: OmegaConf):\n    \"\"\"Create UNet from a config.\"\"\"\n    stem_blocks = 0\n    if config.stem_stride is not None:\n        stem_blocks = np.log2(config.stem_stride).astype(int)\n    down_blocks = np.log2(config.max_stride).astype(int) - stem_blocks\n    up_blocks = (\n        np.log2(config.max_stride / config.output_stride).astype(int) + stem_blocks\n    )\n    return cls(\n        in_channels=config.in_channels,\n        kernel_size=config.kernel_size,\n        filters=config.filters,\n        filters_rate=config.filters_rate,\n        down_blocks=down_blocks,\n        up_blocks=up_blocks,\n        stem_blocks=stem_blocks,\n        convs_per_block=config.convs_per_block,\n        middle_block=config.middle_block,\n        up_interpolate=config.up_interpolate,\n        stacks=config.stacks,\n        output_stride=config.output_stride,\n    )\n</code></pre>"},{"location":"api/architectures/utils/","title":"utils","text":""},{"location":"api/architectures/utils/#sleap_nn.architectures.utils","title":"<code>sleap_nn.architectures.utils</code>","text":"<p>Miscellaneous utility functions for architectures and modeling.</p> <p>Functions:</p> Name Description <code>get_act_fn</code> <p>Get an instance of an activation function module based on the provided name.</p> <code>get_children_layers</code> <p>Recursively retrieves a flattened list of all children modules and submodules within the given model.</p>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_act_fn","title":"<code>get_act_fn(activation)</code>","text":"<p>Get an instance of an activation function module based on the provided name.</p> <p>This function returns an instance of a PyTorch activation function module corresponding to the given activation function name.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>str</code> <p>Name of the activation function. Supported values are 'relu', 'sigmoid', 'tanh', 'softmax', and 'identity'.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: An instance of the requested activation function module.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the provided activation function name is not one of the supported values.</p> Example Source code in <code>sleap_nn/architectures/utils.py</code> <pre><code>def get_act_fn(activation: str) -&gt; nn.Module:\n    \"\"\"Get an instance of an activation function module based on the provided name.\n\n    This function returns an instance of a PyTorch activation function module\n    corresponding to the given activation function name.\n\n    Args:\n        activation (str): Name of the activation function. Supported values are 'relu', 'sigmoid', 'tanh', 'softmax', and 'identity'.\n\n    Returns:\n        nn.Module: An instance of the requested activation function module.\n\n    Raises:\n        KeyError: If the provided activation function name is not one of the supported values.\n\n    Example:\n        # Get an instance of the ReLU activation function\n        relu_fn = get_act_fn('relu')\n\n        # Apply the activation function to an input tensor\n        input_tensor = torch.randn(1, 64, 64)\n        output = relu_fn(input_tensor)\n    \"\"\"\n    activations = {\n        \"relu\": nn.ReLU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"tanh\": nn.Tanh(),\n        \"softmax\": nn.Softmax(dim=-1),\n        \"identity\": nn.Identity(),\n    }\n\n    if activation not in activations:\n        message = f\"Unsupported activation function: {activation}. Supported activations are: {', '.join(activations.keys())}\"\n        logger.error(message)\n        raise KeyError(message)\n\n    return activations[activation]\n</code></pre>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_act_fn--get-an-instance-of-the-relu-activation-function","title":"Get an instance of the ReLU activation function","text":"<p>relu_fn = get_act_fn('relu')</p>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_act_fn--apply-the-activation-function-to-an-input-tensor","title":"Apply the activation function to an input tensor","text":"<p>input_tensor = torch.randn(1, 64, 64) output = relu_fn(input_tensor)</p>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_children_layers","title":"<code>get_children_layers(model)</code>","text":"<p>Recursively retrieves a flattened list of all children modules and submodules within the given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to extract children from.</p> required <p>Returns:</p> Type Description <p>list of nn.Module: A flattened list containing all children modules and submodules.</p> Source code in <code>sleap_nn/architectures/utils.py</code> <pre><code>def get_children_layers(model: torch.nn.Module):\n    \"\"\"Recursively retrieves a flattened list of all children modules and submodules within the given model.\n\n    Args:\n        model: The PyTorch model to extract children from.\n\n    Returns:\n        list of nn.Module: A flattened list containing all children modules and submodules.\n    \"\"\"\n    children = list(model.children())\n    flattened_children = []\n    if len(children) == 0:\n        return model\n    else:\n        for child in children:\n            try:\n                flattened_children.extend(get_children_layers(child))\n            except TypeError:\n                flattened_children.append(get_children_layers(child))\n    return flattened_children\n</code></pre>"},{"location":"api/config/","title":"config","text":""},{"location":"api/config/#sleap_nn.config","title":"<code>sleap_nn.config</code>","text":"<p>Configuration modules for sleap-nn.</p> <p>Modules:</p> Name Description <code>data_config</code> <p>Serializable configuration classes for specifying all data configuration parameters.</p> <code>get_config</code> <p>This module contains functions to get the configuration for the data, model, and trainer.</p> <code>model_config</code> <p>Serializable configuration classes for specifying all model config parameters.</p> <code>trainer_config</code> <p>Serializable configuration classes for specifying all trainer config parameters.</p> <code>training_job_config</code> <p>Serializable configuration classes for specifying all training job parameters.</p> <code>utils</code> <p>Utilities for config building and validation.</p>"},{"location":"api/config/data_config/","title":"data_config","text":""},{"location":"api/config/data_config/#sleap_nn.config.data_config","title":"<code>sleap_nn.config.data_config</code>","text":"<p>Serializable configuration classes for specifying all data configuration parameters.</p> <p>These configuration classes are intended to specify all the parameters required to initialize the data config.</p> <p>Classes:</p> Name Description <code>AugmentationConfig</code> <p>Configuration of Augmentation.</p> <code>DataConfig</code> <p>Data configuration.</p> <code>GeometricConfig</code> <p>Configuration of Geometric (Optional).</p> <code>IntensityConfig</code> <p>Configuration of Intensity (Optional).</p> <code>PreprocessingConfig</code> <p>Configuration of Preprocessing.</p> <p>Functions:</p> Name Description <code>data_mapper</code> <p>Maps the legacy data configuration to the new data configuration.</p> <code>validate_proportion</code> <p>General Proportion Validation.</p>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.AugmentationConfig","title":"<code>AugmentationConfig</code>","text":"<p>Configuration of Augmentation.</p> <p>Attributes:</p> Name Type Description <code>intensity</code> <code>Optional[IntensityConfig]</code> <p>Configuration options for intensity-based augmentations like brightness, contrast, etc. If None, no intensity augmentations will be applied.</p> <code>geometric</code> <code>Optional[GeometricConfig]</code> <p>Configuration options for geometric augmentations like rotation, scaling, translation etc. If None, no geometric augmentations will be applied.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass AugmentationConfig:\n    \"\"\"Configuration of Augmentation.\n\n    Attributes:\n        intensity: Configuration options for intensity-based augmentations like brightness, contrast, etc. If None, no intensity augmentations will be applied.\n        geometric: Configuration options for geometric augmentations like rotation, scaling, translation etc. If None, no geometric augmentations will be applied.\n    \"\"\"\n\n    intensity: Optional[IntensityConfig] = None\n    geometric: Optional[GeometricConfig] = None\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.DataConfig","title":"<code>DataConfig</code>","text":"<p>Data configuration.</p> <p>Attributes:</p> Name Type Description <code>train_labels_path</code> <code>Optional[List[str]]</code> <p>(List[str]) List of paths to training data (<code>.slp</code> file(s)). Default: <code>None</code>.</p> <code>val_labels_path</code> <code>Optional[List[str]]</code> <p>(List[str]) List of paths to validation data (<code>.slp</code> file(s)). Default: <code>None</code>.</p> <code>validation_fraction</code> <code>float</code> <p>(float) Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the <code>validation_labels</code> are already specified, this has no effect. Default: <code>0.1</code>.</p> <code>test_file_path</code> <code>Optional[str]</code> <p>(str) Path to test dataset (<code>.slp</code> file or <code>.mp4</code> file). Note: This is used only with CLI to get evaluation on test set after training is completed. Default: <code>None</code>.</p> <code>provider</code> <code>str</code> <p>(str) Provider class to read the input sleap files. Only \"LabelsReader\" is currently supported for the training pipeline. Default: <code>\"LabelsReader\"</code>.</p> <code>user_instances_only</code> <code>bool</code> <p>(bool) <code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used. Default: <code>True</code>.</p> <code>data_pipeline_fw</code> <code>str</code> <p>(str) Framework to create the data loaders. One of [<code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code>]. Default: <code>\"torch_dataset\"</code>. (Note: When using <code>torch_dataset</code>, <code>num_workers</code> in <code>trainer_config</code> should be set to 0 as multiprocessing doesn't work with pickling video backends.)</p> <code>cache_img_path</code> <code>Optional[str]</code> <p>(str) Path to save <code>.jpg</code> images created with <code>torch_dataset_cache_img_disk</code> data pipeline framework. If <code>None</code>, the path provided in <code>trainer_config.save_ckpt</code> is used. The <code>train_imgs</code> and <code>val_imgs</code> dirs are created inside this path. Default: <code>None</code>.</p> <code>use_existing_imgs</code> <code>bool</code> <p>(bool) Use existing train and val images/ chunks in the <code>cache_img_path</code> for <code>torch_dataset_cache_img_disk</code> frameworks. If <code>True</code>, the <code>cache_img_path</code> should have <code>train_imgs</code> and <code>val_imgs</code> dirs. Default: <code>False</code>.</p> <code>delete_cache_imgs_after_training</code> <code>bool</code> <p>(bool) If <code>False</code>, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. Default: <code>True</code>.</p> <code>preprocessing</code> <code>PreprocessingConfig</code> <p>Configuration options related to data preprocessing.</p> <code>use_augmentations_train</code> <code>bool</code> <p>(bool) True if the data augmentation should be applied to the training data, else False. Default: <code>False</code>.</p> <code>augmentation_config</code> <code>Optional[AugmentationConfig]</code> <p>Configurations related to augmentation. (only if <code>use_augmentations_train</code> is <code>True</code>)</p> <code>skeletons</code> <code>Optional[list]</code> <p>skeleton configuration for the <code>.slp</code> file. This will be pulled from the train dataset and saved to the <code>training_config.yaml</code></p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass DataConfig:\n    \"\"\"Data configuration.\n\n    Attributes:\n        train_labels_path: (List[str]) List of paths to training data (`.slp` file(s)). *Default*: `None`.\n        val_labels_path: (List[str]) List of paths to validation data (`.slp` file(s)). *Default*: `None`.\n        validation_fraction: (float) Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the `validation_labels` are already specified, this has no effect. *Default*: `0.1`.\n        test_file_path: (str) Path to test dataset (`.slp` file or `.mp4` file). *Note*: This is used only with CLI to get evaluation on test set after training is completed. *Default*: `None`.\n        provider: (str) Provider class to read the input sleap files. Only \"LabelsReader\" is currently supported for the training pipeline. *Default*: `\"LabelsReader\"`.\n        user_instances_only: (bool) `True` if only user labeled instances should be used for training. If `False`, both user labeled and predicted instances would be used. *Default*: `True`.\n        data_pipeline_fw: (str) Framework to create the data loaders. One of [`torch_dataset`, `torch_dataset_cache_img_memory`, `torch_dataset_cache_img_disk`]. *Default*: `\"torch_dataset\"`. (Note: When using `torch_dataset`, `num_workers` in `trainer_config` should be set to 0 as multiprocessing doesn't work with pickling video backends.)\n        cache_img_path: (str) Path to save `.jpg` images created with `torch_dataset_cache_img_disk` data pipeline framework. If `None`, the path provided in `trainer_config.save_ckpt` is used. The `train_imgs` and `val_imgs` dirs are created inside this path. *Default*: `None`.\n        use_existing_imgs: (bool) Use existing train and val images/ chunks in the `cache_img_path` for `torch_dataset_cache_img_disk` frameworks. If `True`, the `cache_img_path` should have `train_imgs` and `val_imgs` dirs. *Default*: `False`.\n        delete_cache_imgs_after_training: (bool) If `False`, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. *Default*: `True`.\n        preprocessing: Configuration options related to data preprocessing.\n        use_augmentations_train: (bool) True if the data augmentation should be applied to the training data, else False. *Default*: `False`.\n        augmentation_config: Configurations related to augmentation. (only if `use_augmentations_train` is `True`)\n        skeletons: skeleton configuration for the `.slp` file. This will be pulled from the train dataset and saved to the `training_config.yaml`\n    \"\"\"\n\n    train_labels_path: Optional[List[str]] = None\n    val_labels_path: Optional[List[str]] = None  # TODO : revisit MISSING!\n    validation_fraction: float = 0.1\n    test_file_path: Optional[str] = None\n    provider: str = \"LabelsReader\"\n    user_instances_only: bool = True\n    data_pipeline_fw: str = \"torch_dataset\"\n    cache_img_path: Optional[str] = None\n    use_existing_imgs: bool = False\n    delete_cache_imgs_after_training: bool = True\n    preprocessing: PreprocessingConfig = field(factory=PreprocessingConfig)\n    use_augmentations_train: bool = False\n    augmentation_config: Optional[AugmentationConfig] = None\n    skeletons: Optional[list] = None\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.GeometricConfig","title":"<code>GeometricConfig</code>","text":"<p>Configuration of Geometric (Optional).</p> <p>Attributes:</p> Name Type Description <code>rotation_min</code> <code>float</code> <p>(float) Minimum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. Default: <code>-15.0</code>.</p> <code>rotation_max</code> <code>float</code> <p>(float) Maximum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. Default: <code>15.0</code>.</p> <code>scale_min</code> <code>float</code> <p>(float) Minimum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. Default: <code>0.9</code>.</p> <code>scale_max</code> <code>float</code> <p>(float) Maximum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. Default: <code>1.1</code>.</p> <code>translate_width</code> <code>float</code> <p>(float) Maximum absolute fraction for horizontal translation. For example, if translate_width=a, then horizontal shift is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a. Will not translate by default. Default: <code>0.0</code>.</p> <code>translate_height</code> <code>float</code> <p>(float) Maximum absolute fraction for vertical translation. For example, if translate_height=a, then vertical shift is randomly sampled in the range -img_height * a &lt; dy &lt; img_height * a. Will not translate by default. Default: <code>0.0</code>.</p> <code>affine_p</code> <code>float</code> <p>(float) Probability of applying random affine transformations. Default: <code>0.0</code>.</p> <code>erase_scale_min</code> <code>float</code> <p>(float) Minimum value of range of proportion of erased area against input image. Default: <code>0.0001</code>.</p> <code>erase_scale_max</code> <code>float</code> <p>(float) Maximum value of range of proportion of erased area against input image. Default: <code>0.01</code>.</p> <code>erase_ratio_min</code> <code>float</code> <p>(float) Minimum value of range of aspect ratio of erased area. Default: <code>1.0</code>.</p> <code>erase_ratio_max</code> <code>float</code> <p>(float) Maximum value of range of aspect ratio of erased area. Default: <code>1.0</code>.</p> <code>erase_p</code> <code>float</code> <p>(float) Probability of applying random erase. Default: <code>1.0</code>.</p> <code>mixup_lambda_min</code> <code>float</code> <p>(float) Minimum mixup strength value. Default: <code>0.01</code>.</p> <code>mixup_lambda_max</code> <code>float</code> <p>(float) Maximum mixup strength value. Default: <code>0.05</code>.</p> <code>mixup_p</code> <code>float</code> <p>(float) Probability of applying random mixup v2. Default: <code>0.0</code>.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass GeometricConfig:\n    \"\"\"Configuration of Geometric (Optional).\n\n    Attributes:\n        rotation_min: (float) Minimum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. *Default*: `-15.0`.\n        rotation_max: (float) Maximum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. *Default*: `15.0`.\n        scale_min: (float) Minimum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. *Default*: `0.9`.\n        scale_max: (float) Maximum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. *Default*: `1.1`.\n        translate_width: (float) Maximum absolute fraction for horizontal translation. For example, if translate_width=a, then horizontal shift is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a. Will not translate by default. *Default*: `0.0`.\n        translate_height: (float) Maximum absolute fraction for vertical translation. For example, if translate_height=a, then vertical shift is randomly sampled in the range -img_height * a &lt; dy &lt; img_height * a. Will not translate by default. *Default*: `0.0`.\n        affine_p: (float) Probability of applying random affine transformations. *Default*: `0.0`.\n        erase_scale_min: (float) Minimum value of range of proportion of erased area against input image. *Default*: `0.0001`.\n        erase_scale_max: (float) Maximum value of range of proportion of erased area against input image. *Default*: `0.01`.\n        erase_ratio_min: (float) Minimum value of range of aspect ratio of erased area. *Default*: `1.0`.\n        erase_ratio_max: (float) Maximum value of range of aspect ratio of erased area. *Default*: `1.0`.\n        erase_p: (float) Probability of applying random erase. *Default*: `1.0`.\n        mixup_lambda_min: (float) Minimum mixup strength value. *Default*: `0.01`.\n        mixup_lambda_max: (float) Maximum mixup strength value. *Default*: `0.05`.\n        mixup_p: (float) Probability of applying random mixup v2. *Default*: `0.0`.\n    \"\"\"\n\n    rotation_min: float = field(default=-15.0, validator=validators.ge(-180))\n    rotation_max: float = field(default=15.0, validator=validators.le(180))\n    scale_min: float = field(default=0.9, validator=validators.ge(0))\n    scale_max: float = field(default=1.1, validator=validators.ge(0))\n    translate_width: float = 0.0\n    translate_height: float = 0.0\n    affine_p: float = field(default=0.0, validator=validate_proportion)\n    erase_scale_min: float = 0.0001\n    erase_scale_max: float = 0.01\n    erase_ratio_min: float = 1.0\n    erase_ratio_max: float = 1.0\n    erase_p: float = field(default=0.0, validator=validate_proportion)\n    mixup_lambda_min: float = field(default=0.01, validator=validators.ge(0))\n    mixup_lambda_max: float = field(default=0.05, validator=validators.le(1))\n    mixup_p: float = field(default=0.0, validator=validate_proportion)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.IntensityConfig","title":"<code>IntensityConfig</code>","text":"<p>Configuration of Intensity (Optional).</p> <p>Attributes:</p> Name Type Description <code>uniform_noise_min</code> <code>float</code> <p>(float) Minimum value for uniform noise (uniform_noise_min &gt;=0). Default: <code>0.0</code>.</p> <code>uniform_noise_max</code> <code>float</code> <p>(float) Maximum value for uniform noise (uniform_noise_max &lt;&gt;=1). Default: <code>1.0</code>.</p> <code>uniform_noise_p</code> <code>float</code> <p>(float) Probability of applying random uniform noise. Default: <code>0.0</code>.</p> <code>gaussian_noise_mean</code> <code>float</code> <p>(float) The mean of the gaussian noise distribution. Default: <code>0.0</code>.</p> <code>gaussian_noise_std</code> <code>float</code> <p>(float) The standard deviation of the gaussian noise distribution. Default: <code>1.0</code>.</p> <code>gaussian_noise_p</code> <code>float</code> <p>(float) Probability of applying random gaussian noise. Default: <code>0.0</code>.</p> <code>contrast_min</code> <code>float</code> <p>(float) Minimum contrast factor to apply. Default: <code>0.9</code>.</p> <code>contrast_max</code> <code>float</code> <p>(float) Maximum contrast factor to apply. Default: <code>1.1</code>.</p> <code>contrast_p</code> <code>float</code> <p>(float) Probability of applying random contrast. Default: <code>0.0</code>.</p> <code>brightness_min</code> <code>float</code> <p>(float) Minimum brightness factor to apply. Default: <code>1.0</code>.</p> <code>brightness_max</code> <code>float</code> <p>(float) Maximum brightness factor to apply. Default: <code>1.0</code>.</p> <code>brightness_p</code> <code>float</code> <p>(float) Probability of applying random brightness. Default: <code>0.0</code>.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass IntensityConfig:\n    \"\"\"Configuration of Intensity (Optional).\n\n    Attributes:\n        uniform_noise_min: (float) Minimum value for uniform noise (uniform_noise_min &gt;=0). *Default*: `0.0`.\n        uniform_noise_max: (float) Maximum value for uniform noise (uniform_noise_max &lt;&gt;=1). *Default*: `1.0`.\n        uniform_noise_p: (float) Probability of applying random uniform noise. *Default*: `0.0`.\n        gaussian_noise_mean: (float) The mean of the gaussian noise distribution. *Default*: `0.0`.\n        gaussian_noise_std: (float) The standard deviation of the gaussian noise distribution. *Default*: `1.0`.\n        gaussian_noise_p: (float) Probability of applying random gaussian noise. *Default*: `0.0`.\n        contrast_min: (float) Minimum contrast factor to apply. *Default*: `0.9`.\n        contrast_max: (float) Maximum contrast factor to apply. *Default*: `1.1`.\n        contrast_p: (float) Probability of applying random contrast. *Default*: `0.0`.\n        brightness_min: (float) Minimum brightness factor to apply. *Default*: `1.0`.\n        brightness_max: (float) Maximum brightness factor to apply. *Default*: `1.0`.\n        brightness_p: (float) Probability of applying random brightness. *Default*: `0.0`.\n    \"\"\"\n\n    uniform_noise_min: float = field(default=0.0, validator=validators.ge(0))\n    uniform_noise_max: float = field(default=1.0, validator=validators.le(1))\n    uniform_noise_p: float = field(default=0.0, validator=validate_proportion)\n    gaussian_noise_mean: float = 0.0\n    gaussian_noise_std: float = 1.0\n    gaussian_noise_p: float = field(default=0.0, validator=validate_proportion)\n    contrast_min: float = field(default=0.9, validator=validators.ge(0))\n    contrast_max: float = field(default=1.1, validator=validators.ge(0))\n    contrast_p: float = field(default=0.0, validator=validate_proportion)\n    brightness_min: float = field(default=1.0, validator=validators.ge(0))\n    brightness_max: float = field(default=1.0, validator=validators.le(2))\n    brightness_p: float = field(default=0.0, validator=validate_proportion)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.PreprocessingConfig","title":"<code>PreprocessingConfig</code>","text":"<p>Configuration of Preprocessing.</p> <p>Attributes:</p> Name Type Description <code>ensure_rgb</code> <code>bool</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to <code>True</code>, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: <code>False</code>.</p> <code>ensure_grayscale</code> <code>bool</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: <code>False</code>.</p> <code>max_height</code> <code>Optional[int]</code> <p>(int) Maximum height the image should be padded to. If not provided, the original image size will be retained. Default: <code>None</code>.</p> <code>max_width</code> <code>Optional[int]</code> <p>(int) Maximum width the image should be padded to. If not provided, the original image size will be retained. Default: <code>None</code>.</p> <code>scale</code> <code>float</code> <p>(float) Factor to resize the image dimensions by, specified as a float. Default: <code>1.0</code>.</p> <code>crop_hw</code> <code>Optional[Tuple[int, int]]</code> <p>(Tuple[int]) Crop height and width of each instance (h, w) for centered-instance model. If <code>None</code>, this would be automatically computed based on the largest instance in the <code>sio.Labels</code> file. Default: <code>None</code>.</p> <code>min_crop_size</code> <code>Optional[int]</code> <p>(int) Minimum crop size to be used if <code>crop_hw</code> is <code>None</code>. Default: <code>100</code>.</p> <p>Methods:</p> Name Description <code>validate_scale</code> <p>Scale Validation.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass PreprocessingConfig:\n    \"\"\"Configuration of Preprocessing.\n\n    Attributes:\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to `True`, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. *Default*: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. *Default*: `False`.\n        max_height: (int) Maximum height the image should be padded to. If not provided, the original image size will be retained. *Default*: `None`.\n        max_width: (int) Maximum width the image should be padded to. If not provided, the original image size will be retained. *Default*: `None`.\n        scale: (float) Factor to resize the image dimensions by, specified as a float. *Default*: `1.0`.\n        crop_hw: (Tuple[int]) Crop height and width of each instance (h, w) for centered-instance model. If `None`, this would be automatically computed based on the largest instance in the `sio.Labels` file. *Default*: `None`.\n        min_crop_size: (int) Minimum crop size to be used if `crop_hw` is `None`. *Default*: `100`.\n    \"\"\"\n\n    ensure_rgb: bool = False\n    ensure_grayscale: bool = False\n    max_height: Optional[int] = None\n    max_width: Optional[int] = None\n    scale: float = field(\n        default=1.0, validator=lambda instance, attr, value: instance.validate_scale()\n    )\n    crop_hw: Optional[Tuple[int, int]] = None\n    min_crop_size: Optional[int] = 100  # to help app work in case of error\n\n    def validate_scale(self):\n        \"\"\"Scale Validation.\n\n        Ensures PreprocessingConfig's scale is a float&gt;=0 or list of floats&gt;=0\n        \"\"\"\n        if isinstance(self.scale, float) and self.scale &gt;= 0:\n            return\n        if isinstance(self.scale, list) and all(\n            isinstance(x, float) and x &gt;= 0 for x in self.scale\n        ):\n            return\n        message = \"PreprocessingConfig's scale must be a float or a list of floats.\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.PreprocessingConfig.validate_scale","title":"<code>validate_scale()</code>","text":"<p>Scale Validation.</p> <p>Ensures PreprocessingConfig's scale is a float&gt;=0 or list of floats&gt;=0</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>def validate_scale(self):\n    \"\"\"Scale Validation.\n\n    Ensures PreprocessingConfig's scale is a float&gt;=0 or list of floats&gt;=0\n    \"\"\"\n    if isinstance(self.scale, float) and self.scale &gt;= 0:\n        return\n    if isinstance(self.scale, list) and all(\n        isinstance(x, float) and x &gt;= 0 for x in self.scale\n    ):\n        return\n    message = \"PreprocessingConfig's scale must be a float or a list of floats.\"\n    logger.error(message)\n    raise ValueError(message)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.data_mapper","title":"<code>data_mapper(legacy_config)</code>","text":"<p>Maps the legacy data configuration to the new data configuration.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_config</code> <code>dict</code> <p>A dictionary containing the legacy data configuration.</p> required <p>Returns:</p> Type Description <code>DataConfig</code> <p>An instance of <code>DataConfig</code> with the mapped configuration.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>def data_mapper(legacy_config: dict) -&gt; DataConfig:\n    \"\"\"Maps the legacy data configuration to the new data configuration.\n\n    Args:\n        legacy_config: A dictionary containing the legacy data configuration.\n\n    Returns:\n        An instance of `DataConfig` with the mapped configuration.\n    \"\"\"\n    legacy_config_data = legacy_config.get(\"data\", {})\n    legacy_config_optimization = legacy_config.get(\"optimization\", {})\n    train_labels_path = legacy_config_data.get(\"labels\", {}).get(\n        \"training_labels\", None\n    )\n    val_labels_path = legacy_config_data.get(\"labels\", {}).get(\n        \"validation_labels\", None\n    )\n\n    # get skeleton(s)\n    json_skeletons = legacy_config_data.get(\"labels\", {}).get(\"skeletons\", None)\n    skeletons_list = None\n    if json_skeletons is not None:\n        skeletons_list = []\n        skeletons = SkeletonDecoder().decode(json_skeletons)\n        skeletons = yaml.safe_load(SkeletonYAMLEncoder().encode(skeletons))\n        for skl_name in skeletons.keys():\n            skl = skeletons[skl_name]\n            skl[\"name\"] = skl_name\n            skeletons_list.append(skl)\n\n    data_cfg_args = {}\n    preprocessing_args = {}\n    intensity_args = {}\n    geometric_args = {}\n\n    if train_labels_path is not None:\n        data_cfg_args[\"train_labels_path\"] = [train_labels_path]\n    if val_labels_path is not None:\n        data_cfg_args[\"val_labels_path\"] = [val_labels_path]\n    if (\n        legacy_config_data.get(\"labels\", {}).get(\"validation_fraction\", None)\n        is not None\n    ):\n        data_cfg_args[\"validation_fraction\"] = legacy_config_data[\"labels\"][\n            \"validation_fraction\"\n        ]\n    if legacy_config_data.get(\"labels\", {}).get(\"test_labels\", None) is not None:\n        data_cfg_args[\"test_file_path\"] = legacy_config_data[\"labels\"][\"test_labels\"]\n\n    # preprocessing\n    if legacy_config_data.get(\"preprocessing\", {}).get(\"ensure_rgb\", None) is not None:\n        preprocessing_args[\"ensure_rgb\"] = legacy_config_data[\"preprocessing\"][\n            \"ensure_rgb\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"ensure_grayscale\", None)\n        is not None\n    ):\n        preprocessing_args[\"ensure_grayscale\"] = legacy_config_data[\"preprocessing\"][\n            \"ensure_grayscale\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"target_height\", None)\n        is not None\n    ):\n        preprocessing_args[\"max_height\"] = legacy_config_data[\"preprocessing\"][\n            \"target_height\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"target_width\", None)\n        is not None\n    ):\n        preprocessing_args[\"max_width\"] = legacy_config_data[\"preprocessing\"][\n            \"target_width\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"input_scaling\", None)\n        is not None\n    ):\n        preprocessing_args[\"scale\"] = legacy_config_data[\"preprocessing\"][\n            \"input_scaling\"\n        ]\n    if (\n        legacy_config_data.get(\"instance_cropping\", {}).get(\"crop_size\", None)\n        is not None\n    ):\n        size = legacy_config_data[\"instance_cropping\"][\"crop_size\"]\n        preprocessing_args[\"crop_hw\"] = (size, size)\n\n    # augmentation\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"uniform_noise_min_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"uniform_noise_min\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"uniform_noise_min_val\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"uniform_noise_max_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"uniform_noise_max\"] = min(\n            legacy_config_optimization[\"augmentation_config\"][\"uniform_noise_max_val\"],\n            1.0,\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"uniform_noise\", None\n        )\n        is not None\n    ):\n        intensity_args[\"uniform_noise_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"uniform_noise\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"gaussian_noise_mean\", None\n        )\n        is not None\n    ):\n        intensity_args[\"gaussian_noise_mean\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"gaussian_noise_mean\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"gaussian_noise_stddev\", None\n        )\n        is not None\n    ):\n        intensity_args[\"gaussian_noise_std\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"gaussian_noise_stddev\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"gaussian_noise\", None\n        )\n        is not None\n    ):\n        intensity_args[\"gaussian_noise_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"gaussian_noise\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"contrast_min_gamma\", None\n        )\n        is not None\n    ):\n        intensity_args[\"contrast_min\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"contrast_min_gamma\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"contrast_max_gamma\", None\n        )\n        is not None\n    ):\n        intensity_args[\"contrast_max\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"contrast_max_gamma\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"contrast\", None)\n        is not None\n    ):\n        intensity_args[\"contrast_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"contrast\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"brightness_min_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"brightness_min\"] = min(\n            legacy_config_optimization[\"augmentation_config\"][\"brightness_min_val\"], 2.0\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"brightness_max_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"brightness_max\"] = min(\n            legacy_config_optimization[\"augmentation_config\"][\"brightness_max_val\"], 2.0\n        )  # kornia brightness_max can only be 2.0\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"brightness\", None\n        )\n        is not None\n    ):\n        intensity_args[\"brightness_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"brightness\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"rotation_min_angle\", None\n        )\n        is not None\n    ):\n        geometric_args[\"rotation_min\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"rotation_min_angle\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"rotation_max_angle\", None\n        )\n        is not None\n    ):\n        geometric_args[\"rotation_max\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"rotation_max_angle\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"scale_min\", None)\n        is not None\n    ):\n        geometric_args[\"scale_min\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_min\"\n        ]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"scale_max\", None)\n        is not None\n    ):\n        geometric_args[\"scale_max\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_max\"\n        ]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"scale\", None)\n        is not None\n    ):\n        geometric_args[\"scale_min\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_min\"\n        ]\n        geometric_args[\"scale_max\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_max\"\n        ]\n\n    geometric_args[\"affine_p\"] = (\n        1.0\n        if any(\n            [\n                legacy_config_optimization.get(\"augmentation_config\", {}).get(\n                    \"rotate\", False\n                ),\n                legacy_config_optimization.get(\"augmentation_config\", {}).get(\n                    \"scale\", False\n                ),\n            ]\n        )\n        else 0.0\n    )\n\n    data_cfg_args[\"preprocessing\"] = PreprocessingConfig(**preprocessing_args)\n    data_cfg_args[\"augmentation_config\"] = AugmentationConfig(\n        intensity=IntensityConfig(**intensity_args),\n        geometric=GeometricConfig(**geometric_args),\n    )\n\n    data_cfg_args[\"use_augmentations_train\"] = (\n        True if any(intensity_args.values()) or any(geometric_args.values()) else False\n    )\n    data_cfg_args[\"skeletons\"] = skeletons_list\n\n    return DataConfig(**data_cfg_args)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.validate_proportion","title":"<code>validate_proportion(instance, attribute, value)</code>","text":"<p>General Proportion Validation.</p> <p>Ensures all proportions are a 0&lt;=float&lt;=1.0</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>def validate_proportion(instance, attribute, value):\n    \"\"\"General Proportion Validation.\n\n    Ensures all proportions are a 0&lt;=float&lt;=1.0\n    \"\"\"\n    if not (0.0 &lt;= value &lt;= 1.0):\n        message = f\"{attribute.name} must be between 0.0 and 1.0, got {value}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/get_config/","title":"get_config","text":""},{"location":"api/config/get_config/#sleap_nn.config.get_config","title":"<code>sleap_nn.config.get_config</code>","text":"<p>This module contains functions to get the configuration for the data, model, and trainer.</p> <p>Functions:</p> Name Description <code>get_aug_config</code> <p>Create an augmentation configuration for training data.</p> <code>get_backbone_config</code> <p>Create a backbone configuration for neural network architecture.</p> <code>get_data_config</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p> <code>get_head_configs</code> <p>Create head configurations for pose estimation model outputs.</p> <code>get_model_config</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p> <code>get_trainer_config</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config","title":"<code>get_aug_config(intensity_aug=None, geometric_aug=None)</code>","text":"<p>Create an augmentation configuration for training data.</p> <p>This method creates an <code>AugmentationConfig</code> object based on the user-provided parameters for intensity and geometric augmentations. The function supports both string-based preset configurations and custom dictionary-based configurations.</p> <p>Parameters:</p> Name Type Description Default <code>intensity_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>Intensity augmentation configuration. Can be: - String: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom configuration matching <code>IntensityConfig</code> structure - None: No intensity augmentation applied</p> <code>None</code> <code>geometric_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>Geometric augmentation configuration. Can be: - String: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom configuration matching <code>GeometricConfig</code> structure - None: No geometric augmentation applied</p> <code>None</code> <p>Returns:</p> Name Type Description <code>AugmentationConfig</code> <p>Configured augmentation object with intensity and geometric settings.</p> <p>Examples:</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config--string-based-configuration","title":"String-based configuration","text":"<p>aug_config = get_aug_config(\"contrast\", \"rotation\")</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config--list-based-configuration","title":"List-based configuration","text":"<p>aug_config = get_aug_config([\"contrast\", \"brightness\"], [\"scale\", \"translate\"])</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config--dictionary-based-configuration","title":"Dictionary-based configuration","text":"<p>intensity_dict = {     \"uniform_noise_min\": 0.0,     \"uniform_noise_max\": 0.1,     \"uniform_noise_p\": 0.5,     \"contrast_p\": 1.0 } geometric_dict = {     \"rotation\": 15.0,     \"scale\": (0.9, 1.1),     \"affine_p\": 1.0 } aug_config = get_aug_config(intensity_dict, geometric_dict)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid augmentation options are provided.</p> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_aug_config(\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n):\n    \"\"\"Create an augmentation configuration for training data.\n\n    This method creates an `AugmentationConfig` object based on the user-provided parameters\n    for intensity and geometric augmentations. The function supports both string-based\n    preset configurations and custom dictionary-based configurations.\n\n    Args:\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"]\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom configuration matching `IntensityConfig` structure\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"]\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom configuration matching `GeometricConfig` structure\n            - None: No geometric augmentation applied\n\n    Returns:\n        AugmentationConfig: Configured augmentation object with intensity and geometric settings.\n\n    Examples:\n        # String-based configuration\n        aug_config = get_aug_config(\"contrast\", \"rotation\")\n\n        # List-based configuration\n        aug_config = get_aug_config([\"contrast\", \"brightness\"], [\"scale\", \"translate\"])\n\n        # Dictionary-based configuration\n        intensity_dict = {\n            \"uniform_noise_min\": 0.0,\n            \"uniform_noise_max\": 0.1,\n            \"uniform_noise_p\": 0.5,\n            \"contrast_p\": 1.0\n        }\n        geometric_dict = {\n            \"rotation\": 15.0,\n            \"scale\": (0.9, 1.1),\n            \"affine_p\": 1.0\n        }\n        aug_config = get_aug_config(intensity_dict, geometric_dict)\n\n    Raises:\n        ValueError: If invalid augmentation options are provided.\n    \"\"\"\n    aug_config = AugmentationConfig(\n        intensity=IntensityConfig(), geometric=GeometricConfig()\n    )\n    if isinstance(intensity_aug, str) or isinstance(intensity_aug, list):\n        if isinstance(intensity_aug, str):\n            intensity_aug = [intensity_aug]\n\n        for i in intensity_aug:\n            if i == \"uniform_noise\":\n                aug_config.intensity.uniform_noise_p = 1.0\n            elif i == \"gaussian_noise\":\n                aug_config.intensity.gaussian_noise_p = 1.0\n            elif i == \"contrast\":\n                aug_config.intensity.contrast_p = 1.0\n            elif i == \"brightness\":\n                aug_config.intensity.brightness_p = 1.0\n            else:\n                raise ValueError(\n                    f\"`{intensity_aug}` is not a valid intensity augmentation option. Please use one of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\"\n                )\n\n    elif isinstance(intensity_aug, dict):\n        aug_config.intensity = IntensityConfig(**intensity_aug)\n\n    if isinstance(geometric_aug, str) or isinstance(geometric_aug, list):\n        if isinstance(geometric_aug, str):\n            geometric_aug = [geometric_aug]\n\n        for g in geometric_aug:\n            if g == \"rotation\":\n                aug_config.geometric.affine_p = 1.0\n                aug_config.geometric.scale_min = 1.0\n                aug_config.geometric.scale_max = 1.0\n                aug_config.geometric.translate_height = 0\n                aug_config.geometric.translate_width = 0\n            elif g == \"scale\":\n                aug_config.geometric.scale_min = 0.9\n                aug_config.geometric.scale_max = 1.1\n                aug_config.geometric.affine_p = 1.0\n                aug_config.geometric.rotation_min = 0\n                aug_config.geometric.rotation_max = 0\n                aug_config.geometric.translate_height = 0\n                aug_config.geometric.translate_width = 0\n            elif g == \"translate\":\n                aug_config.geometric.translate_height = 0.2\n                aug_config.geometric.translate_width = 0.2\n                aug_config.geometric.affine_p = 1.0\n                aug_config.geometric.rotation_min = 0\n                aug_config.geometric.rotation_max = 0\n                aug_config.geometric.scale_min = 1.0\n                aug_config.geometric.scale_max = 1.0\n            elif g == \"erase_scale\":\n                aug_config.geometric.erase_p = 1.0\n            elif g == \"mixup\":\n                aug_config.geometric.mixup_p = 1.0\n            else:\n                raise ValueError(\n                    f\"`{geometric_aug}` is not a valid geometric augmentation option. Please use one of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\"\n                )\n\n    elif isinstance(geometric_aug, dict):\n        aug_config.geometric = GeometricConfig(**geometric_aug)\n\n    return aug_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_backbone_config","title":"<code>get_backbone_config(backbone_cfg)</code>","text":"<p>Create a backbone configuration for neural network architecture.</p> <p>This method creates a <code>BackboneConfig</code> object based on the user-provided parameters for the neural network backbone architecture. The function supports both string-based preset configurations and custom dictionary-based configurations for UNet, ConvNeXt, and Swin Transformer architectures.</p> <p>Parameters:</p> Name Type Description Default <code>backbone_cfg</code> <code>Union[str, Dict[str, Any]]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary.</p> required <p>Returns:</p> Name Type Description <code>BackboneConfig</code> <p>Configured backbone object with architecture-specific settings.</p> <p>Examples:</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_backbone_config--string-based-configuration","title":"String-based configuration","text":"<p>backbone_config = get_backbone_config(\"unet\") backbone_config = get_backbone_config(\"convnext_tiny\") backbone_config = get_backbone_config(\"swint_base\")</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_backbone_config--dictionary-based-configuration","title":"Dictionary-based configuration","text":"<p>unet_dict = {     \"unet\": {         \"in_channels\": 3,         \"filters\": 64,         \"max_stride\": 32,         \"output_stride\": 2,         \"kernel_size\": 3,         \"filters_rate\": 2.0     } } backbone_config = get_backbone_config(unet_dict)</p> <p>convnext_dict = {     \"convnext\": {         \"model_type\": \"tiny\",         \"in_channels\": 3,         \"pre_trained_weights\": \"ConvNeXt_Tiny_Weights\"     } } backbone_config = get_backbone_config(convnext_dict)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid backbone type is provided.</p> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_backbone_config(backbone_cfg: Union[str, Dict[str, Any]]):\n    \"\"\"Create a backbone configuration for neural network architecture.\n\n    This method creates a `BackboneConfig` object based on the user-provided parameters\n    for the neural network backbone architecture. The function supports both string-based\n    preset configurations and custom dictionary-based configurations for UNet, ConvNeXt,\n    and Swin Transformer architectures.\n\n    Args:\n        backbone_cfg: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n\n    Returns:\n        BackboneConfig: Configured backbone object with architecture-specific settings.\n\n    Examples:\n        # String-based configuration\n        backbone_config = get_backbone_config(\"unet\")\n        backbone_config = get_backbone_config(\"convnext_tiny\")\n        backbone_config = get_backbone_config(\"swint_base\")\n\n        # Dictionary-based configuration\n        unet_dict = {\n            \"unet\": {\n                \"in_channels\": 3,\n                \"filters\": 64,\n                \"max_stride\": 32,\n                \"output_stride\": 2,\n                \"kernel_size\": 3,\n                \"filters_rate\": 2.0\n            }\n        }\n        backbone_config = get_backbone_config(unet_dict)\n\n        convnext_dict = {\n            \"convnext\": {\n                \"model_type\": \"tiny\",\n                \"in_channels\": 3,\n                \"pre_trained_weights\": \"ConvNeXt_Tiny_Weights\"\n            }\n        }\n        backbone_config = get_backbone_config(convnext_dict)\n\n    Raises:\n        ValueError: If invalid backbone type is provided.\n    \"\"\"\n    backbone_config = BackboneConfig()\n    unet_config_mapper = {\n        \"unet\": UNetConfig(),\n        \"unet_medium_rf\": UNetMediumRFConfig(),\n        \"unet_large_rf\": UNetLargeRFConfig(),\n    }\n    convnext_config_mapper = {\n        \"convnext\": ConvNextConfig(),\n        \"convnext_tiny\": ConvNextConfig(),\n        \"convnext_small\": ConvNextSmallConfig(),\n        \"convnext_base\": ConvNextBaseConfig(),\n        \"convnext_large\": ConvNextLargeConfig(),\n    }\n    swint_config_mapper = {\n        \"swint\": SwinTConfig(),\n        \"swint_tiny\": SwinTConfig(),\n        \"swint_small\": SwinTSmallConfig(),\n        \"swint_base\": SwinTBaseConfig(),\n    }\n    if isinstance(backbone_cfg, str):\n        if backbone_cfg.startswith(\"unet\"):\n            backbone_config.unet = unet_config_mapper[backbone_cfg]\n        elif backbone_cfg.startswith(\"convnext\"):\n            backbone_config.convnext = convnext_config_mapper[backbone_cfg]\n        elif backbone_cfg.startswith(\"swint\"):\n            backbone_config.swint = swint_config_mapper[backbone_cfg]\n        else:\n            raise ValueError(\n                f\"{backbone_cfg} is not a valid backbone. Please choose one of ['unet', 'unet_medium_rf', 'unet_large_rf', 'convnext', 'convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large', 'swint', 'swint_tiny', 'swint_small', 'swint_base']\"\n            )\n\n    elif isinstance(backbone_cfg, dict):\n        backbone_config = BackboneConfig()\n        if \"unet\" in backbone_cfg:\n            backbone_config.unet = UNetConfig(**backbone_cfg[\"unet\"])\n        elif \"convnext\" in backbone_cfg:\n            backbone_config.convnext = ConvNextConfig(**backbone_cfg[\"convnext\"])\n        elif \"swint\" in backbone_cfg:\n            backbone_config.swint = SwinTConfig(**backbone_cfg[\"swint\"])\n\n    return backbone_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_data_config","title":"<code>get_data_config(train_labels_path=None, val_labels_path=None, validation_fraction=0.1, test_file_path=None, provider='LabelsReader', user_instances_only=True, data_pipeline_fw='torch_dataset', cache_img_path=None, use_existing_imgs=False, delete_cache_imgs_after_training=True, ensure_rgb=False, ensure_grayscale=False, scale=1.0, max_height=None, max_width=None, crop_hw=None, min_crop_size=100, use_augmentations_train=False, intensity_aug=None, geometry_aug=None)</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to training data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>val_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to validation data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>validation_fraction</code> <code>float</code> <p>Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the <code>validation_labels</code> are already specified, this has no effect. Default: 0.1.</p> <code>0.1</code> <code>test_file_path</code> <code>Optional[str]</code> <p>Path to test dataset (<code>.slp</code> file or <code>.mp4</code> file). Note: This is used to get evaluation on test set after training is completed.</p> <code>None</code> <code>provider</code> <code>str</code> <p>Provider class to read the input sleap files. Only \"LabelsReader\" supported for the training pipeline. Default: \"LabelsReader\".</p> <code>'LabelsReader'</code> <code>user_instances_only</code> <code>bool</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used. Default: <code>True</code>.</p> <code>True</code> <code>data_pipeline_fw</code> <code>str</code> <p>Framework to create the data loaders. One of [<code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code>]. Default: \"torch_dataset\".</p> <code>'torch_dataset'</code> <code>cache_img_path</code> <code>Optional[str]</code> <p>Path to save <code>.jpg</code> images created with <code>torch_dataset_cache_img_disk</code> data pipeline framework. If <code>None</code>, the path provided in <code>trainer_config.save_ckpt</code> is used (else working dir is used). The <code>train_imgs</code> and <code>val_imgs</code> dirs are created inside this path. Default: None.</p> <code>None</code> <code>use_existing_imgs</code> <code>bool</code> <p>Use existing train and val images/ chunks in the <code>cache_img_path</code> for <code>torch_dataset_cache_img_disk</code> frameworks. If <code>True</code>, the <code>cache_img_path</code> should have <code>train_imgs</code> and <code>val_imgs</code> dirs. Default: False.</p> <code>False</code> <code>delete_cache_imgs_after_training</code> <code>bool</code> <p>If <code>False</code>, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. Default: True.</p> <code>True</code> <code>ensure_rgb</code> <code>bool</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>False</code> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> required <code>ensure_grayscale</code> <code>bool</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>False</code> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> required <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>1.0</code> <code>max_height</code> <code>Optional[int]</code> <p>Maximum height the image should be padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>Maximum width the image should be padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>crop_hw</code> <code>Optional[Tuple[int, int]]</code> <p>Crop height and width of each instance (h, w) for centered-instance model. If <code>None</code>, this would be automatically computed based on the largest instance in the <code>sio.Labels</code> file. Default: None.</p> <code>None</code> <code>min_crop_size</code> <code>Optional[int]</code> <p>Minimum crop size to be used if <code>crop_hw</code> is <code>None</code>. Default: 100.</p> <code>100</code> <code>use_augmentations_train</code> <code>bool</code> <p>True if the data augmentation should be applied to the training data, else False. Default: False.</p> <code>False</code> <code>intensity_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"] or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.IntensityConfig</code>. For eg: {             \"uniform_noise_min\": 1.0,             \"uniform_noise_p\": 1.0         }</p> <code>None</code> <code>geometry_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"]. or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.GeometryConfig</code>. For eg: {             \"rotation\": 45,             \"affine_p\": 1.0         }</p> <code>None</code> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_data_config(\n    train_labels_path: Optional[List[str]] = None,\n    val_labels_path: Optional[List[str]] = None,\n    validation_fraction: float = 0.1,\n    test_file_path: Optional[str] = None,\n    provider: str = \"LabelsReader\",\n    user_instances_only: bool = True,\n    data_pipeline_fw: str = \"torch_dataset\",\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    delete_cache_imgs_after_training: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    scale: float = 1.0,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n    crop_hw: Optional[Tuple[int, int]] = None,\n    min_crop_size: Optional[int] = 100,\n    use_augmentations_train: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometry_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        train_labels_path: List of paths to training data (`.slp` file). Default: `None`\n        val_labels_path: List of paths to validation data (`.slp` file). Default: `None`\n        validation_fraction: Float between 0 and 1 specifying the fraction of the\n            training set to sample for generating the validation set. The remaining\n            labeled frames will be left in the training set. If the `validation_labels`\n            are already specified, this has no effect. Default: 0.1.\n        test_file_path: Path to test dataset (`.slp` file or `.mp4` file).\n            Note: This is used to get evaluation on test set after training is completed.\n        provider: Provider class to read the input sleap files. Only \"LabelsReader\"\n            supported for the training pipeline. Default: \"LabelsReader\".\n        user_instances_only: `True` if only user labeled instances should be used for\n            training. If `False`, both user labeled and predicted instances would be used.\n            Default: `True`.\n        data_pipeline_fw: Framework to create the data loaders. One of [`torch_dataset`,\n            `torch_dataset_cache_img_memory`, `torch_dataset_cache_img_disk`]. Default: \"torch_dataset\".\n        cache_img_path: Path to save `.jpg` images created with `torch_dataset_cache_img_disk` data pipeline\n            framework. If `None`, the path provided in `trainer_config.save_ckpt` is used (else working dir is used). The `train_imgs` and `val_imgs` dirs are created inside this path. Default: None.\n        use_existing_imgs: Use existing train and val images/ chunks in the `cache_img_path` for `torch_dataset_cache_img_disk` frameworks. If `True`, the `cache_img_path` should have `train_imgs` and `val_imgs` dirs.\n            Default: False.\n        delete_cache_imgs_after_training: If `False`, the images (torch_dataset_cache_img_disk) are\n            retained after training. Else, the files are deleted. Default: True.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        max_height: Maximum height the image should be padded to. If not provided, the\n            original image size will be retained. Default: None.\n        max_width: Maximum width the image should be padded to. If not provided, the\n            original image size will be retained. Default: None.\n        crop_hw: Crop height and width of each instance (h, w) for centered-instance model.\n            If `None`, this would be automatically computed based on the largest instance\n            in the `sio.Labels` file. Default: None.\n        min_crop_size: Minimum crop size to be used if `crop_hw` is `None`. Default: 100.\n        use_augmentations_train: True if the data augmentation should be applied to the\n            training data, else False. Default: False.\n        intensity_aug: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"]\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.IntensityConfig`.\n            For eg: {\n                        \"uniform_noise_min\": 1.0,\n                        \"uniform_noise_p\": 1.0\n                    }\n        geometry_aug: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"].\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.GeometryConfig`.\n            For eg: {\n                        \"rotation\": 45,\n                        \"affine_p\": 1.0\n                    }\n    \"\"\"\n    preprocessing_config = PreprocessingConfig(\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        max_height=max_height,\n        max_width=max_width,\n        scale=scale,\n        crop_hw=crop_hw,\n        min_crop_size=min_crop_size,\n    )\n    augmentation_config = None\n    if use_augmentations_train:\n        augmentation_config = get_aug_config(\n            intensity_aug=intensity_aug, geometric_aug=geometry_aug\n        )\n\n    # construct data config\n    data_config = DataConfig(\n        train_labels_path=train_labels_path,\n        val_labels_path=val_labels_path,\n        validation_fraction=validation_fraction,\n        test_file_path=test_file_path,\n        provider=provider,\n        user_instances_only=user_instances_only,\n        data_pipeline_fw=data_pipeline_fw,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        delete_cache_imgs_after_training=delete_cache_imgs_after_training,\n        preprocessing=preprocessing_config,\n        use_augmentations_train=use_augmentations_train,\n        augmentation_config=augmentation_config,\n    )\n\n    return data_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_head_configs","title":"<code>get_head_configs(head_cfg)</code>","text":"<p>Create head configurations for pose estimation model outputs.</p> <p>This method creates a <code>HeadConfig</code> object based on the user-provided parameters for the pose estimation model head layers. The function supports both string-based preset configurations and custom dictionary-based configurations for different model types including Single Instance, Centroid, Centered Instance, Bottom-Up, and Multi-Class variants.</p> <p>Parameters:</p> Name Type Description Default <code>head_cfg</code> <code>Union[str, Dict[str, Any]]</code> <p>Head configuration. Can be: - String: One of the preset head types:     - [\"single_instance\", \"centroid\", \"centered_instance\", \"bottomup\", \"multi_class_bottomup\", \"multi_class_topdown\"] - Dictionary: Custom configuration with structure:     {         \"single_instance\": {             \"confmaps\": {SingleInstanceConfMapsConfig parameters}         },         \"centroid\": {             \"confmaps\": {CentroidConfMapsConfig parameters}         },         \"centered_instance\": {             \"confmaps\": {CenteredInstanceConfMapsConfig parameters}         },         \"bottomup\": {             \"confmaps\": {BottomUpConfMapsConfig parameters},             \"pafs\": {PAFConfig parameters}         },         \"multi_class_bottomup\": {             \"confmaps\": {BottomUpConfMapsConfig parameters},             \"class_maps\": {ClassMapConfig parameters}         },         \"multi_class_topdown\": {             \"confmaps\": {CenteredInstanceConfMapsConfig parameters},             \"class_vectors\": {ClassVectorsConfig parameters}         }     }     Only one head type should be specified in the dictionary.</p> required <p>Returns:</p> Name Type Description <code>HeadConfig</code> <p>Configured head object with model-specific settings.</p> <p>Examples:</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_head_configs--string-based-configuration","title":"String-based configuration","text":"<p>head_configs = get_head_configs(\"single_instance\") head_configs = get_head_configs(\"bottomup\") head_configs = get_head_configs(\"multi_class_topdown\")</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_head_configs--dictionary-based-configuration","title":"Dictionary-based configuration","text":"<p>single_instance_dict = {     \"single_instance\": {         \"confmaps\": {             \"part_names\": [\"head\", \"tail\"],             \"sigma\": 2.5,             \"output_stride\": 2         }     } } head_configs = get_head_configs(single_instance_dict)</p> <p>bottomup_dict = {     \"bottomup\": {         \"confmaps\": {             \"part_names\": [\"head\", \"tail\"],             \"sigma\": 5.0,             \"output_stride\": 4,             \"loss_weight\": 1.0         },         \"pafs\": {             \"edges\": [(\"head\", \"tail\")],             \"sigma\": 15.0,             \"output_stride\": 4,             \"loss_weight\": 1.0         }     } } head_configs = get_head_configs(bottomup_dict)</p> <p>multi_class_dict = {     \"multi_class_topdown\": {         \"confmaps\": {             \"part_names\": [\"head\", \"tail\"],             \"sigma\": 5.0,             \"output_stride\": 16,             \"loss_weight\": 1.0         },         \"class_vectors\": {             \"classes\": None,  # Auto-inferred from track names             \"num_fc_layers\": 1,             \"num_fc_units\": 64,             \"output_stride\": 16,             \"loss_weight\": 1.0         }     } } head_configs = get_head_configs(multi_class_dict)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid head type is provided.</p> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_head_configs(head_cfg: Union[str, Dict[str, Any]]):\n    \"\"\"Create head configurations for pose estimation model outputs.\n\n    This method creates a `HeadConfig` object based on the user-provided parameters\n    for the pose estimation model head layers. The function supports both string-based\n    preset configurations and custom dictionary-based configurations for different model\n    types including Single Instance, Centroid, Centered Instance, Bottom-Up, and\n    Multi-Class variants.\n\n    Args:\n        head_cfg: Head configuration. Can be:\n            - String: One of the preset head types:\n                - [\"single_instance\", \"centroid\", \"centered_instance\", \"bottomup\", \"multi_class_bottomup\", \"multi_class_topdown\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"single_instance\": {\n                        \"confmaps\": {SingleInstanceConfMapsConfig parameters}\n                    },\n                    \"centroid\": {\n                        \"confmaps\": {CentroidConfMapsConfig parameters}\n                    },\n                    \"centered_instance\": {\n                        \"confmaps\": {CenteredInstanceConfMapsConfig parameters}\n                    },\n                    \"bottomup\": {\n                        \"confmaps\": {BottomUpConfMapsConfig parameters},\n                        \"pafs\": {PAFConfig parameters}\n                    },\n                    \"multi_class_bottomup\": {\n                        \"confmaps\": {BottomUpConfMapsConfig parameters},\n                        \"class_maps\": {ClassMapConfig parameters}\n                    },\n                    \"multi_class_topdown\": {\n                        \"confmaps\": {CenteredInstanceConfMapsConfig parameters},\n                        \"class_vectors\": {ClassVectorsConfig parameters}\n                    }\n                }\n                Only one head type should be specified in the dictionary.\n\n    Returns:\n        HeadConfig: Configured head object with model-specific settings.\n\n    Examples:\n        # String-based configuration\n        head_configs = get_head_configs(\"single_instance\")\n        head_configs = get_head_configs(\"bottomup\")\n        head_configs = get_head_configs(\"multi_class_topdown\")\n\n        # Dictionary-based configuration\n        single_instance_dict = {\n            \"single_instance\": {\n                \"confmaps\": {\n                    \"part_names\": [\"head\", \"tail\"],\n                    \"sigma\": 2.5,\n                    \"output_stride\": 2\n                }\n            }\n        }\n        head_configs = get_head_configs(single_instance_dict)\n\n        bottomup_dict = {\n            \"bottomup\": {\n                \"confmaps\": {\n                    \"part_names\": [\"head\", \"tail\"],\n                    \"sigma\": 5.0,\n                    \"output_stride\": 4,\n                    \"loss_weight\": 1.0\n                },\n                \"pafs\": {\n                    \"edges\": [(\"head\", \"tail\")],\n                    \"sigma\": 15.0,\n                    \"output_stride\": 4,\n                    \"loss_weight\": 1.0\n                }\n            }\n        }\n        head_configs = get_head_configs(bottomup_dict)\n\n        multi_class_dict = {\n            \"multi_class_topdown\": {\n                \"confmaps\": {\n                    \"part_names\": [\"head\", \"tail\"],\n                    \"sigma\": 5.0,\n                    \"output_stride\": 16,\n                    \"loss_weight\": 1.0\n                },\n                \"class_vectors\": {\n                    \"classes\": None,  # Auto-inferred from track names\n                    \"num_fc_layers\": 1,\n                    \"num_fc_units\": 64,\n                    \"output_stride\": 16,\n                    \"loss_weight\": 1.0\n                }\n            }\n        }\n        head_configs = get_head_configs(multi_class_dict)\n\n    Raises:\n        ValueError: If invalid head type is provided.\n    \"\"\"\n    head_configs = HeadConfig()\n    if isinstance(head_cfg, str):\n        if head_cfg == \"centered_instance\":\n            head_configs.centered_instance = CenteredInstanceConfig(\n                confmaps=CenteredInstanceConfMapsConfig\n            )\n        elif head_cfg == \"single_instance\":\n            head_configs.single_instance = SingleInstanceConfig(\n                confmaps=SingleInstanceConfMapsConfig\n            )\n        elif head_cfg == \"centroid\":\n            head_configs.centroid = CentroidConfig(confmaps=CentroidConfMapsConfig)\n        elif head_cfg == \"bottomup\":\n            head_configs.bottomup = BottomUpConfig(\n                confmaps=BottomUpConfMapsConfig, pafs=PAFConfig\n            )\n        elif head_cfg == \"multi_class_bottomup\":\n            head_configs.multi_class_bottomup = BottomUpMultiClassConfig(\n                confmaps=BottomUpConfMapsConfig, class_maps=ClassMapConfig\n            )\n        elif head_cfg == \"multi_class_topdown\":\n            head_configs.multi_class_topdown = TopDownCenteredInstanceMultiClassConfig(\n                confmaps=CenteredInstanceConfMapsConfig,\n                class_vectors=ClassVectorsConfig,\n            )\n        else:\n            raise ValueError(\n                f\"{head_cfg} is not a valid head type. Please choose one of ['bottomup', 'centered_instance', 'centroid', 'single_instance', 'multi_class_bottomup', 'multi_class_topdown']\"\n            )\n\n    elif isinstance(head_cfg, dict):\n        head_configs = HeadConfig()\n        if \"single_instance\" in head_cfg and head_cfg[\"single_instance\"] is not None:\n            head_configs.single_instance = SingleInstanceConfig(\n                confmaps=SingleInstanceConfMapsConfig(\n                    **head_cfg[\"single_instance\"][\"confmaps\"]\n                )\n            )\n        elif \"centroid\" in head_cfg and head_cfg[\"centroid\"] is not None:\n            head_configs.centroid = CentroidConfig(\n                confmaps=CentroidConfMapsConfig(**head_cfg[\"centroid\"][\"confmaps\"])\n            )\n        elif (\n            \"centered_instance\" in head_cfg\n            and head_cfg[\"centered_instance\"] is not None\n        ):\n            head_configs.centered_instance = CenteredInstanceConfig(\n                confmaps=CenteredInstanceConfMapsConfig(\n                    **head_cfg[\"centered_instance\"][\"confmaps\"]\n                )\n            )\n        elif \"bottomup\" in head_cfg and head_cfg[\"bottomup\"] is not None:\n            head_configs.bottomup = BottomUpConfig(\n                confmaps=BottomUpConfMapsConfig(\n                    **head_cfg[\"bottomup\"][\"confmaps\"],\n                ),\n                pafs=PAFConfig(**head_cfg[\"bottomup\"][\"pafs\"]),\n            )\n        elif (\n            \"multi_class_bottomup\" in head_cfg\n            and head_cfg[\"multi_class_bottomup\"] is not None\n        ):\n            head_configs.multi_class_bottomup = BottomUpMultiClassConfig(\n                confmaps=BottomUpConfMapsConfig(\n                    **head_cfg[\"multi_class_bottomup\"][\"confmaps\"]\n                ),\n                class_maps=ClassMapConfig(\n                    **head_cfg[\"multi_class_bottomup\"][\"class_maps\"]\n                ),\n            )\n        elif (\n            \"multi_class_topdown\" in head_cfg\n            and head_cfg[\"multi_class_topdown\"] is not None\n        ):\n            head_configs.multi_class_topdown = TopDownCenteredInstanceMultiClassConfig(\n                confmaps=CenteredInstanceConfMapsConfig(\n                    **head_cfg[\"multi_class_topdown\"][\"confmaps\"]\n                ),\n                class_vectors=ClassVectorsConfig(\n                    **head_cfg[\"multi_class_topdown\"][\"class_vectors\"]\n                ),\n            )\n\n    return head_configs\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_model_config","title":"<code>get_model_config(init_weight='default', pretrained_backbone_weights=None, pretrained_head_weights=None, backbone_config='unet', head_configs=None)</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>init_weight</code> <code>str</code> <p>model weights initialization method. \"default\" uses kaiming uniform initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".</p> <code>'default'</code> <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the backbone is initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the head layers are initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>backbone_config</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set, then pass a dictionary with the structure: {     \"unet((or) convnext (or)swint)\":         {(params in the corresponding architecture given in <code>sleap_nn.config.model_config.backbone_config</code>)         } }. For eg: {             \"unet\":                 {                     \"in_channels\": 3,                     \"filters\": 64,                     \"max_stride\": 32,                     \"output_stride\": 2                 }         }</p> <code>'unet'</code> <code>head_configs</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"]. The default <code>sigma</code> and <code>output_strides</code> are used if a string is passed. To set custom parameters, pass in a dictionary with the structure: {     \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):         {             \"confmaps\":                 {                     # params in the corresponding head type given in <code>sleap_nn.config.model_config.head_configs</code>                 },             \"pafs\":                 {                     # only for bottomup                 }         } }. For eg: {             \"single_instance\":                 {                     \"confmaps\":                         {                             \"part_names\": None,                             \"sigma\": 2.5,                             \"output_stride\": 2                         }                 }         }</p> <code>None</code> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_model_config(\n    init_weight: str = \"default\",\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    backbone_config: Union[str, Dict[str, Any]] = \"unet\",\n    head_configs: Union[str, Dict[str, Any]] = None,\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        init_weight: model weights initialization method. \"default\" uses kaiming uniform\n            initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".\n        pretrained_backbone_weights: Path of the `ckpt` (or `.h5` file from SLEAP) file with which the backbone is\n            initialized. If `None`, random init is used. Default: None.\n        pretrained_head_weights: Path of the `ckpt` (or `.h5` file from SLEAP) file with which the head layers are\n            initialized. If `None`, random init is used. Default: None.\n        backbone_config: One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\",\n            \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\",\n            \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set,\n            then pass a dictionary with the structure:\n            {\n                \"unet((or) convnext (or)swint)\":\n                    {(params in the corresponding architecture given in `sleap_nn.config.model_config.backbone_config`)\n                    }\n            }.\n            For eg: {\n                        \"unet\":\n                            {\n                                \"in_channels\": 3,\n                                \"filters\": 64,\n                                \"max_stride\": 32,\n                                \"output_stride\": 2\n                            }\n                    }\n        head_configs: One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"].\n            The default `sigma` and `output_strides` are used if a string is passed. To\n            set custom parameters, pass in a dictionary with the structure:\n            {\n                \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):\n                    {\n                        \"confmaps\":\n                            {\n                                # params in the corresponding head type given in `sleap_nn.config.model_config.head_configs`\n                            },\n                        \"pafs\":\n                            {\n                                # only for bottomup\n                            }\n                    }\n            }.\n            For eg: {\n                        \"single_instance\":\n                            {\n                                \"confmaps\":\n                                    {\n                                        \"part_names\": None,\n                                        \"sigma\": 2.5,\n                                        \"output_stride\": 2\n                                    }\n                            }\n                    }\n\n    \"\"\"\n    backbone_config = get_backbone_config(backbone_cfg=backbone_config)\n    head_configs = get_head_configs(head_cfg=head_configs)\n    model_config = ModelConfig(\n        init_weights=init_weight,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n    )\n    return model_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_trainer_config","title":"<code>get_trainer_config(batch_size=1, shuffle_train=False, num_workers=0, ckpt_save_top_k=1, ckpt_save_last=None, trainer_num_devices='auto', trainer_accelerator='auto', enable_progress_bar=True, min_train_steps_per_epoch=200, train_steps_per_epoch=None, visualize_preds_during_training=False, keep_viz=False, max_epochs=10, seed=0, use_wandb=False, save_ckpt=False, save_ckpt_path=None, resume_ckpt_path=None, wandb_entity=None, wandb_project=None, wandb_name=None, wandb_api_key=None, wandb_mode=None, wandb_resume_prv_runid=None, wandb_group_name=None, optimizer='Adam', learning_rate=0.001, amsgrad=False, lr_scheduler=None, early_stopping=False, early_stopping_min_delta=0.0, early_stopping_patience=1, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, zmq_publish_address=None, zmq_controller_address=None, zmq_controller_timeout=10)</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples per batch or batch size for training data. Default: 4.</p> <code>1</code> <code>shuffle_train</code> <code>bool</code> <p>True to have the train data reshuffled at every epoch. Default: False.</p> <code>False</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: 0.</p> <code>0</code> <code>ckpt_save_top_k</code> <code>int</code> <p>If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. Default: 1.</p> <code>1</code> <code>ckpt_save_last</code> <code>Optional[bool]</code> <p>When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. Default: False.</p> <code>None</code> <code>trainer_num_devices</code> <code>Union[str, int]</code> <p>Number of devices to train on (int), which devices to train on (list or str), or \"auto\" to select automatically. Default: \"auto\".</p> <code>'auto'</code> <code>trainer_accelerator</code> <code>str</code> <p>One of the (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the <code>Trainer</code> to be connected to. Default: \"auto\".</p> <code>'auto'</code> <code>enable_progress_bar</code> <code>bool</code> <p>When True, enables printing the logs during training. Default: False.</p> <code>True</code> <code>min_train_steps_per_epoch</code> <code>int</code> <p>Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer <code>limit_train_batches</code> parameter of Torch <code>Trainer</code>. Default: 200.</p> <code>200</code> <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data or <code>min_train_steps_per_epoch</code>, whichever is largest. Default: <code>None</code>. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>None</code> <code>visualize_preds_during_training</code> <code>bool</code> <p>If set to <code>True</code>, sample predictions (keypoints  + confidence maps) are saved to <code>viz</code> folder in the ckpt dir and in wandb table.</p> <code>False</code> <code>keep_viz</code> <code>bool</code> <p>If set to <code>True</code>, the <code>viz</code> folder will be kept after training. If <code>False</code>, the <code>viz</code> folder will be deleted after training. Only applies when <code>visualize_preds_during_training</code> is <code>True</code>.</p> <code>False</code> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs to run. Default: 100.</p> <code>10</code> <code>seed</code> <code>int</code> <p>Seed value for the current experiment. default: 1000.</p> <code>0</code> <code>save_ckpt</code> <code>bool</code> <p>True to enable checkpointing. Default: False.</p> <code>False</code> <code>save_ckpt_path</code> <code>Optional[str]</code> <p>Directory path to save the training config and checkpoint files. If <code>None</code> and <code>save_ckpt</code> is <code>True</code>, then the current working dir is used as the ckpt path. Default: None</p> <code>None</code> <code>resume_ckpt_path</code> <code>Optional[str]</code> <p>Path to <code>.ckpt</code> file from which training is resumed. Default: None.</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>True to enable wandb logging. Default: False.</p> <code>False</code> <code>wandb_entity</code> <code>Optional[str]</code> <p>Entity of wandb project. Default: None. (The default entity in the user profile settings is used)</p> <code>None</code> <code>wandb_project</code> <code>Optional[str]</code> <p>Project name for the current wandb run. Default: None.</p> <code>None</code> <code>wandb_name</code> <code>Optional[str]</code> <p>Name of the current wandb run. Default: None.</p> <code>None</code> <code>wandb_api_key</code> <code>Optional[str]</code> <p>API key. The API key is masked when saved to config files. Default: None.</p> <code>None</code> <code>wandb_mode</code> <code>Optional[str]</code> <p>\"offline\" if only local logging is required. Default: None.</p> <code>None</code> <code>wandb_resume_prv_runid</code> <code>Optional[str]</code> <p>Previous run ID if training should be resumed from a previous ckpt. Default: None</p> <code>None</code> <code>wandb_group_name</code> <code>Optional[str]</code> <p>Group name for the wandb run. Default: None.</p> <code>None</code> <code>optimizer</code> <code>str</code> <p>Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".</p> <code>'Adam'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of type float. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>bool</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <code>lr_scheduler</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in <code>sleap_nn.config.trainer_config</code> are used). To use custom values, pass a dictionary with the structure in <code>sleap_nn.config.trainer_config.LRSchedulerConfig</code>. For eg, {             \"step_lr\":                 {                     (params in <code>sleap_nn.config.trainer_config.StepLRConfig</code>)                 }         }</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>True if early stopping should be enabled. Default: False.</p> <code>False</code> <code>early_stopping_min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. Default: 0.0.</p> <code>0.0</code> <code>early_stopping_patience</code> <code>int</code> <p>Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. Default: 1.</p> <code>1</code> <code>online_mining</code> <code>bool</code> <p>If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss.</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>float</code> <p>The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>int</code> <p>The minimum number of keypoints that will be considered as \"hard\", even if they are not below the <code>hard_to_easy_ratio</code>.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether.</p> <code>None</code> <code>loss_scale</code> <code>float</code> <p>Factor to scale the hard keypoint losses by for oks.</p> <code>5.0</code> <code>zmq_publish_address</code> <code>Optional[str]</code> <p>(str) Specifies the address and port to which the training logs (loss values) should be sent to.</p> <code>None</code> <code>zmq_controller_address</code> <code>Optional[str]</code> <p>(str) Specifies the address and port to listen to to stop the training (specific to SLEAP GUI).</p> <code>None</code> <code>zmq_controller_timeout</code> <code>int</code> <p>(int) Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed.</p> <code>10</code> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_trainer_config(\n    batch_size: int = 1,\n    shuffle_train: bool = False,\n    num_workers: int = 0,\n    ckpt_save_top_k: int = 1,\n    ckpt_save_last: Optional[bool] = None,\n    trainer_num_devices: Union[str, int] = \"auto\",\n    trainer_accelerator: str = \"auto\",\n    enable_progress_bar: bool = True,\n    min_train_steps_per_epoch: int = 200,\n    train_steps_per_epoch: Optional[int] = None,\n    visualize_preds_during_training: bool = False,\n    keep_viz: bool = False,\n    max_epochs: int = 10,\n    seed: int = 0,\n    use_wandb: bool = False,\n    save_ckpt: bool = False,\n    save_ckpt_path: Optional[str] = None,\n    resume_ckpt_path: Optional[str] = None,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_name: Optional[str] = None,\n    wandb_api_key: Optional[str] = None,\n    wandb_mode: Optional[str] = None,\n    wandb_resume_prv_runid: Optional[str] = None,\n    wandb_group_name: Optional[str] = None,\n    optimizer: str = \"Adam\",\n    learning_rate: float = 1e-3,\n    amsgrad: bool = False,\n    lr_scheduler: Optional[Union[str, Dict[str, Any]]] = None,\n    early_stopping: bool = False,\n    early_stopping_min_delta: float = 0.0,\n    early_stopping_patience: int = 1,\n    online_mining: bool = False,\n    hard_to_easy_ratio: float = 2.0,\n    min_hard_keypoints: int = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: float = 5.0,\n    zmq_publish_address: Optional[str] = None,\n    zmq_controller_address: Optional[str] = None,\n    zmq_controller_timeout: int = 10,\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        batch_size: Number of samples per batch or batch size for training data. Default: 4.\n        shuffle_train: True to have the train data reshuffled at every epoch. Default: False.\n        num_workers: Number of subprocesses to use for data loading. 0 means that the data\n            will be loaded in the main process. Default: 0.\n        ckpt_save_top_k: If save_top_k == k, the best k models according to the quantity\n            monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1,\n            all models are saved. Please note that the monitors are checked every every_n_epochs\n            epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an\n            epoch, the name of the saved file will be appended with a version count starting\n            with v1 unless enable_version_counter is set to False. Default: 1.\n        ckpt_save_last: When True, saves a last.ckpt whenever a checkpoint file gets saved.\n            On a local filesystem, this will be a symbolic link, and otherwise a copy of\n            the checkpoint file. This allows accessing the latest checkpoint in a deterministic\n            manner. Default: False.\n        trainer_num_devices: Number of devices to train on (int), which devices to train\n            on (list or str), or \"auto\" to select automatically. Default: \"auto\".\n        trainer_accelerator: One of the (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\"). \"auto\" recognises\n            the machine the model is running on and chooses the appropriate accelerator for\n            the `Trainer` to be connected to. Default: \"auto\".\n        enable_progress_bar: When True, enables printing the logs during training. Default: False.\n        min_train_steps_per_epoch: Minimum number of iterations in a single epoch. (Useful if model\n            is trained with very few data points). Refer `limit_train_batches` parameter\n            of Torch `Trainer`. Default: 200.\n        train_steps_per_epoch: Number of minibatches (steps) to train for in an epoch. If set to `None`,\n            this is set to the number of batches in the training data or `min_train_steps_per_epoch`,\n            whichever is largest. Default: `None`. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        visualize_preds_during_training: If set to `True`, sample predictions (keypoints  + confidence maps)\n            are saved to `viz` folder in the ckpt dir and in wandb table.\n        keep_viz: If set to `True`, the `viz` folder will be kept after training. If `False`, the `viz` folder\n            will be deleted after training. Only applies when `visualize_preds_during_training` is `True`.\n        max_epochs: Maximum number of epochs to run. Default: 100.\n        seed: Seed value for the current experiment. default: 1000.\n        save_ckpt: True to enable checkpointing. Default: False.\n        save_ckpt_path: Directory path to save the training config and checkpoint files.\n            If `None` and `save_ckpt` is `True`, then the current working dir is used as\n            the ckpt path. Default: None\n        resume_ckpt_path: Path to `.ckpt` file from which training is resumed. Default: None.\n        use_wandb: True to enable wandb logging. Default: False.\n        wandb_entity: Entity of wandb project. Default: None.\n            (The default entity in the user profile settings is used)\n        wandb_project: Project name for the current wandb run. Default: None.\n        wandb_name: Name of the current wandb run. Default: None.\n        wandb_api_key: API key. The API key is masked when saved to config files. Default: None.\n        wandb_mode: \"offline\" if only local logging is required. Default: None.\n        wandb_resume_prv_runid: Previous run ID if training should be resumed from a previous\n            ckpt. Default: None\n        wandb_group_name: Group name for the wandb run. Default: None.\n        optimizer: Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".\n        learning_rate: Learning rate of type float. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n        lr_scheduler: One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in\n            `sleap_nn.config.trainer_config` are used). To use custom values, pass a\n            dictionary with the structure in `sleap_nn.config.trainer_config.LRSchedulerConfig`.\n            For eg, {\n                        \"step_lr\":\n                            {\n                                (params in `sleap_nn.config.trainer_config.StepLRConfig`)\n                            }\n                    }\n        early_stopping: True if early stopping should be enabled. Default: False.\n        early_stopping_min_delta: Minimum change in the monitored quantity to qualify as\n            an improvement, i.e. an absolute change of less than or equal to min_delta,\n            will count as no improvement. Default: 0.0.\n        early_stopping_patience: Number of checks with no improvement after which training\n            will be stopped. Under the default configuration, one check happens after every\n            training epoch. Default: 1.\n        online_mining: If True, online hard keypoint mining (OHKM) will be enabled. When\n            this is enabled, the loss is computed per keypoint (or edge for PAFs) and\n            sorted from lowest (easy) to highest (hard). The hard keypoint loss will be\n            scaled to have a higher weight in the total loss, encouraging the training\n            to focus on tricky body parts that are more difficult to learn.\n            If False, no mining will be performed and all keypoints will be weighted\n            equally in the loss.\n        hard_to_easy_ratio: The minimum ratio of the individual keypoint loss with\n            respect to the lowest keypoint loss in order to be considered as \"hard\".\n            This helps to switch focus on across groups of keypoints during training.\n        min_hard_keypoints: The minimum number of keypoints that will be considered as\n            \"hard\", even if they are not below the `hard_to_easy_ratio`.\n        max_hard_keypoints: The maximum number of hard keypoints to apply scaling to.\n            This can help when there are few very easy keypoints which may skew the\n            ratio and result in loss scaling being applied to most keypoints, which can\n            reduce the impact of hard mining altogether.\n        loss_scale: Factor to scale the hard keypoint losses by for oks.\n        zmq_publish_address: (str) Specifies the address and port to which the training logs (loss values) should be sent to.\n        zmq_controller_address: (str) Specifies the address and port to listen to to stop the training (specific to SLEAP GUI).\n        zmq_controller_timeout: (int) Polling timeout in microseconds specified as an integer. This controls how long the poller\n            should wait to receive a response and should be set to a small value to minimize the impact on training speed.\n    \"\"\"\n    # constrict trainer config\n    train_dataloader_cfg = DataLoaderConfig(\n        batch_size=batch_size, shuffle=shuffle_train, num_workers=num_workers\n    )\n    val_dataloader_cfg = DataLoaderConfig(\n        batch_size=batch_size, shuffle=False, num_workers=num_workers\n    )\n\n    lr_scheduler_cfg = LRSchedulerConfig()\n    if isinstance(lr_scheduler, str):\n        if lr_scheduler == \"step_lr\":\n            lr_scheduler_cfg.step_lr = StepLRConfig()\n        elif lr_scheduler == \"reduce_lr_on_plateau\":\n            lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig()\n        else:\n            message = f\"{lr_scheduler} is not a valid scheduler. Please choose one of ['step_lr', 'reduce_lr_on_plateau']\"\n            logger.error(message)\n            raise ValueError(message)\n    elif isinstance(lr_scheduler, dict):\n        if lr_scheduler is None:\n            lr_scheduler = {\n                \"step_lr\": None,\n                \"reduce_lr_on_plateau\": None,\n            }\n        for k, v in lr_scheduler.items():\n            if v is not None:\n                if k == \"step_lr\":\n                    lr_scheduler_cfg.step_lr = StepLRConfig(**v)\n                    break\n                elif k == \"reduce_lr_on_plateau\":\n                    lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig(**v)\n                    break\n\n    trainer_config = TrainerConfig(\n        train_data_loader=train_dataloader_cfg,\n        val_data_loader=val_dataloader_cfg,\n        model_ckpt=ModelCkptConfig(\n            save_top_k=ckpt_save_top_k, save_last=ckpt_save_last\n        ),\n        trainer_devices=trainer_num_devices,\n        trainer_accelerator=trainer_accelerator,\n        enable_progress_bar=enable_progress_bar,\n        min_train_steps_per_epoch=min_train_steps_per_epoch,\n        train_steps_per_epoch=train_steps_per_epoch,\n        visualize_preds_during_training=visualize_preds_during_training,\n        keep_viz=keep_viz,\n        max_epochs=max_epochs,\n        seed=seed,\n        use_wandb=use_wandb,\n        wandb=WandBConfig(\n            entity=wandb_entity,\n            project=wandb_project,\n            name=wandb_name,\n            api_key=wandb_api_key,\n            wandb_mode=wandb_mode,\n            prv_runid=wandb_resume_prv_runid,\n            group=wandb_group_name,\n        ),\n        save_ckpt=save_ckpt,\n        save_ckpt_path=save_ckpt_path,\n        resume_ckpt_path=resume_ckpt_path,\n        optimizer_name=optimizer,\n        optimizer=OptimizerConfig(lr=learning_rate, amsgrad=amsgrad),\n        lr_scheduler=lr_scheduler_cfg,\n        early_stopping=EarlyStoppingConfig(\n            min_delta=early_stopping_min_delta,\n            patience=early_stopping_patience,\n            stop_training_on_plateau=early_stopping,\n        ),\n        online_hard_keypoint_mining=HardKeypointMiningConfig(\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n        ),\n        zmq=ZMQConfig(\n            controller_address=zmq_controller_address,\n            controller_polling_timeout=zmq_controller_timeout,\n            publish_address=zmq_publish_address,\n        ),\n    )\n    return trainer_config\n</code></pre>"},{"location":"api/config/model_config/","title":"model_config","text":""},{"location":"api/config/model_config/#sleap_nn.config.model_config","title":"<code>sleap_nn.config.model_config</code>","text":"<p>Serializable configuration classes for specifying all model config parameters.</p> <p>These configuration classes are intended to specify all the parameters required to initialize the model config.</p> <p>Classes:</p> Name Description <code>BackboneConfig</code> <p>Configurations related to model backbone configuration.</p> <code>BottomUpConfMapsConfig</code> <p>Bottomup configuration map.</p> <code>BottomUpConfig</code> <p>bottomup head_config.</p> <code>BottomUpMultiClassConfig</code> <p>Head config for BottomUp Id models.</p> <code>CenteredInstanceConfMapsConfig</code> <p>Centered Instance configuration map.</p> <code>CenteredInstanceConfig</code> <p>centered_instance head_config.</p> <code>CentroidConfMapsConfig</code> <p>Centroid configuration map.</p> <code>CentroidConfig</code> <p>centroid head_config.</p> <code>ClassMapConfig</code> <p>Class map head config.</p> <code>ClassVectorsConfig</code> <p>Configurations for class vectors heads.</p> <code>ConvNextBaseConfig</code> <p>Convnext configuration for backbone.</p> <code>ConvNextConfig</code> <p>Convnext configuration for backbone.</p> <code>ConvNextLargeConfig</code> <p>Convnext configuration for backbone.</p> <code>ConvNextSmallConfig</code> <p>Convnext configuration for backbone.</p> <code>HeadConfig</code> <p>Configurations related to the model output head type.</p> <code>ModelConfig</code> <p>Configurations related to model architecture.</p> <code>PAFConfig</code> <p>PAF configuration map.</p> <code>SingleInstanceConfMapsConfig</code> <p>Single Instance configuration map.</p> <code>SingleInstanceConfig</code> <p>single instance head_config.</p> <code>SwinTBaseConfig</code> <p>SwinT configuration for backbone.</p> <code>SwinTConfig</code> <p>SwinT configuration (tiny) for backbone.</p> <code>SwinTSmallConfig</code> <p>SwinT configuration (small) for backbone.</p> <code>TopDownCenteredInstanceMultiClassConfig</code> <p>Head config for TopDown centered instance ID models.</p> <code>UNetConfig</code> <p>UNet config for backbone.</p> <code>UNetLargeRFConfig</code> <p>UNet config for backbone with large receptive field.</p> <code>UNetMediumRFConfig</code> <p>UNet config for backbone with medium receptive field.</p> <p>Functions:</p> Name Description <code>model_mapper</code> <p>Map the legacy model configuration to the new model configuration.</p>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BackboneConfig","title":"<code>BackboneConfig</code>","text":"<p>Configurations related to model backbone configuration.</p> <p>Attributes:</p> Name Type Description <code>unet</code> <code>Optional[UNetConfig]</code> <p>An instance of <code>UNetConfig</code>.</p> <code>convnext</code> <code>Optional[ConvNextConfig]</code> <p>An instance of <code>ConvNextConfig</code>.</p> <code>swint</code> <code>Optional[SwinTConfig]</code> <p>An instance of <code>SwinTConfig</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@oneof\n@define\nclass BackboneConfig:\n    \"\"\"Configurations related to model backbone configuration.\n\n    Attributes:\n        unet: An instance of `UNetConfig`.\n        convnext: An instance of `ConvNextConfig`.\n        swint: An instance of `SwinTConfig`.\n    \"\"\"\n\n    unet: Optional[UNetConfig] = None\n    convnext: Optional[ConvNextConfig] = None\n    swint: Optional[SwinTConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BottomUpConfMapsConfig","title":"<code>BottomUpConfMapsConfig</code>","text":"<p>Bottomup configuration map.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>Optional[List[str]]</code> <p>(List[str]) None if nodes from sio.Labels file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>Optional[float]</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass BottomUpConfMapsConfig:\n    \"\"\"Bottomup configuration map.\n\n    Attributes:\n        part_names: (List[str]) None if nodes from sio.Labels file can be used directly.\n            Else provide text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of channels\n            in the output. If not specified, all body parts in the skeleton will be used.\n            This config does not apply for 'PartAffinityFieldsHead'.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn\n            as they have a lower density within the image space. Larger values are easier\n            to learn but are less precise with respect to the peak coordinate. This spread\n            is in units of pixels of the model input image, i.e., the image resolution\n            after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    part_names: Optional[List[str]] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n    loss_weight: Optional[float] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BottomUpConfig","title":"<code>BottomUpConfig</code>","text":"<p>bottomup head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass BottomUpConfig:\n    \"\"\"bottomup head_config.\"\"\"\n\n    confmaps: Optional[BottomUpConfMapsConfig] = None\n    pafs: Optional[PAFConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BottomUpMultiClassConfig","title":"<code>BottomUpMultiClassConfig</code>","text":"<p>Head config for BottomUp Id models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass BottomUpMultiClassConfig:\n    \"\"\"Head config for BottomUp Id models.\"\"\"\n\n    confmaps: Optional[BottomUpConfMapsConfig] = None\n    class_maps: Optional[ClassMapConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CenteredInstanceConfMapsConfig","title":"<code>CenteredInstanceConfMapsConfig</code>","text":"<p>Centered Instance configuration map.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>Optional[List[str]]</code> <p>(List[str]) None if nodes from sio.Labels file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'.</p> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) Node name to use as the anchor point. If None, the midpoint of the bounding box of all visible instance points will be used as the anchor. The bounding box midpoint will also be used if the anchor part is specified but not visible in the instance. Setting a reliable anchor point can significantly improve topdown model accuracy as they benefit from a consistent geometry of the body parts relative to the center of the image. Default is None.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>float</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CenteredInstanceConfMapsConfig:\n    \"\"\"Centered Instance configuration map.\n\n    Attributes:\n        part_names: (List[str]) None if nodes from sio.Labels file can be used directly.\n            Else provide text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of channels\n            in the output. If not specified, all body parts in the skeleton will be used.\n            This config does not apply for 'PartAffinityFieldsHead'.\n        anchor_part: (str) Node name to use as the anchor point. If None, the midpoint of the\n            bounding box of all visible instance points will be used as the anchor. The bounding\n            box midpoint will also be used if the anchor part is specified but not visible in the\n            instance. Setting a reliable anchor point can significantly improve topdown model\n            accuracy as they benefit from a consistent geometry of the body parts relative to the\n            center of the image. Default is None.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn\n            as they have a lower density within the image space. Larger values are\n            easier to learn but are less precise with respect to the peak coordinate.\n            This spread is in units of pixels of the model input image, i.e., the image\n            resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    part_names: Optional[List[str]] = None\n    anchor_part: Optional[str] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n    loss_weight: float = 1.0\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CenteredInstanceConfig","title":"<code>CenteredInstanceConfig</code>","text":"<p>centered_instance head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CenteredInstanceConfig:\n    \"\"\"centered_instance head_config.\"\"\"\n\n    confmaps: Optional[CenteredInstanceConfMapsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CentroidConfMapsConfig","title":"<code>CentroidConfMapsConfig</code>","text":"<p>Centroid configuration map.</p> <p>Attributes:</p> Name Type Description <code>anchor_part</code> <code>Optional[str]</code> <p>(str) Node name to use as the anchor point. If None, the midpoint of the bounding box of all visible instance points will be used as the anchor. The bounding box midpoint will also be used if the anchor part is specified but not visible in the instance. Setting a reliable anchor point can significantly improve topdown model accuracy as they benefit from a consistent geometry of the body parts relative to the center of the image. Default is None.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CentroidConfMapsConfig:\n    \"\"\"Centroid configuration map.\n\n    Attributes:\n        anchor_part: (str) Node name to use as the anchor point. If None, the midpoint of the\n            bounding box of all visible instance points will be used as the anchor. The bounding\n            box midpoint will also be used if the anchor part is specified but not visible in the\n            instance. Setting a reliable anchor point can significantly improve topdown model\n            accuracy as they benefit from a consistent geometry of the body parts relative to the\n            center of the image. Default is None.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn as\n            they have a lower density within the image space. Larger values are easier to\n            learn but are less precise with respect to the peak coordinate. This spread is\n            in units of pixels of the model input image, i.e., the image resolution after\n            any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n    \"\"\"\n\n    anchor_part: Optional[str] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CentroidConfig","title":"<code>CentroidConfig</code>","text":"<p>centroid head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CentroidConfig:\n    \"\"\"centroid head_config.\"\"\"\n\n    confmaps: Optional[CentroidConfMapsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ClassMapConfig","title":"<code>ClassMapConfig</code>","text":"<p>Class map head config.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>Optional[List[str]]</code> <p>(List[str]) List of class (track) names. Default is <code>None</code>. When <code>None</code>, these are inferred from the track names in the labels file.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>Optional[float]</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ClassMapConfig:\n    \"\"\"Class map head config.\n\n    Attributes:\n        classes: (List[str]) List of class (track) names. Default is `None`. When `None`, these are inferred from the track names in the labels file.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as\n            a scalar float. Smaller values are more precise but may be difficult to\n            learn as they have a lower density within the image space. Larger values\n            are easier to learn but are less precise with respect to the peak\n            coordinate. This spread is in units of pixels of the model input image,\n            i.e., the image resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to\n            the input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the\n            input. Increasing this value can considerably speed up model performance\n            and decrease memory requirements, at the cost of decreased spatial\n            resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    classes: Optional[List[str]] = None\n    sigma: float = 15.0\n    output_stride: int = 1\n    loss_weight: Optional[float] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ClassVectorsConfig","title":"<code>ClassVectorsConfig</code>","text":"<p>Configurations for class vectors heads.</p> <p>These heads are used in top-down multi-instance models that classify detected points using a fixed set of learned classes (e.g., animal identities).</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>Optional[List[str]]</code> <p>List of string names of the classes that this head will predict.</p> <code>num_fc_layers</code> <code>int</code> <p>Number of fully-connected layers before the classification output layer. These can help in transforming general image features into classification-specific features.</p> <code>num_fc_units</code> <code>int</code> <p>Number of units (dimensions) in the fully-connected layers before classification. Increasing this can improve the representational capacity in the pre-classification layers.</p> <code>output_stride</code> <code>int</code> <p>(Ideally this should be same as the backbone's maxstride). The stride of the output class maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in maps that are 0.5x the size of the input. This should be the same size as the confidence maps they are associated with.</p> <code>loss_weight</code> <code>float</code> <p>Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ClassVectorsConfig:\n    \"\"\"Configurations for class vectors heads.\n\n    These heads are used in top-down multi-instance models that classify detected\n    points using a fixed set of learned classes (e.g., animal identities).\n\n    Attributes:\n        classes: List of string names of the classes that this head will predict.\n        num_fc_layers: Number of fully-connected layers before the classification output\n            layer. These can help in transforming general image features into\n            classification-specific features.\n        num_fc_units: Number of units (dimensions) in the fully-connected layers before\n            classification. Increasing this can improve the representational capacity in\n            the pre-classification layers.\n        output_stride: (Ideally this should be same as the backbone's maxstride).\n            The stride of the output class maps relative to the input image.\n            This is the reciprocal of the resolution, e.g., an output stride of 2\n            results in maps that are 0.5x the size of the input. This should be the same\n            size as the confidence maps they are associated with.\n        loss_weight: Scalar float used to weigh the loss term for this head during\n            training. Increase this to encourage the optimization to focus on improving\n            this specific output in multi-head models.\n    \"\"\"\n\n    classes: Optional[List[str]] = None\n    num_fc_layers: int = 1\n    num_fc_units: int = 64\n    global_pool: bool = True\n    output_stride: int = 1\n    loss_weight: float = 1.0\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextBaseConfig","title":"<code>ConvNextBaseConfig</code>","text":"<p>               Bases: <code>ConvNextConfig</code></p> <p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3]. channels: (List(int)) Number of channels in each block. Default:     [96, 192, 384, 768].</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default is 2.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default is 1.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default is 3.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default is 2.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default is 2.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: True.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextBaseConfig(ConvNextConfig):\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type\n            is provided)\n            depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3].\n            channels: (List(int)) Number of channels in each block. Default:\n                [96, 192, 384, 768].\n        model_type: (str) One of the ConvNext architecture types:\n            [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer.\n            Default is 4.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. Default is 2.\n        in_channels: (int) Number of input channels. Default is 1.\n        kernel_size: (int) Size of the convolutional kernels. Default is 3.\n        filters_rate: (float) Factor to adjust the number of filters per block.\n            Default is 2.\n        convs_per_block: (int) Number of convolutional layers per block. Default is 2.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales. Default: True.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `32` for all convnext architectures.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"base\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextBaseConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextConfig","title":"<code>ConvNextConfig</code>","text":"<p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List[int]) Number of layers in each block. Default: <code>[3, 3, 9, 3]</code>. channels: (List[int]) Number of channels in each block. Default: <code>[96, 192, 384, 768]</code>.</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: <code>\"tiny\"</code>.</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default: <code>2</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures. Default: <code>32</code>.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextConfig:\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type is provided)\n            depths: (List[int]) Number of layers in each block. *Default*: `[3, 3, 9, 3]`.\n            channels: (List[int]) Number of channels in each block. *Default*: `[96, 192, 384, 768]`.\n        model_type: (str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. *Default*: `\"tiny\"`.\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer. *Default*: `4`.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. *Default*: `2`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all convnext architectures. *Default*: `32`.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"tiny\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextLargeConfig","title":"<code>ConvNextLargeConfig</code>","text":"<p>               Bases: <code>ConvNextConfig</code></p> <p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3]. channels: (List(int)) Number of channels in each block. Default:     [96, 192, 384, 768].</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default is 2.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default is 1.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default is 3.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default is 2.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default is 2.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: True.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextLargeConfig(ConvNextConfig):\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type\n            is provided)\n            depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3].\n            channels: (List(int)) Number of channels in each block. Default:\n                [96, 192, 384, 768].\n        model_type: (str) One of the ConvNext architecture types:\n            [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer.\n            Default is 4.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. Default is 2.\n        in_channels: (int) Number of input channels. Default is 1.\n        kernel_size: (int) Size of the convolutional kernels. Default is 3.\n        filters_rate: (float) Factor to adjust the number of filters per block.\n            Default is 2.\n        convs_per_block: (int) Number of convolutional layers per block. Default is 2.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales. Default: True.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `32` for all convnext architectures.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"large\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextLargeConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextSmallConfig","title":"<code>ConvNextSmallConfig</code>","text":"<p>               Bases: <code>ConvNextConfig</code></p> <p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3]. channels: (List(int)) Number of channels in each block. Default:     [96, 192, 384, 768].</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default is 2.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default is 1.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default is 3.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default is 2.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default is 2.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: True.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextSmallConfig(ConvNextConfig):\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type\n            is provided)\n            depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3].\n            channels: (List(int)) Number of channels in each block. Default:\n                [96, 192, 384, 768].\n        model_type: (str) One of the ConvNext architecture types:\n            [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer.\n            Default is 4.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. Default is 2.\n        in_channels: (int) Number of input channels. Default is 1.\n        kernel_size: (int) Size of the convolutional kernels. Default is 3.\n        filters_rate: (float) Factor to adjust the number of filters per block.\n            Default is 2.\n        convs_per_block: (int) Number of convolutional layers per block. Default is 2.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales. Default: True.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `32` for all convnext architectures.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"small\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextSmallConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.HeadConfig","title":"<code>HeadConfig</code>","text":"<p>Configurations related to the model output head type.</p> <p>Only one attribute of this class can be set, which defines the model output type.</p> <p>Attributes:</p> Name Type Description <code>single_instance</code> <code>Optional[SingleInstanceConfig]</code> <p>An instance of <code>SingleInstanceConfmapsHeadConfig</code>.</p> <code>centroid</code> <code>Optional[CentroidConfig]</code> <p>An instance of <code>CentroidsHeadConfig</code>.</p> <code>centered_instance</code> <code>Optional[CenteredInstanceConfig]</code> <p>An instance of <code>CenteredInstanceConfmapsHeadConfig</code>.</p> <code>bottomup</code> <code>Optional[BottomUpConfig]</code> <p>An instance of <code>BottomUpConfig</code>.</p> <code>multi_class_bottomup</code> <code>Optional[BottomUpMultiClassConfig]</code> <p>An instance of <code>BottomUpMultiClassConfig</code>.</p> <code>multi_class_topdown</code> <code>Optional[TopDownCenteredInstanceMultiClassConfig]</code> <p>An instance of <code>TopDownCenteredInstanceMultiClassConfig</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@oneof\n@define\nclass HeadConfig:\n    \"\"\"Configurations related to the model output head type.\n\n    Only one attribute of this class can be set, which defines the model output type.\n\n    Attributes:\n        single_instance: An instance of `SingleInstanceConfmapsHeadConfig`.\n        centroid: An instance of `CentroidsHeadConfig`.\n        centered_instance: An instance of `CenteredInstanceConfmapsHeadConfig`.\n        bottomup: An instance of `BottomUpConfig`.\n        multi_class_bottomup: An instance of `BottomUpMultiClassConfig`.\n        multi_class_topdown: An instance of `TopDownCenteredInstanceMultiClassConfig`.\n    \"\"\"\n\n    single_instance: Optional[SingleInstanceConfig] = None\n    centroid: Optional[CentroidConfig] = None\n    centered_instance: Optional[CenteredInstanceConfig] = None\n    bottomup: Optional[BottomUpConfig] = None\n    multi_class_bottomup: Optional[BottomUpMultiClassConfig] = None\n    multi_class_topdown: Optional[TopDownCenteredInstanceMultiClassConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>Configurations related to model architecture.</p> <p>Attributes:</p> Name Type Description <code>init_weights</code> <code>str</code> <p>(str) model weights initialization method. \"default\" uses kaiming uniform initialization and \"xavier\" uses Xavier initialization method.</p> <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the backbone is initialized. If <code>None</code>, random init is used.</p> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP) file with which the head layers are initialized. If <code>None</code>, random init is used.</p> <code>backbone_config</code> <code>BackboneConfig</code> <p>initialize either UNetConfig, ConvNextConfig, or SwinTConfig based on input from backbone_type</p> <code>head_configs</code> <code>HeadConfig</code> <p>(Dict) Dictionary with the following keys having head configs for the model to be trained. Note: Configs should be provided only for the model to train and others should be None</p> <code>total_params</code> <code>Optional[int]</code> <p>(int) Total number of parameters in the model. This is automatically computed when the training starts.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ModelConfig:\n    \"\"\"Configurations related to model architecture.\n\n    Attributes:\n        init_weights: (str) model weights initialization method. \"default\" uses kaiming\n            uniform initialization and \"xavier\" uses Xavier initialization method.\n        pretrained_backbone_weights: Path of the `ckpt` (or `.h5` file from SLEAP) file with which the backbone\n            is initialized. If `None`, random init is used.\n        pretrained_head_weights: Path of the `ckpt` (or `.h5` file from SLEAP) file with which the head layers\n            are initialized. If `None`, random init is used.\n        backbone_config: initialize either UNetConfig, ConvNextConfig, or SwinTConfig\n            based on input from backbone_type\n        head_configs: (Dict) Dictionary with the following keys having head configs for\n            the model to be trained. Note: Configs should be provided only for the model\n            to train and others should be None\n        total_params: (int) Total number of parameters in the model. This is automatically\n            computed when the training starts.\n    \"\"\"\n\n    init_weights: str = \"default\"\n    pretrained_backbone_weights: Optional[str] = None\n    pretrained_head_weights: Optional[str] = None\n    backbone_config: BackboneConfig = field(factory=BackboneConfig)\n    head_configs: HeadConfig = field(factory=HeadConfig)\n    total_params: Optional[int] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.PAFConfig","title":"<code>PAFConfig</code>","text":"<p>PAF configuration map.</p> <p>Attributes:</p> Name Type Description <code>edges</code> <code>Optional[List[List[str]]]</code> <p>(List[str]) None if edges from sio.Labels file can be used directly. Note: Only for 'PartAffinityFieldsHead'. List of indices (src, dest) that form an edge.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>Optional[float]</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass PAFConfig:\n    \"\"\"PAF configuration map.\n\n    Attributes:\n        edges: (List[str]) None if edges from sio.Labels file can be used directly.\n            Note: Only for 'PartAffinityFieldsHead'. List of indices (src, dest) that\n            form an edge.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as\n            a scalar float. Smaller values are more precise but may be difficult to\n            learn as they have a lower density within the image space. Larger values\n            are easier to learn but are less precise with respect to the peak\n            coordinate. This spread is in units of pixels of the model input image,\n            i.e., the image resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to\n            the input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the\n            input. Increasing this value can considerably speed up model performance\n            and decrease memory requirements, at the cost of decreased spatial\n            resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    edges: Optional[List[List[str]]] = None\n    sigma: float = 15.0\n    output_stride: int = 1\n    loss_weight: Optional[float] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SingleInstanceConfMapsConfig","title":"<code>SingleInstanceConfMapsConfig</code>","text":"<p>Single Instance configuration map.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>Optional[List[str]]</code> <p>(List[str]) None if nodes from sio.Labels file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SingleInstanceConfMapsConfig:\n    \"\"\"Single Instance configuration map.\n\n    Attributes:\n        part_names: (List[str]) None if nodes from sio.Labels file can be used directly.\n            Else provide text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of channels\n            in the output. If not specified, all body parts in the skeleton will be used.\n            This config does not apply for 'PartAffinityFieldsHead'.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn\n            as they have a lower density within the image space. Larger values are\n            easier to learn but are less precise with respect to the peak coordinate.\n            This spread is in units of pixels of the model input image,\n            i.e., the image resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n    \"\"\"\n\n    part_names: Optional[List[str]] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SingleInstanceConfig","title":"<code>SingleInstanceConfig</code>","text":"<p>single instance head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SingleInstanceConfig:\n    \"\"\"single instance head_config.\"\"\"\n\n    confmaps: Optional[SingleInstanceConfMapsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTBaseConfig","title":"<code>SwinTBaseConfig</code>","text":"<p>               Bases: <code>SwinTConfig</code></p> <p>SwinT configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].</p> <code>model_type</code> <code>str</code> <p>(str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. Default: <code>\"base\"</code>.</p> <code>arch</code> <code>Optional[dict]</code> <p>Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. Default: <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all swint architectures. Default: <code>32</code>.</p> <code>patch_size</code> <code>int</code> <p>(int) Patch size for the stem layer of SwinT. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Stride for the patch. Default: <code>2</code>.</p> <code>window_size</code> <code>int</code> <p>(int) Window size. Default: <code>7</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <p>Methods:</p> Name Description <code>validate_model_type</code> <p>Validate model_type.</p> <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SwinTBaseConfig(SwinTConfig):\n    \"\"\"SwinT configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].\n        model_type: (str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. *Default*: `\"base\"`.\n        arch: Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. *Default*: `None`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all swint architectures. *Default*: `32`.\n        patch_size: (int) Patch size for the stem layer of SwinT. *Default*: `4`.\n        stem_patch_stride: (int) Stride for the patch. *Default*: `2`.\n        window_size: (int) Window size. *Default*: `7`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = field(\n        default=\"base\",\n        validator=lambda instance, attr, value: instance.validate_model_type(value),\n    )\n    arch: Optional[dict] = None\n    max_stride: int = 32\n    patch_size: int = 4\n    stem_patch_stride: int = 2\n    window_size: int = 7\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n\n    def validate_model_type(self, value):\n        \"\"\"Validate model_type.\n\n        Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n        \"\"\"\n        valid_types = [\"tiny\", \"small\", \"base\"]\n        if value not in valid_types:\n            message = f\"Invalid model_type. Must be one of {valid_types}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        swint_weights are one of\n        (\n            \"Swin_T_Weights\",\n            \"Swin_S_Weights\",\n            \"Swin_B_Weights\"\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n        if value not in swint_weights:\n            message = (\n                f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTBaseConfig.validate_model_type","title":"<code>validate_model_type(value)</code>","text":"<p>Validate model_type.</p> <p>Ensure model_type is one of \"tiny\", \"small\", or \"base\".</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_model_type(self, value):\n    \"\"\"Validate model_type.\n\n    Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n    \"\"\"\n    valid_types = [\"tiny\", \"small\", \"base\"]\n    if value not in valid_types:\n        message = f\"Invalid model_type. Must be one of {valid_types}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTBaseConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: swint_weights are one of (     \"Swin_T_Weights\",     \"Swin_S_Weights\",     \"Swin_B_Weights\" )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    swint_weights are one of\n    (\n        \"Swin_T_Weights\",\n        \"Swin_S_Weights\",\n        \"Swin_B_Weights\"\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n    if value not in swint_weights:\n        message = (\n            f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTConfig","title":"<code>SwinTConfig</code>","text":"<p>SwinT configuration (tiny) for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].</p> <code>model_type</code> <code>str</code> <p>(str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. Default: <code>\"tiny\"</code>.</p> <code>arch</code> <code>Optional[dict]</code> <p>Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. Default: <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all swint architectures. Default: <code>32</code>.</p> <code>patch_size</code> <code>int</code> <p>(int) Patch size for the stem layer of SwinT. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Stride for the patch. Default: <code>2</code>.</p> <code>window_size</code> <code>int</code> <p>(int) Window size. Default: <code>7</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <p>Methods:</p> Name Description <code>validate_model_type</code> <p>Validate model_type.</p> <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SwinTConfig:\n    \"\"\"SwinT configuration (tiny) for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].\n        model_type: (str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. *Default*: `\"tiny\"`.\n        arch: Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. *Default*: `None`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all swint architectures. *Default*: `32`.\n        patch_size: (int) Patch size for the stem layer of SwinT. *Default*: `4`.\n        stem_patch_stride: (int) Stride for the patch. *Default*: `2`.\n        window_size: (int) Window size. *Default*: `7`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = field(\n        default=\"tiny\",\n        validator=lambda instance, attr, value: instance.validate_model_type(value),\n    )\n    arch: Optional[dict] = None\n    max_stride: int = 32\n    patch_size: int = 4\n    stem_patch_stride: int = 2\n    window_size: int = 7\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n\n    def validate_model_type(self, value):\n        \"\"\"Validate model_type.\n\n        Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n        \"\"\"\n        valid_types = [\"tiny\", \"small\", \"base\"]\n        if value not in valid_types:\n            message = f\"Invalid model_type. Must be one of {valid_types}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        swint_weights are one of\n        (\n            \"Swin_T_Weights\",\n            \"Swin_S_Weights\",\n            \"Swin_B_Weights\"\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n        if value not in swint_weights:\n            message = (\n                f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTConfig.validate_model_type","title":"<code>validate_model_type(value)</code>","text":"<p>Validate model_type.</p> <p>Ensure model_type is one of \"tiny\", \"small\", or \"base\".</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_model_type(self, value):\n    \"\"\"Validate model_type.\n\n    Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n    \"\"\"\n    valid_types = [\"tiny\", \"small\", \"base\"]\n    if value not in valid_types:\n        message = f\"Invalid model_type. Must be one of {valid_types}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: swint_weights are one of (     \"Swin_T_Weights\",     \"Swin_S_Weights\",     \"Swin_B_Weights\" )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    swint_weights are one of\n    (\n        \"Swin_T_Weights\",\n        \"Swin_S_Weights\",\n        \"Swin_B_Weights\"\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n    if value not in swint_weights:\n        message = (\n            f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTSmallConfig","title":"<code>SwinTSmallConfig</code>","text":"<p>               Bases: <code>SwinTConfig</code></p> <p>SwinT configuration (small) for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].</p> <code>model_type</code> <code>str</code> <p>(str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. Default: <code>\"small\"</code>.</p> <code>arch</code> <code>Optional[dict]</code> <p>Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. Default: <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all swint architectures. Default: <code>32</code>.</p> <code>patch_size</code> <code>int</code> <p>(int) Patch size for the stem layer of SwinT. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Stride for the patch. Default: <code>2</code>.</p> <code>window_size</code> <code>int</code> <p>(int) Window size. Default: <code>7</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <p>Methods:</p> Name Description <code>validate_model_type</code> <p>Validate model_type.</p> <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SwinTSmallConfig(SwinTConfig):\n    \"\"\"SwinT configuration (small) for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].\n        model_type: (str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. *Default*: `\"small\"`.\n        arch: Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. *Default*: `None`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all swint architectures. *Default*: `32`.\n        patch_size: (int) Patch size for the stem layer of SwinT. *Default*: `4`.\n        stem_patch_stride: (int) Stride for the patch. *Default*: `2`.\n        window_size: (int) Window size. *Default*: `7`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = field(\n        default=\"small\",\n        validator=lambda instance, attr, value: instance.validate_model_type(value),\n    )\n    arch: Optional[dict] = None\n    max_stride: int = 32\n    patch_size: int = 4\n    stem_patch_stride: int = 2\n    window_size: int = 7\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n\n    def validate_model_type(self, value):\n        \"\"\"Validate model_type.\n\n        Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n        \"\"\"\n        valid_types = [\"tiny\", \"small\", \"base\"]\n        if value not in valid_types:\n            message = f\"Invalid model_type. Must be one of {valid_types}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        swint_weights are one of\n        (\n            \"Swin_T_Weights\",\n            \"Swin_S_Weights\",\n            \"Swin_B_Weights\"\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n        if value not in swint_weights:\n            message = (\n                f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTSmallConfig.validate_model_type","title":"<code>validate_model_type(value)</code>","text":"<p>Validate model_type.</p> <p>Ensure model_type is one of \"tiny\", \"small\", or \"base\".</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_model_type(self, value):\n    \"\"\"Validate model_type.\n\n    Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n    \"\"\"\n    valid_types = [\"tiny\", \"small\", \"base\"]\n    if value not in valid_types:\n        message = f\"Invalid model_type. Must be one of {valid_types}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTSmallConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: swint_weights are one of (     \"Swin_T_Weights\",     \"Swin_S_Weights\",     \"Swin_B_Weights\" )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    swint_weights are one of\n    (\n        \"Swin_T_Weights\",\n        \"Swin_S_Weights\",\n        \"Swin_B_Weights\"\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n    if value not in swint_weights:\n        message = (\n            f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.TopDownCenteredInstanceMultiClassConfig","title":"<code>TopDownCenteredInstanceMultiClassConfig</code>","text":"<p>Head config for TopDown centered instance ID models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass TopDownCenteredInstanceMultiClassConfig:\n    \"\"\"Head config for TopDown centered instance ID models.\"\"\"\n\n    confmaps: Optional[CenteredInstanceConfMapsConfig] = None\n    class_vectors: Optional[ClassVectorsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.UNetConfig","title":"<code>UNetConfig</code>","text":"<p>UNet config for backbone.</p> <p>Attributes:</p> Name Type Description <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters</code> <code>int</code> <p>(int) Base number of filters in the network. Default: <code>32</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>1.5</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Scalar integer specifying the maximum stride that the image must be divisible by. Default: <code>16</code>.</p> <code>stem_stride</code> <code>Optional[int]</code> <p>(int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. Default: <code>None</code>.</p> <code>middle_block</code> <code>bool</code> <p>(bool) If True, add an additional block at the end of the encoder. Default: <code>True</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>stacks</code> <code>int</code> <p>(int) Number of upsampling blocks in the decoder. Default: <code>1</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass UNetConfig:\n    \"\"\"UNet config for backbone.\n\n    Attributes:\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters: (int) Base number of filters in the network. *Default*: `32`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `1.5`.\n        max_stride: (int) Scalar integer specifying the maximum stride that the image must be divisible by. *Default*: `16`.\n        stem_stride: (int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. *Default*: `None`.\n        middle_block: (bool) If True, add an additional block at the end of the encoder. *Default*: `True`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        stacks: (int) Number of upsampling blocks in the decoder. *Default*: `1`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters: int = 32\n    filters_rate: float = 1.5\n    max_stride: int = 16\n    stem_stride: Optional[int] = None\n    middle_block: bool = True\n    up_interpolate: bool = True\n    stacks: int = 1\n    convs_per_block: int = 2\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.UNetLargeRFConfig","title":"<code>UNetLargeRFConfig</code>","text":"<p>               Bases: <code>UNetConfig</code></p> <p>UNet config for backbone with large receptive field.</p> <p>Attributes:</p> Name Type Description <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters</code> <code>int</code> <p>(int) Base number of filters in the network. Default: <code>24</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>1.5</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Scalar integer specifying the maximum stride that the image must be divisible by. Default: <code>32</code>.</p> <code>stem_stride</code> <code>Optional[int]</code> <p>(int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. Default: <code>None</code>.</p> <code>middle_block</code> <code>bool</code> <p>(bool) If True, add an additional block at the end of the encoder. Default: <code>True</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>stacks</code> <code>int</code> <p>(int) Number of upsampling blocks in the decoder. Default: <code>1</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass UNetLargeRFConfig(UNetConfig):\n    \"\"\"UNet config for backbone with large receptive field.\n\n    Attributes:\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters: (int) Base number of filters in the network. *Default*: `24`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `1.5`.\n        max_stride: (int) Scalar integer specifying the maximum stride that the image must be divisible by. *Default*: `32`.\n        stem_stride: (int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. *Default*: `None`.\n        middle_block: (bool) If True, add an additional block at the end of the encoder. *Default*: `True`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        stacks: (int) Number of upsampling blocks in the decoder. *Default*: `1`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters: int = 24\n    filters_rate: float = 1.5\n    max_stride: int = 32\n    stem_stride: Optional[int] = None\n    middle_block: bool = True\n    up_interpolate: bool = True\n    stacks: int = 1\n    convs_per_block: int = 2\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.UNetMediumRFConfig","title":"<code>UNetMediumRFConfig</code>","text":"<p>               Bases: <code>UNetConfig</code></p> <p>UNet config for backbone with medium receptive field.</p> <p>Attributes:</p> Name Type Description <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters</code> <code>int</code> <p>(int) Base number of filters in the network. Default: <code>32</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Scalar integer specifying the maximum stride that the image must be divisible by. Default: <code>16</code>.</p> <code>stem_stride</code> <code>Optional[int]</code> <p>(int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. Default: <code>None</code>.</p> <code>middle_block</code> <code>bool</code> <p>(bool) If True, add an additional block at the end of the encoder. Default: <code>True</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>stacks</code> <code>int</code> <p>(int) Number of upsampling blocks in the decoder. Default: <code>1</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass UNetMediumRFConfig(UNetConfig):\n    \"\"\"UNet config for backbone with medium receptive field.\n\n    Attributes:\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters: (int) Base number of filters in the network. *Default*: `32`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        max_stride: (int) Scalar integer specifying the maximum stride that the image must be divisible by. *Default*: `16`.\n        stem_stride: (int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. *Default*: `None`.\n        middle_block: (bool) If True, add an additional block at the end of the encoder. *Default*: `True`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        stacks: (int) Number of upsampling blocks in the decoder. *Default*: `1`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters: int = 32\n    filters_rate: float = 2\n    max_stride: int = 16\n    stem_stride: Optional[int] = None\n    middle_block: bool = True\n    up_interpolate: bool = True\n    stacks: int = 1\n    convs_per_block: int = 2\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.model_mapper","title":"<code>model_mapper(legacy_config)</code>","text":"<p>Map the legacy model configuration to the new model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_config</code> <code>dict</code> <p>A dictionary containing the legacy model configuration.</p> required <p>Returns:</p> Type Description <code>ModelConfig</code> <p>An instance of <code>ModelConfig</code> with the mapped configuration.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def model_mapper(legacy_config: dict) -&gt; ModelConfig:\n    \"\"\"Map the legacy model configuration to the new model configuration.\n\n    Args:\n        legacy_config: A dictionary containing the legacy model configuration.\n\n    Returns:\n        An instance of `ModelConfig` with the mapped configuration.\n    \"\"\"\n    legacy_config_model = legacy_config.get(\"model\", {})\n    backbone_cfg_args = {}\n    head_cfg_args = {}\n    if legacy_config_model.get(\"backbone\", {}).get(\"unet\", None) is not None:\n        backbone_cfg_args[\"unet\"] = UNetConfig(\n            filters=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"filters\", 32),\n            filters_rate=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"filters_rate\", 1.5),\n            max_stride=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"max_stride\", 16),\n            stem_stride=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"stem_stride\", 16),\n            middle_block=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"middle_block\", True),\n            up_interpolate=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"up_interpolate\", True),\n            stacks=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"stacks\", 1),\n            output_stride=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"output_stride\", 1),\n        )\n\n    backbone_cfg = BackboneConfig(**backbone_cfg_args)\n\n    if legacy_config_model.get(\"heads\", {}).get(\"single_instance\", None) is not None:\n        head_cfg_args[\"single_instance\"] = SingleInstanceConfig(\n            confmaps=SingleInstanceConfMapsConfig(\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"single_instance\", {})\n                .get(\"part_names\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"single_instance\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"single_instance\", {})\n                .get(\"output_stride\", 1),\n            )\n        )\n    if legacy_config_model.get(\"heads\", {}).get(\"centroid\", None) is not None:\n        head_cfg_args[\"centroid\"] = CentroidConfig(\n            confmaps=CentroidConfMapsConfig(\n                anchor_part=legacy_config_model.get(\"heads\", {})\n                .get(\"centroid\", {})\n                .get(\"anchor_part\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"centroid\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"centroid\", {})\n                .get(\"output_stride\", 1),\n            )\n        )\n    if legacy_config_model.get(\"heads\", {}).get(\"centered_instance\", None) is not None:\n        head_cfg_args[\"centered_instance\"] = CenteredInstanceConfig(\n            confmaps=CenteredInstanceConfMapsConfig(\n                anchor_part=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"anchor_part\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"output_stride\", 1),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"part_names\", None),\n            )\n        )\n    if legacy_config_model.get(\"heads\", {}).get(\"multi_instance\", None) is not None:\n        head_cfg_args[\"bottomup\"] = BottomUpConfig(\n            confmaps=BottomUpConfMapsConfig(\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"loss_weight\", 1.0),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"output_stride\", 1),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"part_names\", None),\n            ),\n            pafs=PAFConfig(\n                edges=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"edges\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"sigma\", 15.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"output_stride\", 1),\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"loss_weight\", 1.0),\n            ),\n        )\n    if (\n        legacy_config_model.get(\"heads\", {}).get(\"multi_class_bottomup\", None)\n        is not None\n    ):\n        head_cfg_args[\"multi_class_bottomup\"] = BottomUpMultiClassConfig(\n            confmaps=BottomUpConfMapsConfig(\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"loss_weight\", 1.0),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"output_stride\", 1),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"part_names\", None),\n            ),\n            class_maps=ClassMapConfig(\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"sigma\", 15.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"output_stride\", 1),\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"loss_weight\", 1.0),\n                classes=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"classes\", None),\n            ),\n        )\n\n    if (\n        legacy_config_model.get(\"heads\", {}).get(\"multi_class_topdown\", None)\n        is not None\n    ):\n        head_cfg_args[\"multi_class_topdown\"] = TopDownCenteredInstanceMultiClassConfig(\n            confmaps=CenteredInstanceConfMapsConfig(\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"loss_weight\", 1.0),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"output_stride\", 1),\n                anchor_part=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"anchor_part\", None),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"part_names\", None),\n            ),\n            class_vectors=ClassVectorsConfig(\n                classes=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"classes\", None),\n                num_fc_layers=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"num_fc_layers\", 2),\n                num_fc_units=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"num_fc_units\", 1024),\n                global_pool=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"global_pool\", True),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"output_stride\", 1),\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"loss_weight\", 1.0),\n            ),\n        )\n\n    head_cfg = HeadConfig(**head_cfg_args)\n\n    return ModelConfig(backbone_config=backbone_cfg, head_configs=head_cfg)\n</code></pre>"},{"location":"api/config/trainer_config/","title":"trainer_config","text":""},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config","title":"<code>sleap_nn.config.trainer_config</code>","text":"<p>Serializable configuration classes for specifying all trainer config parameters.</p> <p>These configuration classes are intended to specify all the parameters required to initialize the trainer config.</p> <p>Classes:</p> Name Description <code>DataLoaderConfig</code> <p>Train and val DataLoaderConfig.</p> <code>EarlyStoppingConfig</code> <p>Configuration for early_stopping.</p> <code>HardKeypointMiningConfig</code> <p>Configuration for online hard keypoint mining.</p> <code>LRSchedulerConfig</code> <p>Configuration for lr_scheduler.</p> <code>ModelCkptConfig</code> <p>Configuration for model checkpoint.</p> <code>OptimizerConfig</code> <p>Configuration for optimizer.</p> <code>ReduceLROnPlateauConfig</code> <p>Configuration for ReduceLROnPlateau scheduler.</p> <code>StepLRConfig</code> <p>Configuration for StepLR scheduler.</p> <code>TrainerConfig</code> <p>Configuration for trainer.</p> <code>WandBConfig</code> <p>Configuration for WandB.</p> <code>ZMQConfig</code> <p>Configuration of ZeroMQ-based monitoring of the training.</p> <p>Functions:</p> Name Description <code>trainer_mapper</code> <p>Map the legacy trainer configuration to the new trainer configuration.</p>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.DataLoaderConfig","title":"<code>DataLoaderConfig</code>","text":"<p>Train and val DataLoaderConfig.</p> <p>Any parameters from Torch's DataLoader could be used.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch or batch size for training/validation data. Default: <code>1</code>.</p> <code>shuffle</code> <code>bool</code> <p>(bool) True to have the data reshuffled at every epoch. Default: <code>False</code>.</p> <code>num_workers</code> <code>int</code> <p>(int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: <code>0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass DataLoaderConfig:\n    \"\"\"Train and val DataLoaderConfig.\n\n    Any parameters from Torch's DataLoader could be used.\n\n    Attributes:\n        batch_size: (int) Number of samples per batch or batch size for training/validation data. *Default*: `1`.\n        shuffle: (bool) True to have the data reshuffled at every epoch. *Default*: `False`.\n        num_workers: (int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. *Default*: `0`.\n    \"\"\"\n\n    batch_size: int = 1\n    shuffle: bool = False\n    num_workers: int = 0\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.EarlyStoppingConfig","title":"<code>EarlyStoppingConfig</code>","text":"<p>Configuration for early_stopping.</p> <p>Attributes:</p> Name Type Description <code>stop_training_on_plateau</code> <code>bool</code> <p>(bool) True if early stopping should be enabled. Default: <code>False</code>.</p> <code>min_delta</code> <code>float</code> <p>(float) Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. Default: <code>0.0</code>.</p> <code>patience</code> <code>int</code> <p>(int) Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. Default: <code>1</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass EarlyStoppingConfig:\n    \"\"\"Configuration for early_stopping.\n\n    Attributes:\n        stop_training_on_plateau: (bool) True if early stopping should be enabled. *Default*: `False`.\n        min_delta: (float) Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. *Default*: `0.0`.\n        patience: (int) Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. *Default*: `1`.\n    \"\"\"\n\n    min_delta: float = field(default=0.0, validator=validators.ge(0))\n    patience: int = field(default=1, validator=validators.ge(0))\n    stop_training_on_plateau: bool = False\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.HardKeypointMiningConfig","title":"<code>HardKeypointMiningConfig</code>","text":"<p>Configuration for online hard keypoint mining.</p> <p>Attributes:</p> Name Type Description <code>online_mining</code> <code>bool</code> <p>If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss. Default: <code>False</code>.</p> <code>hard_to_easy_ratio</code> <code>float</code> <p>The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training. Default: <code>2.0</code>.</p> <code>min_hard_keypoints</code> <code>int</code> <p>The minimum number of keypoints that will be considered as \"hard\", even if they are not below the <code>hard_to_easy_ratio</code>. Default: <code>2</code>.</p> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether. Default: <code>None</code>.</p> <code>loss_scale</code> <code>float</code> <p>Factor to scale the hard keypoint losses by. Default: <code>5.0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass HardKeypointMiningConfig:\n    \"\"\"Configuration for online hard keypoint mining.\n\n    Attributes:\n        online_mining: If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss. *Default*: `False`.\n        hard_to_easy_ratio: The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training. *Default*: `2.0`.\n        min_hard_keypoints: The minimum number of keypoints that will be considered as \"hard\", even if they are not below the `hard_to_easy_ratio`. *Default*: `2`.\n        max_hard_keypoints: The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether. *Default*: `None`.\n        loss_scale: Factor to scale the hard keypoint losses by. *Default*: `5.0`.\n    \"\"\"\n\n    online_mining: bool = False\n    hard_to_easy_ratio: float = 2.0\n    min_hard_keypoints: int = 2\n    max_hard_keypoints: Optional[int] = None\n    loss_scale: float = 5.0\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.LRSchedulerConfig","title":"<code>LRSchedulerConfig</code>","text":"<p>Configuration for lr_scheduler.</p> <p>Attributes:</p> Name Type Description <code>step_lr</code> <code>Optional[StepLRConfig]</code> <p>Configuration for StepLR scheduler.</p> <code>reduce_lr_on_plateau</code> <code>Optional[ReduceLROnPlateauConfig]</code> <p>Configuration for ReduceLROnPlateau scheduler.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass LRSchedulerConfig:\n    \"\"\"Configuration for lr_scheduler.\n\n    Attributes:\n        step_lr: Configuration for StepLR scheduler.\n        reduce_lr_on_plateau: Configuration for ReduceLROnPlateau scheduler.\n    \"\"\"\n\n    step_lr: Optional[StepLRConfig] = None\n    reduce_lr_on_plateau: Optional[ReduceLROnPlateauConfig] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ModelCkptConfig","title":"<code>ModelCkptConfig</code>","text":"<p>Configuration for model checkpoint.</p> <p>Any parameters from Lightning's ModelCheckpoint could be used.</p> <p>Attributes:</p> Name Type Description <code>save_top_k</code> <code>int</code> <p>(int) If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. Default: <code>1</code>.</p> <code>save_last</code> <code>Optional[bool]</code> <p>(bool) When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. Default: <code>None</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass ModelCkptConfig:\n    \"\"\"Configuration for model checkpoint.\n\n    Any parameters from Lightning's ModelCheckpoint could be used.\n\n    Attributes:\n        save_top_k: (int) If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. *Default*: `1`.\n        save_last: (bool) When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. *Default*: `None`.\n    \"\"\"\n\n    save_top_k: int = 1\n    save_last: Optional[bool] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.OptimizerConfig","title":"<code>OptimizerConfig</code>","text":"<p>Configuration for optimizer.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>(float) Learning rate of type float. Default: <code>1e-3</code>.</p> <code>amsgrad</code> <code>bool</code> <p>(bool) Enable AMSGrad with the optimizer. Default: <code>False</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass OptimizerConfig:\n    \"\"\"Configuration for optimizer.\n\n    Attributes:\n        lr: (float) Learning rate of type float. *Default*: `1e-3`.\n        amsgrad: (bool) Enable AMSGrad with the optimizer. *Default*: `False`.\n    \"\"\"\n\n    lr: float = field(default=1e-3, validator=validators.gt(0))\n    amsgrad: bool = False\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ReduceLROnPlateauConfig","title":"<code>ReduceLROnPlateauConfig</code>","text":"<p>Configuration for ReduceLROnPlateau scheduler.</p> <p>Attributes:</p> Name Type Description <code>threshold</code> <code>float</code> <p>(float) Threshold for measuring the new optimum, to only focus on significant changes. Default: <code>1e-4</code>.</p> <code>threshold_mode</code> <code>str</code> <p>(str) One of \"rel\", \"abs\". In rel mode, dynamic_threshold = best * ( 1 + threshold ) in max mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: <code>\"rel\"</code>.</p> <code>cooldown</code> <code>int</code> <p>(int) Number of epochs to wait before resuming normal operation after lr has been reduced. Default: <code>0</code>.</p> <code>patience</code> <code>int</code> <p>(int) Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the third epoch if the loss still hasn't improved then. Default: <code>10</code>.</p> <code>factor</code> <code>float</code> <p>(float) Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: <code>0.1</code>.</p> <code>min_lr</code> <code>Any</code> <p>(float or List[float]) A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: <code>0.0</code>.</p> <p>Methods:</p> Name Description <code>validate_min_lr</code> <p>min_lr Validation.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass ReduceLROnPlateauConfig:\n    \"\"\"Configuration for ReduceLROnPlateau scheduler.\n\n    Attributes:\n        threshold: (float) Threshold for measuring the new optimum, to only focus on significant changes. *Default*: `1e-4`.\n        threshold_mode: (str) One of \"rel\", \"abs\". In rel mode, dynamic_threshold = best * ( 1 + threshold ) in max mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. *Default*: `\"rel\"`.\n        cooldown: (int) Number of epochs to wait before resuming normal operation after lr has been reduced. *Default*: `0`.\n        patience: (int) Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the third epoch if the loss still hasn't improved then. *Default*: `10`.\n        factor: (float) Factor by which the learning rate will be reduced. new_lr = lr * factor. *Default*: `0.1`.\n        min_lr: (float or List[float]) A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. *Default*: `0.0`.\n    \"\"\"\n\n    threshold: float = 1e-4\n    threshold_mode: str = \"rel\"\n    cooldown: int = 0\n    patience: int = 10\n    factor: float = 0.1\n    min_lr: Any = field(\n        default=0.0, validator=lambda instance, attr, value: instance.validate_min_lr()\n    )\n\n    def validate_min_lr(self):\n        \"\"\"min_lr Validation.\n\n        Ensures min_lr is a float&gt;=0 or list of floats&gt;=0\n        \"\"\"\n        if isinstance(self.min_lr, float) and self.min_lr &gt;= 0:\n            return\n        if isinstance(self.min_lr, list) and all(\n            isinstance(x, float) and x &gt;= 0 for x in self.min_lr\n        ):\n            return\n        message = \"min_lr must be a float or a list of floats.\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ReduceLROnPlateauConfig.validate_min_lr","title":"<code>validate_min_lr()</code>","text":"<p>min_lr Validation.</p> <p>Ensures min_lr is a float&gt;=0 or list of floats&gt;=0</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>def validate_min_lr(self):\n    \"\"\"min_lr Validation.\n\n    Ensures min_lr is a float&gt;=0 or list of floats&gt;=0\n    \"\"\"\n    if isinstance(self.min_lr, float) and self.min_lr &gt;= 0:\n        return\n    if isinstance(self.min_lr, list) and all(\n        isinstance(x, float) and x &gt;= 0 for x in self.min_lr\n    ):\n        return\n    message = \"min_lr must be a float or a list of floats.\"\n    logger.error(message)\n    raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.StepLRConfig","title":"<code>StepLRConfig</code>","text":"<p>Configuration for StepLR scheduler.</p> <p>Attributes:</p> Name Type Description <code>step_size</code> <code>int</code> <p>(int) Period of learning rate decay. If step_size=10, then every 10 epochs, learning rate will be reduced by a factor of gamma. Default: <code>10</code>.</p> <code>gamma</code> <code>float</code> <p>(float) Multiplicative factor of learning rate decay. Default: <code>0.1</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass StepLRConfig:\n    \"\"\"Configuration for StepLR scheduler.\n\n    Attributes:\n        step_size: (int) Period of learning rate decay. If step_size=10, then every 10 epochs, learning rate will be reduced by a factor of gamma. *Default*: `10`.\n        gamma: (float) Multiplicative factor of learning rate decay. *Default*: `0.1`.\n    \"\"\"\n\n    step_size: int = field(default=10, validator=validators.gt(0))\n    gamma: float = 0.1\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.TrainerConfig","title":"<code>TrainerConfig</code>","text":"<p>Configuration for trainer.</p> <p>Attributes:</p> Name Type Description <code>train_data_loader</code> <code>DataLoaderConfig</code> <p>(Note: Any parameters from Torch's DataLoader could be used.)</p> <code>val_data_loader</code> <code>DataLoaderConfig</code> <p>(Similar to train_data_loader)</p> <code>model_ckpt</code> <code>ModelCkptConfig</code> <p>(Note: Any parameters from Lightning's ModelCheckpoint could be used.)</p> <code>trainer_devices</code> <code>Any</code> <p>(int) Number of devices to train on (int), which devices to train on (list or str), or \"auto\" to select automatically. Default: <code>\"auto\"</code>.</p> <code>trainer_accelerator</code> <code>str</code> <p>(str) One of the (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the Trainer to be connected to. Default: <code>\"auto\"</code>.</p> <code>profiler</code> <code>Optional[str]</code> <p>(str) Profiler for pytorch Trainer. One of [\"advanced\", \"passthrough\", \"pytorch\", \"simple\"]. Default: <code>None</code>.</p> <code>trainer_strategy</code> <code>str</code> <p>(str) Training strategy, one of [\"auto\", \"ddp\", \"fsdp\", \"ddp_find_unused_parameters_false\", \"ddp_find_unused_parameters_true\", ...]. This supports any training strategy that is supported by <code>lightning.Trainer</code>. Default: <code>\"auto\"</code>.</p> <code>enable_progress_bar</code> <code>bool</code> <p>(bool) When True, enables printing the logs during training. Default: <code>True</code>.</p> <code>min_train_steps_per_epoch</code> <code>int</code> <p>(int) Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer limit_train_batches parameter of Torch Trainer. Default: <code>200</code>.</p> <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>(int) Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data or <code>min_train_steps_per_epoch</code>, whichever is largest. Default: <code>None</code>. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>visualize_preds_during_training</code> <code>bool</code> <p>(bool) If set to <code>True</code>, sample predictions (keypoints + confidence maps) are saved to <code>viz</code> folder in the ckpt dir and in wandb table. Default: <code>False</code>.</p> <code>keep_viz</code> <code>bool</code> <p>(bool) If set to <code>True</code>, the <code>viz</code> folder will be kept after training. If <code>False</code>, the <code>viz</code> folder will be deleted after training. Only applies when <code>visualize_preds_during_training</code> is <code>True</code>. Default: <code>False</code>.</p> <code>max_epochs</code> <code>int</code> <p>(int) Maximum number of epochs to run. Default: <code>10</code>.</p> <code>seed</code> <code>int</code> <p>(int) Seed value for the current experiment. Default: <code>0</code>.</p> <code>use_wandb</code> <code>bool</code> <p>(bool) True to enable wandb logging. Default: <code>False</code>.</p> <code>save_ckpt</code> <code>bool</code> <p>(bool) True to enable checkpointing. Default: <code>False</code>.</p> <code>save_ckpt_path</code> <code>Optional[str]</code> <p>(str) Directory path to save the training config and checkpoint files. Default: <code>None</code>.</p> <code>resume_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file from which training is resumed. Default: <code>None</code>.</p> <code>wandb</code> <code>WandBConfig</code> <p>(Only if use_wandb is True, else skip this)</p> <code>optimizer_name</code> <code>str</code> <p>(str) Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: <code>\"Adam\"</code>.</p> <code>optimizer</code> <code>OptimizerConfig</code> <p>create an optimizer configuration</p> <code>lr_scheduler</code> <code>Optional[LRSchedulerConfig]</code> <p>create an lr_scheduler configuration</p> <code>early_stopping</code> <code>Optional[EarlyStoppingConfig]</code> <p>create an early_stopping configuration</p> <code>zmq</code> <code>Optional[ZMQConfig]</code> <p>Zmq config with publish and controller port addresses.</p> <p>Methods:</p> Name Description <code>validate_optimizer_name</code> <p>Validate that optimizer_name is one of the allowed values.</p> <code>validate_trainer_devices</code> <p>Validate the value of trainer_devices.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass TrainerConfig:\n    \"\"\"Configuration for trainer.\n\n    Attributes:\n        train_data_loader: (Note: Any parameters from Torch's DataLoader could be used.)\n        val_data_loader: (Similar to train_data_loader)\n        model_ckpt: (Note: Any parameters from Lightning's ModelCheckpoint could be used.)\n        trainer_devices: (int) Number of devices to train on (int), which devices to train on (list or str), or \"auto\" to select automatically. *Default*: `\"auto\"`.\n        trainer_accelerator: (str) One of the (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the Trainer to be connected to. *Default*: `\"auto\"`.\n        profiler: (str) Profiler for pytorch Trainer. One of [\"advanced\", \"passthrough\", \"pytorch\", \"simple\"]. *Default*: `None`.\n        trainer_strategy: (str) Training strategy, one of [\"auto\", \"ddp\", \"fsdp\", \"ddp_find_unused_parameters_false\", \"ddp_find_unused_parameters_true\", ...]. This supports any training strategy that is supported by `lightning.Trainer`. *Default*: `\"auto\"`.\n        enable_progress_bar: (bool) When True, enables printing the logs during training. *Default*: `True`.\n        min_train_steps_per_epoch: (int) Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer limit_train_batches parameter of Torch Trainer. *Default*: `200`.\n        train_steps_per_epoch: (int) Number of minibatches (steps) to train for in an epoch. If set to `None`, this is set to the number of batches in the training data or `min_train_steps_per_epoch`, whichever is largest. *Default*: `None`. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        visualize_preds_during_training: (bool) If set to `True`, sample predictions (keypoints + confidence maps) are saved to `viz` folder in the ckpt dir and in wandb table. *Default*: `False`.\n        keep_viz: (bool) If set to `True`, the `viz` folder will be kept after training. If `False`, the `viz` folder will be deleted after training. Only applies when `visualize_preds_during_training` is `True`. *Default*: `False`.\n        max_epochs: (int) Maximum number of epochs to run. *Default*: `10`.\n        seed: (int) Seed value for the current experiment. *Default*: `0`.\n        use_wandb: (bool) True to enable wandb logging. *Default*: `False`.\n        save_ckpt: (bool) True to enable checkpointing. *Default*: `False`.\n        save_ckpt_path: (str) Directory path to save the training config and checkpoint files. *Default*: `None`.\n        resume_ckpt_path: (str) Path to `.ckpt` file from which training is resumed. *Default*: `None`.\n        wandb: (Only if use_wandb is True, else skip this)\n        optimizer_name: (str) Optimizer to be used. One of [\"Adam\", \"AdamW\"]. *Default*: `\"Adam\"`.\n        optimizer: create an optimizer configuration\n        lr_scheduler: create an lr_scheduler configuration\n        early_stopping: create an early_stopping configuration\n        zmq: Zmq config with publish and controller port addresses.\n    \"\"\"\n\n    train_data_loader: DataLoaderConfig = field(factory=DataLoaderConfig)\n    val_data_loader: DataLoaderConfig = field(factory=DataLoaderConfig)\n    model_ckpt: ModelCkptConfig = field(factory=ModelCkptConfig)\n    trainer_devices: Any = field(\n        default=\"auto\",\n        validator=lambda inst, attr, val: TrainerConfig.validate_trainer_devices(val),\n    )\n    trainer_accelerator: str = \"auto\"\n    profiler: Optional[str] = None\n    trainer_strategy: str = \"auto\"\n    enable_progress_bar: bool = True\n    min_train_steps_per_epoch: int = 200\n    train_steps_per_epoch: Optional[int] = None\n    visualize_preds_during_training: bool = False\n    keep_viz: bool = False\n    max_epochs: int = 10\n    seed: int = 0\n    use_wandb: bool = False\n    save_ckpt: bool = False\n    save_ckpt_path: Optional[str] = None\n    resume_ckpt_path: Optional[str] = None\n    wandb: WandBConfig = field(factory=WandBConfig)\n    optimizer_name: str = field(\n        default=\"Adam\",\n        validator=lambda inst, attr, val: TrainerConfig.validate_optimizer_name(val),\n    )\n    optimizer: OptimizerConfig = field(factory=OptimizerConfig)\n    lr_scheduler: Optional[LRSchedulerConfig] = None\n    early_stopping: Optional[EarlyStoppingConfig] = None\n    online_hard_keypoint_mining: Optional[HardKeypointMiningConfig] = field(\n        factory=HardKeypointMiningConfig\n    )\n    zmq: Optional[ZMQConfig] = field(factory=ZMQConfig)  # Required for SLEAP GUI\n\n    @staticmethod\n    def validate_optimizer_name(value):\n        \"\"\"Validate that optimizer_name is one of the allowed values.\"\"\"\n        if value not in [\"Adam\", \"AdamW\"]:\n            message = \"optimizer_name must be one of: Adam, AdamW\"\n            logger.error(message)\n            raise ValueError(message)\n        return True\n\n    @staticmethod\n    def validate_trainer_devices(value):\n        \"\"\"Validate the value of trainer_devices.\"\"\"\n        if isinstance(value, int) and value &gt;= 0:\n            return\n        if isinstance(value, list) and all(\n            isinstance(x, int) and x &gt;= 0 for x in value\n        ):\n            return\n        if isinstance(value, str) and value == \"auto\":\n            return\n        message = \"trainer_devices must be an integer &gt;= 0, a list of integers &gt;= 0, or the string 'auto'.\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.TrainerConfig.validate_optimizer_name","title":"<code>validate_optimizer_name(value)</code>  <code>staticmethod</code>","text":"<p>Validate that optimizer_name is one of the allowed values.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@staticmethod\ndef validate_optimizer_name(value):\n    \"\"\"Validate that optimizer_name is one of the allowed values.\"\"\"\n    if value not in [\"Adam\", \"AdamW\"]:\n        message = \"optimizer_name must be one of: Adam, AdamW\"\n        logger.error(message)\n        raise ValueError(message)\n    return True\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.TrainerConfig.validate_trainer_devices","title":"<code>validate_trainer_devices(value)</code>  <code>staticmethod</code>","text":"<p>Validate the value of trainer_devices.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@staticmethod\ndef validate_trainer_devices(value):\n    \"\"\"Validate the value of trainer_devices.\"\"\"\n    if isinstance(value, int) and value &gt;= 0:\n        return\n    if isinstance(value, list) and all(\n        isinstance(x, int) and x &gt;= 0 for x in value\n    ):\n        return\n    if isinstance(value, str) and value == \"auto\":\n        return\n    message = \"trainer_devices must be an integer &gt;= 0, a list of integers &gt;= 0, or the string 'auto'.\"\n    logger.error(message)\n    raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.WandBConfig","title":"<code>WandBConfig</code>","text":"<p>Configuration for WandB.</p> <p>Only if use_wandb is True, else skip this</p> <p>Attributes:</p> Name Type Description <code>entity</code> <code>Optional[str]</code> <p>(str) Entity of wandb project. Default: <code>None</code>.</p> <code>project</code> <code>Optional[str]</code> <p>(str) Project name for the wandb project. Default: <code>None</code>.</p> <code>name</code> <code>Optional[str]</code> <p>(str) Name of the current run. Default: <code>None</code>.</p> <code>api_key</code> <code>Optional[str]</code> <p>(str) API key. The API key is masked when saved to config files. Default: <code>None</code>.</p> <code>wandb_mode</code> <code>Optional[str]</code> <p>(str) \"offline\" if only local logging is required. Default: <code>\"None\"</code>.</p> <code>prv_runid</code> <code>Optional[str]</code> <p>(str) Previous run ID if training should be resumed from a previous ckpt. Default: <code>None</code>.</p> <code>group</code> <code>Optional[str]</code> <p>(str) Group for wandb logging. Default: <code>None</code>.</p> <code>current_run_id</code> <code>Optional[str]</code> <p>(str) Run ID for the current model training. (stored once the training starts). Default: <code>None</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass WandBConfig:\n    \"\"\"Configuration for WandB.\n\n    Only if use_wandb is True, else skip this\n\n    Attributes:\n        entity: (str) Entity of wandb project. *Default*: `None`.\n        project: (str) Project name for the wandb project. *Default*: `None`.\n        name: (str) Name of the current run. *Default*: `None`.\n        api_key: (str) API key. The API key is masked when saved to config files. *Default*: `None`.\n        wandb_mode: (str) \"offline\" if only local logging is required. *Default*: `\"None\"`.\n        prv_runid: (str) Previous run ID if training should be resumed from a previous ckpt. *Default*: `None`.\n        group: (str) Group for wandb logging. *Default*: `None`.\n        current_run_id: (str) Run ID for the current model training. (stored once the training starts). *Default*: `None`.\n    \"\"\"\n\n    entity: Optional[str] = None\n    project: Optional[str] = None\n    name: Optional[str] = None\n    api_key: Optional[str] = None\n    wandb_mode: Optional[str] = None\n    prv_runid: Optional[str] = None\n    group: Optional[str] = None\n    current_run_id: Optional[str] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ZMQConfig","title":"<code>ZMQConfig</code>","text":"<p>Configuration of ZeroMQ-based monitoring of the training.</p> <p>Attributes:</p> Name Type Description <code>controller_address</code> <code>Optional[str]</code> <p>IP address/hostname and port number of the endpoint to listen for command messages from. For TCP-based endpoints, this must be in the form of \"tcp://{ip_address}:{port_number}\". Defaults to None. Default: <code>None</code>.</p> <code>controller_polling_timeout</code> <code>int</code> <p>Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed. Default: <code>10</code>.</p> <code>publish_address</code> <code>Optional[str]</code> <p>IP address/hostname and port number of the endpoint to publish updates to. For TCP-based endpoints, this must be in the form of \"tcp://{ip_address}:{port_number}\". Sample: \"tcp://127.0.0.1:9001\". Defaults to None. Default: <code>None</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass ZMQConfig:\n    \"\"\"Configuration of ZeroMQ-based monitoring of the training.\n\n    Attributes:\n        controller_address: IP address/hostname and port number of the endpoint to listen for command messages from. For TCP-based endpoints, this must be in the form of \"tcp://{ip_address}:{port_number}\". Defaults to None. *Default*: `None`.\n        controller_polling_timeout: Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed. *Default*: `10`.\n        publish_address: IP address/hostname and port number of the endpoint to publish updates to. For TCP-based endpoints, this must be in the form of \"tcp://{ip_address}:{port_number}\". Sample: \"tcp://127.0.0.1:9001\". Defaults to None. *Default*: `None`.\n    \"\"\"\n\n    controller_address: Optional[str] = None\n    controller_polling_timeout: int = 10\n    publish_address: Optional[str] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.trainer_mapper","title":"<code>trainer_mapper(legacy_config)</code>","text":"<p>Map the legacy trainer configuration to the new trainer configuration.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_config</code> <code>dict</code> <p>A dictionary containing the legacy trainer configuration.</p> required <p>Returns:</p> Type Description <code>TrainerConfig</code> <p>An instance of <code>TrainerConfig</code> with the mapped configuration.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>def trainer_mapper(legacy_config: dict) -&gt; TrainerConfig:\n    \"\"\"Map the legacy trainer configuration to the new trainer configuration.\n\n    Args:\n        legacy_config: A dictionary containing the legacy trainer configuration.\n\n    Returns:\n        An instance of `TrainerConfig` with the mapped configuration.\n    \"\"\"\n    legacy_config_optimization = legacy_config.get(\"optimization\", {})\n    legacy_config_outputs = legacy_config.get(\"outputs\", {})\n    resume_ckpt_path = legacy_config.get(\"model\", {}).get(\"base_checkpoint\", None)\n    resume_ckpt_path = (\n        (Path(resume_ckpt_path) / \"best.ckpt\").as_posix()\n        if resume_ckpt_path is not None\n        else None\n    )\n    run_name = legacy_config_outputs.get(\"run_name\", None)\n    run_name = run_name if run_name is not None else \"\"\n    run_name_prefix = legacy_config_outputs.get(\"run_name_prefix\", \"\")\n    run_name_suffix = legacy_config_outputs.get(\"run_name_suffix\", \"\")\n    run_name = (\n        run_name_prefix\n        if run_name_prefix is not None\n        else \"\" + run_name + run_name_suffix if run_name_prefix is not None else \"\"\n    )\n\n    trainer_cfg_args = {}\n    train_dataloader_cfg_args = {}\n    val_dataloader_cfg_args = {}\n    model_ckpt_cfg_args = {}\n    optimizer_cfg_args = {}\n    lr_scheduler_cfg_args = {}\n    reduce_lr_on_plateau_cfg_args = {}\n    early_stopping_cfg_args = {}\n    zmq_cfg_args = {}\n    online_hard_keypoint_mining_cfg_args = {}\n\n    # train dataloader\n    if legacy_config_optimization.get(\"batch_size\", None) is not None:\n        train_dataloader_cfg_args[\"batch_size\"] = legacy_config_optimization[\n            \"batch_size\"\n        ]\n\n    if legacy_config_optimization.get(\"online_shuffling\", None) is not None:\n        train_dataloader_cfg_args[\"shuffle\"] = legacy_config_optimization[\n            \"online_shuffling\"\n        ]\n\n    if legacy_config_optimization.get(\"num_workers\", None) is not None:\n        train_dataloader_cfg_args[\"num_workers\"] = legacy_config_optimization[\n            \"num_workers\"\n        ]\n\n    trainer_cfg_args[\"train_data_loader\"] = DataLoaderConfig(\n        **train_dataloader_cfg_args\n    )\n\n    # val dataloader\n    if legacy_config_optimization.get(\"batch_size\", None) is not None:\n        val_dataloader_cfg_args[\"batch_size\"] = legacy_config_optimization[\"batch_size\"]\n\n    if legacy_config_optimization.get(\"num_workers\", None) is not None:\n        val_dataloader_cfg_args[\"num_workers\"] = legacy_config_optimization[\n            \"num_workers\"\n        ]\n\n    trainer_cfg_args[\"val_data_loader\"] = DataLoaderConfig(**val_dataloader_cfg_args)\n\n    # model ckpt\n    if (\n        legacy_config_outputs.get(\"checkpointing\", {}).get(\"latest_model\", None)\n        is not None\n    ):\n        model_ckpt_cfg_args[\"save_last\"] = legacy_config_outputs[\"checkpointing\"][\n            \"latest_model\"\n        ]\n\n    trainer_cfg_args[\"model_ckpt\"] = ModelCkptConfig(**model_ckpt_cfg_args)\n\n    if legacy_config_outputs.get(\"save_visualizations\", None) is not None:\n        trainer_cfg_args[\"visualize_preds_during_training\"] = legacy_config_outputs[\n            \"save_visualizations\"\n        ]\n\n    # Handle legacy delete_viz_images parameter\n    if legacy_config_outputs.get(\"keep_viz_images\", None) is not None:\n        trainer_cfg_args[\"keep_viz\"] = legacy_config_outputs[\"keep_viz_images\"]\n\n    if legacy_config_optimization.get(\"epochs\", None) is not None:\n        trainer_cfg_args[\"max_epochs\"] = legacy_config_optimization[\"epochs\"]\n\n    if legacy_config_optimization.get(\"min_batches_per_epoch\", None) is not None:\n        trainer_cfg_args[\"min_train_steps_per_epoch\"] = legacy_config_optimization[\n            \"min_batches_per_epoch\"\n        ]\n\n    if legacy_config_optimization.get(\"batches_per_epoch\", None) is not None:\n        trainer_cfg_args[\"train_steps_per_epoch\"] = legacy_config_optimization[\n            \"batches_per_epoch\"\n        ]\n\n    trainer_cfg_args[\"save_ckpt\"] = True\n    trainer_cfg_args[\"save_ckpt_path\"] = (\n        Path(legacy_config_outputs.get(\"runs_folder\", \".\")) / run_name\n    ).as_posix()\n    trainer_cfg_args[\"resume_ckpt_path\"] = resume_ckpt_path\n\n    trainer_cfg_args[\"optimizer_name\"] = re.sub(\n        r\"^[a-z]\",\n        lambda x: x.group().upper(),\n        legacy_config_optimization.get(\"optimizer\", \"adam\"),\n    )\n    if legacy_config_optimization.get(\"initial_learning_rate\", None) is not None:\n        optimizer_cfg_args[\"lr\"] = legacy_config_optimization[\"initial_learning_rate\"]\n\n    trainer_cfg_args[\"optimizer\"] = OptimizerConfig(**optimizer_cfg_args)\n\n    if (\n        legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n            \"reduce_on_plateau\", None\n        )\n        is not None\n    ):\n        if legacy_config_optimization[\"learning_rate_schedule\"][\"reduce_on_plateau\"]:\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"plateau_min_delta\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"threshold\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"plateau_min_delta\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"plateau_cooldown\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"cooldown\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"plateau_cooldown\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"reduction_factor\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"factor\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"reduction_factor\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"plateau_patience\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"patience\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"plateau_patience\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"min_learning_rate\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"min_lr\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"min_learning_rate\"]\n\n            lr_scheduler_cfg_args[\"reduce_lr_on_plateau\"] = ReduceLROnPlateauConfig(\n                **reduce_lr_on_plateau_cfg_args\n            )\n\n    trainer_cfg_args[\"lr_scheduler\"] = LRSchedulerConfig(**lr_scheduler_cfg_args)\n\n    if (\n        legacy_config_optimization.get(\"early_stopping\", {}).get(\n            \"stop_training_on_plateau\", None\n        )\n        is not None\n    ):\n        early_stopping_cfg_args[\"stop_training_on_plateau\"] = (\n            legacy_config_optimization[\"early_stopping\"][\"stop_training_on_plateau\"]\n        )\n        if (\n            legacy_config_optimization.get(\"early_stopping\", {}).get(\n                \"plateau_min_delta\", None\n            )\n            is not None\n        ):\n            early_stopping_cfg_args[\"min_delta\"] = legacy_config_optimization[\n                \"early_stopping\"\n            ][\"plateau_min_delta\"]\n\n        if (\n            legacy_config_optimization.get(\"early_stopping\", {}).get(\n                \"plateau_patience\", None\n            )\n            is not None\n        ):\n            early_stopping_cfg_args[\"patience\"] = legacy_config_optimization[\n                \"early_stopping\"\n            ][\"plateau_patience\"]\n\n    trainer_cfg_args[\"early_stopping\"] = EarlyStoppingConfig(**early_stopping_cfg_args)\n\n    if (\n        legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n            \"online_mining\", None\n        )\n        is not None\n    ):\n        if legacy_config_optimization[\"hard_keypoint_mining\"][\"online_mining\"]:\n            online_hard_keypoint_mining_cfg_args[\"online_mining\"] = True\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"hard_to_easy_ratio\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"hard_to_easy_ratio\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"hard_to_easy_ratio\"]\n            )\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"min_hard_keypoints\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"min_hard_keypoints\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"min_hard_keypoints\"]\n            )\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"max_hard_keypoints\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"max_hard_keypoints\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"max_hard_keypoints\"]\n            )\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"loss_scale\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"loss_scale\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"loss_scale\"]\n            )\n\n    trainer_cfg_args[\"online_hard_keypoint_mining\"] = HardKeypointMiningConfig(\n        **online_hard_keypoint_mining_cfg_args\n    )\n\n    if (\n        legacy_config_outputs.get(\"zmq\", {}).get(\"subscribe_to_controller\", None)\n        is not None\n    ):\n        zmq_cfg_args[\"controller_address\"] = legacy_config_outputs[\"zmq\"][\n            \"controller_address\"\n        ]\n\n    if legacy_config_outputs.get(\"zmq\", {}).get(\"publish_updates\", None) is not None:\n        zmq_cfg_args[\"publish_address\"] = legacy_config_outputs[\"zmq\"][\n            \"publish_address\"\n        ]\n\n    if (\n        legacy_config_outputs.get(\"zmq\", {}).get(\"controller_polling_timeout\", None)\n        is not None\n    ):\n        zmq_cfg_args[\"controller_polling_timeout\"] = legacy_config_outputs[\"zmq\"][\n            \"controller_polling_timeout\"\n        ]\n\n    trainer_cfg_args[\"zmq\"] = ZMQConfig(**zmq_cfg_args)\n\n    return TrainerConfig(**trainer_cfg_args)\n</code></pre>"},{"location":"api/config/training_job_config/","title":"training_job_config","text":""},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config","title":"<code>sleap_nn.config.training_job_config</code>","text":"<p>Serializable configuration classes for specifying all training job parameters.</p> <p>These configuration classes are intended to specify all the parameters required to run a training job or perform inference from a serialized one.</p> <p>They are explicitly not intended to implement any of the underlying functionality that they parametrize. This serves two purposes:</p> <pre><code>1. Parameter specification through simple attributes. These can be read/edited by a\n    human, as well as easily be serialized/deserialized to/from simple dictionaries\n    and YAML.\n\n2. Decoupling from the implementation. This makes it easier to design functional\n    modules with attributes/parameters that contain objects that may not be easily\n    serializable or may implement additional logic that relies on runtime\n    information or other parameters.\n</code></pre> <p>In general, classes that implement the actual functionality related to these configuration classes should provide a classmethod for instantiation from the configuration class instances. This makes it easier to implement other logic not related to the high level parameters at creation time.</p> <p>Conveniently, this format also provides a single location where all user-facing parameters are aggregated and documented for end users (as opposed to developers).</p> <p>Classes:</p> Name Description <code>TrainingJobConfig</code> <p>Configuration of a training job.</p> <p>Functions:</p> Name Description <code>check_must_be_set</code> <p>Check that all required fields are set in the BackboneConfig and HeadConfig.</p> <code>verify_training_cfg</code> <p>Get sleap-nn training config from a DictConfig object.</p>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig","title":"<code>TrainingJobConfig</code>","text":"<p>Configuration of a training job.</p> <p>Attributes:</p> Name Type Description <code>data_config</code> <code>DataConfig</code> <p>Configuration options related to the training data.</p> <code>model_config</code> <code>ModelConfig</code> <p>Configuration options related to the model architecture.</p> <code>trainer_config</code> <code>TrainerConfig</code> <p>Configuration ooptions related to model training.</p> <code>outputs</code> <code>TrainerConfig</code> <p>Configuration options related to outputs during training.</p> <code>name</code> <code>Optional[Text]</code> <p>Optional name for this configuration profile.</p> <code>description</code> <code>Optional[Text]</code> <p>Optional description of the configuration.</p> <code>sleap_nn_version</code> <code>Optional[Text]</code> <p>Version of SLEAP that generated this configuration.</p> <code>filename</code> <code>Optional[Text]</code> <p>Path to this config file if it was loaded from disk.</p> <p>Methods:</p> Name Description <code>load_sleap_config</code> <p>Load a SLEAP configuration from a JSON file and convert it to OmegaConf.</p> <code>load_sleap_config_from_json</code> <p>Load a SLEAP configuration from a JSON string and convert it to OmegaConf.</p> <code>to_sleap_nn_cfg</code> <p>Convert the attrs class to OmegaConf object.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>@define\nclass TrainingJobConfig:\n    \"\"\"Configuration of a training job.\n\n    Attributes:\n        data_config: Configuration options related to the training data.\n        model_config: Configuration options related to the model architecture.\n        trainer_config: Configuration ooptions related to model training.\n        outputs: Configuration options related to outputs during training.\n        name: Optional name for this configuration profile.\n        description: Optional description of the configuration.\n        sleap_nn_version: Version of SLEAP that generated this configuration.\n        filename: Path to this config file if it was loaded from disk.\n    \"\"\"\n\n    data_config: DataConfig = field(factory=DataConfig)\n    model_config: ModelConfig = field(factory=ModelConfig)\n    trainer_config: TrainerConfig = field(factory=TrainerConfig)\n    name: Optional[Text] = \"\"\n    description: Optional[Text] = \"\"\n    sleap_nn_version: Optional[Text] = sleap_nn.__version__\n    filename: Optional[Text] = \"\"\n\n    def to_sleap_nn_cfg(self) -&gt; DictConfig:\n        \"\"\"Convert the attrs class to OmegaConf object.\"\"\"\n        config = OmegaConf.structured(self)\n        OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n        return config\n\n    @classmethod\n    def load_sleap_config(cls, json_file_path: str) -&gt; OmegaConf:\n        \"\"\"Load a SLEAP configuration from a JSON file and convert it to OmegaConf.\n\n        Args:\n            cls: The class to instantiate with the loaded configuration.\n            json_file_path: Path to a JSON file containing the SLEAP configuration.\n\n        Returns:\n            An OmegaConf instance with the loaded configuration.\n        \"\"\"\n        with open(json_file_path, \"r\") as f:\n            old_config = json.load(f)\n\n        return cls.load_sleap_config_from_json(old_config)\n\n    @classmethod\n    def load_sleap_config_from_json(cls, json_str: str) -&gt; OmegaConf:\n        \"\"\"Load a SLEAP configuration from a JSON string and convert it to OmegaConf.\n\n        Args:\n            cls: The class to instantiate with the loaded configuration.\n            json_str: JSON-formatted string containing the SLEAP configuration.\n\n        Returns:\n            An OmegaConf instance with the loaded configuration.\n        \"\"\"\n        data_config = data_mapper(json_str)\n        model_config = model_mapper(json_str)\n        trainer_config = trainer_mapper(json_str)\n\n        config = cls(\n            data_config=data_config,\n            model_config=model_config,\n            trainer_config=trainer_config,\n        )\n\n        schema = OmegaConf.structured(config)\n        config_omegaconf = OmegaConf.merge(schema, OmegaConf.create(asdict(config)))\n        OmegaConf.to_container(config_omegaconf, resolve=True, throw_on_missing=True)\n\n        return config_omegaconf\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig.load_sleap_config","title":"<code>load_sleap_config(json_file_path)</code>  <code>classmethod</code>","text":"<p>Load a SLEAP configuration from a JSON file and convert it to OmegaConf.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class to instantiate with the loaded configuration.</p> required <code>json_file_path</code> <code>str</code> <p>Path to a JSON file containing the SLEAP configuration.</p> required <p>Returns:</p> Type Description <code>OmegaConf</code> <p>An OmegaConf instance with the loaded configuration.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>@classmethod\ndef load_sleap_config(cls, json_file_path: str) -&gt; OmegaConf:\n    \"\"\"Load a SLEAP configuration from a JSON file and convert it to OmegaConf.\n\n    Args:\n        cls: The class to instantiate with the loaded configuration.\n        json_file_path: Path to a JSON file containing the SLEAP configuration.\n\n    Returns:\n        An OmegaConf instance with the loaded configuration.\n    \"\"\"\n    with open(json_file_path, \"r\") as f:\n        old_config = json.load(f)\n\n    return cls.load_sleap_config_from_json(old_config)\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig.load_sleap_config_from_json","title":"<code>load_sleap_config_from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Load a SLEAP configuration from a JSON string and convert it to OmegaConf.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class to instantiate with the loaded configuration.</p> required <code>json_str</code> <code>str</code> <p>JSON-formatted string containing the SLEAP configuration.</p> required <p>Returns:</p> Type Description <code>OmegaConf</code> <p>An OmegaConf instance with the loaded configuration.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>@classmethod\ndef load_sleap_config_from_json(cls, json_str: str) -&gt; OmegaConf:\n    \"\"\"Load a SLEAP configuration from a JSON string and convert it to OmegaConf.\n\n    Args:\n        cls: The class to instantiate with the loaded configuration.\n        json_str: JSON-formatted string containing the SLEAP configuration.\n\n    Returns:\n        An OmegaConf instance with the loaded configuration.\n    \"\"\"\n    data_config = data_mapper(json_str)\n    model_config = model_mapper(json_str)\n    trainer_config = trainer_mapper(json_str)\n\n    config = cls(\n        data_config=data_config,\n        model_config=model_config,\n        trainer_config=trainer_config,\n    )\n\n    schema = OmegaConf.structured(config)\n    config_omegaconf = OmegaConf.merge(schema, OmegaConf.create(asdict(config)))\n    OmegaConf.to_container(config_omegaconf, resolve=True, throw_on_missing=True)\n\n    return config_omegaconf\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig.to_sleap_nn_cfg","title":"<code>to_sleap_nn_cfg()</code>","text":"<p>Convert the attrs class to OmegaConf object.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>def to_sleap_nn_cfg(self) -&gt; DictConfig:\n    \"\"\"Convert the attrs class to OmegaConf object.\"\"\"\n    config = OmegaConf.structured(self)\n    OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n    return config\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.check_must_be_set","title":"<code>check_must_be_set(config)</code>","text":"<p>Check that all required fields are set in the BackboneConfig and HeadConfig.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>def check_must_be_set(config: DictConfig) -&gt; None:\n    \"\"\"Check that all required fields are set in the BackboneConfig and HeadConfig.\"\"\"\n    backbone_config = config.model_config.backbone_config\n    head_config = config.model_config.head_configs\n\n    backbone_attributes = [k for k, v in backbone_config.items() if v is not None]\n\n    head_config_attributes = [k for k, v in head_config.items() if v is not None]\n\n    if len(backbone_attributes) == 0:\n        message = \"BackboneConfig: At least one attribute of this class must be set.\"\n        raise ValueError(message)\n\n    if len(head_config_attributes) == 0:\n        message = \"HeadConfig: At least one attribute of this class must be set.\"\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.verify_training_cfg","title":"<code>verify_training_cfg(cfg)</code>","text":"<p>Get sleap-nn training config from a DictConfig object.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>def verify_training_cfg(cfg: DictConfig) -&gt; DictConfig:\n    \"\"\"Get sleap-nn training config from a DictConfig object.\"\"\"\n    schema = OmegaConf.structured(TrainingJobConfig())\n    config = OmegaConf.merge(schema, cfg)\n    OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n\n    # Verify configs with @oneof class is valid\n    _ = OmegaConf.to_object(config)\n\n    # Verify required fields are set\n    check_must_be_set(config)\n    return config\n</code></pre>"},{"location":"api/config/utils/","title":"utils","text":""},{"location":"api/config/utils/#sleap_nn.config.utils","title":"<code>sleap_nn.config.utils</code>","text":"<p>Utilities for config building and validation.</p> <p>Functions:</p> Name Description <code>check_output_strides</code> <p>Check max_stride and output_stride in backbone_config with head_config.</p> <code>get_backbone_type_from_cfg</code> <p>Return the backbone type from the config. One of [unet, swint, convnext].</p> <code>get_model_type_from_cfg</code> <p>Return the model type from the config. One of [single_instance, centroid, centered_instance, bottomup].</p> <code>get_output_strides_from_heads</code> <p>Get list of output strides from head configs.</p> <code>oneof</code> <p>Ensure that the decorated attrs class only has a single attribute set.</p>"},{"location":"api/config/utils/#sleap_nn.config.utils.check_output_strides","title":"<code>check_output_strides(config)</code>","text":"<p>Check max_stride and output_stride in backbone_config with head_config.</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def check_output_strides(config: OmegaConf) -&gt; OmegaConf:\n    \"\"\"Check max_stride and output_stride in backbone_config with head_config.\"\"\"\n    output_strides = get_output_strides_from_heads(config.model_config.head_configs)\n    backbone_type = get_backbone_type_from_cfg(config)\n    if output_strides:\n        config.model_config.backbone_config[f\"{backbone_type}\"][\"output_stride\"] = min(\n            output_strides\n        )\n        if config.model_config.backbone_config[f\"{backbone_type}\"][\"max_stride\"] &lt; max(\n            output_strides\n        ):\n            config.model_config.backbone_config[f\"{backbone_type}\"][\"max_stride\"] = max(\n                output_strides\n            )\n\n    model_type = get_model_type_from_cfg(config)\n    if model_type == \"multi_class_topdown\":\n        config.model_config.head_configs.multi_class_topdown.class_vectors.output_stride = config.model_config.backbone_config[\n            f\"{backbone_type}\"\n        ][\n            \"max_stride\"\n        ]\n    return config\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.get_backbone_type_from_cfg","title":"<code>get_backbone_type_from_cfg(config)</code>","text":"<p>Return the backbone type from the config. One of [unet, swint, convnext].</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def get_backbone_type_from_cfg(config: DictConfig):\n    \"\"\"Return the backbone type from the config. One of [unet, swint, convnext].\"\"\"\n    backbone_type = None\n    for k, v in config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n    return backbone_type\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.get_model_type_from_cfg","title":"<code>get_model_type_from_cfg(config)</code>","text":"<p>Return the model type from the config. One of [single_instance, centroid, centered_instance, bottomup].</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def get_model_type_from_cfg(config: DictConfig):\n    \"\"\"Return the model type from the config. One of [single_instance, centroid, centered_instance, bottomup].\"\"\"\n    model_type = None\n    for k, v in config.model_config.head_configs.items():\n        if v is not None:\n            model_type = k\n            break\n    return model_type\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.get_output_strides_from_heads","title":"<code>get_output_strides_from_heads(head_configs)</code>","text":"<p>Get list of output strides from head configs.</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def get_output_strides_from_heads(head_configs: DictConfig):\n    \"\"\"Get list of output strides from head configs.\"\"\"\n    output_strides_from_heads = []\n    for head_type in head_configs:\n        if head_configs[head_type] is not None:\n            for head_layer in head_configs[head_type]:\n                output_strides_from_heads.append(\n                    head_configs[head_type][head_layer][\"output_stride\"]\n                )\n    return output_strides_from_heads\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.oneof","title":"<code>oneof(attrs_cls, must_be_set=False)</code>","text":"<p>Ensure that the decorated attrs class only has a single attribute set.</p> <p>This decorator is inspired by the <code>oneof</code> protobuffer field behavior.</p> <p>Parameters:</p> Name Type Description Default <code>attrs_cls</code> <p>An attrs decorated class.</p> required <code>must_be_set</code> <code>bool</code> <p>If True, raise an error if none of the attributes are set. If not, error will only be raised if more than one attribute is set.</p> <code>False</code> <p>Returns:</p> Type Description <p>The <code>attrs_cls</code> with an <code>__init__</code> method that checks for the number of attributes that are set.</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def oneof(attrs_cls, must_be_set: bool = False):\n    \"\"\"Ensure that the decorated attrs class only has a single attribute set.\n\n    This decorator is inspired by the `oneof` protobuffer field behavior.\n\n    Args:\n        attrs_cls: An attrs decorated class.\n        must_be_set: If True, raise an error if none of the attributes are set. If not,\n            error will only be raised if more than one attribute is set.\n\n    Returns:\n        The `attrs_cls` with an `__init__` method that checks for the number of\n        attributes that are set.\n    \"\"\"\n    # Check if the class is an attrs class at all.\n    if not hasattr(attrs_cls, \"__attrs_attrs__\"):\n        message = \"Classes decorated with oneof must also be attr.s decorated.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    # Pull out attrs generated class attributes.\n    attribs = attrs_cls.__attrs_attrs__\n    init_fn = attrs_cls.__init__\n\n    # Define a new __init__ function that wraps the attrs generated one.\n    def new_init_fn(self, *args, **kwargs):\n        # Execute the standard attrs-generated __init__.\n        init_fn(self, *args, **kwargs)\n\n        # Check for attribs with set values.\n        attribs_with_value = [\n            attrib for attrib in attribs if getattr(self, attrib.name) is not None\n        ]\n\n        class_name = self.__class__.__name__\n\n        if len(attribs_with_value) &gt; 1:\n            # Raise error if more than one attribute is set.\n            message = (\n                f\"{class_name}: Only one attribute of this class can be set (not None).\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        if len(attribs_with_value) == 0 and must_be_set:\n            # Raise error if none are set.\n            message = f\"{class_name}: At least one attribute of this class must be set.\"\n            logger.error(message)\n            raise ValueError(message)\n\n    # Replace with wrapped __init__.\n    attrs_cls.__init__ = new_init_fn\n\n    # Define convenience method for getting the set attribute.\n    def which_oneof_attrib_name(self):\n        attribs_with_value = [\n            attrib for attrib in attribs if getattr(self, attrib.name) is not None\n        ]\n        class_name = self.__class__.__name__\n\n        if len(attribs_with_value) &gt; 1:\n            # Raise error if more than one attribute is set.\n            message = (\n                f\"{class_name}: Only one attribute of this class can be set (not None).\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        if len(attribs_with_value) == 0:\n            if must_be_set:\n                # Raise error if none are set.\n                message = (\n                    f\"{class_name}: At least one attribute of this class must be set.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n            else:\n                return None\n\n        return attribs_with_value[0].name\n\n    def which_oneof(self):\n        attrib_name = self.which_oneof_attrib_name()\n\n        if attrib_name is None:\n            return None\n\n        return getattr(self, attrib_name)\n\n    attrs_cls.which_oneof_attrib_name = which_oneof_attrib_name\n    attrs_cls.which_oneof = which_oneof\n\n    return attrs_cls\n</code></pre>"},{"location":"api/data/","title":"data","text":""},{"location":"api/data/#sleap_nn.data","title":"<code>sleap_nn.data</code>","text":"<p>Modules related to data loading and processing.</p> <p>Modules:</p> Name Description <code>augmentation</code> <p>This module implements data pipeline blocks for augmentation operations.</p> <code>confidence_maps</code> <p>Generate confidence maps.</p> <code>custom_datasets</code> <p>Custom <code>torch.utils.data.Dataset</code>s for different model types.</p> <code>edge_maps</code> <p>Transformers for generating edge confidence maps and part affinity fields.</p> <code>identity</code> <p>Utilities for generating data for track identity models.</p> <code>instance_centroids</code> <p>Handle calculation of instance centroids.</p> <code>instance_cropping</code> <p>Handle cropping of instances.</p> <code>normalization</code> <p>This module implements data pipeline blocks for normalization operations.</p> <code>providers</code> <p>This module implements pipeline blocks for reading input data such as labels.</p> <code>resizing</code> <p>This module implements image resizing and padding.</p> <code>utils</code> <p>Miscellaneous utility functions for data processing.</p>"},{"location":"api/data/augmentation/","title":"augmentation","text":""},{"location":"api/data/augmentation/#sleap_nn.data.augmentation","title":"<code>sleap_nn.data.augmentation</code>","text":"<p>This module implements data pipeline blocks for augmentation operations.</p> <p>Classes:</p> Name Description <code>RandomUniformNoise</code> <p>Data transformer for applying random uniform noise to input images.</p> <p>Functions:</p> Name Description <code>apply_geometric_augmentation</code> <p>Apply kornia geometric augmentation on image and instances.</p> <code>apply_intensity_augmentation</code> <p>Apply kornia intensity augmentation on image and instances.</p>"},{"location":"api/data/augmentation/#sleap_nn.data.augmentation.RandomUniformNoise","title":"<code>RandomUniformNoise</code>","text":"<p>               Bases: <code>IntensityAugmentationBase2D</code></p> <p>Data transformer for applying random uniform noise to input images.</p> <p>This is a custom Kornia augmentation inheriting from <code>IntensityAugmentationBase2D</code>. Uniform noise within (min_val, max_val) is applied to the entire input image.</p> <p>Note: Inverse transform is not implemented and re-applying the same transformation in the example below does not work when included in an AugmentationSequential class.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>Tuple[float, float]</code> <p>2-tuple (min_val, max_val); 0.0 &lt;= min_val &lt;= max_val &lt;= 1.0.</p> required <code>p</code> <code>float</code> <p>probability for applying an augmentation. This param controls the augmentation probabilities element-wise for a batch.</p> <code>0.5</code> <code>p_batch</code> <code>float</code> <p>probability for applying an augmentation to a batch. This param controls the augmentation probabilities batch-wise.</p> <code>1.0</code> <code>same_on_batch</code> <code>bool</code> <p>apply the same transformation across the batch.</p> <code>False</code> <code>keepdim</code> <code>bool</code> <p>whether to keep the output shape the same as input <code>True</code> or broadcast it to the batch form <code>False</code>.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rng = torch.manual_seed(0)\n&gt;&gt;&gt; img = torch.rand(1, 1, 2, 2)\n&gt;&gt;&gt; RandomUniformNoise(min_val=0., max_val=0.1, p=1.)(img)\ntensor([[[[0.9607, 0.5865],\n          [0.2705, 0.5920]]]])\n</code></pre> <p>To apply the exact augmentation again, you may take the advantage of the previous parameter state:     &gt;&gt;&gt; input = torch.rand(1, 3, 32, 32)     &gt;&gt;&gt; aug = RandomUniformNoise(min_val=0., max_val=0.1, p=1.)     &gt;&gt;&gt; (aug(input) == aug(input, params=aug._params)).all()     tensor(True)</p> <p>Ref: <code>kornia.augmentation._2d.intensity.gaussian_noise &lt;https://kornia.readthedocs.io/en/latest/_modules/kornia/augmentation/_2d/intensity/gaussian_noise.html#RandomGaussianNoise&gt;</code>_.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>apply_transform</code> <p>Compute the uniform noise, add, and clamp output.</p> Source code in <code>sleap_nn/data/augmentation.py</code> <pre><code>class RandomUniformNoise(IntensityAugmentationBase2D):\n    \"\"\"Data transformer for applying random uniform noise to input images.\n\n    This is a custom Kornia augmentation inheriting from `IntensityAugmentationBase2D`.\n    Uniform noise within (min_val, max_val) is applied to the entire input image.\n\n    Note: Inverse transform is not implemented and re-applying the same transformation\n    in the example below does not work when included in an AugmentationSequential class.\n\n    Args:\n        noise: 2-tuple (min_val, max_val); 0.0 &lt;= min_val &lt;= max_val &lt;= 1.0.\n        p: probability for applying an augmentation. This param controls the augmentation probabilities\n          element-wise for a batch.\n        p_batch: probability for applying an augmentation to a batch. This param controls the augmentation\n          probabilities batch-wise.\n        same_on_batch: apply the same transformation across the batch.\n        keepdim: whether to keep the output shape the same as input `True` or broadcast it\n          to the batch form `False`.\n\n    Examples:\n        &gt;&gt;&gt; rng = torch.manual_seed(0)\n        &gt;&gt;&gt; img = torch.rand(1, 1, 2, 2)\n        &gt;&gt;&gt; RandomUniformNoise(min_val=0., max_val=0.1, p=1.)(img)\n        tensor([[[[0.9607, 0.5865],\n                  [0.2705, 0.5920]]]])\n\n    To apply the exact augmentation again, you may take the advantage of the previous parameter state:\n        &gt;&gt;&gt; input = torch.rand(1, 3, 32, 32)\n        &gt;&gt;&gt; aug = RandomUniformNoise(min_val=0., max_val=0.1, p=1.)\n        &gt;&gt;&gt; (aug(input) == aug(input, params=aug._params)).all()\n        tensor(True)\n\n    Ref: `kornia.augmentation._2d.intensity.gaussian_noise\n    &lt;https://kornia.readthedocs.io/en/latest/_modules/kornia/augmentation/_2d/intensity/gaussian_noise.html#RandomGaussianNoise&gt;`_.\n    \"\"\"\n\n    def __init__(\n        self,\n        noise: Tuple[float, float],\n        p: float = 0.5,\n        p_batch: float = 1.0,\n        clip_output: bool = True,\n        same_on_batch: bool = False,\n        keepdim: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__(\n            p=p, p_batch=p_batch, same_on_batch=same_on_batch, keepdim=keepdim\n        )\n        self.flags = {\n            \"uniform_noise\": _range_bound(noise, \"uniform_noise\", bounds=(0.0, 1.0))\n        }\n        self.clip_output = clip_output\n\n    def apply_transform(\n        self,\n        input: Tensor,\n        params: Dict[str, Tensor],\n        flags: Dict[str, Any],\n        transform: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Compute the uniform noise, add, and clamp output.\"\"\"\n        if \"uniform_noise\" in params:\n            uniform_noise = params[\"uniform_noise\"]\n        else:\n            uniform_noise = (\n                torch.FloatTensor(input.shape)\n                .uniform_(flags[\"uniform_noise\"][0], flags[\"uniform_noise\"][1])\n                .to(input.device)\n            )\n            self._params[\"uniform_noise\"] = uniform_noise\n        if self.clip_output:\n            return torch.clamp(\n                input + uniform_noise, 0.0, 1.0\n            )  # RandomGaussianNoise doesn't clamp.\n        return input + uniform_noise\n</code></pre>"},{"location":"api/data/augmentation/#sleap_nn.data.augmentation.RandomUniformNoise.__init__","title":"<code>__init__(noise, p=0.5, p_batch=1.0, clip_output=True, same_on_batch=False, keepdim=False)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/data/augmentation.py</code> <pre><code>def __init__(\n    self,\n    noise: Tuple[float, float],\n    p: float = 0.5,\n    p_batch: float = 1.0,\n    clip_output: bool = True,\n    same_on_batch: bool = False,\n    keepdim: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__(\n        p=p, p_batch=p_batch, same_on_batch=same_on_batch, keepdim=keepdim\n    )\n    self.flags = {\n        \"uniform_noise\": _range_bound(noise, \"uniform_noise\", bounds=(0.0, 1.0))\n    }\n    self.clip_output = clip_output\n</code></pre>"},{"location":"api/data/augmentation/#sleap_nn.data.augmentation.RandomUniformNoise.apply_transform","title":"<code>apply_transform(input, params, flags, transform=None)</code>","text":"<p>Compute the uniform noise, add, and clamp output.</p> Source code in <code>sleap_nn/data/augmentation.py</code> <pre><code>def apply_transform(\n    self,\n    input: Tensor,\n    params: Dict[str, Tensor],\n    flags: Dict[str, Any],\n    transform: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Compute the uniform noise, add, and clamp output.\"\"\"\n    if \"uniform_noise\" in params:\n        uniform_noise = params[\"uniform_noise\"]\n    else:\n        uniform_noise = (\n            torch.FloatTensor(input.shape)\n            .uniform_(flags[\"uniform_noise\"][0], flags[\"uniform_noise\"][1])\n            .to(input.device)\n        )\n        self._params[\"uniform_noise\"] = uniform_noise\n    if self.clip_output:\n        return torch.clamp(\n            input + uniform_noise, 0.0, 1.0\n        )  # RandomGaussianNoise doesn't clamp.\n    return input + uniform_noise\n</code></pre>"},{"location":"api/data/augmentation/#sleap_nn.data.augmentation.apply_geometric_augmentation","title":"<code>apply_geometric_augmentation(image, instances, rotation_min=-15.0, rotation_max=15.0, scale_min=0.9, scale_max=1.1, translate_width=0.02, translate_height=0.02, affine_p=0.0, erase_scale_min=0.0001, erase_scale_max=0.01, erase_ratio_min=1, erase_ratio_max=1, erase_p=0.0, mixup_lambda_min=0.01, mixup_lambda_max=0.05, mixup_p=0.0)</code>","text":"<p>Apply kornia geometric augmentation on image and instances.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image. Shape: (n_samples, C, H, W)</p> required <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)</p> required <code>rotation_min</code> <code>Optional[float]</code> <p>Minimum rotation angle in degrees. Default: -15.0.</p> <code>-15.0</code> <code>rotation_max</code> <code>Optional[float]</code> <p>Maximum rotation angle in degrees. Default: 15.0.</p> <code>15.0</code> <code>scale_min</code> <code>Optional[float]</code> <p>Minimum scaling factor for isotropic scaling. Default: 0.9.</p> <code>0.9</code> <code>scale_max</code> <code>Optional[float]</code> <p>Maximum scaling factor for isotropic scaling. Default: 1.1.</p> <code>1.1</code> <code>translate_width</code> <code>Optional[float]</code> <p>Maximum absolute fraction for horizontal translation. Default: 0.02.</p> <code>0.02</code> <code>translate_height</code> <code>Optional[float]</code> <p>Maximum absolute fraction for vertical translation. Default: 0.02.</p> <code>0.02</code> <code>affine_p</code> <code>float</code> <p>Probability of applying random affine transformations. Default: 0.0.</p> <code>0.0</code> <code>erase_scale_min</code> <code>Optional[float]</code> <p>Minimum value of range of proportion of erased area against input image. Default: 0.0001.</p> <code>0.0001</code> <code>erase_scale_max</code> <code>Optional[float]</code> <p>Maximum value of range of proportion of erased area against input image. Default: 0.01.</p> <code>0.01</code> <code>erase_ratio_min</code> <code>Optional[float]</code> <p>Minimum value of range of aspect ratio of erased area. Default: 1.</p> <code>1</code> <code>erase_ratio_max</code> <code>Optional[float]</code> <p>Maximum value of range of aspect ratio of erased area. Default: 1.</p> <code>1</code> <code>erase_p</code> <code>float</code> <p>Probability of applying random erase. Default: 0.0.</p> <code>0.0</code> <code>mixup_lambda_min</code> <code>Optional[float]</code> <p>Minimum mixup strength value. Default: 0.01.</p> <code>0.01</code> <code>mixup_lambda_max</code> <code>Optional[float]</code> <p>Maximum mixup strength value. Default: 0.05.</p> <code>0.05</code> <code>mixup_p</code> <code>float</code> <p>Probability of applying random mixup v2. Default: 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Returns tuple: (image, instances) with augmentation applied.</p> Source code in <code>sleap_nn/data/augmentation.py</code> <pre><code>def apply_geometric_augmentation(\n    image: torch.Tensor,\n    instances: torch.Tensor,\n    rotation_min: Optional[float] = -15.0,\n    rotation_max: Optional[float] = 15.0,\n    scale_min: Optional[float] = 0.9,\n    scale_max: Optional[float] = 1.1,\n    translate_width: Optional[float] = 0.02,\n    translate_height: Optional[float] = 0.02,\n    affine_p: float = 0.0,\n    erase_scale_min: Optional[float] = 0.0001,\n    erase_scale_max: Optional[float] = 0.01,\n    erase_ratio_min: Optional[float] = 1,\n    erase_ratio_max: Optional[float] = 1,\n    erase_p: float = 0.0,\n    mixup_lambda_min: Optional[float] = 0.01,\n    mixup_lambda_max: Optional[float] = 0.05,\n    mixup_p: float = 0.0,\n) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Apply kornia geometric augmentation on image and instances.\n\n    Args:\n        image: Input image. Shape: (n_samples, C, H, W)\n        instances: Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)\n        rotation_min: Minimum rotation angle in degrees. Default: -15.0.\n        rotation_max: Maximum rotation angle in degrees. Default: 15.0.\n        scale_min: Minimum scaling factor for isotropic scaling. Default: 0.9.\n        scale_max: Maximum scaling factor for isotropic scaling. Default: 1.1.\n        translate_width: Maximum absolute fraction for horizontal translation. Default: 0.02.\n        translate_height: Maximum absolute fraction for vertical translation. Default: 0.02.\n        affine_p: Probability of applying random affine transformations. Default: 0.0.\n        erase_scale_min: Minimum value of range of proportion of erased area against input image. Default: 0.0001.\n        erase_scale_max: Maximum value of range of proportion of erased area against input image. Default: 0.01.\n        erase_ratio_min: Minimum value of range of aspect ratio of erased area. Default: 1.\n        erase_ratio_max: Maximum value of range of aspect ratio of erased area. Default: 1.\n        erase_p: Probability of applying random erase. Default: 0.0.\n        mixup_lambda_min: Minimum mixup strength value. Default: 0.01.\n        mixup_lambda_max: Maximum mixup strength value. Default: 0.05.\n        mixup_p: Probability of applying random mixup v2. Default: 0.0.\n\n    Returns:\n        Returns tuple: (image, instances) with augmentation applied.\n    \"\"\"\n    aug_stack = []\n    if affine_p &gt; 0:\n        aug_stack.append(\n            K.augmentation.RandomAffine(\n                degrees=(rotation_min, rotation_max),\n                translate=(translate_width, translate_height),\n                scale=(scale_min, scale_max),\n                p=affine_p,\n                keepdim=True,\n                same_on_batch=True,\n            )\n        )\n\n    if erase_p &gt; 0:\n        aug_stack.append(\n            K.augmentation.RandomErasing(\n                scale=(erase_scale_min, erase_scale_max),\n                ratio=(erase_ratio_min, erase_ratio_max),\n                p=erase_p,\n                keepdim=True,\n                same_on_batch=True,\n            )\n        )\n    if mixup_p &gt; 0:\n        aug_stack.append(\n            K.augmentation.RandomMixUpV2(\n                lambda_val=(mixup_lambda_min, mixup_lambda_max),\n                p=mixup_p,\n                keepdim=True,\n                same_on_batch=True,\n            )\n        )\n\n    augmenter = AugmentationSequential(\n        *aug_stack,\n        data_keys=[\"input\", \"keypoints\"],\n        keepdim=True,\n        same_on_batch=True,\n    )\n\n    inst_shape = instances.shape\n    # Before (full image): (n_samples, C, H, W), (n_samples, n_instances, n_nodes, 2)\n    # or\n    # Before (cropped image): (B=1, C, crop_H, crop_W), (n_samples, n_nodes, 2)\n    instances = instances.reshape(inst_shape[0], -1, 2)\n    # (n_samples, C, H, W), (n_samples, n_instances * n_nodes, 2) OR (n_samples, n_nodes, 2)\n\n    aug_image, aug_instances = augmenter(image, instances)\n\n    # After (full image): (n_samples, C, H, W), (n_samples, n_instances, n_nodes, 2)\n    # or\n    # After (cropped image): (n_samples, C, crop_H, crop_W), (n_samples, n_nodes, 2)\n    return aug_image, aug_instances.reshape(*inst_shape)\n</code></pre>"},{"location":"api/data/augmentation/#sleap_nn.data.augmentation.apply_intensity_augmentation","title":"<code>apply_intensity_augmentation(image, instances, uniform_noise_min=0.0, uniform_noise_max=0.04, uniform_noise_p=0.0, gaussian_noise_mean=0.02, gaussian_noise_std=0.004, gaussian_noise_p=0.0, contrast_min=0.5, contrast_max=2.0, contrast_p=0.0, brightness_min=1.0, brightness_max=1.0, brightness_p=0.0)</code>","text":"<p>Apply kornia intensity augmentation on image and instances.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image. Shape: (n_samples, C, H, W)</p> required <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)</p> required <code>uniform_noise_min</code> <code>Optional[float]</code> <p>Minimum value for uniform noise (uniform_noise_min &gt;=0).</p> <code>0.0</code> <code>uniform_noise_max</code> <code>Optional[float]</code> <p>Maximum value for uniform noise (uniform_noise_max &lt;=1).</p> <code>0.04</code> <code>uniform_noise_p</code> <code>float</code> <p>Probability of applying random uniform noise.</p> <code>0.0</code> <code>gaussian_noise_mean</code> <code>Optional[float]</code> <p>The mean of the gaussian distribution.</p> <code>0.02</code> <code>gaussian_noise_std</code> <code>Optional[float]</code> <p>The standard deviation of the gaussian distribution.</p> <code>0.004</code> <code>gaussian_noise_p</code> <code>float</code> <p>Probability of applying random gaussian noise.</p> <code>0.0</code> <code>contrast_min</code> <code>Optional[float]</code> <p>Minimum contrast factor to apply. Default: 0.5.</p> <code>0.5</code> <code>contrast_max</code> <code>Optional[float]</code> <p>Maximum contrast factor to apply. Default: 2.0.</p> <code>2.0</code> <code>contrast_p</code> <code>float</code> <p>Probability of applying random contrast.</p> <code>0.0</code> <code>brightness_min</code> <code>Optional[float]</code> <p>Minimum brightness factor to apply. Default: 1.0.</p> <code>1.0</code> <code>brightness_max</code> <code>Optional[float]</code> <p>Maximum brightness factor to apply. Default: 1.0.</p> <code>1.0</code> <code>brightness_p</code> <code>float</code> <p>Probability of applying random brightness.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Returns tuple: (image, instances) with augmentation applied.</p> Source code in <code>sleap_nn/data/augmentation.py</code> <pre><code>def apply_intensity_augmentation(\n    image: torch.Tensor,\n    instances: torch.Tensor,\n    uniform_noise_min: Optional[float] = 0.0,\n    uniform_noise_max: Optional[float] = 0.04,\n    uniform_noise_p: float = 0.0,\n    gaussian_noise_mean: Optional[float] = 0.02,\n    gaussian_noise_std: Optional[float] = 0.004,\n    gaussian_noise_p: float = 0.0,\n    contrast_min: Optional[float] = 0.5,\n    contrast_max: Optional[float] = 2.0,\n    contrast_p: float = 0.0,\n    brightness_min: Optional[float] = 1.0,\n    brightness_max: Optional[float] = 1.0,\n    brightness_p: float = 0.0,\n) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Apply kornia intensity augmentation on image and instances.\n\n    Args:\n        image: Input image. Shape: (n_samples, C, H, W)\n        instances: Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)\n        uniform_noise_min: Minimum value for uniform noise (uniform_noise_min &gt;=0).\n        uniform_noise_max: Maximum value for uniform noise (uniform_noise_max &lt;=1).\n        uniform_noise_p: Probability of applying random uniform noise.\n        gaussian_noise_mean: The mean of the gaussian distribution.\n        gaussian_noise_std: The standard deviation of the gaussian distribution.\n        gaussian_noise_p: Probability of applying random gaussian noise.\n        contrast_min: Minimum contrast factor to apply. Default: 0.5.\n        contrast_max: Maximum contrast factor to apply. Default: 2.0.\n        contrast_p: Probability of applying random contrast.\n        brightness_min: Minimum brightness factor to apply. Default: 1.0.\n        brightness_max: Maximum brightness factor to apply. Default: 1.0.\n        brightness_p: Probability of applying random brightness.\n\n    Returns:\n        Returns tuple: (image, instances) with augmentation applied.\n    \"\"\"\n    aug_stack = []\n    if uniform_noise_p &gt; 0:\n        aug_stack.append(\n            RandomUniformNoise(\n                noise=(uniform_noise_min, uniform_noise_max),\n                p=uniform_noise_p,\n                keepdim=True,\n                same_on_batch=True,\n            )\n        )\n    if gaussian_noise_p &gt; 0:\n        aug_stack.append(\n            K.augmentation.RandomGaussianNoise(\n                mean=gaussian_noise_mean,\n                std=gaussian_noise_std,\n                p=gaussian_noise_p,\n                keepdim=True,\n                same_on_batch=True,\n            )\n        )\n    if contrast_p &gt; 0:\n        aug_stack.append(\n            K.augmentation.RandomContrast(\n                contrast=(contrast_min, contrast_max),\n                p=contrast_p,\n                keepdim=True,\n                same_on_batch=True,\n            )\n        )\n    if brightness_p &gt; 0:\n        aug_stack.append(\n            K.augmentation.RandomBrightness(\n                brightness=(brightness_min, brightness_max),\n                p=brightness_p,\n                keepdim=True,\n                same_on_batch=True,\n            )\n        )\n\n    augmenter = AugmentationSequential(\n        *aug_stack,\n        data_keys=[\"input\", \"keypoints\"],\n        keepdim=True,\n        same_on_batch=True,\n    )\n\n    inst_shape = instances.shape\n    # Before (full image): (n_samples, C, H, W), (n_samples, n_instances, n_nodes, 2)\n    # or\n    # Before (cropped image): (B=1, C, crop_H, crop_W), (n_samples, n_nodes, 2)\n    instances = instances.reshape(inst_shape[0], -1, 2)\n    # (n_samples, C, H, W), (n_samples, n_instances * n_nodes, 2) OR (n_samples, n_nodes, 2)\n\n    aug_image, aug_instances = augmenter(image, instances)\n\n    # After (full image): (n_samples, C, H, W), (n_samples, n_instances, n_nodes, 2)\n    # or\n    # After (cropped image): (n_samples, C, crop_H, crop_W), (n_samples, n_nodes, 2)\n    return aug_image, aug_instances.reshape(*inst_shape)\n</code></pre>"},{"location":"api/data/confidence_maps/","title":"confidence_maps","text":""},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps","title":"<code>sleap_nn.data.confidence_maps</code>","text":"<p>Generate confidence maps.</p> <p>Functions:</p> Name Description <code>generate_confmaps</code> <p>Generate Confidence maps.</p> <code>generate_multiconfmaps</code> <p>Generate multi-instance confidence maps.</p> <code>make_confmaps</code> <p>Make confidence maps from a batch of points for multiple instances.</p> <code>make_multi_confmaps</code> <p>Make confidence maps for multiple instances through reduction.</p>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.generate_confmaps","title":"<code>generate_confmaps(instance, img_hw, sigma=1.5, output_stride=2)</code>","text":"<p>Generate Confidence maps.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2).</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <code>int</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps for the input keypoints.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def generate_confmaps(\n    instance: torch.Tensor,\n    img_hw: Tuple[int],\n    sigma: float = 1.5,\n    output_stride: int = 2,\n) -&gt; torch.Tensor:\n    \"\"\"Generate Confidence maps.\n\n    Args:\n        instance: Input keypoints. (n_samples, n_instances, n_nodes, 2) or\n            (n_samples, n_nodes, 2).\n        img_hw: Image size as tuple (height, width).\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n\n    Returns:\n        Confidence maps for the input keypoints.\n    \"\"\"\n    if instance.ndim != 3:\n        instance = instance.view(instance.shape[0], -1, 2)\n        # instances: (n_samples, n_nodes, 2)\n\n    height, width = img_hw\n\n    xv, yv = make_grid_vectors(height, width, output_stride)\n\n    confidence_maps = make_confmaps(\n        instance,\n        xv,\n        yv,\n        sigma * output_stride,\n    )  # (n_samples, n_nodes, height/ output_stride, width/ output_stride)\n\n    return confidence_maps\n</code></pre>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.generate_multiconfmaps","title":"<code>generate_multiconfmaps(instances, img_hw, num_instances, sigma=1.5, output_stride=2, is_centroids=False)</code>","text":"<p>Generate multi-instance confidence maps.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or for centroids - (n_samples, n_instances, 2)</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>num_instances</code> <code>int</code> <p>Original number of instances in the frame.</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <code>int</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <code>is_centroids</code> <code>bool</code> <p>True if confidence maps should be generates for centroids else False. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps for the input keypoints.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def generate_multiconfmaps(\n    instances: torch.Tensor,\n    img_hw: Tuple[int],\n    num_instances: int,\n    sigma: float = 1.5,\n    output_stride: int = 2,\n    is_centroids: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate multi-instance confidence maps.\n\n    Args:\n        instances: Input keypoints. (n_samples, n_instances, n_nodes, 2) or\n            for centroids - (n_samples, n_instances, 2)\n        img_hw: Image size as tuple (height, width).\n        num_instances: Original number of instances in the frame.\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n        is_centroids: True if confidence maps should be generates for centroids else False.\n            Default: False.\n\n    Returns:\n        Confidence maps for the input keypoints.\n    \"\"\"\n    if is_centroids:\n        points = instances[:, :num_instances, :].unsqueeze(dim=-2)\n        # (n_samples, n_instances, 1, 2)\n    else:\n        points = instances[\n            :, :num_instances, :, :\n        ]  # (n_samples, n_instances, n_nodes, 2)\n\n    height, width = img_hw\n\n    xv, yv = make_grid_vectors(height, width, output_stride)\n\n    confidence_maps = make_multi_confmaps(\n        points,\n        xv,\n        yv,\n        sigma * output_stride,\n    )  # (n_samples, n_nodes, height/ output_stride, width/ output_stride).\n    # If `is_centroids`, (n_samples, 1, height/ output_stride, width/ output_stride).\n\n    return confidence_maps\n</code></pre>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.make_confmaps","title":"<code>make_confmaps(points_batch, xv, yv, sigma)</code>","text":"<p>Make confidence maps from a batch of points for multiple instances.</p> <p>Parameters:</p> Name Type Description Default <code>points_batch</code> <code>Tensor</code> <p>A tensor of points of shape <code>(n_samples, n_nodes, 2)</code> and dtype <code>torch.float32</code> where the last axis corresponds to (x, y) pixel coordinates on the image for each instance. These can contain NaNs to indicate missing points.</p> required <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape <code>(grid_width,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape <code>(grid_height,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate confidence maps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps as a tensor of shape <code>(n_samples, n_nodes, grid_height, grid_width)</code> of dtype <code>torch.float32</code>.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def make_confmaps(\n    points_batch: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor, sigma: float\n) -&gt; torch.Tensor:\n    \"\"\"Make confidence maps from a batch of points for multiple instances.\n\n    Args:\n        points_batch: A tensor of points of shape `(n_samples, n_nodes, 2)` and dtype `torch.float32` where\n            the last axis corresponds to (x, y) pixel coordinates on the image for each instance.\n            These can contain NaNs to indicate missing points.\n        xv: Sampling grid vector for x-coordinates of shape `(grid_width,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape `(grid_height,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            confidence maps.\n\n    Returns:\n        Confidence maps as a tensor of shape `(n_samples, n_nodes, grid_height, grid_width)` of\n        dtype `torch.float32`.\n    \"\"\"\n    samples, n_nodes, _ = points_batch.shape\n\n    x = torch.reshape(points_batch[:, :, 0], (samples, n_nodes, 1, 1))\n    y = torch.reshape(points_batch[:, :, 1], (samples, n_nodes, 1, 1))\n\n    xv_reshaped = torch.reshape(xv, (1, 1, 1, -1))\n    yv_reshaped = torch.reshape(yv, (1, 1, -1, 1))\n\n    cm = torch.exp(-((xv_reshaped - x) ** 2 + (yv_reshaped - y) ** 2) / (2 * sigma**2))\n\n    # Replace NaNs with 0.\n    cm = torch.nan_to_num(cm)\n\n    return cm\n</code></pre>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.make_multi_confmaps","title":"<code>make_multi_confmaps(points_batch, xv, yv, sigma)</code>","text":"<p>Make confidence maps for multiple instances through reduction.</p> <p>Parameters:</p> Name Type Description Default <code>points_batch</code> <code>Tensor</code> <p>A tensor of shape <code>(n_samples, n_instances, n_nodes, 2)</code> and dtype <code>tf.float32</code> containing instance points where the last axis corresponds to (x, y) pixel coordinates on the image. This must be rank-3 even if a single instance is present.</p> required <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape <code>(grid_width,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape <code>(grid_height,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate confidence maps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps as a tensor of shape <code>(n_samples, n_nodes, grid_height, grid_width)</code> of dtype <code>torch.float32</code>.</p> <p>Each channel will contain the elementwise maximum of the confidence maps generated from all individual points for the associated node.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def make_multi_confmaps(\n    points_batch: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor, sigma: float\n) -&gt; torch.Tensor:\n    \"\"\"Make confidence maps for multiple instances through reduction.\n\n    Args:\n        points_batch: A tensor of shape `(n_samples, n_instances, n_nodes, 2)`\n            and dtype `tf.float32` containing instance points where the last axis\n            corresponds to (x, y) pixel coordinates on the image. This must be rank-3\n            even if a single instance is present.\n        xv: Sampling grid vector for x-coordinates of shape `(grid_width,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape `(grid_height,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            confidence maps.\n\n    Returns:\n        Confidence maps as a tensor of shape `(n_samples, n_nodes, grid_height, grid_width)` of\n        dtype `torch.float32`.\n\n        Each channel will contain the elementwise maximum of the confidence maps\n        generated from all individual points for the associated node.\n\n    \"\"\"\n    samples, n_inst, n_nodes, _ = points_batch.shape\n    w, h = xv.shape[0], yv.shape[0]\n    cms = torch.zeros((samples, n_nodes, h, w), dtype=torch.float32)\n    points = points_batch.reshape(samples * n_inst, n_nodes, 2)\n    for p in points:\n        cm_instance = make_confmaps(p.unsqueeze(dim=0), xv, yv, sigma)\n        cms = torch.maximum(cms, cm_instance)\n    return cms\n</code></pre>"},{"location":"api/data/custom_datasets/","title":"custom_datasets","text":""},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets","title":"<code>sleap_nn.data.custom_datasets</code>","text":"<p>Custom <code>torch.utils.data.Dataset</code>s for different model types.</p> <p>Classes:</p> Name Description <code>BaseDataset</code> <p>Base class for custom torch Datasets.</p> <code>BottomUpDataset</code> <p>Dataset class for bottom-up models.</p> <code>BottomUpMultiClassDataset</code> <p>Dataset class for bottom-up ID models.</p> <code>CenteredInstanceDataset</code> <p>Dataset class for instance-centered confidence map models.</p> <code>CentroidDataset</code> <p>Dataset class for centroid models.</p> <code>InfiniteDataLoader</code> <p>Dataloader that reuses workers for infinite iteration.</p> <code>SingleInstanceDataset</code> <p>Dataset class for single-instance models.</p> <code>TopDownCenteredInstanceMultiClassDataset</code> <p>Dataset class for instance-centered confidence map ID models.</p> <p>Functions:</p> Name Description <code>get_steps_per_epoch</code> <p>Compute the number of steps (iterations) per epoch for the given dataset.</p> <code>get_train_val_dataloaders</code> <p>Return the train and val dataloaders.</p> <code>get_train_val_datasets</code> <p>Return the train and val datasets.</p>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for custom torch Datasets.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>Source <code>sio.Labels</code> object.</p> <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Returns the sample dict for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> <code>__iter__</code> <p>Returns an iterator.</p> <code>__len__</code> <p>Return the number of samples in the dataset.</p> <code>__next__</code> <p>Get the next sample from the dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class BaseDataset(Dataset):\n    \"\"\"Base class for custom torch Datasets.\n\n    Attributes:\n        labels: Source `sio.Labels` object.\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        max_stride: int,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__()\n        self.labels = labels\n        self.user_instances_only = user_instances_only\n        self.ensure_rgb = ensure_rgb\n        self.ensure_grayscale = ensure_grayscale\n\n        # Handle intensity augmentation\n        if intensity_aug is not None:\n            if not isinstance(intensity_aug, DictConfig):\n                intensity_aug = get_aug_config(intensity_aug=intensity_aug)\n                config = OmegaConf.structured(intensity_aug)\n                OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n                intensity_aug = DictConfig(config.intensity)\n        self.intensity_aug = intensity_aug\n\n        # Handle geometric augmentation\n        if geometric_aug is not None:\n            if not isinstance(geometric_aug, DictConfig):\n                geometric_aug = get_aug_config(geometric_aug=geometric_aug)\n                config = OmegaConf.structured(geometric_aug)\n                OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n                geometric_aug = DictConfig(config.geometric)\n        self.geometric_aug = geometric_aug\n        self.curr_idx = 0\n        self.max_stride = max_stride\n        self.scale = scale\n        self.apply_aug = apply_aug\n        self.max_hw = max_hw\n        self.rank = rank\n        self.max_instances = 0\n        for x in self.labels:\n            max_instances = get_max_instances(x) if x else None\n\n            if max_instances &gt; self.max_instances:\n                self.max_instances = max_instances\n\n        self.lf_idx_list = self._get_lf_idx_list()\n        self.cache_img = cache_img\n        self.cache_img_path = cache_img_path\n        self.use_existing_imgs = use_existing_imgs\n        if self.cache_img is not None and \"disk\" in self.cache_img:\n            if self.cache_img_path is None:\n                self.cache_img_path = \".\"\n            path = (\n                Path(self.cache_img_path)\n                if isinstance(self.cache_img_path, str)\n                else self.cache_img_path\n            )\n            if not path.is_dir():\n                path.mkdir(parents=True, exist_ok=True)\n\n        self.transform_to_pil = T.ToPILImage()\n        self.transform_pil_to_tensor = T.ToTensor()\n        self.cache = {}\n\n        if self.cache_img is not None:\n            if self.cache_img == \"memory\":\n                self._fill_cache()\n            elif self.cache_img == \"disk\" and not self.use_existing_imgs:\n                if self.rank is None or self.rank == -1 or self.rank == 0:\n                    self._fill_cache()\n                # Synchronize all ranks after cache creation\n                if is_distributed_initialized():\n                    dist.barrier()\n\n    def _get_lf_idx_list(self) -&gt; List[Tuple[int]]:\n        \"\"\"Return list of indices of labelled frames.\"\"\"\n        lf_idx_list = []\n        for labels_idx, label in enumerate(self.labels):\n            for lf_idx, lf in enumerate(label):\n                # Filter to user instances\n                if self.user_instances_only:\n                    if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                        lf.instances = lf.user_instances\n                is_empty = True\n                for _, inst in enumerate(lf.instances):\n                    if not inst.is_empty:  # filter all NaN instances.\n                        is_empty = False\n                if not is_empty:\n                    lf_idx_list.append((labels_idx, lf_idx))\n        return lf_idx_list\n\n    def __next__(self):\n        \"\"\"Get the next sample from the dataset.\"\"\"\n        if self.curr_idx &gt;= len(self):\n            raise StopIteration\n\n        sample = self.__getitem__(self.curr_idx)\n        self.curr_idx += 1\n        return sample\n\n    def __iter__(self):\n        \"\"\"Returns an iterator.\"\"\"\n        return self\n\n    def _fill_cache(self):\n        \"\"\"Load all samples to cache.\"\"\"\n\n        def process_sample(args):\n            labels_idx, lf_idx = args\n            img = self.labels[labels_idx][lf_idx].image\n            if img.shape[-1] == 1:\n                img = np.squeeze(img)\n\n            if self.cache_img == \"disk\":\n                f_name = f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                Image.fromarray(img).save(f_name, format=\"JPEG\")\n                return (\n                    labels_idx,\n                    lf_idx,\n                ), None  # Return key and None for disk cache\n\n            if self.cache_img == \"memory\":\n                return (\n                    labels_idx,\n                    lf_idx,\n                ), img  # Return key and image for memory cache\n\n        # Use ThreadPoolExecutor for I/O-bound operations\n        max_workers = min(len(self.lf_idx_list), (os.cpu_count() or 4) * 4)\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all tasks\n            future_to_sample = {\n                executor.submit(process_sample, (labels_idx, lf_idx)): (\n                    labels_idx,\n                    lf_idx,\n                )\n                for labels_idx, lf_idx in self.lf_idx_list\n            }\n\n            # Collect results\n            for future in concurrent.futures.as_completed(future_to_sample):\n                result = future.result()\n                if result is not None:\n                    key, img = result\n                    if img is not None:  # Memory cache\n                        self.cache[key] = img\n\n        # Close videos after all processing is done\n        for label in self.labels:\n            for video in label.videos:\n                if video.is_open:\n                    video.close()\n\n    def _get_video_idx(self, lf, labels_idx):\n        \"\"\"Return indsample of `lf.video` in `labels.videos`.\"\"\"\n        return self.labels[labels_idx].videos.index(lf.video)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples in the dataset.\"\"\"\n        return len(self.lf_idx_list)\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Returns the sample dict for given index.\"\"\"\n        message = \"Subclasses must implement __getitem__\"\n        logger.error(message)\n        raise NotImplementedError(message)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns the sample dict for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Returns the sample dict for given index.\"\"\"\n    message = \"Subclasses must implement __getitem__\"\n    logger.error(message)\n    raise NotImplementedError(message)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__init__","title":"<code>__init__(labels, max_stride, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    max_stride: int,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__()\n    self.labels = labels\n    self.user_instances_only = user_instances_only\n    self.ensure_rgb = ensure_rgb\n    self.ensure_grayscale = ensure_grayscale\n\n    # Handle intensity augmentation\n    if intensity_aug is not None:\n        if not isinstance(intensity_aug, DictConfig):\n            intensity_aug = get_aug_config(intensity_aug=intensity_aug)\n            config = OmegaConf.structured(intensity_aug)\n            OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n            intensity_aug = DictConfig(config.intensity)\n    self.intensity_aug = intensity_aug\n\n    # Handle geometric augmentation\n    if geometric_aug is not None:\n        if not isinstance(geometric_aug, DictConfig):\n            geometric_aug = get_aug_config(geometric_aug=geometric_aug)\n            config = OmegaConf.structured(geometric_aug)\n            OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n            geometric_aug = DictConfig(config.geometric)\n    self.geometric_aug = geometric_aug\n    self.curr_idx = 0\n    self.max_stride = max_stride\n    self.scale = scale\n    self.apply_aug = apply_aug\n    self.max_hw = max_hw\n    self.rank = rank\n    self.max_instances = 0\n    for x in self.labels:\n        max_instances = get_max_instances(x) if x else None\n\n        if max_instances &gt; self.max_instances:\n            self.max_instances = max_instances\n\n    self.lf_idx_list = self._get_lf_idx_list()\n    self.cache_img = cache_img\n    self.cache_img_path = cache_img_path\n    self.use_existing_imgs = use_existing_imgs\n    if self.cache_img is not None and \"disk\" in self.cache_img:\n        if self.cache_img_path is None:\n            self.cache_img_path = \".\"\n        path = (\n            Path(self.cache_img_path)\n            if isinstance(self.cache_img_path, str)\n            else self.cache_img_path\n        )\n        if not path.is_dir():\n            path.mkdir(parents=True, exist_ok=True)\n\n    self.transform_to_pil = T.ToPILImage()\n    self.transform_pil_to_tensor = T.ToTensor()\n    self.cache = {}\n\n    if self.cache_img is not None:\n        if self.cache_img == \"memory\":\n            self._fill_cache()\n        elif self.cache_img == \"disk\" and not self.use_existing_imgs:\n            if self.rank is None or self.rank == -1 or self.rank == 0:\n                self._fill_cache()\n            # Synchronize all ranks after cache creation\n            if is_distributed_initialized():\n                dist.barrier()\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Returns an iterator.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Returns an iterator.\"\"\"\n    return self\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples in the dataset.\"\"\"\n    return len(self.lf_idx_list)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__next__","title":"<code>__next__()</code>","text":"<p>Get the next sample from the dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __next__(self):\n    \"\"\"Get the next sample from the dataset.\"\"\"\n    if self.curr_idx &gt;= len(self):\n        raise StopIteration\n\n    sample = self.__getitem__(self.curr_idx)\n    self.curr_idx += 1\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpDataset","title":"<code>BottomUpDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for bottom-up models.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>Source <code>sio.Labels</code> object.</p> <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section. (required keys: <code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ).</p> <code>pafs_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section (required keys: <code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ) for PAFs.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image, confmaps and pafs for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class BottomUpDataset(BaseDataset):\n    \"\"\"Dataset class for bottom-up models.\n\n    Attributes:\n        labels: Source `sio.Labels` object.\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n            (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type ).\n        pafs_head_config: DictConfig object with all the keys in the `head_config` section\n            (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type )\n            for PAFs.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        pafs_head_config: DictConfig,\n        max_stride: int,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        self.confmap_head_config = confmap_head_config\n        self.pafs_head_config = pafs_head_config\n\n        self.edge_inds = self.labels[0].skeletons[0].edge_inds\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n        (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n        lf = self.labels[labels_idx][lf_idx]\n\n        # load the img\n        if self.cache_img is not None:\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n\n        else:  # load from slp file if not cached\n            img = lf.image\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        video_idx = self._get_video_idx(lf, labels_idx)\n\n        # get dict\n        sample = process_lf(\n            lf,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        # apply normalization\n        sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_multiconfmaps(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n            is_centroids=False,\n        )\n\n        # pafs\n        pafs = generate_pafs(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            sigma=self.pafs_head_config.sigma,\n            output_stride=self.pafs_head_config.output_stride,\n            edge_inds=torch.Tensor(self.edge_inds),\n            flatten_channels=True,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"part_affinity_fields\"] = pafs\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image, confmaps and pafs for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n    (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n    lf = self.labels[labels_idx][lf_idx]\n\n    # load the img\n    if self.cache_img is not None:\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n\n    else:  # load from slp file if not cached\n        img = lf.image\n\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    video_idx = self._get_video_idx(lf, labels_idx)\n\n    # get dict\n    sample = process_lf(\n        lf,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    # apply normalization\n    sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_multiconfmaps(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n        is_centroids=False,\n    )\n\n    # pafs\n    pafs = generate_pafs(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        sigma=self.pafs_head_config.sigma,\n        output_stride=self.pafs_head_config.output_stride,\n        edge_inds=torch.Tensor(self.edge_inds),\n        flatten_channels=True,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"part_affinity_fields\"] = pafs\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpDataset.__init__","title":"<code>__init__(labels, confmap_head_config, pafs_head_config, max_stride, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    pafs_head_config: DictConfig,\n    max_stride: int,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n    )\n    self.confmap_head_config = confmap_head_config\n    self.pafs_head_config = pafs_head_config\n\n    self.edge_inds = self.labels[0].skeletons[0].edge_inds\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpMultiClassDataset","title":"<code>BottomUpMultiClassDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for bottom-up ID models.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>Source <code>sio.Labels</code> object.</p> <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>class_map_threshold</code> <p>Minimum confidence map value below which map values will be replaced with zeros.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section. (required keys: <code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ).</p> <code>class_maps_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section (required keys: <code>sigma</code>, <code>output_stride</code> and <code>classes</code>) for class maps.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image, confmaps and pafs for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class BottomUpMultiClassDataset(BaseDataset):\n    \"\"\"Dataset class for bottom-up ID models.\n\n    Attributes:\n        labels: Source `sio.Labels` object.\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        class_map_threshold: Minimum confidence map value below which map values will be\n            replaced with zeros.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n            (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type ).\n        class_maps_head_config: DictConfig object with all the keys in the `head_config` section\n            (required keys: `sigma`, `output_stride` and `classes`)\n            for class maps.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        class_maps_head_config: DictConfig,\n        max_stride: int,\n        class_map_threshold: float = 0.2,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        self.confmap_head_config = confmap_head_config\n        self.class_maps_head_config = class_maps_head_config\n        self.tracks = []\n        for train_label in self.labels:\n            self.tracks.extend([x.name for x in train_label.tracks if x is not None])\n        self.tracks = list(set(self.tracks))\n        self.class_map_threshold = class_map_threshold\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n        (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n        lf = self.labels[labels_idx][lf_idx]\n\n        # load the img\n        if self.cache_img is not None:\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n\n        else:  # load from slp file if not cached\n            img = lf.image\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        video_idx = self._get_video_idx(lf, labels_idx)\n\n        # get dict\n        sample = process_lf(\n            lf,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        sample[\"track_ids\"] = torch.Tensor(\n            [\n                (\n                    self.tracks.index(lf.instances[idx].track.name)\n                    if lf.instances[idx].track is not None\n                    else -1\n                )\n                for idx in range(sample[\"num_instances\"])\n            ]\n        ).to(torch.int32)\n\n        sample[\"num_tracks\"] = torch.tensor(len(self.tracks), dtype=torch.int32)\n\n        # apply normalization\n        sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_multiconfmaps(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n            is_centroids=False,\n        )\n\n        # class maps\n        class_maps = generate_class_maps(\n            instances=sample[\"instances\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            class_inds=sample[\"track_ids\"],\n            num_tracks=sample[\"num_tracks\"],\n            class_map_threshold=self.class_map_threshold,\n            sigma=self.class_maps_head_config.sigma,\n            output_stride=self.class_maps_head_config.output_stride,\n            is_centroids=False,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"class_maps\"] = class_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpMultiClassDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image, confmaps and pafs for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n    (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n    lf = self.labels[labels_idx][lf_idx]\n\n    # load the img\n    if self.cache_img is not None:\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n\n    else:  # load from slp file if not cached\n        img = lf.image\n\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    video_idx = self._get_video_idx(lf, labels_idx)\n\n    # get dict\n    sample = process_lf(\n        lf,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    sample[\"track_ids\"] = torch.Tensor(\n        [\n            (\n                self.tracks.index(lf.instances[idx].track.name)\n                if lf.instances[idx].track is not None\n                else -1\n            )\n            for idx in range(sample[\"num_instances\"])\n        ]\n    ).to(torch.int32)\n\n    sample[\"num_tracks\"] = torch.tensor(len(self.tracks), dtype=torch.int32)\n\n    # apply normalization\n    sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_multiconfmaps(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n        is_centroids=False,\n    )\n\n    # class maps\n    class_maps = generate_class_maps(\n        instances=sample[\"instances\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        class_inds=sample[\"track_ids\"],\n        num_tracks=sample[\"num_tracks\"],\n        class_map_threshold=self.class_map_threshold,\n        sigma=self.class_maps_head_config.sigma,\n        output_stride=self.class_maps_head_config.output_stride,\n        is_centroids=False,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"class_maps\"] = class_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpMultiClassDataset.__init__","title":"<code>__init__(labels, confmap_head_config, class_maps_head_config, max_stride, class_map_threshold=0.2, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    class_maps_head_config: DictConfig,\n    max_stride: int,\n    class_map_threshold: float = 0.2,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n    )\n    self.confmap_head_config = confmap_head_config\n    self.class_maps_head_config = class_maps_head_config\n    self.tracks = []\n    for train_label in self.labels:\n        self.tracks.extend([x.name for x in train_label.tracks if x is not None])\n    self.tracks = list(set(self.tracks))\n    self.class_map_threshold = class_map_threshold\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset","title":"<code>CenteredInstanceDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for instance-centered confidence map models.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>Source <code>sio.Labels</code> object.</p> <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>anchor_ind</code> <p>Index of the node to use as the anchor point, based on its index in the ordered list of skeleton nodes.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>crop_hw</code> <p>Height and width of the crop in pixels.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <p>Note: If scale is provided for centered-instance model, the images are cropped out from the scaled image with the given crop size.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with cropped image and confmaps of instance for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> <code>__len__</code> <p>Return number of instances in the labels object.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class CenteredInstanceDataset(BaseDataset):\n    \"\"\"Dataset class for instance-centered confidence map models.\n\n    Attributes:\n        labels: Source `sio.Labels` object.\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        anchor_ind: Index of the node to use as the anchor point, based on its index in the\n            ordered list of skeleton nodes.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        crop_hw: Height and width of the crop in pixels.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n\n    Note: If scale is provided for centered-instance model, the images are cropped out\n    from the scaled image with the given crop size.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        crop_hw: Tuple[int],\n        confmap_head_config: DictConfig,\n        max_stride: int,\n        anchor_ind: Optional[int] = None,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        self.crop_hw = crop_hw\n        self.anchor_ind = anchor_ind\n        self.confmap_head_config = confmap_head_config\n        self.instance_idx_list = self._get_instance_idx_list()\n        self.cache_lf = [None, None]\n\n    def _get_instance_idx_list(self) -&gt; List[Tuple[int]]:\n        \"\"\"Return list of tuples with indices of labelled frames and instances.\"\"\"\n        instance_idx_list = []\n        for labels_idx, label in enumerate(self.labels):\n            for lf_idx, lf in enumerate(label):\n                # Filter to user instances\n                if self.user_instances_only:\n                    if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                        lf.instances = lf.user_instances\n                for inst_idx, inst in enumerate(lf.instances):\n                    if not inst.is_empty:  # filter all NaN instances.\n                        instance_idx_list.append((labels_idx, lf_idx, inst_idx))\n        return instance_idx_list\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of instances in the labels object.\"\"\"\n        return len(self.instance_idx_list)\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n        labels_idx, lf_idx, inst_idx = self.instance_idx_list[index]\n        lf = self.labels[labels_idx][lf_idx]\n\n        if lf_idx == self.cache_lf[0]:\n            img = self.cache_lf[1]\n        else:\n            # load the img\n            if self.cache_img is not None:\n                if self.cache_img == \"disk\":\n                    img = np.array(\n                        Image.open(\n                            f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                        )\n                    )\n                elif self.cache_img == \"memory\":\n                    img = self.cache[(labels_idx, lf_idx)].copy()\n\n            else:  # load from slp file if not cached\n                img = lf.image  # TODO: doesn't work when num_workers &gt; 0\n\n            if img.ndim == 2:\n                img = np.expand_dims(img, axis=2)\n\n            self.cache_lf = [lf_idx, img]\n\n        video_idx = self._get_video_idx(lf, labels_idx)\n\n        image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n        instances = []\n        for inst in lf:\n            instances.append(inst.numpy())\n        instances = np.stack(instances, axis=0)\n\n        # Add singleton time dimension for single frames.\n        image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n        instances = np.expand_dims(\n            instances, axis=0\n        )  # (n_samples=1, num_instances, num_nodes, 2)\n\n        instances = torch.from_numpy(instances.astype(\"float32\"))\n        image = torch.from_numpy(image)\n\n        num_instances, _ = instances.shape[1:3]\n        orig_img_height, orig_img_width = image.shape[-2:]\n\n        instances = instances[:, inst_idx]\n\n        # apply normalization\n        image = apply_normalization(image)\n\n        if self.ensure_rgb:\n            image = convert_to_rgb(image)\n        elif self.ensure_grayscale:\n            image = convert_to_grayscale(image)\n\n        # size matcher\n        image, eff_scale = apply_sizematcher(\n            image,\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        instances = instances * eff_scale\n\n        # resize image\n        image, instances = apply_resizer(\n            image,\n            instances,\n            scale=self.scale,\n        )\n\n        # get the centroids based on the anchor idx\n        centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n        instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n        crop_size = np.array(self.crop_hw) * np.sqrt(\n            2\n        )  # crop extra for rotation augmentation\n        crop_size = crop_size.astype(np.int32).tolist()\n\n        sample = generate_crops(image, instance, centroid, crop_size)\n\n        sample[\"frame_idx\"] = torch.tensor(lf.frame_idx, dtype=torch.int32)\n        sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n        sample[\"num_instances\"] = num_instances\n        sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width])\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_intensity_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_geometric_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.geometric_aug,\n                )\n\n        # re-crop to original crop size\n        sample[\"instance_bbox\"] = torch.unsqueeze(\n            make_centered_bboxes(\n                sample[\"centroid\"][0], self.crop_hw[0], self.crop_hw[1]\n            ),\n            0,\n        )  # (n_samples=1, 4, 2)\n\n        sample[\"instance_image\"] = crop_and_resize(\n            sample[\"instance_image\"], boxes=sample[\"instance_bbox\"], size=self.crop_hw\n        )\n        point = sample[\"instance_bbox\"][0][0]\n        center_instance = sample[\"instance\"] - point\n        centered_centroid = sample[\"centroid\"] - point\n\n        sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n        sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n        # Pad the image (if needed) according max stride\n        sample[\"instance_image\"] = apply_pad_to_stride(\n            sample[\"instance_image\"], max_stride=self.max_stride\n        )\n\n        img_hw = sample[\"instance_image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_confmaps(\n            sample[\"instance\"],\n            img_hw=img_hw,\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with cropped image and confmaps of instance for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n    labels_idx, lf_idx, inst_idx = self.instance_idx_list[index]\n    lf = self.labels[labels_idx][lf_idx]\n\n    if lf_idx == self.cache_lf[0]:\n        img = self.cache_lf[1]\n    else:\n        # load the img\n        if self.cache_img is not None:\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n\n        else:  # load from slp file if not cached\n            img = lf.image  # TODO: doesn't work when num_workers &gt; 0\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        self.cache_lf = [lf_idx, img]\n\n    video_idx = self._get_video_idx(lf, labels_idx)\n\n    image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n    instances = []\n    for inst in lf:\n        instances.append(inst.numpy())\n    instances = np.stack(instances, axis=0)\n\n    # Add singleton time dimension for single frames.\n    image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n    instances = np.expand_dims(\n        instances, axis=0\n    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n    instances = torch.from_numpy(instances.astype(\"float32\"))\n    image = torch.from_numpy(image)\n\n    num_instances, _ = instances.shape[1:3]\n    orig_img_height, orig_img_width = image.shape[-2:]\n\n    instances = instances[:, inst_idx]\n\n    # apply normalization\n    image = apply_normalization(image)\n\n    if self.ensure_rgb:\n        image = convert_to_rgb(image)\n    elif self.ensure_grayscale:\n        image = convert_to_grayscale(image)\n\n    # size matcher\n    image, eff_scale = apply_sizematcher(\n        image,\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    instances = instances * eff_scale\n\n    # resize image\n    image, instances = apply_resizer(\n        image,\n        instances,\n        scale=self.scale,\n    )\n\n    # get the centroids based on the anchor idx\n    centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n    instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n    crop_size = np.array(self.crop_hw) * np.sqrt(\n        2\n    )  # crop extra for rotation augmentation\n    crop_size = crop_size.astype(np.int32).tolist()\n\n    sample = generate_crops(image, instance, centroid, crop_size)\n\n    sample[\"frame_idx\"] = torch.tensor(lf.frame_idx, dtype=torch.int32)\n    sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n    sample[\"num_instances\"] = num_instances\n    sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width])\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_intensity_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_geometric_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.geometric_aug,\n            )\n\n    # re-crop to original crop size\n    sample[\"instance_bbox\"] = torch.unsqueeze(\n        make_centered_bboxes(\n            sample[\"centroid\"][0], self.crop_hw[0], self.crop_hw[1]\n        ),\n        0,\n    )  # (n_samples=1, 4, 2)\n\n    sample[\"instance_image\"] = crop_and_resize(\n        sample[\"instance_image\"], boxes=sample[\"instance_bbox\"], size=self.crop_hw\n    )\n    point = sample[\"instance_bbox\"][0][0]\n    center_instance = sample[\"instance\"] - point\n    centered_centroid = sample[\"centroid\"] - point\n\n    sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n    sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n    # Pad the image (if needed) according max stride\n    sample[\"instance_image\"] = apply_pad_to_stride(\n        sample[\"instance_image\"], max_stride=self.max_stride\n    )\n\n    img_hw = sample[\"instance_image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_confmaps(\n        sample[\"instance\"],\n        img_hw=img_hw,\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset.__init__","title":"<code>__init__(labels, crop_hw, confmap_head_config, max_stride, anchor_ind=None, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    crop_hw: Tuple[int],\n    confmap_head_config: DictConfig,\n    max_stride: int,\n    anchor_ind: Optional[int] = None,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n    )\n    self.crop_hw = crop_hw\n    self.anchor_ind = anchor_ind\n    self.confmap_head_config = confmap_head_config\n    self.instance_idx_list = self._get_instance_idx_list()\n    self.cache_lf = [None, None]\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return number of instances in the labels object.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of instances in the labels object.\"\"\"\n    return len(self.instance_idx_list)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CentroidDataset","title":"<code>CentroidDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for centroid models.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>Source <code>sio.Labels</code> object.</p> <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>anchor_ind</code> <p>Index of the node to use as the anchor point, based on its index in the ordered list of skeleton nodes.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section.</p> <code>(required</code> <code>keys</code> <p><code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ).</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image and confmaps for centroids for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class CentroidDataset(BaseDataset):\n    \"\"\"Dataset class for centroid models.\n\n    Attributes:\n        labels: Source `sio.Labels` object.\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        anchor_ind: Index of the node to use as the anchor point, based on its index in the\n            ordered list of skeleton nodes.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n        (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type ).\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        max_stride: int,\n        anchor_ind: Optional[int] = None,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        self.anchor_ind = anchor_ind\n        self.confmap_head_config = confmap_head_config\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image and confmaps for centroids for given index.\"\"\"\n        (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n        lf = self.labels[labels_idx][lf_idx]\n\n        # load the img\n        if self.cache_img is not None:\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n\n        else:  # load from slp file if not cached\n            img = lf.image\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        video_idx = self._get_video_idx(lf, labels_idx)\n\n        # get dict\n        sample = process_lf(\n            lf,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        # apply normalization\n        sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # get the centroids based on the anchor idx\n        centroids = generate_centroids(sample[\"instances\"], anchor_ind=self.anchor_ind)\n\n        sample[\"centroids\"] = centroids\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"centroids\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"centroids\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"centroids\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"centroids\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_multiconfmaps(\n            sample[\"centroids\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n            is_centroids=True,\n        )\n\n        sample[\"centroids_confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CentroidDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image and confmaps for centroids for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image and confmaps for centroids for given index.\"\"\"\n    (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n    lf = self.labels[labels_idx][lf_idx]\n\n    # load the img\n    if self.cache_img is not None:\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n\n    else:  # load from slp file if not cached\n        img = lf.image\n\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    video_idx = self._get_video_idx(lf, labels_idx)\n\n    # get dict\n    sample = process_lf(\n        lf,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    # apply normalization\n    sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # get the centroids based on the anchor idx\n    centroids = generate_centroids(sample[\"instances\"], anchor_ind=self.anchor_ind)\n\n    sample[\"centroids\"] = centroids\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"centroids\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"centroids\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"centroids\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"centroids\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_multiconfmaps(\n        sample[\"centroids\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n        is_centroids=True,\n    )\n\n    sample[\"centroids_confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CentroidDataset.__init__","title":"<code>__init__(labels, confmap_head_config, max_stride, anchor_ind=None, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    max_stride: int,\n    anchor_ind: Optional[int] = None,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n    )\n    self.anchor_ind = anchor_ind\n    self.confmap_head_config = confmap_head_config\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader","title":"<code>InfiniteDataLoader</code>","text":"<p>               Bases: <code>DataLoader</code></p> <p>Dataloader that reuses workers for infinite iteration.</p> <p>This dataloader extends the PyTorch DataLoader to provide infinite recycling of workers, which improves efficiency for training loops that need to iterate through the dataset multiple times without recreating workers.</p> <p>Attributes:</p> Name Type Description <code>batch_sampler</code> <code>_RepeatSampler</code> <p>A sampler that repeats indefinitely.</p> <code>iterator</code> <code>Iterator</code> <p>The iterator from the parent DataLoader.</p> <code>len_dataloader</code> <code>Optional[int]</code> <p>Number of minibatches to be generated. If <code>None</code>, this is set to len(dataset)/batch_size.</p> <p>Methods:</p> Name Description <code>__len__</code> <p>Return the length of the batch sampler's sampler.</p> <code>__iter__</code> <p>Create a sampler that repeats indefinitely.</p> <code>__del__</code> <p>Ensure workers are properly terminated.</p> <code>reset</code> <p>Reset the iterator, useful when modifying dataset settings during training.</p> <p>Examples:</p> <p>Create an infinite dataloader for training</p> <pre><code>&gt;&gt;&gt; dataset = CenteredInstanceDataset(...)\n&gt;&gt;&gt; dataloader = InfiniteDataLoader(dataset, batch_size=16, shuffle=True)\n&gt;&gt;&gt; for batch in dataloader:  # Infinite iteration\n&gt;&gt;&gt;     train_step(batch)\n</code></pre> <p>Source: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/build.py</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class InfiniteDataLoader(DataLoader):\n    \"\"\"Dataloader that reuses workers for infinite iteration.\n\n    This dataloader extends the PyTorch DataLoader to provide infinite recycling of workers, which improves efficiency\n    for training loops that need to iterate through the dataset multiple times without recreating workers.\n\n    Attributes:\n        batch_sampler (_RepeatSampler): A sampler that repeats indefinitely.\n        iterator (Iterator): The iterator from the parent DataLoader.\n        len_dataloader (Optional[int]): Number of minibatches to be generated. If `None`, this is set to len(dataset)/batch_size.\n\n    Methods:\n        __len__: Return the length of the batch sampler's sampler.\n        __iter__: Create a sampler that repeats indefinitely.\n        __del__: Ensure workers are properly terminated.\n        reset: Reset the iterator, useful when modifying dataset settings during training.\n\n    Examples:\n        Create an infinite dataloader for training\n        &gt;&gt;&gt; dataset = CenteredInstanceDataset(...)\n        &gt;&gt;&gt; dataloader = InfiniteDataLoader(dataset, batch_size=16, shuffle=True)\n        &gt;&gt;&gt; for batch in dataloader:  # Infinite iteration\n        &gt;&gt;&gt;     train_step(batch)\n\n    Source: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/build.py\n    \"\"\"\n\n    def __init__(self, len_dataloader: Optional[int] = None, *args: Any, **kwargs: Any):\n        \"\"\"Initialize the InfiniteDataLoader with the same arguments as DataLoader.\"\"\"\n        super().__init__(*args, **kwargs)\n        object.__setattr__(self, \"batch_sampler\", _RepeatSampler(self.batch_sampler))\n        self.iterator = super().__iter__()\n        self.len_dataloader = len_dataloader\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the batch sampler's sampler.\"\"\"\n        # set the len to required number of steps per epoch as Lightning Trainer\n        # doesn't use the `__iter__` directly but instead uses the length to set\n        # the number of steps per epoch. If this is just set to len(sampler), then\n        # it only iterates through the samples in the dataset (and doesn't cycle through)\n        # if the required steps per epoch is more than batches in dataset.\n        return (\n            self.len_dataloader\n            if self.len_dataloader is not None\n            else len(self.batch_sampler.sampler)\n        )\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\n        while True:\n            yield next(self.iterator)\n\n    def __del__(self):\n        \"\"\"Ensure that workers are properly terminated when the dataloader is deleted.\"\"\"\n        try:\n            if not hasattr(self.iterator, \"_workers\"):\n                return\n            for w in self.iterator._workers:  # force terminate\n                if w.is_alive():\n                    w.terminate()\n            self.iterator._shutdown_workers()  # cleanup\n        except Exception:\n            pass\n\n    def reset(self):\n        \"\"\"Reset the iterator to allow modifications to the dataset during training.\"\"\"\n        self.iterator = self._get_iterator()\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__del__","title":"<code>__del__()</code>","text":"<p>Ensure that workers are properly terminated when the dataloader is deleted.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure that workers are properly terminated when the dataloader is deleted.\"\"\"\n    try:\n        if not hasattr(self.iterator, \"_workers\"):\n            return\n        for w in self.iterator._workers:  # force terminate\n            if w.is_alive():\n                w.terminate()\n        self.iterator._shutdown_workers()  # cleanup\n    except Exception:\n        pass\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__init__","title":"<code>__init__(len_dataloader=None, *args, **kwargs)</code>","text":"<p>Initialize the InfiniteDataLoader with the same arguments as DataLoader.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(self, len_dataloader: Optional[int] = None, *args: Any, **kwargs: Any):\n    \"\"\"Initialize the InfiniteDataLoader with the same arguments as DataLoader.\"\"\"\n    super().__init__(*args, **kwargs)\n    object.__setattr__(self, \"batch_sampler\", _RepeatSampler(self.batch_sampler))\n    self.iterator = super().__iter__()\n    self.len_dataloader = len_dataloader\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Create an iterator that yields indefinitely from the underlying iterator.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\n    while True:\n        yield next(self.iterator)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the batch sampler's sampler.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the batch sampler's sampler.\"\"\"\n    # set the len to required number of steps per epoch as Lightning Trainer\n    # doesn't use the `__iter__` directly but instead uses the length to set\n    # the number of steps per epoch. If this is just set to len(sampler), then\n    # it only iterates through the samples in the dataset (and doesn't cycle through)\n    # if the required steps per epoch is more than batches in dataset.\n    return (\n        self.len_dataloader\n        if self.len_dataloader is not None\n        else len(self.batch_sampler.sampler)\n    )\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.reset","title":"<code>reset()</code>","text":"<p>Reset the iterator to allow modifications to the dataset during training.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the iterator to allow modifications to the dataset during training.\"\"\"\n    self.iterator = self._get_iterator()\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.SingleInstanceDataset","title":"<code>SingleInstanceDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for single-instance models.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>Source <code>sio.Labels</code> object.</p> <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section.</p> <code>(required</code> <code>keys</code> <p><code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ).</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image and confmaps for instance for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class SingleInstanceDataset(BaseDataset):\n    \"\"\"Dataset class for single-instance models.\n\n    Attributes:\n        labels: Source `sio.Labels` object.\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n        (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type ).\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        max_stride: int,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        self.confmap_head_config = confmap_head_config\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image and confmaps for instance for given index.\"\"\"\n        (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n        lf = self.labels[labels_idx][lf_idx]\n\n        # load the img\n        if self.cache_img is not None:\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n\n        else:  # load from slp file if not cached\n            img = lf.image\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        video_idx = self._get_video_idx(lf, labels_idx)\n\n        # get dict\n        sample = process_lf(\n            lf,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        # apply normalization\n        sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_confmaps(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.SingleInstanceDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image and confmaps for instance for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image and confmaps for instance for given index.\"\"\"\n    (labels_idx, lf_idx) = self.lf_idx_list[index]\n\n    lf = self.labels[labels_idx][lf_idx]\n\n    # load the img\n    if self.cache_img is not None:\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n\n    else:  # load from slp file if not cached\n        img = lf.image\n\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    video_idx = self._get_video_idx(lf, labels_idx)\n\n    # get dict\n    sample = process_lf(\n        lf,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    # apply normalization\n    sample[\"image\"] = apply_normalization(sample[\"image\"])\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_confmaps(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.SingleInstanceDataset.__init__","title":"<code>__init__(labels, confmap_head_config, max_stride, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    max_stride: int,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n    )\n    self.confmap_head_config = confmap_head_config\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.TopDownCenteredInstanceMultiClassDataset","title":"<code>TopDownCenteredInstanceMultiClassDataset</code>","text":"<p>               Bases: <code>CenteredInstanceDataset</code></p> <p>Dataset class for instance-centered confidence map ID models.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>Source <code>sio.Labels</code> object.</p> <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>anchor_ind</code> <p>Index of the node to use as the anchor point, based on its index in the ordered list of skeleton nodes.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>crop_hw</code> <p>Height and width of the crop in pixels.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <p>Note: If scale is provided for centered-instance model, the images are cropped out from the scaled image with the given crop size.</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with cropped image and confmaps of instance for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class TopDownCenteredInstanceMultiClassDataset(CenteredInstanceDataset):\n    \"\"\"Dataset class for instance-centered confidence map ID models.\n\n    Attributes:\n        labels: Source `sio.Labels` object.\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        anchor_ind: Index of the node to use as the anchor point, based on its index in the\n            ordered list of skeleton nodes.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        crop_hw: Height and width of the crop in pixels.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n\n    Note: If scale is provided for centered-instance model, the images are cropped out\n    from the scaled image with the given crop size.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        crop_hw: Tuple[int],\n        confmap_head_config: DictConfig,\n        max_stride: int,\n        anchor_ind: Optional[int] = None,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            crop_hw=crop_hw,\n            confmap_head_config=confmap_head_config,\n            max_stride=max_stride,\n            anchor_ind=anchor_ind,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        self.tracks = []\n        for train_label in self.labels:\n            self.tracks.extend([x.name for x in train_label.tracks if x is not None])\n        self.tracks = list(set(self.tracks))\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n        labels_idx, lf_idx, inst_idx = self.instance_idx_list[index]\n        lf = self.labels[labels_idx][lf_idx]\n\n        if lf_idx == self.cache_lf[0]:\n            img = self.cache_lf[1]\n        else:\n            # load the img\n            if self.cache_img is not None:\n                if self.cache_img == \"disk\":\n                    img = np.array(\n                        Image.open(\n                            f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                        )\n                    )\n                elif self.cache_img == \"memory\":\n                    img = self.cache[(labels_idx, lf_idx)].copy()\n\n            else:  # load from slp file if not cached\n                img = lf.image  # TODO: doesn't work when num_workers &gt; 0\n\n            if img.ndim == 2:\n                img = np.expand_dims(img, axis=2)\n\n            self.cache_lf = [lf_idx, img]\n\n        video_idx = self._get_video_idx(lf, labels_idx)\n\n        image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n        instances = []\n        for inst in lf:\n            instances.append(inst.numpy())\n        instances = np.stack(instances, axis=0)\n\n        # Add singleton time dimension for single frames.\n        image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n        instances = np.expand_dims(\n            instances, axis=0\n        )  # (n_samples=1, num_instances, num_nodes, 2)\n\n        instances = torch.from_numpy(instances.astype(\"float32\"))\n        image = torch.from_numpy(image)\n\n        num_instances, _ = instances.shape[1:3]\n        orig_img_height, orig_img_width = image.shape[-2:]\n\n        instances = instances[:, inst_idx]\n\n        # apply normalization\n        image = apply_normalization(image)\n\n        if self.ensure_rgb:\n            image = convert_to_rgb(image)\n        elif self.ensure_grayscale:\n            image = convert_to_grayscale(image)\n\n        # size matcher\n        image, eff_scale = apply_sizematcher(\n            image,\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        instances = instances * eff_scale\n\n        # resize image\n        image, instances = apply_resizer(\n            image,\n            instances,\n            scale=self.scale,\n        )\n\n        # get class vectors\n        track_ids = torch.Tensor(\n            [\n                (\n                    self.tracks.index(lf.instances[idx].track.name)\n                    if lf.instances[idx].track is not None\n                    else -1\n                )\n                for idx in range(num_instances)\n            ]\n        ).to(torch.int32)\n        class_vectors = make_class_vectors(\n            class_inds=track_ids,\n            n_classes=torch.tensor(len(self.tracks), dtype=torch.int32),\n        )\n\n        # get the centroids based on the anchor idx\n        centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n        instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n        crop_size = np.array(self.crop_hw) * np.sqrt(\n            2\n        )  # crop extra for rotation augmentation\n        crop_size = crop_size.astype(np.int32).tolist()\n\n        sample = generate_crops(image, instance, centroid, crop_size)\n\n        sample[\"frame_idx\"] = torch.tensor(lf.frame_idx, dtype=torch.int32)\n        sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n        sample[\"num_instances\"] = num_instances\n        sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width])\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_intensity_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_geometric_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.geometric_aug,\n                )\n\n        # re-crop to original crop size\n        sample[\"instance_bbox\"] = torch.unsqueeze(\n            make_centered_bboxes(\n                sample[\"centroid\"][0], self.crop_hw[0], self.crop_hw[1]\n            ),\n            0,\n        )  # (n_samples=1, 4, 2)\n\n        sample[\"instance_image\"] = crop_and_resize(\n            sample[\"instance_image\"], boxes=sample[\"instance_bbox\"], size=self.crop_hw\n        )\n        point = sample[\"instance_bbox\"][0][0]\n        center_instance = sample[\"instance\"] - point\n        centered_centroid = sample[\"centroid\"] - point\n\n        sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n        sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n        # Pad the image (if needed) according max stride\n        sample[\"instance_image\"] = apply_pad_to_stride(\n            sample[\"instance_image\"], max_stride=self.max_stride\n        )\n\n        img_hw = sample[\"instance_image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_confmaps(\n            sample[\"instance\"],\n            img_hw=img_hw,\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n        )\n\n        sample[\"track_id\"] = track_ids[inst_idx]\n        sample[\"class_vectors\"] = class_vectors[inst_idx].to(torch.float32)\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.TopDownCenteredInstanceMultiClassDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with cropped image and confmaps of instance for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n    labels_idx, lf_idx, inst_idx = self.instance_idx_list[index]\n    lf = self.labels[labels_idx][lf_idx]\n\n    if lf_idx == self.cache_lf[0]:\n        img = self.cache_lf[1]\n    else:\n        # load the img\n        if self.cache_img is not None:\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n\n        else:  # load from slp file if not cached\n            img = lf.image  # TODO: doesn't work when num_workers &gt; 0\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        self.cache_lf = [lf_idx, img]\n\n    video_idx = self._get_video_idx(lf, labels_idx)\n\n    image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n    instances = []\n    for inst in lf:\n        instances.append(inst.numpy())\n    instances = np.stack(instances, axis=0)\n\n    # Add singleton time dimension for single frames.\n    image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n    instances = np.expand_dims(\n        instances, axis=0\n    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n    instances = torch.from_numpy(instances.astype(\"float32\"))\n    image = torch.from_numpy(image)\n\n    num_instances, _ = instances.shape[1:3]\n    orig_img_height, orig_img_width = image.shape[-2:]\n\n    instances = instances[:, inst_idx]\n\n    # apply normalization\n    image = apply_normalization(image)\n\n    if self.ensure_rgb:\n        image = convert_to_rgb(image)\n    elif self.ensure_grayscale:\n        image = convert_to_grayscale(image)\n\n    # size matcher\n    image, eff_scale = apply_sizematcher(\n        image,\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    instances = instances * eff_scale\n\n    # resize image\n    image, instances = apply_resizer(\n        image,\n        instances,\n        scale=self.scale,\n    )\n\n    # get class vectors\n    track_ids = torch.Tensor(\n        [\n            (\n                self.tracks.index(lf.instances[idx].track.name)\n                if lf.instances[idx].track is not None\n                else -1\n            )\n            for idx in range(num_instances)\n        ]\n    ).to(torch.int32)\n    class_vectors = make_class_vectors(\n        class_inds=track_ids,\n        n_classes=torch.tensor(len(self.tracks), dtype=torch.int32),\n    )\n\n    # get the centroids based on the anchor idx\n    centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n    instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n    crop_size = np.array(self.crop_hw) * np.sqrt(\n        2\n    )  # crop extra for rotation augmentation\n    crop_size = crop_size.astype(np.int32).tolist()\n\n    sample = generate_crops(image, instance, centroid, crop_size)\n\n    sample[\"frame_idx\"] = torch.tensor(lf.frame_idx, dtype=torch.int32)\n    sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n    sample[\"num_instances\"] = num_instances\n    sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width])\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_intensity_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_geometric_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.geometric_aug,\n            )\n\n    # re-crop to original crop size\n    sample[\"instance_bbox\"] = torch.unsqueeze(\n        make_centered_bboxes(\n            sample[\"centroid\"][0], self.crop_hw[0], self.crop_hw[1]\n        ),\n        0,\n    )  # (n_samples=1, 4, 2)\n\n    sample[\"instance_image\"] = crop_and_resize(\n        sample[\"instance_image\"], boxes=sample[\"instance_bbox\"], size=self.crop_hw\n    )\n    point = sample[\"instance_bbox\"][0][0]\n    center_instance = sample[\"instance\"] - point\n    centered_centroid = sample[\"centroid\"] - point\n\n    sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n    sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n    # Pad the image (if needed) according max stride\n    sample[\"instance_image\"] = apply_pad_to_stride(\n        sample[\"instance_image\"], max_stride=self.max_stride\n    )\n\n    img_hw = sample[\"instance_image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_confmaps(\n        sample[\"instance\"],\n        img_hw=img_hw,\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n    )\n\n    sample[\"track_id\"] = track_ids[inst_idx]\n    sample[\"class_vectors\"] = class_vectors[inst_idx].to(torch.float32)\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.TopDownCenteredInstanceMultiClassDataset.__init__","title":"<code>__init__(labels, crop_hw, confmap_head_config, max_stride, anchor_ind=None, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    crop_hw: Tuple[int],\n    confmap_head_config: DictConfig,\n    max_stride: int,\n    anchor_ind: Optional[int] = None,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        crop_hw=crop_hw,\n        confmap_head_config=confmap_head_config,\n        max_stride=max_stride,\n        anchor_ind=anchor_ind,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n    )\n    self.tracks = []\n    for train_label in self.labels:\n        self.tracks.extend([x.name for x in train_label.tracks if x is not None])\n    self.tracks = list(set(self.tracks))\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.get_steps_per_epoch","title":"<code>get_steps_per_epoch(dataset, batch_size)</code>","text":"<p>Compute the number of steps (iterations) per epoch for the given dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def get_steps_per_epoch(dataset: BaseDataset, batch_size: int):\n    \"\"\"Compute the number of steps (iterations) per epoch for the given dataset.\"\"\"\n    return (len(dataset) // batch_size) + (1 if (len(dataset) % batch_size) else 0)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.get_train_val_dataloaders","title":"<code>get_train_val_dataloaders(train_dataset, val_dataset, config, train_steps_per_epoch=None, val_steps_per_epoch=None, rank=None, trainer_devices=1)</code>","text":"<p>Return the train and val dataloaders.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>BaseDataset</code> <p>Train dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].</p> required <code>val_dataset</code> <code>BaseDataset</code> <p>Val dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].</p> required <code>config</code> <code>DictConfig</code> <p>Sleap-nn config.</p> required <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>None</code> <code>val_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to run validation for in an epoch. If set to <code>None</code>, this is set to the number of batches in the val data.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>None</code> <code>trainer_devices</code> <code>int</code> <p>Number of devices to use for training.</p> <code>1</code> <p>Returns:</p> Type Description <p>A tuple (train_dataloader, val_dataloader).</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def get_train_val_dataloaders(\n    train_dataset: BaseDataset,\n    val_dataset: BaseDataset,\n    config: DictConfig,\n    train_steps_per_epoch: Optional[int] = None,\n    val_steps_per_epoch: Optional[int] = None,\n    rank: Optional[int] = None,\n    trainer_devices: int = 1,\n):\n    \"\"\"Return the train and val dataloaders.\n\n    Args:\n        train_dataset: Train dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].\n        val_dataset: Val dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].\n        config: Sleap-nn config.\n        train_steps_per_epoch: Number of minibatches (steps) to train for in an epoch. If set to `None`, this is set to the number of batches in the training data. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        val_steps_per_epoch: Number of minibatches (steps) to run validation for in an epoch. If set to `None`, this is set to the number of batches in the val data.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        trainer_devices: Number of devices to use for training.\n\n    Returns:\n        A tuple (train_dataloader, val_dataloader).\n    \"\"\"\n    pin_memory = (\n        config.trainer_config.train_data_loader.pin_memory\n        if \"pin_memory\" in config.trainer_config.train_data_loader\n        and config.trainer_config.train_data_loader.pin_memory is not None\n        else True\n    )\n\n    if train_steps_per_epoch is None:\n        train_steps_per_epoch = config.trainer_config.train_steps_per_epoch\n        if train_steps_per_epoch is None:\n            train_steps_per_epoch = get_steps_per_epoch(\n                dataset=train_dataset,\n                batch_size=config.trainer_config.train_data_loader.batch_size,\n            )\n\n    if val_steps_per_epoch is None:\n        val_steps_per_epoch = get_steps_per_epoch(\n            dataset=val_dataset,\n            batch_size=config.trainer_config.val_data_loader.batch_size,\n        )\n\n    train_sampler = (\n        DistributedSampler(\n            dataset=train_dataset,\n            shuffle=config.trainer_config.train_data_loader.shuffle,\n            rank=rank if rank is not None else 0,\n            num_replicas=trainer_devices,\n        )\n        if trainer_devices &gt; 1\n        else None\n    )\n\n    train_data_loader = InfiniteDataLoader(\n        dataset=train_dataset,\n        sampler=train_sampler,\n        len_dataloader=(round(train_steps_per_epoch / trainer_devices)),\n        shuffle=(\n            config.trainer_config.train_data_loader.shuffle\n            if train_sampler is None\n            else None\n        ),\n        batch_size=config.trainer_config.train_data_loader.batch_size,\n        num_workers=config.trainer_config.train_data_loader.num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(\n            True if config.trainer_config.train_data_loader.num_workers &gt; 0 else None\n        ),\n        prefetch_factor=(\n            config.trainer_config.train_data_loader.batch_size\n            if config.trainer_config.train_data_loader.num_workers &gt; 0\n            else None\n        ),\n    )\n\n    val_sampler = (\n        DistributedSampler(\n            dataset=val_dataset,\n            shuffle=False,\n            rank=rank if rank is not None else 0,\n            num_replicas=trainer_devices,\n        )\n        if trainer_devices &gt; 1\n        else None\n    )\n    val_data_loader = InfiniteDataLoader(\n        dataset=val_dataset,\n        shuffle=False if val_sampler is None else None,\n        sampler=val_sampler,\n        len_dataloader=(\n            round(val_steps_per_epoch / trainer_devices)\n            if trainer_devices &gt; 1\n            else None\n        ),\n        batch_size=config.trainer_config.val_data_loader.batch_size,\n        num_workers=config.trainer_config.val_data_loader.num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(\n            True if config.trainer_config.val_data_loader.num_workers &gt; 0 else None\n        ),\n        prefetch_factor=(\n            config.trainer_config.val_data_loader.batch_size\n            if config.trainer_config.val_data_loader.num_workers &gt; 0\n            else None\n        ),\n    )\n\n    return train_data_loader, val_data_loader\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.get_train_val_datasets","title":"<code>get_train_val_datasets(train_labels, val_labels, config, rank=None)</code>","text":"<p>Return the train and val datasets.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels</code> <code>List[Labels]</code> <p>List of train labels.</p> required <code>val_labels</code> <code>List[Labels]</code> <p>List of val labels.</p> required <code>config</code> <code>DictConfig</code> <p>Sleap-nn config.</p> required <code>rank</code> <code>Optional[int]</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple (train_dataset, val_dataset).</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def get_train_val_datasets(\n    train_labels: List[sio.Labels],\n    val_labels: List[sio.Labels],\n    config: DictConfig,\n    rank: Optional[int] = None,\n):\n    \"\"\"Return the train and val datasets.\n\n    Args:\n        train_labels: List of train labels.\n        val_labels: List of val labels.\n        config: Sleap-nn config.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n\n    Returns:\n        A tuple (train_dataset, val_dataset).\n    \"\"\"\n    cache_imgs = (\n        config.data_config.data_pipeline_fw.split(\"_\")[-1]\n        if \"cache_img\" in config.data_config.data_pipeline_fw\n        else None\n    )\n    base_cache_img_path = config.data_config.cache_img_path\n    train_cache_img_path, val_cache_img_path = None, None\n\n    if cache_imgs == \"disk\":\n        train_cache_img_path = Path(base_cache_img_path) / \"train_imgs\"\n        val_cache_img_path = Path(base_cache_img_path) / \"val_imgs\"\n    use_existing_imgs = config.data_config.use_existing_imgs\n\n    model_type = get_model_type_from_cfg(config=config)\n    backbone_type = get_backbone_type_from_cfg(config=config)\n\n    if cache_imgs == \"disk\" and use_existing_imgs:\n        if not (\n            train_cache_img_path.exists()\n            and train_cache_img_path.is_dir()\n            and any(train_cache_img_path.glob(\"*.jpg\"))\n        ):\n            message = f\"There are no images in the path: {train_cache_img_path}\"\n            logger.error(message)\n            raise Exception(message)\n\n        if not (\n            val_cache_img_path.exists()\n            and val_cache_img_path.is_dir()\n            and any(val_cache_img_path.glob(\"*.jpg\"))\n        ):\n            message = f\"There are no images in the path: {val_cache_img_path}\"\n            logger.error(message)\n            raise Exception(message)\n\n    if model_type == \"bottomup\":\n        train_dataset = BottomUpDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.bottomup.confmaps,\n            pafs_head_config=config.model_config.head_configs.bottomup.pafs,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        val_dataset = BottomUpDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.bottomup.confmaps,\n            pafs_head_config=config.model_config.head_configs.bottomup.pafs,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n\n    elif model_type == \"multi_class_bottomup\":\n        train_dataset = BottomUpMultiClassDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_bottomup.confmaps,\n            class_maps_head_config=config.model_config.head_configs.multi_class_bottomup.class_maps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        val_dataset = BottomUpMultiClassDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_bottomup.confmaps,\n            class_maps_head_config=config.model_config.head_configs.multi_class_bottomup.class_maps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n\n    elif model_type == \"centered_instance\":\n        nodes = config.model_config.head_configs.centered_instance.confmaps.part_names\n        anchor_part = (\n            config.model_config.head_configs.centered_instance.confmaps.anchor_part\n        )\n        anchor_ind = nodes.index(anchor_part) if anchor_part is not None else None\n        train_dataset = CenteredInstanceDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.centered_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            crop_hw=list(config.data_config.preprocessing.crop_hw),\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        val_dataset = CenteredInstanceDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.centered_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            crop_hw=list(config.data_config.preprocessing.crop_hw),\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n\n    elif model_type == \"multi_class_topdown\":\n        nodes = config.model_config.head_configs.multi_class_topdown.confmaps.part_names\n        anchor_part = (\n            config.model_config.head_configs.multi_class_topdown.confmaps.anchor_part\n        )\n        anchor_ind = nodes.index(anchor_part) if anchor_part is not None else None\n        train_dataset = TopDownCenteredInstanceMultiClassDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_topdown.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            crop_hw=list(config.data_config.preprocessing.crop_hw),\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        val_dataset = TopDownCenteredInstanceMultiClassDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_topdown.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            crop_hw=list(config.data_config.preprocessing.crop_hw),\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n\n    elif model_type == \"centroid\":\n        nodes = [x[\"name\"] for x in config.data_config.skeletons[0][\"nodes\"]]\n        anchor_part = config.model_config.head_configs.centroid.confmaps.anchor_part\n        anchor_ind = nodes.index(anchor_part) if anchor_part is not None else None\n        train_dataset = CentroidDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.centroid.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        val_dataset = CentroidDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.centroid.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n\n    else:\n        train_dataset = SingleInstanceDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.single_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n        val_dataset = SingleInstanceDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.single_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n        )\n\n    # If using caching, close the videos to prevent `h5py objects can't be pickled error` when num_workers &gt; 0.\n    if \"cache_img\" in config.data_config.data_pipeline_fw:\n        for train, val in zip(train_labels, val_labels):\n            for video in train.videos:\n                if video.is_open:\n                    video.close()\n            for video in val.videos:\n                if video.is_open:\n                    video.close()\n\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"api/data/edge_maps/","title":"edge_maps","text":""},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps","title":"<code>sleap_nn.data.edge_maps</code>","text":"<p>Transformers for generating edge confidence maps and part affinity fields.</p> <p>Functions:</p> Name Description <code>distance_to_edge</code> <p>Compute pairwise distance between points and undirected edges.</p> <code>generate_pafs</code> <p>Generate part-affinity fields.</p> <code>get_edge_points</code> <p>Return the points in each instance that form a directed graph.</p> <code>make_edge_maps</code> <p>Generate confidence maps for a set of undirected edges.</p> <code>make_multi_pafs</code> <p>Make multiple instance PAFs with addition reduction.</p> <code>make_pafs</code> <p>Generate part affinity fields for a set of directed edges.</p>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.distance_to_edge","title":"<code>distance_to_edge(points, edge_source, edge_destination)</code>","text":"<p>Compute pairwise distance between points and undirected edges.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (d_0, ..., d_n, 2) where the last axis corresponds to x- and y-coordinates. Distances will be broadcast across all point dimensions.</p> required <code>edge_source</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destination</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of dtype torch.float32 of shape (d_0, ..., d_n, n_edges) where the first axes correspond to the initial dimensions of <code>points</code>, and the last indicates the distance of each point to each edge.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def distance_to_edge(\n    points: torch.Tensor, edge_source: torch.Tensor, edge_destination: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute pairwise distance between points and undirected edges.\n\n    Args:\n        points: Tensor of dtype torch.float32 of shape (d_0, ..., d_n, 2) where the last\n            axis corresponds to x- and y-coordinates. Distances will be broadcast across\n            all point dimensions.\n        edge_source: Tensor of dtype torch.float32 of shape (n_edges, 2) where the last\n            axis corresponds to x- and y-coordinates of the source points of each edge.\n        edge_destination: Tensor of dtype torch.float32 of shape (n_edges, 2) where the\n            last axis corresponds to x- and y-coordinates of the source points of each\n            edge.\n\n    Returns:\n        A tensor of dtype torch.float32 of shape (d_0, ..., d_n, n_edges) where the first\n        axes correspond to the initial dimensions of `points`, and the last indicates\n        the distance of each point to each edge.\n    \"\"\"\n    # Ensure all points are at least rank 2.\n    points = expand_to_rank(points, 2)\n    edge_source = expand_to_rank(edge_source, 2)\n    edge_destination = expand_to_rank(edge_destination, 2)\n\n    # Compute number of point dimensions.\n    n_pt_dims = points.dim() - 1\n\n    # Direction vector.\n    direction_vector = edge_destination - edge_source  # (n_edges, 2)\n\n    # Edge length.\n    edge_length = torch.maximum(\n        direction_vector.square().sum(dim=1), torch.tensor(1.0)\n    )  # (n_edges,)\n\n    # Adjust query points relative to edge source point.\n    source_relative_points = torch.unsqueeze(points, dim=-2) - expand_to_rank(\n        edge_source, n_pt_dims + 2\n    )  # (..., n_edges, 2)\n\n    # Project points to edge line.\n    line_projections = torch.sum(\n        source_relative_points * expand_to_rank(direction_vector, n_pt_dims + 2), dim=3\n    ) / expand_to_rank(\n        edge_length, n_pt_dims + 1\n    )  # (..., n_edges)\n\n    # Crop to line segment.\n    line_projections = torch.clamp(line_projections, min=0, max=1)\n\n    # Compute distance from each point to the edge.\n    distances = torch.sum(\n        torch.square(\n            (\n                line_projections.unsqueeze(-1)\n                * expand_to_rank(direction_vector, n_pt_dims + 2)\n            )\n            - source_relative_points\n        ),\n        dim=-1,\n    )  # (..., n_edges)\n\n    return distances\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.generate_pafs","title":"<code>generate_pafs(instances, img_hw, sigma=1.5, output_stride=2, edge_inds=attrs.field(default=None, converter=(attrs.converters.optional(ensure_list))), flatten_channels=False)</code>","text":"<p>Generate part-affinity fields.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>Input instances. (n_samples, n_instances, n_nodes, 2)</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <code>edge_inds</code> <code>Optional[Tensor]</code> <p><code>torch.Tensor</code> to use for looking up the index of the edges.</p> <code>field(default=None, converter=optional(ensure_list))</code> <code>flatten_channels</code> <code>bool</code> <p>If False, the generated tensors are of shape [n_edges, 2, height, width]. If True, generated tensors are of shape [n_edges * 2, height, width] by flattening the last 2 axes.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The \"part_affinity_fields\" key will be a tensor of shape (n_edges, 2, grid_height, grid_width) containing the combined part affinity fields of all instances in the frame.</p> <p>If the <code>flatten_channels</code> attribute is set to True, the last 2 axes of the \"part_affinity_fields\" are flattened to produce a tensor of shape (n_edges * 2, grid_height, grid_width). This is a convenient form when training models as a rank-4 (batched) tensor will generally be expected.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def generate_pafs(\n    instances: torch.Tensor,\n    img_hw: Tuple[int],\n    sigma: float = 1.5,\n    output_stride=2,\n    edge_inds: Optional[torch.Tensor] = attrs.field(\n        default=None, converter=attrs.converters.optional(ensure_list)\n    ),\n    flatten_channels: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate part-affinity fields.\n\n    Args:\n        instances: Input instances. (n_samples, n_instances, n_nodes, 2)\n        img_hw: Image size as tuple (height, width).\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n        edge_inds: `torch.Tensor` to use for looking up the index of the\n            edges.\n        flatten_channels: If False, the generated tensors are of shape\n            [n_edges, 2, height, width]. If True, generated tensors are of shape\n            [n_edges * 2, height, width] by flattening the last 2 axes.\n\n    Returns:\n        The \"part_affinity_fields\" key will be a tensor of shape\n        (n_edges, 2, grid_height, grid_width) containing the combined part affinity\n        fields of all instances in the frame.\n\n        If the `flatten_channels` attribute is set to True, the last 2 axes of the\n        \"part_affinity_fields\" are flattened to produce a tensor of shape\n        (n_edges * 2, grid_height, grid_width). This is a convenient form when\n        training models as a rank-4 (batched) tensor will generally be expected.\n    \"\"\"\n    image_height, image_width = img_hw\n\n    # Generate sampling grid vectors.\n    xv, yv = make_grid_vectors(\n        image_height=image_height,\n        image_width=image_width,\n        output_stride=output_stride,\n    )\n    grid_height = len(yv)\n    grid_width = len(xv)\n    n_edges = len(edge_inds)\n\n    instances = instances[0]  # n_samples=1\n    in_img = (instances &gt; 0) &amp; (instances &lt; torch.stack([xv[-1], yv[-1]]).view(1, 1, 2))\n    in_img = in_img.all(dim=-1).any(dim=1)\n    assert len(in_img.shape) == 1\n    instances = instances[in_img]\n\n    edge_sources, edge_destinations = get_edge_points(instances, edge_inds)\n    assert len(edge_sources.shape) == 3\n    assert edge_sources.shape[1:] == (n_edges, 2)\n\n    assert len(edge_destinations.shape) == 3\n    assert edge_destinations.shape[1:] == (n_edges, 2)\n\n    pafs = make_multi_pafs(\n        xv=xv,\n        yv=yv,\n        edge_sources=edge_sources,\n        edge_destinations=edge_destinations,\n        sigma=sigma,\n    )\n    assert pafs.shape == (n_edges, 2, grid_height, grid_width)\n\n    if flatten_channels:\n        pafs = pafs.reshape(n_edges * 2, grid_height, grid_width)\n        assert pafs.shape == (n_edges * 2, grid_height, grid_width)\n\n    return pafs\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.get_edge_points","title":"<code>get_edge_points(instances, edge_inds)</code>","text":"<p>Return the points in each instance that form a directed graph.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>A tensor of shape (n_instances, n_nodes, 2) and dtype torch.float32 containing instance points where the last axis corresponds to (x, y) pixel coordinates on the image. This must be rank-3 even if a single instance is present.</p> required <code>edge_inds</code> <code>Tensor</code> <p>A tensor of shape (n_edges, 2) and dtype torch.int32 containing the node indices that define a directed graph, where the last axis corresponds to the source and destination node indices.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (edge_sources, edge_destinations) containing the edge and destination points respectively. Both will be tensors of shape (n_instances, n_edges, 2), where the last axis corresponds to (x, y) pixel coordinates on the image.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def get_edge_points(\n    instances: torch.Tensor, edge_inds: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Return the points in each instance that form a directed graph.\n\n    Args:\n        instances: A tensor of shape (n_instances, n_nodes, 2) and dtype torch.float32\n            containing instance points where the last axis corresponds to (x, y) pixel\n            coordinates on the image. This must be rank-3 even if a single instance is\n            present.\n        edge_inds: A tensor of shape (n_edges, 2) and dtype torch.int32 containing the node\n            indices that define a directed graph, where the last axis corresponds to the\n            source and destination node indices.\n\n    Returns:\n        Tuple of (edge_sources, edge_destinations) containing the edge and destination\n        points respectively. Both will be tensors of shape (n_instances, n_edges, 2),\n        where the last axis corresponds to (x, y) pixel coordinates on the image.\n    \"\"\"\n    source_inds = edge_inds[:, 0].to(torch.int32)\n    destination_inds = edge_inds[:, 1].to(torch.int32)\n\n    edge_sources = instances[:, source_inds]\n    edge_destinations = instances[:, destination_inds]\n    return edge_sources, edge_destinations\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.make_edge_maps","title":"<code>make_edge_maps(xv, yv, edge_source, edge_destination, sigma)</code>","text":"<p>Generate confidence maps for a set of undirected edges.</p> <p>Parameters:</p> Name Type Description Default <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape (grid_width,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape (grid_height,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>edge_source</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destination</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the destination points of each edge.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate confidence maps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A set of confidence maps corresponding to the probability of each point on a sampling grid being on each edge. These will be in a tensor of shape (grid_height, grid_width, n_edges) of dtype torch.float32.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def make_edge_maps(\n    xv: torch.Tensor,\n    yv: torch.Tensor,\n    edge_source: torch.Tensor,\n    edge_destination: torch.Tensor,\n    sigma: float,\n) -&gt; torch.Tensor:\n    \"\"\"Generate confidence maps for a set of undirected edges.\n\n    Args:\n        xv: Sampling grid vector for x-coordinates of shape (grid_width,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape (grid_height,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        edge_source: Tensor of dtype torch.float32 of shape (n_edges, 2) where the last\n            axis corresponds to x- and y-coordinates of the source points of each edge.\n        edge_destination: Tensor of dtype torch.float32 of shape (n_edges, 2) where the\n            last axis corresponds to x- and y-coordinates of the destination points of\n            each edge.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            confidence maps.\n\n    Returns:\n        A set of confidence maps corresponding to the probability of each point on a\n        sampling grid being on each edge. These will be in a tensor of shape\n        (grid_height, grid_width, n_edges) of dtype torch.float32.\n    \"\"\"\n    yy, xx = torch.meshgrid(yv, xv, indexing=\"ij\")\n    sampling_grid = torch.stack((xx, yy), dim=-1)  # (height, width, 2)\n\n    distances = distance_to_edge(\n        sampling_grid, edge_source=edge_source, edge_destination=edge_destination\n    )\n    edge_maps = gaussian_pdf(distances, sigma=sigma)\n    return edge_maps\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.make_multi_pafs","title":"<code>make_multi_pafs(xv, yv, edge_sources, edge_destinations, sigma)</code>","text":"<p>Make multiple instance PAFs with addition reduction.</p> <p>Parameters:</p> Name Type Description Default <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape (grid_width,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape (grid_height,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>edge_sources</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destinations</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2) where the last axis corresponds to x- and y-coordinates of the destination points of each edge.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate the edge maps for masking the PAFs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A set of part affinity fields generated for each instance. These will be in a tensor of shape (n_edges, 2, grid_height, grid_width). If multiple instance PAFs are defined on the same pixel, they will be summed.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def make_multi_pafs(\n    xv: torch.Tensor,\n    yv: torch.Tensor,\n    edge_sources: torch.Tensor,\n    edge_destinations: torch.Tensor,\n    sigma: float,\n) -&gt; torch.Tensor:\n    \"\"\"Make multiple instance PAFs with addition reduction.\n\n    Args:\n        xv: Sampling grid vector for x-coordinates of shape (grid_width,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape (grid_height,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        edge_sources: Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2)\n            where the last axis corresponds to x- and y-coordinates of the source points\n            of each edge.\n        edge_destinations: Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2)\n            where the last axis corresponds to x- and y-coordinates of the destination\n            points of each edge.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            the edge maps for masking the PAFs.\n\n    Returns:\n        A set of part affinity fields generated for each instance. These will be in a\n        tensor of shape (n_edges, 2, grid_height, grid_width). If multiple instance\n        PAFs are defined on the same pixel, they will be summed.\n    \"\"\"\n    grid_height = yv.shape[0]\n    grid_width = xv.shape[0]\n    n_edges = edge_sources.shape[1]\n    n_instances = edge_sources.shape[0]\n\n    pafs = torch.zeros((n_edges, 2, grid_height, grid_width), dtype=torch.float32)\n\n    for i in range(n_instances):\n        edge_source = edge_sources[i, :]\n        edge_destination = edge_destinations[i, :]\n\n        paf = make_pafs(\n            xv=xv,\n            yv=yv,\n            edge_source=edge_source,\n            edge_destination=edge_destination,\n            sigma=sigma,\n        )\n\n        paf[torch.isnan(paf)] = 0.0\n\n        pafs += paf\n\n    return pafs\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.make_pafs","title":"<code>make_pafs(xv, yv, edge_source, edge_destination, sigma)</code>","text":"<p>Generate part affinity fields for a set of directed edges.</p> <p>Parameters:</p> Name Type Description Default <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape (grid_width,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape (grid_height,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>edge_source</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destination</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the destination points of each edge.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate the edge maps for masking the PAFs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A set of part affinity fields corresponding to the unit vector pointing along the direction of each edge weighted by the probability of each point on a sampling grid being on each edge. These will be in a tensor of shape (n_edges, 2, grid_height, grid_width) of dtype torch.float32. The last axis corresponds to the x- and y-coordinates of the unit vectors.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def make_pafs(\n    xv: torch.Tensor,\n    yv: torch.Tensor,\n    edge_source: torch.Tensor,\n    edge_destination: torch.Tensor,\n    sigma: float,\n) -&gt; torch.Tensor:\n    \"\"\"Generate part affinity fields for a set of directed edges.\n\n    Args:\n        xv: Sampling grid vector for x-coordinates of shape (grid_width,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape (grid_height,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        edge_source: Tensor of dtype torch.float32 of shape (n_edges, 2) where the last\n            axis corresponds to x- and y-coordinates of the source points of each edge.\n        edge_destination: Tensor of dtype torch.float32 of shape (n_edges, 2) where the\n            last axis corresponds to x- and y-coordinates of the destination points of\n            each edge.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            the edge maps for masking the PAFs.\n\n    Returns:\n        A set of part affinity fields corresponding to the unit vector pointing along\n        the direction of each edge weighted by the probability of each point on a\n        sampling grid being on each edge. These will be in a tensor of shape\n        (n_edges, 2, grid_height, grid_width) of dtype torch.float32. The last axis\n        corresponds to the x- and y-coordinates of the unit vectors.\n    \"\"\"\n    unit_vectors = edge_destination - edge_source\n    unit_vectors = unit_vectors / torch.norm(unit_vectors, dim=-1, keepdim=True)\n    edge_confidence_map = make_edge_maps(\n        xv=xv,\n        yv=yv,\n        edge_source=edge_source,\n        edge_destination=edge_destination,\n        sigma=sigma,\n    )\n    pafs = torch.unsqueeze(edge_confidence_map, dim=-1) * expand_to_rank(\n        unit_vectors, 4\n    )\n    pafs = pafs.permute(2, 3, 0, 1)\n    return pafs\n</code></pre>"},{"location":"api/data/identity/","title":"identity","text":""},{"location":"api/data/identity/#sleap_nn.data.identity","title":"<code>sleap_nn.data.identity</code>","text":"<p>Utilities for generating data for track identity models.</p> <p>Functions:</p> Name Description <code>generate_class_maps</code> <p>Generate class maps from track indices.</p> <code>make_class_maps</code> <p>Generate identity class maps using instance-wise confidence maps.</p> <code>make_class_vectors</code> <p>Make a binary class vectors from class indices.</p>"},{"location":"api/data/identity/#sleap_nn.data.identity.generate_class_maps","title":"<code>generate_class_maps(instances, img_hw, num_instances, class_inds, num_tracks, class_map_threshold=0.2, sigma=1.5, output_stride=2, is_centroids=False)</code>","text":"<p>Generate class maps from track indices.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples=1, n_instances, n_nodes, 2) or for centroids - (n_samples=1, n_instances, 2)</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>num_instances</code> <code>int</code> <p>Original number of instances in the frame.</p> required <code>class_inds</code> <code>Tensor</code> <p>Class indices as <code>torch.int32</code> tensor of shape <code>(n_instances)</code>.</p> required <code>num_tracks</code> <code>int</code> <p>Total number of tracks in the dataset.</p> required <code>class_map_threshold</code> <code>float</code> <p>Minimum confidence map value below which map values will be replaced with zeros.</p> <code>0.2</code> <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <code>int</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <code>is_centroids</code> <code>bool</code> <p>True if confidence maps should be generates for centroids else False. Default: False.</p> <code>False</code> Source code in <code>sleap_nn/data/identity.py</code> <pre><code>def generate_class_maps(\n    instances: torch.Tensor,\n    img_hw: Tuple[int],\n    num_instances: int,\n    class_inds: torch.Tensor,\n    num_tracks: int,\n    class_map_threshold: float = 0.2,\n    sigma: float = 1.5,\n    output_stride: int = 2,\n    is_centroids: bool = False,\n):\n    \"\"\"Generate class maps from track indices.\n\n    Args:\n        instances: Input keypoints. (n_samples=1, n_instances, n_nodes, 2) or\n            for centroids - (n_samples=1, n_instances, 2)\n        img_hw: Image size as tuple (height, width).\n        num_instances: Original number of instances in the frame.\n        class_inds: Class indices as `torch.int32` tensor of shape `(n_instances)`.\n        num_tracks: Total number of tracks in the dataset.\n        class_map_threshold: Minimum confidence map value below which map values will be\n            replaced with zeros.\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n        is_centroids: True if confidence maps should be generates for centroids else False.\n            Default: False.\n\n    \"\"\"\n    height, width = img_hw\n    xv, yv = make_grid_vectors(height, width, output_stride)\n\n    if is_centroids:\n        points = instances[:, :num_instances, :].unsqueeze(dim=-3)\n        # (n_samples=1, 1, n_instances, 2)\n    else:\n        points = instances[:, :num_instances, :, :].permute(\n            0, 2, 1, 3\n        )  # (n_samples=1, n_nodes, n_instances, 2)\n\n    # Generate confidene maps for masking.\n    cms = make_multi_confmaps(\n        points, xv, yv, sigma * output_stride\n    )  # (n_samples=1, n_instances, height/ output_stride, width/ output_stride).\n\n    class_maps = make_class_maps(\n        cms,\n        class_inds=class_inds,\n        n_classes=num_tracks,\n        threshold=class_map_threshold,\n    )  # (n_samples=1, n_classes, height/ output_stride, width/ output_stride)\n    return class_maps\n</code></pre>"},{"location":"api/data/identity/#sleap_nn.data.identity.make_class_maps","title":"<code>make_class_maps(confmaps, class_inds, n_classes, threshold=0.2)</code>","text":"<p>Generate identity class maps using instance-wise confidence maps.</p> <p>This is useful for making class maps defined on local neighborhoods around the peaks.</p> <p>Parameters:</p> Name Type Description Default <code>confmaps</code> <code>Tensor</code> <p>Confidence maps for the same points as the offset maps as a <code>torch.Tensor</code> of shape <code>(n_samples=1, n_instances, grid_height, grid_width)</code>. This can be generated by <code>sleap_nn.data.confidence_maps.make_confmaps</code>.</p> required <code>class_inds</code> <code>Tensor</code> <p>Class indices as <code>torch.int32</code> tensor of shape <code>(n_instances)</code>.</p> required <code>n_classes</code> <code>int</code> <p>Integer number of maximum classes.</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence map value below which map values will be replaced with zeros.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The class maps with shape <code>(n_samples=1, n_classes, grid_height, grid_width)</code> and dtype <code>torch.float32</code> where each channel will be a binary mask with 1 where the instance confidence maps were higher than the threshold.</p> Notes <p>Pixels that have confidence map values from more than one animal will have the class vectors weighed by the relative contribution of each instance.</p> Source code in <code>sleap_nn/data/identity.py</code> <pre><code>def make_class_maps(\n    confmaps: torch.Tensor,\n    class_inds: torch.Tensor,\n    n_classes: int,\n    threshold: float = 0.2,\n) -&gt; torch.Tensor:\n    \"\"\"Generate identity class maps using instance-wise confidence maps.\n\n    This is useful for making class maps defined on local neighborhoods around the\n    peaks.\n\n    Args:\n        confmaps: Confidence maps for the same points as the offset maps as a\n            `torch.Tensor` of shape `(n_samples=1, n_instances, grid_height, grid_width)`. This can be generated by\n            `sleap_nn.data.confidence_maps.make_confmaps`.\n        class_inds: Class indices as `torch.int32` tensor of shape `(n_instances)`.\n        n_classes: Integer number of maximum classes.\n        threshold: Minimum confidence map value below which map values will be replaced\n            with zeros.\n\n    Returns:\n        The class maps with shape `(n_samples=1, n_classes, grid_height, grid_width)` and dtype\n        `torch.float32` where each channel will be a binary mask with 1 where the instance\n        confidence maps were higher than the threshold.\n\n    Notes:\n        Pixels that have confidence map values from more than one animal will have the\n        class vectors weighed by the relative contribution of each instance.\n\n    \"\"\"\n    n_instances = confmaps.shape[-3]\n    class_vectors = make_class_vectors(class_inds, n_classes)\n    class_vectors = torch.reshape(\n        class_vectors.to(torch.float32),\n        [n_classes, n_instances, 1, 1],\n    )\n\n    # Normalize instance mask\n    mask = confmaps / torch.sum(confmaps, dim=-3, keepdim=True)\n    mask = torch.where(\n        confmaps &gt; threshold,\n        mask,\n        torch.tensor(0.0, dtype=mask.dtype, device=mask.device),\n    )  # (1, num_instances, H, W)\n\n    # Apply mask to vectors and reduce over instances\n    class_maps = torch.max(mask * class_vectors, dim=-3).values\n\n    return class_maps.unsqueeze(0)  # (n_samples=1, n_classes, H, W)\n</code></pre>"},{"location":"api/data/identity/#sleap_nn.data.identity.make_class_vectors","title":"<code>make_class_vectors(class_inds, n_classes)</code>","text":"<p>Make a binary class vectors from class indices.</p> <p>Parameters:</p> Name Type Description Default <code>class_inds</code> <code>Tensor</code> <p>Class indices as <code>torch.Tensor</code> of dtype <code>torch.int32</code> and shape <code>(n_instances,)</code>. Indices of <code>-1</code> will be interpreted as having no class.</p> required <code>n_classes</code> <code>int</code> <p>Integer number of maximum classes.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with binary class vectors of shape <code>(n_instances, n_classes)</code> of dtype <code>torch.int32</code>. Instances with no class will have all zeros in their row.</p> <p>Notes: A class index can be used to represent a track index.</p> Source code in <code>sleap_nn/data/identity.py</code> <pre><code>def make_class_vectors(class_inds: torch.Tensor, n_classes: int) -&gt; torch.Tensor:\n    \"\"\"Make a binary class vectors from class indices.\n\n    Args:\n        class_inds: Class indices as `torch.Tensor` of dtype `torch.int32` and shape\n            `(n_instances,)`. Indices of `-1` will be interpreted as having no class.\n        n_classes: Integer number of maximum classes.\n\n    Returns:\n        A tensor with binary class vectors of shape `(n_instances, n_classes)` of dtype\n        `torch.int32`. Instances with no class will have all zeros in their row.\n\n    Notes: A class index can be used to represent a track index.\n    \"\"\"\n    # Create mask of valid IDs\n    mask = class_inds &gt;= 0\n    class_inds_masked = class_inds.clone()\n    class_inds_masked[~mask] = 0\n\n    one_hot = F.one_hot(class_inds_masked.long(), num_classes=n_classes)\n    one_hot[~mask] = 0  # zero out invalids\n    return one_hot.to(torch.int32)\n</code></pre>"},{"location":"api/data/instance_centroids/","title":"instance_centroids","text":""},{"location":"api/data/instance_centroids/#sleap_nn.data.instance_centroids","title":"<code>sleap_nn.data.instance_centroids</code>","text":"<p>Handle calculation of instance centroids.</p> <p>Functions:</p> Name Description <code>find_points_bbox_midpoint</code> <p>Find the midpoint of the bounding box of a set of points.</p> <code>generate_centroids</code> <p>Return centroids, falling back to bounding box midpoints.</p>"},{"location":"api/data/instance_centroids/#sleap_nn.data.instance_centroids.find_points_bbox_midpoint","title":"<code>find_points_bbox_midpoint(points)</code>","text":"<p>Find the midpoint of the bounding box of a set of points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>A torch.Tensor of dtype torch.float32 and of shape (..., n_points, 2), i.e., rank &gt;= 2.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The midpoints between the bounds of each set of points. The output will be of shape (..., 2), reducing the rank of the input by 1. NaNs will be ignored in the calculation.</p> Notes <p>The midpoint is calculated as:     xy_mid = xy_min + ((xy_max - xy_min) / 2)            = ((2 * xy_min) / 2) + ((xy_max - xy_min) / 2)            = (2 * xy_min + xy_max - xy_min) / 2            = (xy_min + xy_max) / 2</p> Source code in <code>sleap_nn/data/instance_centroids.py</code> <pre><code>def find_points_bbox_midpoint(points: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Find the midpoint of the bounding box of a set of points.\n\n    Args:\n        points: A torch.Tensor of dtype torch.float32 and of shape (..., n_points, 2),\n            i.e., rank &gt;= 2.\n\n    Returns:\n        The midpoints between the bounds of each set of points. The output will be of\n        shape (..., 2), reducing the rank of the input by 1. NaNs will be ignored in the\n        calculation.\n\n    Notes:\n        The midpoint is calculated as:\n            xy_mid = xy_min + ((xy_max - xy_min) / 2)\n                   = ((2 * xy_min) / 2) + ((xy_max - xy_min) / 2)\n                   = (2 * xy_min + xy_max - xy_min) / 2\n                   = (xy_min + xy_max) / 2\n    \"\"\"\n    pts_min = torch.min(\n        torch.where(torch.isnan(points), torch.inf, points), dim=-2\n    ).values\n    pts_max = torch.max(\n        torch.where(torch.isnan(points), -torch.inf, points), dim=-2\n    ).values\n\n    return (pts_max + pts_min) * 0.5\n</code></pre>"},{"location":"api/data/instance_centroids/#sleap_nn.data.instance_centroids.generate_centroids","title":"<code>generate_centroids(points, anchor_ind=None)</code>","text":"<p>Return centroids, falling back to bounding box midpoints.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>A torch.Tensor of dtype torch.float32 and of shape (..., n_nodes, 2), i.e., rank &gt;= 2.</p> required <code>anchor_ind</code> <code>Optional[int]</code> <p>The index of the node to use as the anchor for the centroid. If not provided or if not present in the instance, the midpoint of the bounding box is used instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The centroids of the instances. The output will be of shape (..., 2), reducing the rank of the input by 1. NaNs will be ignored in the calculation.</p> Source code in <code>sleap_nn/data/instance_centroids.py</code> <pre><code>def generate_centroids(\n    points: torch.Tensor, anchor_ind: Optional[int] = None\n) -&gt; torch.Tensor:\n    \"\"\"Return centroids, falling back to bounding box midpoints.\n\n    Args:\n        points: A torch.Tensor of dtype torch.float32 and of shape (..., n_nodes, 2),\n            i.e., rank &gt;= 2.\n        anchor_ind: The index of the node to use as the anchor for the centroid. If not\n            provided or if not present in the instance, the midpoint of the bounding box\n            is used instead.\n\n    Returns:\n        The centroids of the instances. The output will be of shape (..., 2), reducing\n        the rank of the input by 1. NaNs will be ignored in the calculation.\n    \"\"\"\n    if anchor_ind is not None:\n        centroids = points[..., anchor_ind, :].clone()\n    else:\n        centroids = torch.full_like(points[..., 0, :], torch.nan).clone()\n\n    missing_anchors = torch.isnan(centroids).any(dim=-1)\n    if missing_anchors.any():\n        centroids[missing_anchors] = find_points_bbox_midpoint(points[missing_anchors])\n\n    return centroids  # (..., n_instances, 2)\n</code></pre>"},{"location":"api/data/instance_cropping/","title":"instance_cropping","text":""},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping","title":"<code>sleap_nn.data.instance_cropping</code>","text":"<p>Handle cropping of instances.</p> <p>Functions:</p> Name Description <code>find_instance_crop_size</code> <p>Compute the size of the largest instance bounding box from labels.</p> <code>generate_crops</code> <p>Generate cropped image for the given centroid.</p> <code>make_centered_bboxes</code> <p>Create centered bounding boxes around centroid.</p>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.find_instance_crop_size","title":"<code>find_instance_crop_size(labels, padding=0, maximum_stride=2, input_scaling=1.0, min_crop_size=None)</code>","text":"<p>Compute the size of the largest instance bounding box from labels.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>A <code>sio.Labels</code> containing user-labeled instances.</p> required <code>padding</code> <code>int</code> <p>Integer number of pixels to add to the bounds as margin padding.</p> <code>0</code> <code>maximum_stride</code> <code>int</code> <p>Ensure that the returned crop size is divisible by this value. Useful for ensuring that the crop size will not be truncated in a given architecture.</p> <code>2</code> <code>input_scaling</code> <code>float</code> <p>Float factor indicating the scale of the input images if any scaling will be done before cropping.</p> <code>1.0</code> <code>min_crop_size</code> <code>Optional[int]</code> <p>The crop size set by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>An integer crop size denoting the length of the side of the bounding boxes that will contain the instances when cropped. The returned crop size will be larger or equal to the input <code>min_crop_size</code>.</p> <p>This accounts for stride, padding and scaling when ensuring divisibility.</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def find_instance_crop_size(\n    labels: sio.Labels,\n    padding: int = 0,\n    maximum_stride: int = 2,\n    input_scaling: float = 1.0,\n    min_crop_size: Optional[int] = None,\n) -&gt; int:\n    \"\"\"Compute the size of the largest instance bounding box from labels.\n\n    Args:\n        labels: A `sio.Labels` containing user-labeled instances.\n        padding: Integer number of pixels to add to the bounds as margin padding.\n        maximum_stride: Ensure that the returned crop size is divisible by this value.\n            Useful for ensuring that the crop size will not be truncated in a given\n            architecture.\n        input_scaling: Float factor indicating the scale of the input images if any\n            scaling will be done before cropping.\n        min_crop_size: The crop size set by the user.\n\n    Returns:\n        An integer crop size denoting the length of the side of the bounding boxes that\n        will contain the instances when cropped. The returned crop size will be larger\n        or equal to the input `min_crop_size`.\n\n        This accounts for stride, padding and scaling when ensuring divisibility.\n    \"\"\"\n    # Check if user-specified crop size is divisible by max stride\n    min_crop_size = 0 if min_crop_size is None else min_crop_size\n    if (min_crop_size &gt; 0) and (min_crop_size % maximum_stride == 0):\n        return min_crop_size\n\n    # Calculate crop size\n    min_crop_size_no_pad = min_crop_size - padding\n    max_length = 0.0\n    for lf in labels:\n        for inst in lf.instances:\n            pts = inst.numpy()\n            pts *= input_scaling\n            diff_x = np.nanmax(pts[:, 0]) - np.nanmin(pts[:, 0])\n            diff_x = 0 if np.isnan(diff_x) else diff_x\n            max_length = np.maximum(max_length, diff_x)\n            diff_y = np.nanmax(pts[:, 1]) - np.nanmin(pts[:, 1])\n            diff_y = 0 if np.isnan(diff_y) else diff_y\n            max_length = np.maximum(max_length, diff_y)\n            max_length = np.maximum(max_length, min_crop_size_no_pad)\n\n    max_length += float(padding)\n    crop_size = math.ceil(max_length / float(maximum_stride)) * maximum_stride\n\n    return int(crop_size)\n</code></pre>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.generate_crops","title":"<code>generate_crops(image, instance, centroid, crop_size)</code>","text":"<p>Generate cropped image for the given centroid.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input source image. (n_samples, C, H, W)</p> required <code>instance</code> <code>Tensor</code> <p>Keypoints for the instance to be cropped. (n_nodes, 2)</p> required <code>centroid</code> <code>Tensor</code> <p>Centroid of the instance to be cropped. (2)</p> required <code>crop_size</code> <code>Tuple[int]</code> <p>(height, width) of the crop to be generated.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary with cropped images, bounding box for the cropped instance, keypoints and centroids adjusted to the crop.</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def generate_crops(\n    image: torch.Tensor,\n    instance: torch.Tensor,\n    centroid: torch.Tensor,\n    crop_size: Tuple[int],\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Generate cropped image for the given centroid.\n\n    Args:\n        image: Input source image. (n_samples, C, H, W)\n        instance: Keypoints for the instance to be cropped. (n_nodes, 2)\n        centroid: Centroid of the instance to be cropped. (2)\n        crop_size: (height, width) of the crop to be generated.\n\n    Returns:\n        A dictionary with cropped images, bounding box for the cropped instance, keypoints and\n        centroids adjusted to the crop.\n    \"\"\"\n    box_size = crop_size\n\n    # Generate bounding boxes from centroid.\n    instance_bbox = torch.unsqueeze(\n        make_centered_bboxes(centroid, box_size[0], box_size[1]), 0\n    )  # (n_samples=1, 4, 2)\n\n    # Generate cropped image of shape (n_samples, C, crop_H, crop_W)\n    instance_image = crop_and_resize(\n        image,\n        boxes=instance_bbox,\n        size=box_size,\n    )\n\n    # Access top left point (x,y) of bounding box and subtract this offset from\n    # position of nodes.\n    point = instance_bbox[0][0]\n    center_instance = (instance - point).unsqueeze(0)  # (n_samples=1, n_nodes, 2)\n    centered_centroid = (centroid - point).unsqueeze(0)  # (n_samples=1, 2)\n\n    cropped_sample = {\n        \"instance_image\": instance_image,\n        \"instance_bbox\": instance_bbox,\n        \"instance\": center_instance,\n        \"centroid\": centered_centroid,\n    }\n\n    return cropped_sample\n</code></pre>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.make_centered_bboxes","title":"<code>make_centered_bboxes(centroids, box_height, box_width)</code>","text":"<p>Create centered bounding boxes around centroid.</p> <p>To be used with <code>kornia.geometry.transform.crop_and_resize</code>in the following (clockwise) order: top-left, top-right, bottom-right and bottom-left.</p> <p>Parameters:</p> Name Type Description Default <code>centroids</code> <code>Tensor</code> <p>A tensor of centroids with shape (n_centroids, 2), where n_centroids is the number of centroids, and the last dimension represents x and y coordinates.</p> required <code>box_height</code> <code>int</code> <p>The desired height of the bounding boxes.</p> required <code>box_width</code> <code>int</code> <p>The desired width of the bounding boxes.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing bounding box coordinates for each centroid.     The output tensor has shape (n_centroids, 4, 2), where n_centroids is the number     of centroids, and the second dimension represents the four corner points of     the bounding boxes, each with x and y coordinates. The order of the corners     follows a clockwise arrangement: top-left, top-right, bottom-right, and     bottom-left.</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def make_centered_bboxes(\n    centroids: torch.Tensor, box_height: int, box_width: int\n) -&gt; torch.Tensor:\n    \"\"\"Create centered bounding boxes around centroid.\n\n    To be used with `kornia.geometry.transform.crop_and_resize`in the following\n    (clockwise) order: top-left, top-right, bottom-right and bottom-left.\n\n    Args:\n        centroids: A tensor of centroids with shape (n_centroids, 2), where n_centroids is the\n            number of centroids, and the last dimension represents x and y coordinates.\n        box_height: The desired height of the bounding boxes.\n        box_width: The desired width of the bounding boxes.\n\n    Returns:\n        torch.Tensor: A tensor containing bounding box coordinates for each centroid.\n            The output tensor has shape (n_centroids, 4, 2), where n_centroids is the number\n            of centroids, and the second dimension represents the four corner points of\n            the bounding boxes, each with x and y coordinates. The order of the corners\n            follows a clockwise arrangement: top-left, top-right, bottom-right, and\n            bottom-left.\n    \"\"\"\n    half_h = box_height / 2\n    half_w = box_width / 2\n\n    # Get x and y values from the centroids tensor.\n    x = centroids[..., 0]\n    y = centroids[..., 1]\n\n    # Calculate the corner points.\n    top_left = torch.stack([x - half_w, y - half_h], dim=-1)\n    top_right = torch.stack([x + half_w, y - half_h], dim=-1)\n    bottom_left = torch.stack([x - half_w, y + half_h], dim=-1)\n    bottom_right = torch.stack([x + half_w, y + half_h], dim=-1)\n\n    # Get bounding box.\n    corners = torch.stack([top_left, top_right, bottom_right, bottom_left], dim=-2)\n\n    offset = torch.tensor([[+0.5, +0.5], [-0.5, +0.5], [-0.5, -0.5], [+0.5, -0.5]]).to(\n        corners.device\n    )\n\n    return corners + offset\n</code></pre>"},{"location":"api/data/normalization/","title":"normalization","text":""},{"location":"api/data/normalization/#sleap_nn.data.normalization","title":"<code>sleap_nn.data.normalization</code>","text":"<p>This module implements data pipeline blocks for normalization operations.</p> <p>Functions:</p> Name Description <code>apply_normalization</code> <p>Normalize image tensor.</p> <code>convert_to_grayscale</code> <p>Convert given image to Grayscale image (single-channel).</p> <code>convert_to_rgb</code> <p>Convert given image to RGB image (three-channel image).</p>"},{"location":"api/data/normalization/#sleap_nn.data.normalization.apply_normalization","title":"<code>apply_normalization(image)</code>","text":"<p>Normalize image tensor.</p> Source code in <code>sleap_nn/data/normalization.py</code> <pre><code>def apply_normalization(image: torch.Tensor):\n    \"\"\"Normalize image tensor.\"\"\"\n    if not torch.is_floating_point(image):\n        image = image.to(torch.float32) / 255.0\n    return image\n</code></pre>"},{"location":"api/data/normalization/#sleap_nn.data.normalization.convert_to_grayscale","title":"<code>convert_to_grayscale(image)</code>","text":"<p>Convert given image to Grayscale image (single-channel).</p> <p>This functions converts the input image to grayscale only if the given image is not a single-channeled image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Tensor image of shape (..., 3, H, W)</p> required <p>Returns:</p> Type Description <p>Tensor image of shape (..., 1, H, W).</p> Source code in <code>sleap_nn/data/normalization.py</code> <pre><code>def convert_to_grayscale(image: torch.Tensor):\n    \"\"\"Convert given image to Grayscale image (single-channel).\n\n    This functions converts the input image to grayscale only if the given image is not\n    a single-channeled image.\n\n    Args:\n        image: Tensor image of shape (..., 3, H, W)\n\n    Returns:\n        Tensor image of shape (..., 1, H, W).\n    \"\"\"\n    if image.shape[-3] != 1:\n        image = F.rgb_to_grayscale(image, num_output_channels=1)\n    return image\n</code></pre>"},{"location":"api/data/normalization/#sleap_nn.data.normalization.convert_to_rgb","title":"<code>convert_to_rgb(image)</code>","text":"<p>Convert given image to RGB image (three-channel image).</p> <p>This functions converts the input image to RGB only if the given image is not a RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Tensor image of shape (..., 1, H, W)</p> required <p>Returns:</p> Type Description <p>Tensor image of shape (..., 3, H, W).</p> Source code in <code>sleap_nn/data/normalization.py</code> <pre><code>def convert_to_rgb(image: torch.Tensor):\n    \"\"\"Convert given image to RGB image (three-channel image).\n\n    This functions converts the input image to RGB only if the given image is not\n    a RGB image.\n\n    Args:\n        image: Tensor image of shape (..., 1, H, W)\n\n    Returns:\n        Tensor image of shape (..., 3, H, W).\n    \"\"\"\n    if image.shape[-3] != 3:\n        image = image.repeat(1, 3, 1, 1)\n    return image\n</code></pre>"},{"location":"api/data/providers/","title":"providers","text":""},{"location":"api/data/providers/#sleap_nn.data.providers","title":"<code>sleap_nn.data.providers</code>","text":"<p>This module implements pipeline blocks for reading input data such as labels.</p> <p>Classes:</p> Name Description <code>LabelsReader</code> <p>Thread module for reading images from sleap-io Labels object.</p> <code>VideoReader</code> <p>Thread module for reading frames from sleap-io Video object.</p> <p>Functions:</p> Name Description <code>get_max_height_width</code> <p>Return <code>(height, width)</code> that is the maximum of all videos.</p> <code>get_max_instances</code> <p>Get the maximum number of instances in a single LabeledFrame.</p> <code>process_lf</code> <p>Get sample dict from <code>sio.LabeledFrame</code>.</p>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader","title":"<code>LabelsReader</code>","text":"<p>               Bases: <code>Thread</code></p> <p>Thread module for reading images from sleap-io Labels object.</p> <p>This module will load the images from <code>.slp</code> files and pushes them as Tensors into a buffer queue as a dictionary with (image, frame index, video index, (height, width)) which are then batched and consumed during the inference process.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>sleap_io.Labels object that contains LabeledFrames that will be     accessed through a torchdata DataPipe.</p> <code>frame_buffer</code> <p>Frame buffer queue.</p> <code>instances_key</code> <p>If <code>True</code>, then instances are appended to the output dictionary.</p> <code>only_labeled_frames</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>only_suggested_frames</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attribute of the class.</p> <code>from_filename</code> <p>Create LabelsReader from a .slp filename.</p> <code>run</code> <p>Adds frames to the buffer queue.</p> <code>total_len</code> <p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>class LabelsReader(Thread):\n    \"\"\"Thread module for reading images from sleap-io Labels object.\n\n    This module will load the images from `.slp` files and pushes them as Tensors into a\n    buffer queue as a dictionary with (image, frame index, video index, (height, width))\n    which are then batched and consumed during the inference process.\n\n    Attributes:\n        labels: sleap_io.Labels object that contains LabeledFrames that will be\n                accessed through a torchdata DataPipe.\n        frame_buffer: Frame buffer queue.\n        instances_key: If `True`, then instances are appended to the output dictionary.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: sio.Labels,\n        frame_buffer: Queue,\n        instances_key: bool = False,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n    ):\n        \"\"\"Initialize attribute of the class.\"\"\"\n        super().__init__()\n        self.labels = labels\n        self.frame_buffer = frame_buffer\n        self.instances_key = instances_key\n        self.max_instances = get_max_instances(self.labels)\n\n        self.only_labeled_frames = only_labeled_frames\n        self.only_suggested_frames = only_suggested_frames\n\n        # Filter to only user labeled instances\n        if self.only_labeled_frames:\n            self.filtered_lfs = []\n            for lf in self.labels:\n                if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                    lf.instances = lf.user_instances\n                    self.filtered_lfs.append(lf)\n\n        # Filter to only unlabeled suggested instances\n        elif self.only_suggested_frames:\n            self.filtered_lfs = []\n            for suggestion in self.labels.suggestions:\n                lf = self.labels.find(suggestion.video, suggestion.frame_idx)[0]\n                if lf is None or not lf.has_user_instances:\n                    self.filtered_lfs.append(lf)\n\n        else:\n            self.filtered_lfs = [lf for lf in self.labels]\n\n    def total_len(self):\n        \"\"\"Returns the total number of frames in the video.\"\"\"\n        return len(self.filtered_lfs)\n\n    @property\n    def max_height_and_width(self) -&gt; Tuple[int, int]:\n        \"\"\"Return `(height, width)` of frames in the video.\"\"\"\n        return max(video.shape[1] for video in self.labels.videos), max(\n            video.shape[2] for video in self.labels.videos\n        )\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename: str,\n        queue_maxsize: int,\n        instances_key: bool = False,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n    ):\n        \"\"\"Create LabelsReader from a .slp filename.\"\"\"\n        labels = sio.load_slp(filename)\n        frame_buffer = Queue(maxsize=queue_maxsize)\n        return cls(\n            labels,\n            frame_buffer,\n            instances_key,\n            only_labeled_frames,\n            only_suggested_frames,\n        )\n\n    def run(self):\n        \"\"\"Adds frames to the buffer queue.\"\"\"\n        try:\n            for lf in self.filtered_lfs:\n                img = lf.image\n                img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n                img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n                sample = {\n                    \"image\": torch.from_numpy(img),\n                    \"frame_idx\": torch.tensor(lf.frame_idx, dtype=torch.int32),\n                    \"video_idx\": torch.tensor(\n                        self.labels.videos.index(lf.video), dtype=torch.int32\n                    ),\n                    \"orig_size\": torch.Tensor(img.shape[-2:]),\n                }\n\n                if self.instances_key:\n                    instances = []\n                    for inst in lf:\n                        if not inst.is_empty:\n                            instances.append(inst.numpy())\n                    instances = np.stack(instances, axis=0)\n\n                    # Add singleton time dimension for single frames.\n                    instances = np.expand_dims(\n                        instances, axis=0\n                    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n                    instances = torch.from_numpy(instances.astype(\"float32\"))\n\n                    num_instances, nodes = instances.shape[1:3]\n\n                    # append with nans for broadcasting\n                    if self.max_instances != 1:\n                        nans = torch.full(\n                            (1, np.abs(self.max_instances - num_instances), nodes, 2),\n                            torch.nan,\n                        )\n                        instances = torch.cat(\n                            [instances, nans], dim=1\n                        )  # (n_samples, max_instances, num_nodes, 2)\n\n                    sample[\"instances\"] = instances\n\n                self.frame_buffer.put(sample)\n\n        except Exception as e:\n            logger.error(\n                f\"Error when reading labelled frame. Stopping labels reader.\\n{e}\"\n            )\n\n        finally:\n            self.frame_buffer.put(\n                {\n                    \"image\": None,\n                    \"frame_idx\": None,\n                    \"video_idx\": None,\n                    \"orig_size\": None,\n                }\n            )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.max_height_and_width","title":"<code>max_height_and_width</code>  <code>property</code>","text":"<p>Return <code>(height, width)</code> of frames in the video.</p>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.__init__","title":"<code>__init__(labels, frame_buffer, instances_key=False, only_labeled_frames=False, only_suggested_frames=False)</code>","text":"<p>Initialize attribute of the class.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def __init__(\n    self,\n    labels: sio.Labels,\n    frame_buffer: Queue,\n    instances_key: bool = False,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n):\n    \"\"\"Initialize attribute of the class.\"\"\"\n    super().__init__()\n    self.labels = labels\n    self.frame_buffer = frame_buffer\n    self.instances_key = instances_key\n    self.max_instances = get_max_instances(self.labels)\n\n    self.only_labeled_frames = only_labeled_frames\n    self.only_suggested_frames = only_suggested_frames\n\n    # Filter to only user labeled instances\n    if self.only_labeled_frames:\n        self.filtered_lfs = []\n        for lf in self.labels:\n            if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                lf.instances = lf.user_instances\n                self.filtered_lfs.append(lf)\n\n    # Filter to only unlabeled suggested instances\n    elif self.only_suggested_frames:\n        self.filtered_lfs = []\n        for suggestion in self.labels.suggestions:\n            lf = self.labels.find(suggestion.video, suggestion.frame_idx)[0]\n            if lf is None or not lf.has_user_instances:\n                self.filtered_lfs.append(lf)\n\n    else:\n        self.filtered_lfs = [lf for lf in self.labels]\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.from_filename","title":"<code>from_filename(filename, queue_maxsize, instances_key=False, only_labeled_frames=False, only_suggested_frames=False)</code>  <code>classmethod</code>","text":"<p>Create LabelsReader from a .slp filename.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>@classmethod\ndef from_filename(\n    cls,\n    filename: str,\n    queue_maxsize: int,\n    instances_key: bool = False,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n):\n    \"\"\"Create LabelsReader from a .slp filename.\"\"\"\n    labels = sio.load_slp(filename)\n    frame_buffer = Queue(maxsize=queue_maxsize)\n    return cls(\n        labels,\n        frame_buffer,\n        instances_key,\n        only_labeled_frames,\n        only_suggested_frames,\n    )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.run","title":"<code>run()</code>","text":"<p>Adds frames to the buffer queue.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def run(self):\n    \"\"\"Adds frames to the buffer queue.\"\"\"\n    try:\n        for lf in self.filtered_lfs:\n            img = lf.image\n            img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n            img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n            sample = {\n                \"image\": torch.from_numpy(img),\n                \"frame_idx\": torch.tensor(lf.frame_idx, dtype=torch.int32),\n                \"video_idx\": torch.tensor(\n                    self.labels.videos.index(lf.video), dtype=torch.int32\n                ),\n                \"orig_size\": torch.Tensor(img.shape[-2:]),\n            }\n\n            if self.instances_key:\n                instances = []\n                for inst in lf:\n                    if not inst.is_empty:\n                        instances.append(inst.numpy())\n                instances = np.stack(instances, axis=0)\n\n                # Add singleton time dimension for single frames.\n                instances = np.expand_dims(\n                    instances, axis=0\n                )  # (n_samples=1, num_instances, num_nodes, 2)\n\n                instances = torch.from_numpy(instances.astype(\"float32\"))\n\n                num_instances, nodes = instances.shape[1:3]\n\n                # append with nans for broadcasting\n                if self.max_instances != 1:\n                    nans = torch.full(\n                        (1, np.abs(self.max_instances - num_instances), nodes, 2),\n                        torch.nan,\n                    )\n                    instances = torch.cat(\n                        [instances, nans], dim=1\n                    )  # (n_samples, max_instances, num_nodes, 2)\n\n                sample[\"instances\"] = instances\n\n            self.frame_buffer.put(sample)\n\n    except Exception as e:\n        logger.error(\n            f\"Error when reading labelled frame. Stopping labels reader.\\n{e}\"\n        )\n\n    finally:\n        self.frame_buffer.put(\n            {\n                \"image\": None,\n                \"frame_idx\": None,\n                \"video_idx\": None,\n                \"orig_size\": None,\n            }\n        )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.total_len","title":"<code>total_len()</code>","text":"<p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def total_len(self):\n    \"\"\"Returns the total number of frames in the video.\"\"\"\n    return len(self.filtered_lfs)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader","title":"<code>VideoReader</code>","text":"<p>               Bases: <code>Thread</code></p> <p>Thread module for reading frames from sleap-io Video object.</p> <p>This module will load the frames from video and pushes them as Tensors into a buffer queue as a dictionary with (image, frame index, video index, (height, width)) which are then batched and consumed during the inference process.</p> <p>Attributes:</p> Name Type Description <code>video</code> <p>sleap_io.Video object that contains images that will be     accessed through a torchdata DataPipe.</p> <code>frame_buffer</code> <p>Frame buffer queue.</p> <code>frames</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attribute of the class.</p> <code>from_filename</code> <p>Create VideoReader from a .slp filename.</p> <code>from_video</code> <p>Create VideoReader from a video object.</p> <code>run</code> <p>Adds frames to the buffer queue.</p> <code>total_len</code> <p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>class VideoReader(Thread):\n    \"\"\"Thread module for reading frames from sleap-io Video object.\n\n    This module will load the frames from video and pushes them as Tensors into a buffer\n    queue as a dictionary with (image, frame index, video index, (height, width))\n    which are then batched and consumed during the inference process.\n\n    Attributes:\n        video: sleap_io.Video object that contains images that will be\n                accessed through a torchdata DataPipe.\n        frame_buffer: Frame buffer queue.\n        frames: List of frames indices. If `None`, all frames in the video are used.\n    \"\"\"\n\n    def __init__(\n        self,\n        video: sio.Video,\n        frame_buffer: Queue,\n        frames: Optional[list] = None,\n    ):\n        \"\"\"Initialize attribute of the class.\"\"\"\n        super().__init__()\n        self.video = video\n        self.frame_buffer = frame_buffer\n        self.frames = frames\n        if self.frames is None:\n            self.frames = [x for x in range(0, len(self.video))]\n\n    def total_len(self):\n        \"\"\"Returns the total number of frames in the video.\"\"\"\n        return len(self.frames)\n\n    @property\n    def max_height_and_width(self) -&gt; Tuple[int, int]:\n        \"\"\"Return `(height, width)` of frames in the video.\"\"\"\n        return self.video.shape[1], self.video.shape[2]\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename: str,\n        queue_maxsize: int,\n        frames: Optional[list] = None,\n        dataset: Optional[str] = None,\n        input_format: str = \"channels_last\",\n    ):\n        \"\"\"Create VideoReader from a .slp filename.\"\"\"\n        video = sio.load_video(filename, dataset=dataset, input_format=input_format)\n        frame_buffer = Queue(maxsize=queue_maxsize)\n        return cls(video, frame_buffer, frames)\n\n    @classmethod\n    def from_video(\n        cls,\n        video: sio.Video,\n        queue_maxsize: int,\n        frames: Optional[list] = None,\n    ):\n        \"\"\"Create VideoReader from a video object.\"\"\"\n        frame_buffer = Queue(maxsize=queue_maxsize)\n        return cls(video, frame_buffer, frames)\n\n    def run(self):\n        \"\"\"Adds frames to the buffer queue.\"\"\"\n        try:\n            for idx in self.frames:\n                img = self.video[idx]\n                img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n                img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n                self.frame_buffer.put(\n                    {\n                        \"image\": torch.from_numpy(img),\n                        \"frame_idx\": torch.tensor(idx, dtype=torch.int32),\n                        \"video_idx\": torch.tensor(0, dtype=torch.int32),\n                        \"orig_size\": torch.Tensor(img.shape[-2:]),\n                    }\n                )\n\n        except Exception as e:\n            logger.error(f\"Error when reading video frame. Stopping video reader.\\n{e}\")\n\n        finally:\n            self.frame_buffer.put(\n                {\n                    \"image\": None,\n                    \"frame_idx\": None,\n                    \"video_idx\": None,\n                    \"orig_size\": None,\n                }\n            )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.max_height_and_width","title":"<code>max_height_and_width</code>  <code>property</code>","text":"<p>Return <code>(height, width)</code> of frames in the video.</p>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.__init__","title":"<code>__init__(video, frame_buffer, frames=None)</code>","text":"<p>Initialize attribute of the class.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def __init__(\n    self,\n    video: sio.Video,\n    frame_buffer: Queue,\n    frames: Optional[list] = None,\n):\n    \"\"\"Initialize attribute of the class.\"\"\"\n    super().__init__()\n    self.video = video\n    self.frame_buffer = frame_buffer\n    self.frames = frames\n    if self.frames is None:\n        self.frames = [x for x in range(0, len(self.video))]\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.from_filename","title":"<code>from_filename(filename, queue_maxsize, frames=None, dataset=None, input_format='channels_last')</code>  <code>classmethod</code>","text":"<p>Create VideoReader from a .slp filename.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>@classmethod\ndef from_filename(\n    cls,\n    filename: str,\n    queue_maxsize: int,\n    frames: Optional[list] = None,\n    dataset: Optional[str] = None,\n    input_format: str = \"channels_last\",\n):\n    \"\"\"Create VideoReader from a .slp filename.\"\"\"\n    video = sio.load_video(filename, dataset=dataset, input_format=input_format)\n    frame_buffer = Queue(maxsize=queue_maxsize)\n    return cls(video, frame_buffer, frames)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.from_video","title":"<code>from_video(video, queue_maxsize, frames=None)</code>  <code>classmethod</code>","text":"<p>Create VideoReader from a video object.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>@classmethod\ndef from_video(\n    cls,\n    video: sio.Video,\n    queue_maxsize: int,\n    frames: Optional[list] = None,\n):\n    \"\"\"Create VideoReader from a video object.\"\"\"\n    frame_buffer = Queue(maxsize=queue_maxsize)\n    return cls(video, frame_buffer, frames)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.run","title":"<code>run()</code>","text":"<p>Adds frames to the buffer queue.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def run(self):\n    \"\"\"Adds frames to the buffer queue.\"\"\"\n    try:\n        for idx in self.frames:\n            img = self.video[idx]\n            img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n            img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n            self.frame_buffer.put(\n                {\n                    \"image\": torch.from_numpy(img),\n                    \"frame_idx\": torch.tensor(idx, dtype=torch.int32),\n                    \"video_idx\": torch.tensor(0, dtype=torch.int32),\n                    \"orig_size\": torch.Tensor(img.shape[-2:]),\n                }\n            )\n\n    except Exception as e:\n        logger.error(f\"Error when reading video frame. Stopping video reader.\\n{e}\")\n\n    finally:\n        self.frame_buffer.put(\n            {\n                \"image\": None,\n                \"frame_idx\": None,\n                \"video_idx\": None,\n                \"orig_size\": None,\n            }\n        )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.total_len","title":"<code>total_len()</code>","text":"<p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def total_len(self):\n    \"\"\"Returns the total number of frames in the video.\"\"\"\n    return len(self.frames)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.get_max_height_width","title":"<code>get_max_height_width(labels)</code>","text":"<p>Return <code>(height, width)</code> that is the maximum of all videos.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def get_max_height_width(labels: sio.Labels) -&gt; Tuple[int, int]:\n    \"\"\"Return `(height, width)` that is the maximum of all videos.\"\"\"\n    return int(max(video.shape[1] for video in labels.videos)), int(\n        max(video.shape[2] for video in labels.videos)\n    )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.get_max_instances","title":"<code>get_max_instances(labels)</code>","text":"<p>Get the maximum number of instances in a single LabeledFrame.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>sleap_io.Labels object that contains LabeledFrames.</p> required <p>Returns:</p> Type Description <p>Maximum number of instances that could occur in a single LabeledFrame.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def get_max_instances(labels: sio.Labels):\n    \"\"\"Get the maximum number of instances in a single LabeledFrame.\n\n    Args:\n        labels: sleap_io.Labels object that contains LabeledFrames.\n\n    Returns:\n        Maximum number of instances that could occur in a single LabeledFrame.\n    \"\"\"\n    max_instances = -1\n    for lf in labels:\n        num_inst = len(lf.instances)\n        if num_inst &gt; max_instances:\n            max_instances = num_inst\n    return max_instances\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.process_lf","title":"<code>process_lf(lf, video_idx, max_instances, user_instances_only=True)</code>","text":"<p>Get sample dict from <code>sio.LabeledFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LabeledFrame</code> <p>Input <code>sio.LabeledFrame</code>.</p> required <code>video_idx</code> <code>int</code> <p>Video index of the given lf.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances that could occur in a single LabeledFrame.</p> required <code>user_instances_only</code> <code>bool</code> <p>True if filter labels only to user instances else False. Default: True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with image, instancs, frame index, video index, original image size and number of instances.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def process_lf(\n    lf: sio.LabeledFrame,\n    video_idx: int,\n    max_instances: int,\n    user_instances_only: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"Get sample dict from `sio.LabeledFrame`.\n\n    Args:\n        lf: Input `sio.LabeledFrame`.\n        video_idx: Video index of the given lf.\n        max_instances: Maximum number of instances that could occur in a single LabeledFrame.\n        user_instances_only: True if filter labels only to user instances else False.\n            Default: True.\n\n    Returns:\n        Dict with image, instancs, frame index, video index, original image size and\n        number of instances.\n\n    \"\"\"\n    # Filter to user instances\n    if user_instances_only:\n        if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n            lf.instances = lf.user_instances\n\n    image = np.transpose(lf.image, (2, 0, 1))  # HWC -&gt; CHW\n\n    instances = []\n    for inst in lf:\n        if not inst.is_empty:\n            instances.append(inst.numpy())\n    instances = np.stack(instances, axis=0)\n\n    # Add singleton time dimension for single frames.\n    image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n    instances = np.expand_dims(\n        instances, axis=0\n    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n    instances = torch.from_numpy(instances.astype(\"float32\"))\n\n    num_instances, nodes = instances.shape[1:3]\n    img_height, img_width = image.shape[-2:]\n\n    # append with nans for broadcasting\n    if max_instances != 1:\n        nans = torch.full(\n            (1, np.abs(max_instances - num_instances), nodes, 2), torch.nan\n        )\n        instances = torch.cat(\n            [instances, nans], dim=1\n        )  # (n_samples, max_instances, num_nodes, 2)\n\n    ex = {\n        \"image\": torch.from_numpy(image),\n        \"instances\": instances,\n        \"video_idx\": torch.tensor(video_idx, dtype=torch.int32),\n        \"frame_idx\": torch.tensor(lf.frame_idx, dtype=torch.int32),\n        \"orig_size\": torch.Tensor([img_height, img_width]),\n        \"num_instances\": num_instances,\n    }\n\n    return ex\n</code></pre>"},{"location":"api/data/resizing/","title":"resizing","text":""},{"location":"api/data/resizing/#sleap_nn.data.resizing","title":"<code>sleap_nn.data.resizing</code>","text":"<p>This module implements image resizing and padding.</p> <p>Functions:</p> Name Description <code>apply_pad_to_stride</code> <p>Pad an image to meet a max stride constraint.</p> <code>apply_resizer</code> <p>Rescale image and keypoints by a scale factor.</p> <code>apply_sizematcher</code> <p>Apply scaling and padding to image to (max_height, max_width) shape.</p> <code>find_padding_for_stride</code> <p>Compute padding required to ensure image is divisible by a stride.</p> <code>resize_image</code> <p>Rescale an image by a scale factor.</p>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.apply_pad_to_stride","title":"<code>apply_pad_to_stride(image, max_stride)</code>","text":"<p>Pad an image to meet a max stride constraint.</p> <p>This is useful for ensuring there is no size mismatch between an image and the output tensors after multiple downsampling and upsampling steps.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Single image tensor of shape (..., channels, height, width).</p> required <code>max_stride</code> <code>int</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by. This is the ratio between the length of the image and the length of the smallest tensor it is converted to. This is typically <code>2 ** n_down_blocks</code>, where <code>n_down_blocks</code> is the number of 2-strided reduction layers in the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The input image with 0-padding applied to the bottom and/or right such that the new shape's height and width are both divisible by <code>max_stride</code>.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def apply_pad_to_stride(image: torch.Tensor, max_stride: int) -&gt; torch.Tensor:\n    \"\"\"Pad an image to meet a max stride constraint.\n\n    This is useful for ensuring there is no size mismatch between an image and the\n    output tensors after multiple downsampling and upsampling steps.\n\n    Args:\n        image: Single image tensor of shape (..., channels, height, width).\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by. This is the ratio between the length of the image and the\n            length of the smallest tensor it is converted to. This is typically\n            `2 ** n_down_blocks`, where `n_down_blocks` is the number of 2-strided\n            reduction layers in the model.\n\n    Returns:\n        The input image with 0-padding applied to the bottom and/or right such that the\n        new shape's height and width are both divisible by `max_stride`.\n    \"\"\"\n    if max_stride &gt; 1:\n        image_height, image_width = image.shape[-2:]\n        pad_height, pad_width = find_padding_for_stride(\n            image_height=image_height,\n            image_width=image_width,\n            max_stride=max_stride,\n        )\n\n        if pad_height &gt; 0 or pad_width &gt; 0:\n            image = F.pad(\n                image,\n                (0, pad_width, 0, pad_height),\n                mode=\"constant\",\n            ).to(torch.float32)\n    return image\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.apply_resizer","title":"<code>apply_resizer(image, instances, scale=1.0)</code>","text":"<p>Rescale image and keypoints by a scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Image tensor of shape (..., channels, height, width)</p> required <code>instances</code> <code>Tensor</code> <p>Keypoints tensor.</p> required <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float scalar. Default: 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>Tuple with resized image and corresponding keypoints.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def apply_resizer(image: torch.Tensor, instances: torch.Tensor, scale: float = 1.0):\n    \"\"\"Rescale image and keypoints by a scale factor.\n\n    Args:\n        image: Image tensor of shape (..., channels, height, width)\n        instances: Keypoints tensor.\n        scale: Factor to resize the image dimensions by, specified as a float\n            scalar. Default: 1.0.\n\n    Returns:\n        Tuple with resized image and corresponding keypoints.\n    \"\"\"\n    if scale != 1.0:\n        image = resize_image(image, scale)\n        instances = instances * scale\n    return image, instances\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.apply_sizematcher","title":"<code>apply_sizematcher(image, max_height=None, max_width=None)</code>","text":"<p>Apply scaling and padding to image to (max_height, max_width) shape.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def apply_sizematcher(\n    image: torch.Tensor,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n):\n    \"\"\"Apply scaling and padding to image to (max_height, max_width) shape.\"\"\"\n    img_height, img_width = image.shape[-2:]\n    # pad images to max_height and max_width\n    if max_height is None:\n        max_height = img_height\n    if max_width is None:\n        max_width = img_width\n    if img_height != max_height or img_width != max_width:\n        hratio = max_height / img_height\n        wratio = max_width / img_width\n\n        if hratio &gt; wratio:\n            eff_scale_ratio = wratio\n            target_h = int(round(img_height * wratio))\n            target_w = int(round(img_width * wratio))\n        else:\n            eff_scale_ratio = hratio\n            target_w = int(round(img_width * hratio))\n            target_h = int(round(img_height * hratio))\n\n        image = tvf.resize(image, size=(target_h, target_w))\n\n        pad_height = max_height - target_h\n        pad_width = max_width - target_w\n\n        image = F.pad(\n            image,\n            (0, pad_width, 0, pad_height),\n            mode=\"constant\",\n        ).to(torch.float32)\n\n        return image, eff_scale_ratio\n    else:\n        return image, 1.0\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.find_padding_for_stride","title":"<code>find_padding_for_stride(image_height, image_width, max_stride)</code>","text":"<p>Compute padding required to ensure image is divisible by a stride.</p> <p>This function is useful for determining how to pad images such that they will not have issues with divisibility after repeated pooling steps.</p> <p>Parameters:</p> Name Type Description Default <code>image_height</code> <code>int</code> <p>Scalar integer specifying the image height (rows).</p> required <code>image_width</code> <code>int</code> <p>Scalar integer specifying the image height (columns).</p> required <code>max_stride</code> <code>int</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>A tuple of (pad_height, pad_width), integers with the number of pixels that the image would need to be padded by to meet the divisibility requirement.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def find_padding_for_stride(\n    image_height: int, image_width: int, max_stride: int\n) -&gt; Tuple[int, int]:\n    \"\"\"Compute padding required to ensure image is divisible by a stride.\n\n    This function is useful for determining how to pad images such that they will not\n    have issues with divisibility after repeated pooling steps.\n\n    Args:\n        image_height: Scalar integer specifying the image height (rows).\n        image_width: Scalar integer specifying the image height (columns).\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n\n    Returns:\n        A tuple of (pad_height, pad_width), integers with the number of pixels that the\n        image would need to be padded by to meet the divisibility requirement.\n    \"\"\"\n    # The outer-most modulo handles edge case when image_height % max_stride == 0\n    pad_height = (max_stride - (image_height % max_stride)) % max_stride\n    pad_width = (max_stride - (image_width % max_stride)) % max_stride\n    return pad_height, pad_width\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.resize_image","title":"<code>resize_image(image, scale)</code>","text":"<p>Rescale an image by a scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Single image tensor of shape (..., channels, height, width).</p> required <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float scalar.</p> required <p>Returns:</p> Type Description <p>The resized image tensor of the same dtype but scaled height and width.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def resize_image(image: torch.Tensor, scale: float):\n    \"\"\"Rescale an image by a scale factor.\n\n    Args:\n        image: Single image tensor of shape (..., channels, height, width).\n        scale: Factor to resize the image dimensions by, specified as a float\n            scalar.\n\n    Returns:\n        The resized image tensor of the same dtype but scaled height and width.\n    \"\"\"\n    img_height, img_width = image.shape[-2:]\n    new_size = [int(img_height * scale), int(img_width * scale)]\n    image = tvf.resize(image, size=new_size)\n    return image\n</code></pre>"},{"location":"api/data/utils/","title":"utils","text":""},{"location":"api/data/utils/#sleap_nn.data.utils","title":"<code>sleap_nn.data.utils</code>","text":"<p>Miscellaneous utility functions for data processing.</p> <p>Functions:</p> Name Description <code>check_cache_memory</code> <p>Check memory requirements for in-memory caching dataset pipeline.</p> <code>check_memory</code> <p>Return memory required for caching the image samples from a single labels object.</p> <code>ensure_list</code> <p>Convert the input into a list if it is not already.</p> <code>expand_to_rank</code> <p>Expand a tensor to a target rank by adding singleton dimensions in PyTorch.</p> <code>gaussian_pdf</code> <p>Compute the PDF of an unnormalized 0-centered Gaussian distribution.</p> <code>make_grid_vectors</code> <p>Make sampling grid vectors from image dimensions.</p>"},{"location":"api/data/utils/#sleap_nn.data.utils.check_cache_memory","title":"<code>check_cache_memory(train_labels, val_labels, memory_buffer=0.2)</code>","text":"<p>Check memory requirements for in-memory caching dataset pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels</code> <code>List[Labels]</code> <p>List of <code>sleap_io.Labels</code> objects for training data.</p> required <code>val_labels</code> <code>List[Labels]</code> <p>List of <code>sleap_io.Labels</code> objects for validation data.</p> required <code>memory_buffer</code> <code>float</code> <p>Fraction of the total image memory required for caching that should be reserved as a buffer.</p> <code>0.2</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the total memory required for caching is within available system     memory, False otherwise.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def check_cache_memory(\n    train_labels: List[sio.Labels],\n    val_labels: List[sio.Labels],\n    memory_buffer: float = 0.2,\n) -&gt; bool:\n    \"\"\"Check memory requirements for in-memory caching dataset pipeline.\n\n    Args:\n        train_labels: List of `sleap_io.Labels` objects for training data.\n        val_labels: List of `sleap_io.Labels` objects for validation data.\n        memory_buffer: Fraction of the total image memory required for caching that\n            should be reserved as a buffer.\n\n    Returns:\n        bool: True if the total memory required for caching is within available system\n            memory, False otherwise.\n    \"\"\"\n    train_cache_memory_final = 0\n    val_cache_memory_final = 0\n    for train, val in zip(train_labels, val_labels):\n        train_cache_memory = check_memory(train)\n        val_cache_memory = check_memory(val)\n        train_cache_memory_final += train_cache_memory\n        val_cache_memory_final += val_cache_memory\n\n    total_cache_memory = train_cache_memory_final + val_cache_memory_final\n    total_cache_memory += memory_buffer * total_cache_memory  # memory required in bytes\n    available_memory = psutil.virtual_memory().available  # available memory in bytes\n\n    if total_cache_memory &gt; available_memory:\n        return False\n    return True\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.check_memory","title":"<code>check_memory(labels)</code>","text":"<p>Return memory required for caching the image samples from a single labels object.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>A <code>sleap_io.Labels</code> object containing the labels for a single dataset.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Memory in bytes required to cache the image samples from the labels object.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def check_memory(\n    labels: sio.Labels,\n) -&gt; int:\n    \"\"\"Return memory required for caching the image samples from a single labels object.\n\n    Args:\n        labels: A `sleap_io.Labels` object containing the labels for a single dataset.\n\n    Returns:\n        Memory in bytes required to cache the image samples from the labels object.\n    \"\"\"\n    imgs_bytes = []\n    for label in labels:\n        if label.image is not None:\n            img = label.image\n            img_bytes = img.nbytes\n            imgs_bytes.append(img_bytes)\n        else:\n            raise ValueError(\n                \"Labels object contains a label with no image data, which is required for training.\"\n            )\n    img_mem = sum(imgs_bytes)\n    return img_mem\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.ensure_list","title":"<code>ensure_list(x)</code>","text":"<p>Convert the input into a list if it is not already.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def ensure_list(x: Any) -&gt; List[Any]:\n    \"\"\"Convert the input into a list if it is not already.\"\"\"\n    if not isinstance(x, list):\n        return [x]\n    return x\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.expand_to_rank","title":"<code>expand_to_rank(x, target_rank, prepend=True)</code>","text":"<p>Expand a tensor to a target rank by adding singleton dimensions in PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Any <code>torch.Tensor</code> with rank &lt;= <code>target_rank</code>. If the rank is higher than <code>target_rank</code>, the tensor will be returned with the same shape.</p> required <code>target_rank</code> <code>int</code> <p>Rank to expand the input to.</p> required <code>prepend</code> <code>bool</code> <p>If True, singleton dimensions are added before the first axis of the data. If False, singleton dimensions are added after the last axis.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The expanded tensor of the same dtype as the input, but with rank <code>target_rank</code>. The output has the same exact data as the input tensor and will be identical if they are both flattened.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def expand_to_rank(\n    x: torch.Tensor, target_rank: int, prepend: bool = True\n) -&gt; torch.Tensor:\n    \"\"\"Expand a tensor to a target rank by adding singleton dimensions in PyTorch.\n\n    Args:\n        x: Any `torch.Tensor` with rank &lt;= `target_rank`. If the rank is higher than\n            `target_rank`, the tensor will be returned with the same shape.\n        target_rank: Rank to expand the input to.\n        prepend: If True, singleton dimensions are added before the first axis of the\n            data. If False, singleton dimensions are added after the last axis.\n\n    Returns:\n        The expanded tensor of the same dtype as the input, but with rank `target_rank`.\n        The output has the same exact data as the input tensor and will be identical if\n        they are both flattened.\n    \"\"\"\n    n_singleton_dims = max(target_rank - x.dim(), 0)\n    singleton_dims = [1] * n_singleton_dims\n    if prepend:\n        new_shape = singleton_dims + list(x.shape)\n    else:\n        new_shape = list(x.shape) + singleton_dims\n    return x.reshape(new_shape)\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.gaussian_pdf","title":"<code>gaussian_pdf(x, sigma)</code>","text":"<p>Compute the PDF of an unnormalized 0-centered Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor of dtype torch.float32 with values to compute the PDF for.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of the same shape as <code>x</code>, but with values of a PDF of an unnormalized Gaussian distribution. Values of 0 have an unnormalized PDF value of 1.0.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def gaussian_pdf(x: torch.Tensor, sigma: float) -&gt; torch.Tensor:\n    \"\"\"Compute the PDF of an unnormalized 0-centered Gaussian distribution.\n\n    Args:\n        x: A tensor of dtype torch.float32 with values to compute the PDF for.\n        sigma: Standard deviation of the Gaussian distribution.\n\n    Returns:\n        A tensor of the same shape as `x`, but with values of a PDF of an unnormalized\n        Gaussian distribution. Values of 0 have an unnormalized PDF value of 1.0.\n    \"\"\"\n    return torch.exp(-(x**2) / (2 * sigma**2))\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.make_grid_vectors","title":"<code>make_grid_vectors(image_height, image_width, output_stride=1)</code>","text":"<p>Make sampling grid vectors from image dimensions.</p> <p>This is a useful function for creating the x- and y-vectors that define a sampling grid over an image space. These vectors can be used to generate a full meshgrid or for equivalent broadcasting operations.</p> <p>Parameters:</p> Name Type Description Default <code>image_height</code> <code>int</code> <p>Height of the image grid that will be sampled, specified as a scalar integer.</p> required <code>image_width</code> <code>int</code> <p>width of the image grid that will be sampled, specified as a scalar integer.</p> required <code>output_stride</code> <code>int</code> <p>Sampling step size, specified as a scalar integer. This can be used to specify a sampling grid that has a smaller shape than the image grid but with values span the same range. This can be thought of as the reciprocal of the output scale, i.e., it will induce subsampling when set to values greater than 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of grid vectors (xv, yv). These are tensors of dtype tf.float32 with shapes (grid_width,) and (grid_height,) respectively.</p> <p>The grid dimensions are calculated as:     grid_width = image_width // output_stride     grid_height = image_height // output_stride</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def make_grid_vectors(\n    image_height: int, image_width: int, output_stride: int = 1\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Make sampling grid vectors from image dimensions.\n\n    This is a useful function for creating the x- and y-vectors that define a sampling\n    grid over an image space. These vectors can be used to generate a full meshgrid or\n    for equivalent broadcasting operations.\n\n    Args:\n        image_height: Height of the image grid that will be sampled, specified as a\n            scalar integer.\n        image_width: width of the image grid that will be sampled, specified as a\n            scalar integer.\n        output_stride: Sampling step size, specified as a scalar integer. This can be\n            used to specify a sampling grid that has a smaller shape than the image\n            grid but with values span the same range. This can be thought of as the\n            reciprocal of the output scale, i.e., it will induce subsampling when set to\n            values greater than 1.\n\n    Returns:\n        Tuple of grid vectors (xv, yv). These are tensors of dtype tf.float32 with\n        shapes (grid_width,) and (grid_height,) respectively.\n\n        The grid dimensions are calculated as:\n            grid_width = image_width // output_stride\n            grid_height = image_height // output_stride\n    \"\"\"\n    xv = torch.arange(0, image_width, step=output_stride, dtype=torch.float32)\n    yv = torch.arange(0, image_height, step=output_stride, dtype=torch.float32)\n    return xv, yv\n</code></pre>"},{"location":"api/inference/","title":"inference","text":""},{"location":"api/inference/#sleap_nn.inference","title":"<code>sleap_nn.inference</code>","text":"<p>Inference-related modules.</p> <p>Modules:</p> Name Description <code>bottomup</code> <p>Inference modules for BottomUp models.</p> <code>identity</code> <p>Utilities for models that learn identity.</p> <code>paf_grouping</code> <p>This module provides a set of utilities for grouping peaks based on PAFs.</p> <code>peak_finding</code> <p>Peak finding for inference.</p> <code>predictors</code> <p>Predictors for running inference.</p> <code>single_instance</code> <p>Inference modules for SingleInstance models.</p> <code>topdown</code> <p>Inference modules for TopDown centroid and centered-instance models.</p> <code>utils</code> <p>Miscellaneous utility functions for Inference modules.</p>"},{"location":"api/inference/bottomup/","title":"bottomup","text":""},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup","title":"<code>sleap_nn.inference.bottomup</code>","text":"<p>Inference modules for BottomUp models.</p> <p>Classes:</p> Name Description <code>BottomUpInferenceModel</code> <p>BottomUp Inference model.</p> <code>BottomUpMultiClassInferenceModel</code> <p>BottomUp Inference model for multi-class models.</p>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpInferenceModel","title":"<code>BottomUpInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>BottomUp Inference model.</p> <p>This model encapsulates the bottom-up approach. The images are passed to a peak detector to get the predicted instances and then fed into PAF to combine nodes belonging to the same instance.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on MultiInstanceConfMaps.</p> <code>paf_scorer</code> <p>A <code>sleap_nn.inference.paf_grouping.PAFScorer</code> instance configured to group instances based on peaks and PAFs produced by the model.</p> <code>cms_output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>pafs_output_stride</code> <p>Output stride of the model, denoting the scale of the output pafs relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>return_pafs</code> <p>If <code>True</code>, the part affinity fields will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>return_paf_graph</code> <p>If <code>True</code>, the part affinity field graph will be returned together with the predicted instances. The graph is obtained by parsing the part affinity fields with the <code>paf_scorer</code> instance and is an intermediate representation used during instance grouping.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>class BottomUpInferenceModel(L.LightningModule):\n    \"\"\"BottomUp Inference model.\n\n    This model encapsulates the bottom-up approach. The images are passed to a peak detector\n    to get the predicted instances and then fed into PAF to combine nodes belonging to\n    the same instance.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            MultiInstanceConfMaps.\n        paf_scorer: A `sleap_nn.inference.paf_grouping.PAFScorer` instance configured to group\n            instances based on peaks and PAFs produced by the model.\n        cms_output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        pafs_output_stride: Output stride of the model, denoting the scale of the output\n            pafs relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        return_pafs: If `True`, the part affinity fields will be returned together with\n            the predicted instances. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        return_paf_graph: If `True`, the part affinity field graph will be returned\n            together with the predicted instances. The graph is obtained by parsing the\n            part affinity fields with the `paf_scorer` instance and is an intermediate\n            representation used during instance grouping.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        paf_scorer: PAFScorer,\n        cms_output_stride: Optional[int] = None,\n        pafs_output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = \"integral\",\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        return_pafs: Optional[bool] = False,\n        return_paf_graph: Optional[bool] = False,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__()\n        self.torch_model = torch_model\n        self.paf_scorer = paf_scorer\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.cms_output_stride = cms_output_stride\n        self.pafs_output_stride = pafs_output_stride\n        self.return_confmaps = return_confmaps\n        self.return_pafs = return_pafs\n        self.return_paf_graph = return_paf_graph\n        self.input_scale = input_scale\n\n    def _generate_cms_peaks(self, cms):\n        # TODO: append nans to batch them -&gt; tensor (vectorize the initial paf grouping steps)\n        peaks, peak_vals, sample_inds, peak_channel_inds = find_local_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n        # Adjust for stride and scale.\n        peaks = peaks * self.cms_output_stride  # (n_centroids, 2)\n\n        cms_peaks, cms_peak_vals, cms_peak_channel_inds = [], [], []\n\n        for b in range(self.batch_size):\n            cms_peaks.append(peaks[sample_inds == b])\n            cms_peak_vals.append(peak_vals[sample_inds == b].to(torch.float32))\n            cms_peak_channel_inds.append(peak_channel_inds[sample_inds == b])\n\n        # cms_peaks: [(#nodes, 2), ...]\n        return cms_peaks, cms_peak_vals, cms_peak_channel_inds\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        Args:\n            inputs: Dictionary with \"image\" as one of the keys.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n                as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n        \"\"\"\n        # Network forward pass.\n        self.batch_size = inputs[\"image\"].shape[0]\n        output = self.torch_model(inputs[\"image\"])\n        cms = output[\"MultiInstanceConfmapsHead\"]\n        pafs = output[\"PartAffinityFieldsHead\"].permute(\n            0, 2, 3, 1\n        )  # (batch, h, w, 2*edges)\n        cms_peaks, cms_peak_vals, cms_peak_channel_inds = self._generate_cms_peaks(cms)\n\n        (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n            edge_inds,\n            edge_peak_inds,\n            line_scores,\n        ) = self.paf_scorer.predict(\n            pafs=pafs,\n            peaks=cms_peaks,\n            peak_vals=cms_peak_vals,\n            peak_channel_inds=cms_peak_channel_inds,\n        )\n\n        predicted_instances = [p / self.input_scale for p in predicted_instances]\n        predicted_instances_adjusted = []\n        for idx, p in enumerate(predicted_instances):\n            predicted_instances_adjusted.append(\n                p / inputs[\"eff_scale\"][idx].to(p.device)\n            )\n        out = {\n            \"pred_instance_peaks\": predicted_instances_adjusted,\n            \"pred_peak_values\": predicted_peak_scores,\n            \"instance_scores\": predicted_instance_scores,\n        }\n\n        if self.return_confmaps:\n            out[\"pred_confmaps\"] = cms.detach()\n        if self.return_pafs:\n            out[\"pred_part_affinity_fields\"] = pafs.detach()\n        if self.return_paf_graph:\n            out[\"peaks\"] = cms_peaks\n            out[\"peak_vals\"] = cms_peak_vals\n            out[\"peak_channel_inds\"] = cms_peak_channel_inds\n            out[\"edge_inds\"] = edge_inds\n            out[\"edge_peak_inds\"] = edge_peak_inds\n            out[\"line_scores\"] = line_scores\n\n        inputs.update(out)\n        return [inputs]\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpInferenceModel.__init__","title":"<code>__init__(torch_model, paf_scorer, cms_output_stride=None, pafs_output_stride=None, peak_threshold=0.0, refinement='integral', integral_patch_size=5, return_confmaps=False, return_pafs=False, return_paf_graph=False, input_scale=1.0)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    paf_scorer: PAFScorer,\n    cms_output_stride: Optional[int] = None,\n    pafs_output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    return_pafs: Optional[bool] = False,\n    return_paf_graph: Optional[bool] = False,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__()\n    self.torch_model = torch_model\n    self.paf_scorer = paf_scorer\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.cms_output_stride = cms_output_stride\n    self.pafs_output_stride = pafs_output_stride\n    self.return_confmaps = return_confmaps\n    self.return_pafs = return_pafs\n    self.return_paf_graph = return_paf_graph\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpInferenceModel.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with \"image\" as one of the keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch     as a <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    Args:\n        inputs: Dictionary with \"image\" as one of the keys.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n            as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n    \"\"\"\n    # Network forward pass.\n    self.batch_size = inputs[\"image\"].shape[0]\n    output = self.torch_model(inputs[\"image\"])\n    cms = output[\"MultiInstanceConfmapsHead\"]\n    pafs = output[\"PartAffinityFieldsHead\"].permute(\n        0, 2, 3, 1\n    )  # (batch, h, w, 2*edges)\n    cms_peaks, cms_peak_vals, cms_peak_channel_inds = self._generate_cms_peaks(cms)\n\n    (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n        edge_inds,\n        edge_peak_inds,\n        line_scores,\n    ) = self.paf_scorer.predict(\n        pafs=pafs,\n        peaks=cms_peaks,\n        peak_vals=cms_peak_vals,\n        peak_channel_inds=cms_peak_channel_inds,\n    )\n\n    predicted_instances = [p / self.input_scale for p in predicted_instances]\n    predicted_instances_adjusted = []\n    for idx, p in enumerate(predicted_instances):\n        predicted_instances_adjusted.append(\n            p / inputs[\"eff_scale\"][idx].to(p.device)\n        )\n    out = {\n        \"pred_instance_peaks\": predicted_instances_adjusted,\n        \"pred_peak_values\": predicted_peak_scores,\n        \"instance_scores\": predicted_instance_scores,\n    }\n\n    if self.return_confmaps:\n        out[\"pred_confmaps\"] = cms.detach()\n    if self.return_pafs:\n        out[\"pred_part_affinity_fields\"] = pafs.detach()\n    if self.return_paf_graph:\n        out[\"peaks\"] = cms_peaks\n        out[\"peak_vals\"] = cms_peak_vals\n        out[\"peak_channel_inds\"] = cms_peak_channel_inds\n        out[\"edge_inds\"] = edge_inds\n        out[\"edge_peak_inds\"] = edge_peak_inds\n        out[\"line_scores\"] = line_scores\n\n    inputs.update(out)\n    return [inputs]\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpMultiClassInferenceModel","title":"<code>BottomUpMultiClassInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>BottomUp Inference model for multi-class models.</p> <p>This model encapsulates the bottom-up approach. The images are passed to a local peak detector to get the predicted instances and then grouped into instances by their identity classifications.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on MultiInstanceConfMaps.</p> <code>cms_output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>class_maps_output_stride</code> <p>Output stride of the model, denoting the scale of the output pafs relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>return_class_maps</code> <p>If <code>True</code>, the class maps will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>class BottomUpMultiClassInferenceModel(L.LightningModule):\n    \"\"\"BottomUp Inference model for multi-class models.\n\n    This model encapsulates the bottom-up approach. The images are passed to a local peak detector\n    to get the predicted instances and then grouped into instances by their identity\n    classifications.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            MultiInstanceConfMaps.\n        cms_output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        class_maps_output_stride: Output stride of the model, denoting the scale of the output\n            pafs relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        return_class_maps: If `True`, the class maps will be returned together with\n            the predicted instances. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        cms_output_stride: Optional[int] = None,\n        class_maps_output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = \"integral\",\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        return_class_maps: Optional[bool] = False,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__()\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.cms_output_stride = cms_output_stride\n        self.class_maps_output_stride = class_maps_output_stride\n        self.return_confmaps = return_confmaps\n        self.return_class_maps = return_class_maps\n        self.input_scale = input_scale\n\n    def _generate_cms_peaks(self, cms):\n        # TODO: append nans to batch them -&gt; tensor (vectorize the initial paf grouping steps)\n        peaks, peak_vals, sample_inds, peak_channel_inds = find_local_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n        # Adjust for stride and scale.\n        peaks = peaks * self.cms_output_stride  # (n_centroids, 2)\n\n        return peaks, peak_vals, sample_inds, peak_channel_inds\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        Args:\n            inputs: Dictionary with \"image\" as one of the keys.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n                as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n        \"\"\"\n        # Network forward pass.\n        self.batch_size = inputs[\"image\"].shape[0]\n        output = self.torch_model(inputs[\"image\"])\n        cms = output[\"MultiInstanceConfmapsHead\"]\n        class_maps = output[\"ClassMapsHead\"]  # (batch, n_classes, h, w)\n        cms_peaks, cms_peak_vals, cms_peak_sample_inds, cms_peak_channel_inds = (\n            self._generate_cms_peaks(cms.detach())\n        )\n\n        cms_peaks = cms_peaks / self.class_maps_output_stride\n        (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n        ) = classify_peaks_from_maps(\n            class_maps.detach(),\n            cms_peaks,\n            cms_peak_vals,\n            cms_peak_sample_inds,\n            cms_peak_channel_inds,\n            n_channels=cms.shape[-3],\n        )\n        predicted_instances = [\n            p * self.class_maps_output_stride for p in predicted_instances\n        ]\n\n        # Adjust for input scaling.\n        if self.input_scale != 1.0:\n            predicted_instances = [p / self.input_scale for p in predicted_instances]\n\n        predicted_instances_adjusted = []\n        for idx, p in enumerate(predicted_instances):\n            predicted_instances_adjusted.append(\n                p / inputs[\"eff_scale\"][idx].to(p.device)\n            )\n        out = {\n            \"pred_instance_peaks\": predicted_instances_adjusted,\n            \"pred_peak_values\": predicted_peak_scores,\n            \"instance_scores\": predicted_instance_scores,\n        }\n\n        if self.return_confmaps:\n            out[\"pred_confmaps\"] = cms.detach()\n        if self.return_class_maps:\n            out[\"pred_class_maps\"] = class_maps.detach()\n\n        inputs.update(out)\n        return [inputs]\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpMultiClassInferenceModel.__init__","title":"<code>__init__(torch_model, cms_output_stride=None, class_maps_output_stride=None, peak_threshold=0.0, refinement='integral', integral_patch_size=5, return_confmaps=False, return_class_maps=False, input_scale=1.0)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    cms_output_stride: Optional[int] = None,\n    class_maps_output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    return_class_maps: Optional[bool] = False,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__()\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.cms_output_stride = cms_output_stride\n    self.class_maps_output_stride = class_maps_output_stride\n    self.return_confmaps = return_confmaps\n    self.return_class_maps = return_class_maps\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpMultiClassInferenceModel.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with \"image\" as one of the keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch     as a <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    Args:\n        inputs: Dictionary with \"image\" as one of the keys.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n            as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n    \"\"\"\n    # Network forward pass.\n    self.batch_size = inputs[\"image\"].shape[0]\n    output = self.torch_model(inputs[\"image\"])\n    cms = output[\"MultiInstanceConfmapsHead\"]\n    class_maps = output[\"ClassMapsHead\"]  # (batch, n_classes, h, w)\n    cms_peaks, cms_peak_vals, cms_peak_sample_inds, cms_peak_channel_inds = (\n        self._generate_cms_peaks(cms.detach())\n    )\n\n    cms_peaks = cms_peaks / self.class_maps_output_stride\n    (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n    ) = classify_peaks_from_maps(\n        class_maps.detach(),\n        cms_peaks,\n        cms_peak_vals,\n        cms_peak_sample_inds,\n        cms_peak_channel_inds,\n        n_channels=cms.shape[-3],\n    )\n    predicted_instances = [\n        p * self.class_maps_output_stride for p in predicted_instances\n    ]\n\n    # Adjust for input scaling.\n    if self.input_scale != 1.0:\n        predicted_instances = [p / self.input_scale for p in predicted_instances]\n\n    predicted_instances_adjusted = []\n    for idx, p in enumerate(predicted_instances):\n        predicted_instances_adjusted.append(\n            p / inputs[\"eff_scale\"][idx].to(p.device)\n        )\n    out = {\n        \"pred_instance_peaks\": predicted_instances_adjusted,\n        \"pred_peak_values\": predicted_peak_scores,\n        \"instance_scores\": predicted_instance_scores,\n    }\n\n    if self.return_confmaps:\n        out[\"pred_confmaps\"] = cms.detach()\n    if self.return_class_maps:\n        out[\"pred_class_maps\"] = class_maps.detach()\n\n    inputs.update(out)\n    return [inputs]\n</code></pre>"},{"location":"api/inference/identity/","title":"identity","text":""},{"location":"api/inference/identity/#sleap_nn.inference.identity","title":"<code>sleap_nn.inference.identity</code>","text":"<p>Utilities for models that learn identity.</p> <p>These functions implement the inference logic for classifying peaks using class maps or classification vectors.</p> <p>Functions:</p> Name Description <code>classify_peaks_from_maps</code> <p>Classify and group local peaks by their class map probability.</p> <code>get_class_inds_from_vectors</code> <p>Get class indices from the probability scores.</p> <code>group_class_peaks</code> <p>Group local peaks using class probabilities, matching peaks to classes using the Hungarian algorithm, per (sample, channel) pair.</p>"},{"location":"api/inference/identity/#sleap_nn.inference.identity.classify_peaks_from_maps","title":"<code>classify_peaks_from_maps(class_maps, peak_points, peak_vals, peak_sample_inds, peak_channel_inds, n_channels)</code>","text":"<p>Classify and group local peaks by their class map probability.</p> <p>Parameters:</p> Name Type Description Default <code>class_maps</code> <code>Tensor</code> <p>Class maps with shape <code>(n_samples, n_classes, height, width, )</code>.</p> required <code>peak_points</code> <code>Tensor</code> <p>Local peak coordinates of shape <code>(n_peaks,)</code>. These should be in the same scale as the class maps.</p> required <code>peak_vals</code> <code>Tensor</code> <p>Confidence map value with shape <code>(n_peaks,)</code>.</p> required <code>peak_sample_inds</code> <code>Tensor</code> <p>Sample index for each peak with shape <code>(n_peaks,)</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>Channel index for each peak with shape <code>(n_peaks,)</code>.</p> required <code>n_channels</code> <code>int</code> <p>Integer number of channels (nodes) the instances should have.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of <code>(points, point_vals, class_probs)</code> containing the grouped peaks.</p> <p><code>points</code>: Predicted instances <code>(n_samples, n_classes, n_peaks, 2)</code>. Missing points will be denoted by     NaNs.</p> <p><code>point_vals</code>: The confidence map values for each point with shape <code>(n_samples, n_classes, n_peaks)</code>.</p> <p><code>class_probs</code>: Classification probabilities for matched points with shape <code>(n_samples, n_classes, n_peaks)</code>.</p> <p>See also: group_class_peaks</p> Source code in <code>sleap_nn/inference/identity.py</code> <pre><code>def classify_peaks_from_maps(\n    class_maps: torch.Tensor,\n    peak_points: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_sample_inds: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    n_channels: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Classify and group local peaks by their class map probability.\n\n    Args:\n        class_maps: Class maps with shape `(n_samples, n_classes, height, width, )`.\n        peak_points: Local peak coordinates of shape `(n_peaks,)`. These should be in the same scale as the class maps.\n        peak_vals: Confidence map value with shape `(n_peaks,)`.\n        peak_sample_inds: Sample index for each peak with shape `(n_peaks,)`.\n        peak_channel_inds: Channel index for each peak with shape `(n_peaks,)`.\n        n_channels: Integer number of channels (nodes) the instances should have.\n\n    Returns:\n        A tuple of `(points, point_vals, class_probs)` containing the grouped peaks.\n\n        `points`: Predicted instances `(n_samples, n_classes, n_peaks, 2)`. Missing points will be denoted by\n            NaNs.\n\n        `point_vals`: The confidence map values for each point with shape `(n_samples, n_classes, n_peaks)`.\n\n        `class_probs`: Classification probabilities for matched points with shape `(n_samples, n_classes, n_peaks)`.\n\n    See also: group_class_peaks\n    \"\"\"\n    # Build subscripts and pull out class probabilities for each peak from class maps.\n    n_samples, n_instances, h, w = class_maps.shape\n    peak_sample_inds = peak_sample_inds.to(torch.int32)\n    peak_channel_inds = peak_channel_inds.to(torch.int32)\n\n    subs = torch.cat(\n        [\n            peak_sample_inds.view(-1, 1),\n            torch.round(torch.flip(peak_points, dims=[1])).to(torch.int32),\n        ],\n        dim=1,\n    )\n    subs[:, 1] = subs[:, 1].clamp(0, h - 1)\n    subs[:, 2] = subs[:, 2].clamp(0, w - 1)\n\n    peak_class_probs = class_maps[subs[:, 0], :, subs[:, 1], subs[:, 2]]\n\n    # Classify the peaks.\n    peak_inds, class_inds = group_class_peaks(\n        peak_class_probs, peak_sample_inds, peak_channel_inds, n_samples, n_channels\n    )\n\n    # Assign the results to fixed size tensors.\n    subs = torch.stack(\n        [peak_sample_inds[peak_inds], class_inds, peak_channel_inds[peak_inds]], dim=1\n    )\n\n    points = torch.full(\n        (n_samples, n_instances, n_channels, 2), float(\"nan\"), device=class_maps.device\n    )\n    point_vals = torch.full(\n        (n_samples, n_instances, n_channels), float(\"nan\"), device=class_maps.device\n    )\n    class_probs = torch.full(\n        (n_samples, n_instances, n_channels), float(\"nan\"), device=class_maps.device\n    )\n\n    points[subs[:, 0], subs[:, 1], subs[:, 2]] = peak_points[peak_inds]\n    point_vals[subs[:, 0], subs[:, 1], subs[:, 2]] = peak_vals[peak_inds]\n\n    gather_inds = torch.stack([peak_inds, class_inds], dim=1)\n    gathered_class_probs = peak_class_probs[gather_inds[:, 0], gather_inds[:, 1]]\n\n    class_probs[subs[:, 0], subs[:, 1], subs[:, 2]] = gathered_class_probs\n\n    return points, point_vals, class_probs\n</code></pre>"},{"location":"api/inference/identity/#sleap_nn.inference.identity.get_class_inds_from_vectors","title":"<code>get_class_inds_from_vectors(peak_class_probs)</code>","text":"<p>Get class indices from the probability scores.</p> <p>Parameters:</p> Name Type Description Default <code>peak_class_probs</code> <code>Tensor</code> <p>(n_samples, n_classes) softmax output for each sample</p> required <p>Returns:</p> Name Type Description <code>class_inds</code> <p>(n_samples,) class index assigned to each sample class_probs: (n_samples,) the probability of the assigned class</p> Source code in <code>sleap_nn/inference/identity.py</code> <pre><code>def get_class_inds_from_vectors(peak_class_probs: torch.Tensor):\n    \"\"\"Get class indices from the probability scores.\n\n    Args:\n        peak_class_probs: (n_samples, n_classes) softmax output for each sample\n\n    Returns:\n        class_inds: (n_samples,) class index assigned to each sample\n        class_probs: (n_samples,) the probability of the assigned class\n    \"\"\"\n    n_samples, n_classes = peak_class_probs.shape\n\n    # Run Hungarian matching on negative probabilities (maximize total confidence)\n    row_inds, col_inds = linear_sum_assignment(-peak_class_probs.cpu().numpy())\n\n    # Initialize result tensors\n    class_inds = torch.full((n_samples,), -1, dtype=torch.int64)\n    class_probs = torch.full((n_samples,), float(\"nan\"))\n\n    # Assign class IDs and probabilities to samples\n    for sample_idx, class_idx in zip(row_inds, col_inds):\n        class_inds[sample_idx] = class_idx\n        class_probs[sample_idx] = peak_class_probs[sample_idx, class_idx]\n\n    return class_inds, class_probs\n</code></pre>"},{"location":"api/inference/identity/#sleap_nn.inference.identity.group_class_peaks","title":"<code>group_class_peaks(peak_class_probs, peak_sample_inds, peak_channel_inds, n_samples, n_channels)</code>","text":"<p>Group local peaks using class probabilities, matching peaks to classes using the Hungarian algorithm, per (sample, channel) pair.</p> Source code in <code>sleap_nn/inference/identity.py</code> <pre><code>def group_class_peaks(\n    peak_class_probs: torch.Tensor,\n    peak_sample_inds: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    n_samples: int,\n    n_channels: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Group local peaks using class probabilities, matching peaks to classes using the Hungarian algorithm, per (sample, channel) pair.\"\"\"\n    peak_inds_list = []\n    class_inds_list = []\n\n    for sample in range(n_samples):\n        for channel in range(n_channels):\n            # Mask to find peaks belonging to this (sample, channel) pair\n            mask = (peak_sample_inds == sample) &amp; (peak_channel_inds == channel)\n            if not torch.any(mask):\n                continue\n\n            # Extract probabilities for current group\n            probs = peak_class_probs[mask]  # (n_peaks_sc, n_classes)\n            if probs.numel() == 0:\n                continue\n\n            # Run Hungarian algorithm (note: maximize =&gt; minimize negative cost)\n            cost = -probs.detach().cpu().numpy()\n            row_ind, col_ind = linear_sum_assignment(cost)\n\n            # Get original indices in peak_class_probs\n            masked_indices = (\n                torch.nonzero(mask, as_tuple=False)\n                .squeeze(1)\n                .to(peak_sample_inds.device)\n            )\n            peak_inds_sc = masked_indices[row_ind]\n            class_inds_sc = torch.tensor(col_ind, dtype=torch.int64).to(\n                peak_sample_inds.device\n            )\n\n            peak_inds_list.append(peak_inds_sc)\n            class_inds_list.append(class_inds_sc)\n\n    if not peak_inds_list:\n        return (\n            torch.empty(0, dtype=torch.int64).to(peak_sample_inds.device),\n            torch.empty(0, dtype=torch.int64).to(peak_sample_inds.device),\n        )\n\n    peak_inds = torch.cat(peak_inds_list, dim=0).to(peak_sample_inds.device)\n    class_inds = torch.cat(class_inds_list, dim=0).to(peak_sample_inds.device)\n\n    # Filter to keep only best class per peak\n    matched_probs = peak_class_probs[peak_inds, class_inds]\n    best_probs = peak_class_probs[peak_inds].max(dim=1).values\n    is_best = matched_probs == best_probs\n\n    return peak_inds[is_best], class_inds[is_best]\n</code></pre>"},{"location":"api/inference/paf_grouping/","title":"paf_grouping","text":""},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping","title":"<code>sleap_nn.inference.paf_grouping</code>","text":"<p>This module provides a set of utilities for grouping peaks based on PAFs.</p> <p>Part affinity fields (PAFs) are a representation used to resolve the peak grouping problem for multi-instance pose estimation [1].</p> <p>They are a convenient way to represent directed graphs with support in image space. For each edge, a PAF can be represented by an image with two channels, corresponding to the x and y components of a unit vector pointing along the direction of the underlying directed graph formed by the connections of the landmarks belonging to an instance.</p> <p>Given a pair of putatively connected landmarks, the agreement between the line segment that connects them and the PAF vectors found at the coordinates along the same line can be used as a measure of \"connectedness\". These scores can then be used to guide the instance-wise grouping of landmarks.</p> <p>This image space representation is particularly useful as it is amenable to neural network-based prediction from unlabeled images.</p> <p>A high-level API for grouping based on PAFs is provided through the <code>PAFScorer</code> class.</p> References <p>.. [1] Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh. Realtime Multi-Person 2D    Pose Estimation using Part Affinity Fields. In CVPR, 2017.</p> <p>Classes:</p> Name Description <code>EdgeConnection</code> <p>Indices to specify a matched connection between two peaks.</p> <code>EdgeType</code> <p>Indices to uniquely identify a single edge type.</p> <code>PAFScorer</code> <p>Scoring pipeline based on part affinity fields.</p> <code>PeakID</code> <p>Indices to uniquely identify a single peak.</p> <p>Functions:</p> Name Description <code>assign_connections_to_instances</code> <p>Assign connected edges to instances via greedy graph partitioning.</p> <code>compute_distance_penalty</code> <p>Compute the distance penalty component of the PAF line integral score.</p> <code>get_connection_candidates</code> <p>Find the indices of all the possible connections formed by the detected peaks.</p> <code>get_paf_lines</code> <p>Get the PAF values at the lines formed between all detected peaks in a sample.</p> <code>group_instances_batch</code> <p>Group matched connections into full instances for a batch.</p> <code>group_instances_sample</code> <p>Group matched connections into full instances for a single sample.</p> <code>make_line_subs</code> <p>Create the lines between candidate connections for evaluating the PAFs.</p> <code>make_predicted_instances</code> <p>Group peaks by assignments and accumulate scores.</p> <code>match_candidates_batch</code> <p>Match candidate connections for a batch based on PAF scores.</p> <code>match_candidates_sample</code> <p>Match candidate connections for a sample based on PAF scores.</p> <code>score_paf_lines</code> <p>Compute the connectivity score for each PAF line in a sample.</p> <code>score_paf_lines_batch</code> <p>Process a batch of images to score the Part Affinity Fields (PAFs) lines formed between connection candidates for each sample.</p> <code>toposort_edges</code> <p>Find a topological ordering for a list of edge types.</p>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.EdgeConnection","title":"<code>EdgeConnection</code>","text":"<p>Indices to specify a matched connection between two peaks.</p> <p>This is a convenience named tuple for use in the matching pipeline.</p> <p>Attributes:</p> Name Type Description <code>src_peak_ind</code> <code>int</code> <p>Index of the source peak within all peaks.</p> <code>dst_peak_ind</code> <code>int</code> <p>Index of the destination peak within all peaks.</p> <code>score</code> <code>float</code> <p>Score of the match.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define(auto_attribs=True)\nclass EdgeConnection:\n    \"\"\"Indices to specify a matched connection between two peaks.\n\n    This is a convenience named tuple for use in the matching pipeline.\n\n    Attributes:\n        src_peak_ind: Index of the source peak within all peaks.\n        dst_peak_ind: Index of the destination peak within all peaks.\n        score: Score of the match.\n    \"\"\"\n\n    src_peak_ind: int\n    dst_peak_ind: int\n    score: float\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.EdgeType","title":"<code>EdgeType</code>","text":"<p>Indices to uniquely identify a single edge type.</p> <p>This is a convenience named tuple for use in the matching pipeline.</p> <p>Attributes:</p> Name Type Description <code>src_node_ind</code> <code>int</code> <p>Index of the source node type within the skeleton edges.</p> <code>dst_node_ind</code> <code>int</code> <p>Index of the destination node type within the skeleton edges.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define(auto_attribs=True, frozen=True)\nclass EdgeType:\n    \"\"\"Indices to uniquely identify a single edge type.\n\n    This is a convenience named tuple for use in the matching pipeline.\n\n    Attributes:\n        src_node_ind: Index of the source node type within the skeleton edges.\n        dst_node_ind: Index of the destination node type within the skeleton edges.\n    \"\"\"\n\n    src_node_ind: int\n    dst_node_ind: int\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer","title":"<code>PAFScorer</code>","text":"<p>Scoring pipeline based on part affinity fields.</p> <p>This class facilitates grouping of predicted peaks based on PAFs. It holds a set of common parameters that are used across different steps of the pipeline.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>List[Text]</code> <p>List of string node names in the skeleton.</p> <code>edges</code> <code>List[Tuple[Text, Text]]</code> <p>List of (src_node, dst_node) names in the skeleton.</p> <code>pafs_stride</code> <code>int</code> <p>Output stride of the part affinity fields. This will be used to adjust the peak coordinates from full image to PAF subscripts.</p> <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching.</p> <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>n_points</code> <code>int</code> <p>Number of points to sample along the line integral.</p> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Minimum number of peaks the instance should have to be considered a real instance. Instances with fewer peaks than this will be discarded (useful for filtering spurious detections).</p> <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones.</p> <code>edge_inds</code> <code>List[Tuple[int, int]]</code> <p>The edges of the skeleton defined as a list of (source, destination) tuples of node indices. This is created automatically on initialization.</p> <code>edge_types</code> <code>List[EdgeType]</code> <p>A list of <code>EdgeType</code> instances representing the edges of the skeleton. This is created automatically on initialization.</p> <code>n_nodes</code> <code>int</code> <p>The number of nodes in the skeleton as a scalar <code>int</code>. This is created automatically on initialization.</p> <code>n_edges</code> <code>int</code> <p>The number of edges in the skeleton as a scalar <code>int</code>. This is created automatically on initialization.</p> <code>sorted_edge_inds</code> <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> Notes <p>This class provides high level APIs for grouping peaks into instances using PAFs.</p> <p>The algorithm has three steps:</p> <pre><code>1. Find all candidate connections between peaks and compute their matching\nscore based on the PAFs.\n\n2. Match candidate connections using the connectivity score such that no\npeak is used in two connections of the same type.\n\n3. Group matched connections into complete instances.\n</code></pre> <p>In general, the output from a peak finder (such as multi-peak confidence map prediction network) can be passed into <code>PAFScorer.predict()</code> to get back complete instances.</p> <p>For finer control over the grouping pipeline steps, use the instance methods in this class or the lower level functions in <code>sleap_nn.paf_grouping</code>.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Cache some computed attributes on initialization.</p> <code>from_config</code> <p>Initialize the PAF scorer from a <code>MultiInstanceConfig</code> head config.</p> <code>group_instances</code> <p>Group matched connections into full instances for a batch.</p> <code>match_candidates</code> <p>Match candidate connections for a batch based on PAF scores.</p> <code>predict</code> <p>Group a batch of predicted peaks into full instance predictions using PAFs.</p> <code>score_paf_lines</code> <p>Create and score PAF lines formed between connection candidates.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define\nclass PAFScorer:\n    \"\"\"Scoring pipeline based on part affinity fields.\n\n    This class facilitates grouping of predicted peaks based on PAFs. It holds a set of\n    common parameters that are used across different steps of the pipeline.\n\n    Attributes:\n        part_names: List of string node names in the skeleton.\n        edges: List of (src_node, dst_node) names in the skeleton.\n        pafs_stride: Output stride of the part affinity fields. This will be used to\n            adjust the peak coordinates from full image to PAF subscripts.\n        max_edge_length_ratio: The maximum expected length of a connected pair of points\n            as a fraction of the image size. Candidate connections longer than this\n            length will be penalized during matching.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n        n_points: Number of points to sample along the line integral.\n        min_instance_peaks: Minimum number of peaks the instance should have to be\n            considered a real instance. Instances with fewer peaks than this will be\n            discarded (useful for filtering spurious detections).\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs. Useful for rejecting spurious detections when\n            there are no better ones.\n        edge_inds: The edges of the skeleton defined as a list of (source, destination)\n            tuples of node indices. This is created automatically on initialization.\n        edge_types: A list of `EdgeType` instances representing the edges of the\n            skeleton. This is created automatically on initialization.\n        n_nodes: The number of nodes in the skeleton as a scalar `int`. This is created\n            automatically on initialization.\n        n_edges: The number of edges in the skeleton as a scalar `int`. This is created\n            automatically on initialization.\n        sorted_edge_inds: A tuple of indices specifying the topological order that the\n            edge types should be accessed in during instance assembly\n            (`assign_connections_to_instances`).\n\n    Notes:\n        This class provides high level APIs for grouping peaks into instances using\n        PAFs.\n\n        The algorithm has three steps:\n\n            1. Find all candidate connections between peaks and compute their matching\n            score based on the PAFs.\n\n            2. Match candidate connections using the connectivity score such that no\n            peak is used in two connections of the same type.\n\n            3. Group matched connections into complete instances.\n\n        In general, the output from a peak finder (such as multi-peak confidence map\n        prediction network) can be passed into `PAFScorer.predict()` to get back\n        complete instances.\n\n        For finer control over the grouping pipeline steps, use the instance methods in\n        this class or the lower level functions in `sleap_nn.paf_grouping`.\n    \"\"\"\n\n    part_names: List[Text]\n    edges: List[Tuple[Text, Text]]\n    pafs_stride: int\n    max_edge_length_ratio: float = 0.25\n    dist_penalty_weight: float = 1.0\n    n_points: int = 10\n    min_instance_peaks: Union[int, float] = 0\n    min_line_scores: float = 0.25\n\n    edge_inds: List[Tuple[int, int]] = attr.ib(init=False)\n    edge_types: List[EdgeType] = attr.ib(init=False)\n    n_nodes: int = attr.ib(init=False)\n    n_edges: int = attr.ib(init=False)\n    sorted_edge_inds: Tuple[int] = attr.ib(init=False)\n\n    def __attrs_post_init__(self):\n        \"\"\"Cache some computed attributes on initialization.\"\"\"\n        self.edge_inds = [\n            (self.part_names.index(src), self.part_names.index(dst))\n            for (src, dst) in self.edges\n        ]\n        self.edge_types = [\n            EdgeType(src_node, dst_node) for src_node, dst_node in self.edge_inds\n        ]\n\n        self.n_nodes = len(self.part_names)\n        self.n_edges = len(self.edges)\n        self.sorted_edge_inds = toposort_edges(self.edge_types)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: OmegaConf,\n        max_edge_length_ratio: float = 0.25,\n        dist_penalty_weight: float = 1.0,\n        n_points: int = 10,\n        min_instance_peaks: Union[int, float] = 0,\n        min_line_scores: float = 0.25,\n    ) -&gt; \"PAFScorer\":\n        \"\"\"Initialize the PAF scorer from a `MultiInstanceConfig` head config.\n\n        Args:\n            config: An `OmegaConf` instance.\n            max_edge_length_ratio: The maximum expected length of a connected pair of\n                points as a fraction of the image size. Candidate connections longer\n                than this length will be penalized during matching.\n            dist_penalty_weight: A coefficient to scale weight of the distance penalty\n                as a scalar float. Set to values greater than 1.0 to enforce the\n                distance penalty more strictly.\n            min_edge_score: Minimum score required to classify a connection as correct.\n            n_points: Number of points to sample along the line integral.\n            min_instance_peaks: Minimum number of peaks the instance should have to be\n                considered a real instance. Instances with fewer peaks than this will be\n                discarded (useful for filtering spurious detections).\n            min_line_scores: Minimum line score (between -1 and 1) required to form a\n                match between candidate point pairs. Useful for rejecting spurious\n                detections when there are no better ones.\n\n        Returns:\n            The initialized instance of `PAFScorer`.\n        \"\"\"\n        return cls(\n            part_names=config.confmaps.part_names,\n            edges=config.pafs.edges,\n            pafs_stride=config.pafs.output_stride,\n            max_edge_length_ratio=max_edge_length_ratio,\n            dist_penalty_weight=dist_penalty_weight,\n            n_points=n_points,\n            min_instance_peaks=min_instance_peaks,\n            min_line_scores=min_line_scores,\n        )\n\n    def score_paf_lines(\n        self,\n        pafs: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_channel_inds: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Create and score PAF lines formed between connection candidates.\n\n        Args:\n            pafs: A nested torch tensor of shape `(n_samples, height, width, 2 * n_edges)`\n                containing the part affinity fields for each sample in the batch.\n            peaks: A nested torch tensor of shape `(n_samples, (n_peaks), 2)` containing the\n                (x, y) coordinates of the detected peaks for each sample.\n            peak_channel_inds: A nested torch tensor of shape `(n_samples, (n_peaks))` indicating\n                the channel (node) index that each peak corresponds to.\n\n        Returns:\n            A tuple containing three lists for each sample in the batch:\n                - A nested torch tensor of shape `(n_samples, (n_connections,))` indicating the indices\n                of the edges that each connection corresponds to.\n                - A nested torch tensor of shape `(n_samples, (n_connections, 2))` containing the indices\n                of the source and destination peaks forming each connection.\n                - A nested torch tensor of shape `(n_samples, (n_connections,))` containing the scores\n                for each connection based on the PAFs.\n\n        Notes:\n            This is a convenience wrapper for the standalone `score_paf_lines_batch()`.\n\n        See also: score_paf_lines_batch\n        \"\"\"\n        return score_paf_lines_batch(\n            pafs,\n            peaks,\n            peak_channel_inds,\n            self.edge_inds,\n            self.n_points,\n            self.pafs_stride,\n            self.max_edge_length_ratio,\n            self.dist_penalty_weight,\n            self.n_nodes,\n        )\n\n    def match_candidates(\n        self,\n        edge_inds: torch.Tensor,\n        edge_peak_inds: torch.Tensor,\n        line_scores: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Match candidate connections for a batch based on PAF scores.\n\n        Args:\n            edge_inds: Sample-grouped edge indices as a nested `torch.Tensor` of shape\n                `(n_samples, (n_candidates))` and dtype `torch.int32` indicating the\n                indices of the edge that each of the candidate connections belongs to.\n                Can be generated using `PAFScorer.score_paf_lines()`.\n            edge_peak_inds: Sample-grouped indices of the peaks that form the source and\n                destination of each candidate connection as a nested `torch.Tensor` of shape\n                `(n_samples, (n_candidates), 2)` and dtype `torch.int32`. Can be generated\n                using `PAFScorer.score_paf_lines()`.\n            line_scores: Sample-grouped scores for each candidate connection as a\n                nested `torch.Tensor` of shape `(n_samples, (n_candidates))` and dtype\n                `torch.float32`. Can be generated using `PAFScorer.score_paf_lines()`.\n\n        Returns:\n            The connection peaks for each edge matched based on score as tuple of\n            `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n            `match_edge_inds`: Sample-grouped indices of the skeleton edge for each\n            connection as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n            and dtype `torch.int32`.\n\n            `match_src_peak_inds`: Sample-grouped indices of the source peaks that form\n            each connection as a nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks in\n            the sample.\n\n            `match_dst_peak_inds`: Sample-grouped indices of the destination peaks that\n            form each connection as a nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks in\n            the sample.\n\n            `match_line_scores`: Sample-grouped PAF line scores of the matched\n            connections as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n            and dtype `torch.float32`.\n\n        Notes:\n            This is a convenience wrapper for the standalone `match_candidates_batch()`.\n\n        See also: PAFScorer.score_paf_lines, match_candidates_batch\n        \"\"\"\n        return match_candidates_batch(\n            edge_inds, edge_peak_inds, line_scores, self.n_edges\n        )\n\n    def group_instances(\n        self,\n        peaks: torch.Tensor,\n        peak_vals: torch.Tensor,\n        peak_channel_inds: torch.Tensor,\n        match_edge_inds: torch.Tensor,\n        match_src_peak_inds: torch.Tensor,\n        match_dst_peak_inds: torch.Tensor,\n        match_line_scores: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Group matched connections into full instances for a batch.\n\n        Args:\n            peaks: The sample-grouped detected peaks in a batch as a nested tensor `torch.Tensor`\n                of shape `(n_samples, (n_peaks), 2)` and dtype `torch.float32`. These\n                should be `(x, y)` coordinates of each peak in the image scale.\n            peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n                nested tensor `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n                `torch.float32`.\n            peak_channel_inds: The sample-grouped indices of the channel (node) that\n                each detected peak is associated with as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_peaks))` and dtype `torch.int32`.\n            match_edge_inds: Sample-grouped indices of the skeleton edge that each\n                connection corresponds to as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_connections))` and dtype `torch.int32`. This can be\n                generated by `PAFScorer.match_candidates()`.\n            match_src_peak_inds: Sample-grouped indices of the source peaks that form\n                each connection as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n                indices correspond to the edge-grouped peaks, not the set of all peaks\n                in each sample. This can be generated by `PAFScorer.match_candidates()`.\n            match_dst_peak_inds: Sample-grouped indices of the destination peaks that\n                form each connection as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n                indices correspond to the edge-grouped peaks, not the set of all peaks\n                in the sample. This can be generated by `PAFScorer.match_candidates()`.\n            match_line_scores: Sample-grouped PAF line scores of the matched connections\n                as a nested tensor `torch.Tensor` of shape `(n_samples, (n_connections))` and dtype\n                `torch.float32`. This can be generated by `PAFScorer.match_candidates()`.\n\n        Returns:\n            A tuple of arrays with the grouped instances for the whole batch grouped by\n            sample:\n\n            `predicted_instances`: The sample- and instance-grouped coordinates for each\n            instance as nested `torch.Tensor` of shape\n            `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n            peaks are represented by `NaN`s.\n\n            `predicted_peak_scores`: The sample- and instance-grouped confidence map\n            values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n            and dtype `torch.float32`.\n\n            `predicted_instance_scores`: The sample-grouped instance grouping score for\n            each instance as an array of shape `(n_samples, (n_instances))` and dtype\n            `torch.float32`.\n\n        Notes:\n            This is a convenience wrapper for the standalone `group_instances_batch()`.\n\n        See also: PAFScorer.match_candidates, group_instances_batch\n        \"\"\"\n        return group_instances_batch(\n            peaks,\n            peak_vals,\n            peak_channel_inds,\n            match_edge_inds,\n            match_src_peak_inds,\n            match_dst_peak_inds,\n            match_line_scores,\n            self.n_nodes,\n            self.sorted_edge_inds,\n            self.edge_types,\n            self.min_instance_peaks,\n            min_line_scores=self.min_line_scores,\n        )\n\n    def predict(\n        self,\n        pafs: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_vals: torch.Tensor,\n        peak_channel_inds: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Group a batch of predicted peaks into full instance predictions using PAFs.\n\n        Args:\n            pafs: The batch of part affinity fields as a `torch.Tensor` of shape\n                `(n_samples, height, width, 2 * n_edges)` and type `torch.float32`.\n            peaks: The coordinates of the peaks grouped by sample as a nested `torch.Tensor`\n                of shape `(n_samples, (n_peaks), 2)`.\n            peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n                nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n                `torch.float32`.\n            peak_channel_inds: The channel (node) that each peak in `peaks` corresponds\n                to as a nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n                `torch.int32`.\n\n        Returns:\n            A tuple of arrays with the grouped instances for the whole batch grouped by\n            sample:\n\n            `predicted_instances`: The sample- and instance-grouped coordinates for each\n            instance as nested `torch.Tensor` of shape\n            `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n            peaks are represented by `NaN`s.\n\n            `predicted_peak_scores`: The sample- and instance-grouped confidence map\n            values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n            and dtype `torch.float32`.\n\n            `predicted_instance_scores`: The sample-grouped instance grouping score for\n            each instance as an array of shape `(n_samples, (n_instances))` and dtype\n            `torch.float32`.\n\n        Notes:\n            This is a high level API for grouping peaks into instances using PAFs.\n\n            See the `PAFScorer` class documentation for more details on the algorithm.\n\n        See Also:\n            PAFScorer.score_paf_lines, PAFScorer.match_candidates,\n            PAFScorer.group_instances\n        \"\"\"\n        edge_inds, edge_peak_inds, line_scores = self.score_paf_lines(\n            pafs, peaks, peak_channel_inds\n        )\n        (\n            match_edge_inds,\n            match_src_peak_inds,\n            match_dst_peak_inds,\n            match_line_scores,\n        ) = self.match_candidates(edge_inds, edge_peak_inds, line_scores)\n        (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n        ) = self.group_instances(\n            peaks,\n            peak_vals,\n            peak_channel_inds,\n            match_edge_inds,\n            match_src_peak_inds,\n            match_dst_peak_inds,\n            match_line_scores,\n        )\n        return (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n            edge_inds,\n            edge_peak_inds,\n            line_scores,\n        )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Cache some computed attributes on initialization.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def __attrs_post_init__(self):\n    \"\"\"Cache some computed attributes on initialization.\"\"\"\n    self.edge_inds = [\n        (self.part_names.index(src), self.part_names.index(dst))\n        for (src, dst) in self.edges\n    ]\n    self.edge_types = [\n        EdgeType(src_node, dst_node) for src_node, dst_node in self.edge_inds\n    ]\n\n    self.n_nodes = len(self.part_names)\n    self.n_edges = len(self.edges)\n    self.sorted_edge_inds = toposort_edges(self.edge_types)\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.from_config","title":"<code>from_config(config, max_edge_length_ratio=0.25, dist_penalty_weight=1.0, n_points=10, min_instance_peaks=0, min_line_scores=0.25)</code>  <code>classmethod</code>","text":"<p>Initialize the PAF scorer from a <code>MultiInstanceConfig</code> head config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OmegaConf</code> <p>An <code>OmegaConf</code> instance.</p> required <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching.</p> <code>0.25</code> <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>1.0</code> <code>min_edge_score</code> <p>Minimum score required to classify a connection as correct.</p> required <code>n_points</code> <code>int</code> <p>Number of points to sample along the line integral.</p> <code>10</code> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Minimum number of peaks the instance should have to be considered a real instance. Instances with fewer peaks than this will be discarded (useful for filtering spurious detections).</p> <code>0</code> <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>PAFScorer</code> <p>The initialized instance of <code>PAFScorer</code>.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: OmegaConf,\n    max_edge_length_ratio: float = 0.25,\n    dist_penalty_weight: float = 1.0,\n    n_points: int = 10,\n    min_instance_peaks: Union[int, float] = 0,\n    min_line_scores: float = 0.25,\n) -&gt; \"PAFScorer\":\n    \"\"\"Initialize the PAF scorer from a `MultiInstanceConfig` head config.\n\n    Args:\n        config: An `OmegaConf` instance.\n        max_edge_length_ratio: The maximum expected length of a connected pair of\n            points as a fraction of the image size. Candidate connections longer\n            than this length will be penalized during matching.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty\n            as a scalar float. Set to values greater than 1.0 to enforce the\n            distance penalty more strictly.\n        min_edge_score: Minimum score required to classify a connection as correct.\n        n_points: Number of points to sample along the line integral.\n        min_instance_peaks: Minimum number of peaks the instance should have to be\n            considered a real instance. Instances with fewer peaks than this will be\n            discarded (useful for filtering spurious detections).\n        min_line_scores: Minimum line score (between -1 and 1) required to form a\n            match between candidate point pairs. Useful for rejecting spurious\n            detections when there are no better ones.\n\n    Returns:\n        The initialized instance of `PAFScorer`.\n    \"\"\"\n    return cls(\n        part_names=config.confmaps.part_names,\n        edges=config.pafs.edges,\n        pafs_stride=config.pafs.output_stride,\n        max_edge_length_ratio=max_edge_length_ratio,\n        dist_penalty_weight=dist_penalty_weight,\n        n_points=n_points,\n        min_instance_peaks=min_instance_peaks,\n        min_line_scores=min_line_scores,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.group_instances","title":"<code>group_instances(peaks, peak_vals, peak_channel_inds, match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code>","text":"<p>Group matched connections into full instances for a batch.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <code>Tensor</code> <p>The sample-grouped detected peaks in a batch as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks), 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>peak_vals</code> <code>Tensor</code> <p>The sample-grouped scores of the detected peaks in a batch as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>The sample-grouped indices of the channel (node) that each detected peak is associated with as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.int32</code>.</p> required <code>match_edge_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the skeleton edge that each connection corresponds to as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <code>match_src_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the source peaks that form each connection as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in each sample. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <code>match_dst_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the destination peaks that form each connection as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <code>match_line_scores</code> <code>Tensor</code> <p>Sample-grouped PAF line scores of the matched connections as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of arrays with the grouped instances for the whole batch grouped by sample:</p> <p><code>predicted_instances</code>: The sample- and instance-grouped coordinates for each instance as nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes, 2)</code> and dtype <code>torch.float32</code>. Missing peaks are represented by <code>NaN</code>s.</p> <p><code>predicted_peak_scores</code>: The sample- and instance-grouped confidence map values for each peak as an array of <code>(n_samples, (n_instances), n_nodes)</code> and dtype <code>torch.float32</code>.</p> <p><code>predicted_instance_scores</code>: The sample-grouped instance grouping score for each instance as an array of shape <code>(n_samples, (n_instances))</code> and dtype <code>torch.float32</code>.</p> Notes <p>This is a convenience wrapper for the standalone <code>group_instances_batch()</code>.</p> <p>See also: PAFScorer.match_candidates, group_instances_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def group_instances(\n    self,\n    peaks: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    match_edge_inds: torch.Tensor,\n    match_src_peak_inds: torch.Tensor,\n    match_dst_peak_inds: torch.Tensor,\n    match_line_scores: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Group matched connections into full instances for a batch.\n\n    Args:\n        peaks: The sample-grouped detected peaks in a batch as a nested tensor `torch.Tensor`\n            of shape `(n_samples, (n_peaks), 2)` and dtype `torch.float32`. These\n            should be `(x, y)` coordinates of each peak in the image scale.\n        peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n            nested tensor `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n            `torch.float32`.\n        peak_channel_inds: The sample-grouped indices of the channel (node) that\n            each detected peak is associated with as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_peaks))` and dtype `torch.int32`.\n        match_edge_inds: Sample-grouped indices of the skeleton edge that each\n            connection corresponds to as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. This can be\n            generated by `PAFScorer.match_candidates()`.\n        match_src_peak_inds: Sample-grouped indices of the source peaks that form\n            each connection as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks\n            in each sample. This can be generated by `PAFScorer.match_candidates()`.\n        match_dst_peak_inds: Sample-grouped indices of the destination peaks that\n            form each connection as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks\n            in the sample. This can be generated by `PAFScorer.match_candidates()`.\n        match_line_scores: Sample-grouped PAF line scores of the matched connections\n            as a nested tensor `torch.Tensor` of shape `(n_samples, (n_connections))` and dtype\n            `torch.float32`. This can be generated by `PAFScorer.match_candidates()`.\n\n    Returns:\n        A tuple of arrays with the grouped instances for the whole batch grouped by\n        sample:\n\n        `predicted_instances`: The sample- and instance-grouped coordinates for each\n        instance as nested `torch.Tensor` of shape\n        `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n        peaks are represented by `NaN`s.\n\n        `predicted_peak_scores`: The sample- and instance-grouped confidence map\n        values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n        and dtype `torch.float32`.\n\n        `predicted_instance_scores`: The sample-grouped instance grouping score for\n        each instance as an array of shape `(n_samples, (n_instances))` and dtype\n        `torch.float32`.\n\n    Notes:\n        This is a convenience wrapper for the standalone `group_instances_batch()`.\n\n    See also: PAFScorer.match_candidates, group_instances_batch\n    \"\"\"\n    return group_instances_batch(\n        peaks,\n        peak_vals,\n        peak_channel_inds,\n        match_edge_inds,\n        match_src_peak_inds,\n        match_dst_peak_inds,\n        match_line_scores,\n        self.n_nodes,\n        self.sorted_edge_inds,\n        self.edge_types,\n        self.min_instance_peaks,\n        min_line_scores=self.min_line_scores,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.match_candidates","title":"<code>match_candidates(edge_inds, edge_peak_inds, line_scores)</code>","text":"<p>Match candidate connections for a batch based on PAF scores.</p> <p>Parameters:</p> Name Type Description Default <code>edge_inds</code> <code>Tensor</code> <p>Sample-grouped edge indices as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>PAFScorer.score_paf_lines()</code>.</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the peaks that form the source and destination of each candidate connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_candidates), 2)</code> and dtype <code>torch.int32</code>. Can be generated using <code>PAFScorer.score_paf_lines()</code>.</p> required <code>line_scores</code> <code>Tensor</code> <p>Sample-grouped scores for each candidate connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.float32</code>. Can be generated using <code>PAFScorer.score_paf_lines()</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>The connection peaks for each edge matched based on score as tuple of <code>(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code></p> <p><code>match_edge_inds</code>: Sample-grouped indices of the skeleton edge for each connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>.</p> <p><code>match_src_peak_inds</code>: Sample-grouped indices of the source peaks that form each connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_dst_peak_inds</code>: Sample-grouped indices of the destination peaks that form each connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_line_scores</code>: Sample-grouped PAF line scores of the matched connections as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>.</p> Notes <p>This is a convenience wrapper for the standalone <code>match_candidates_batch()</code>.</p> <p>See also: PAFScorer.score_paf_lines, match_candidates_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def match_candidates(\n    self,\n    edge_inds: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    line_scores: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Match candidate connections for a batch based on PAF scores.\n\n    Args:\n        edge_inds: Sample-grouped edge indices as a nested `torch.Tensor` of shape\n            `(n_samples, (n_candidates))` and dtype `torch.int32` indicating the\n            indices of the edge that each of the candidate connections belongs to.\n            Can be generated using `PAFScorer.score_paf_lines()`.\n        edge_peak_inds: Sample-grouped indices of the peaks that form the source and\n            destination of each candidate connection as a nested `torch.Tensor` of shape\n            `(n_samples, (n_candidates), 2)` and dtype `torch.int32`. Can be generated\n            using `PAFScorer.score_paf_lines()`.\n        line_scores: Sample-grouped scores for each candidate connection as a\n            nested `torch.Tensor` of shape `(n_samples, (n_candidates))` and dtype\n            `torch.float32`. Can be generated using `PAFScorer.score_paf_lines()`.\n\n    Returns:\n        The connection peaks for each edge matched based on score as tuple of\n        `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n        `match_edge_inds`: Sample-grouped indices of the skeleton edge for each\n        connection as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n        and dtype `torch.int32`.\n\n        `match_src_peak_inds`: Sample-grouped indices of the source peaks that form\n        each connection as a nested `torch.Tensor` of shape\n        `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n        indices correspond to the edge-grouped peaks, not the set of all peaks in\n        the sample.\n\n        `match_dst_peak_inds`: Sample-grouped indices of the destination peaks that\n        form each connection as a nested `torch.Tensor` of shape\n        `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n        indices correspond to the edge-grouped peaks, not the set of all peaks in\n        the sample.\n\n        `match_line_scores`: Sample-grouped PAF line scores of the matched\n        connections as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n        and dtype `torch.float32`.\n\n    Notes:\n        This is a convenience wrapper for the standalone `match_candidates_batch()`.\n\n    See also: PAFScorer.score_paf_lines, match_candidates_batch\n    \"\"\"\n    return match_candidates_batch(\n        edge_inds, edge_peak_inds, line_scores, self.n_edges\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.predict","title":"<code>predict(pafs, peaks, peak_vals, peak_channel_inds)</code>","text":"<p>Group a batch of predicted peaks into full instance predictions using PAFs.</p> <p>Parameters:</p> Name Type Description Default <code>pafs</code> <code>Tensor</code> <p>The batch of part affinity fields as a <code>torch.Tensor</code> of shape <code>(n_samples, height, width, 2 * n_edges)</code> and type <code>torch.float32</code>.</p> required <code>peaks</code> <code>Tensor</code> <p>The coordinates of the peaks grouped by sample as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks), 2)</code>.</p> required <code>peak_vals</code> <code>Tensor</code> <p>The sample-grouped scores of the detected peaks in a batch as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>The channel (node) that each peak in <code>peaks</code> corresponds to as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.int32</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of arrays with the grouped instances for the whole batch grouped by sample:</p> <p><code>predicted_instances</code>: The sample- and instance-grouped coordinates for each instance as nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes, 2)</code> and dtype <code>torch.float32</code>. Missing peaks are represented by <code>NaN</code>s.</p> <p><code>predicted_peak_scores</code>: The sample- and instance-grouped confidence map values for each peak as an array of <code>(n_samples, (n_instances), n_nodes)</code> and dtype <code>torch.float32</code>.</p> <p><code>predicted_instance_scores</code>: The sample-grouped instance grouping score for each instance as an array of shape <code>(n_samples, (n_instances))</code> and dtype <code>torch.float32</code>.</p> Notes <p>This is a high level API for grouping peaks into instances using PAFs.</p> <p>See the <code>PAFScorer</code> class documentation for more details on the algorithm.</p> See Also <p>PAFScorer.score_paf_lines, PAFScorer.match_candidates, PAFScorer.group_instances</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def predict(\n    self,\n    pafs: torch.Tensor,\n    peaks: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Group a batch of predicted peaks into full instance predictions using PAFs.\n\n    Args:\n        pafs: The batch of part affinity fields as a `torch.Tensor` of shape\n            `(n_samples, height, width, 2 * n_edges)` and type `torch.float32`.\n        peaks: The coordinates of the peaks grouped by sample as a nested `torch.Tensor`\n            of shape `(n_samples, (n_peaks), 2)`.\n        peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n            nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n            `torch.float32`.\n        peak_channel_inds: The channel (node) that each peak in `peaks` corresponds\n            to as a nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n            `torch.int32`.\n\n    Returns:\n        A tuple of arrays with the grouped instances for the whole batch grouped by\n        sample:\n\n        `predicted_instances`: The sample- and instance-grouped coordinates for each\n        instance as nested `torch.Tensor` of shape\n        `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n        peaks are represented by `NaN`s.\n\n        `predicted_peak_scores`: The sample- and instance-grouped confidence map\n        values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n        and dtype `torch.float32`.\n\n        `predicted_instance_scores`: The sample-grouped instance grouping score for\n        each instance as an array of shape `(n_samples, (n_instances))` and dtype\n        `torch.float32`.\n\n    Notes:\n        This is a high level API for grouping peaks into instances using PAFs.\n\n        See the `PAFScorer` class documentation for more details on the algorithm.\n\n    See Also:\n        PAFScorer.score_paf_lines, PAFScorer.match_candidates,\n        PAFScorer.group_instances\n    \"\"\"\n    edge_inds, edge_peak_inds, line_scores = self.score_paf_lines(\n        pafs, peaks, peak_channel_inds\n    )\n    (\n        match_edge_inds,\n        match_src_peak_inds,\n        match_dst_peak_inds,\n        match_line_scores,\n    ) = self.match_candidates(edge_inds, edge_peak_inds, line_scores)\n    (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n    ) = self.group_instances(\n        peaks,\n        peak_vals,\n        peak_channel_inds,\n        match_edge_inds,\n        match_src_peak_inds,\n        match_dst_peak_inds,\n        match_line_scores,\n    )\n    return (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n        edge_inds,\n        edge_peak_inds,\n        line_scores,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.score_paf_lines","title":"<code>score_paf_lines(pafs, peaks, peak_channel_inds)</code>","text":"<p>Create and score PAF lines formed between connection candidates.</p> <p>Parameters:</p> Name Type Description Default <code>pafs</code> <code>Tensor</code> <p>A nested torch tensor of shape <code>(n_samples, height, width, 2 * n_edges)</code> containing the part affinity fields for each sample in the batch.</p> required <code>peaks</code> <code>Tensor</code> <p>A nested torch tensor of shape <code>(n_samples, (n_peaks), 2)</code> containing the (x, y) coordinates of the detected peaks for each sample.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>A nested torch tensor of shape <code>(n_samples, (n_peaks))</code> indicating the channel (node) index that each peak corresponds to.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple containing three lists for each sample in the batch:     - A nested torch tensor of shape <code>(n_samples, (n_connections,))</code> indicating the indices     of the edges that each connection corresponds to.     - A nested torch tensor of shape <code>(n_samples, (n_connections, 2))</code> containing the indices     of the source and destination peaks forming each connection.     - A nested torch tensor of shape <code>(n_samples, (n_connections,))</code> containing the scores     for each connection based on the PAFs.</p> Notes <p>This is a convenience wrapper for the standalone <code>score_paf_lines_batch()</code>.</p> <p>See also: score_paf_lines_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def score_paf_lines(\n    self,\n    pafs: torch.Tensor,\n    peaks: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Create and score PAF lines formed between connection candidates.\n\n    Args:\n        pafs: A nested torch tensor of shape `(n_samples, height, width, 2 * n_edges)`\n            containing the part affinity fields for each sample in the batch.\n        peaks: A nested torch tensor of shape `(n_samples, (n_peaks), 2)` containing the\n            (x, y) coordinates of the detected peaks for each sample.\n        peak_channel_inds: A nested torch tensor of shape `(n_samples, (n_peaks))` indicating\n            the channel (node) index that each peak corresponds to.\n\n    Returns:\n        A tuple containing three lists for each sample in the batch:\n            - A nested torch tensor of shape `(n_samples, (n_connections,))` indicating the indices\n            of the edges that each connection corresponds to.\n            - A nested torch tensor of shape `(n_samples, (n_connections, 2))` containing the indices\n            of the source and destination peaks forming each connection.\n            - A nested torch tensor of shape `(n_samples, (n_connections,))` containing the scores\n            for each connection based on the PAFs.\n\n    Notes:\n        This is a convenience wrapper for the standalone `score_paf_lines_batch()`.\n\n    See also: score_paf_lines_batch\n    \"\"\"\n    return score_paf_lines_batch(\n        pafs,\n        peaks,\n        peak_channel_inds,\n        self.edge_inds,\n        self.n_points,\n        self.pafs_stride,\n        self.max_edge_length_ratio,\n        self.dist_penalty_weight,\n        self.n_nodes,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PeakID","title":"<code>PeakID</code>","text":"<p>Indices to uniquely identify a single peak.</p> <p>This is a convenience named tuple for use in the matching pipeline.</p> <p>Attributes:</p> Name Type Description <code>node_ind</code> <code>int</code> <p>Index of the node type (channel) of the peak.</p> <code>peak_ind</code> <code>int</code> <p>Index of the peak within its node type.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define(auto_attribs=True, frozen=True)\nclass PeakID:\n    \"\"\"Indices to uniquely identify a single peak.\n\n    This is a convenience named tuple for use in the matching pipeline.\n\n    Attributes:\n        node_ind: Index of the node type (channel) of the peak.\n        peak_ind: Index of the peak within its node type.\n    \"\"\"\n\n    node_ind: int\n    peak_ind: int\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.assign_connections_to_instances","title":"<code>assign_connections_to_instances(connections, min_instance_peaks=0, n_nodes=None)</code>","text":"<p>Assign connected edges to instances via greedy graph partitioning.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Dict[EdgeType, List[EdgeConnection]]</code> <p>A dict that maps EdgeType to a list of EdgeConnections found through connection scoring. This can be generated by the filter_connection_candidates function.</p> required <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>If this is greater than 0, grouped instances with fewer assigned peaks than this threshold will be excluded. If a float in the range (0., 1.] is provided, this is interpreted as a fraction of the total number of nodes in the skeleton. If an integer is provided, this is the absolute minimum number of peaks.</p> <code>0</code> <code>n_nodes</code> <code>int</code> <p>Total node type count. Used to convert min_instance_peaks to an absolute number when a fraction is specified. If not provided, the node count is inferred from the unique node inds in connections.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>instance_assignments</code> <code>Dict[PeakID, int]</code> <p>A dict mapping PeakID to a unique instance ID specified as an integer.</p> <p>A PeakID is a tuple of (node_type_ind, peak_ind), where the peak_ind is the index or identifier specified in a EdgeConnection as a src_peak_ind or dst_peak_ind.</p> Note <p>Instance IDs are not necessarily consecutive since some instances may be filtered out during the partitioning or filtering.</p> <p>This function expects connections from a single sample/frame!</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def assign_connections_to_instances(\n    connections: Dict[EdgeType, List[EdgeConnection]],\n    min_instance_peaks: Union[int, float] = 0,\n    n_nodes: int = None,\n) -&gt; Dict[PeakID, int]:\n    \"\"\"Assign connected edges to instances via greedy graph partitioning.\n\n    Args:\n        connections: A dict that maps EdgeType to a list of EdgeConnections found\n            through connection scoring. This can be generated by the\n            filter_connection_candidates function.\n        min_instance_peaks: If this is greater than 0, grouped instances with fewer\n            assigned peaks than this threshold will be excluded. If a float in the\n            range (0., 1.] is provided, this is interpreted as a fraction of the total\n            number of nodes in the skeleton. If an integer is provided, this is the\n            absolute minimum number of peaks.\n        n_nodes: Total node type count. Used to convert min_instance_peaks to an\n            absolute number when a fraction is specified. If not provided, the node\n            count is inferred from the unique node inds in connections.\n\n    Returns:\n        instance_assignments: A dict mapping PeakID to a unique instance ID specified\n        as an integer.\n\n        A PeakID is a tuple of (node_type_ind, peak_ind), where the peak_ind is the\n        index or identifier specified in a EdgeConnection as a src_peak_ind or\n        dst_peak_ind.\n\n    Note:\n        Instance IDs are not necessarily consecutive since some instances may be\n        filtered out during the partitioning or filtering.\n\n        This function expects connections from a single sample/frame!\n    \"\"\"\n    # Grouping table that maps PeakID(node_ind, peak_ind) to an instance_id.\n    instance_assignments = dict()\n\n    # Loop through edge types.\n    for edge_type, edge_connections in connections.items():\n        # Loop through connections for the current edge.\n        for connection in edge_connections:\n            # Notation: specific peaks are identified by (node_ind, peak_ind).\n            src_id = PeakID(edge_type.src_node_ind, connection.src_peak_ind)\n            dst_id = PeakID(edge_type.dst_node_ind, connection.dst_peak_ind)\n\n            # Get instance assignments for the connection peaks.\n            src_instance = instance_assignments.get(src_id, None)\n            dst_instance = instance_assignments.get(dst_id, None)\n\n            if src_instance is None and dst_instance is None:\n                # Case 1: Neither peak is assigned to an instance yet. We'll create a\n                # new instance to hold both.\n                new_instance = max(instance_assignments.values(), default=-1) + 1\n                instance_assignments[src_id] = new_instance\n                instance_assignments[dst_id] = new_instance\n\n            elif src_instance is not None and dst_instance is None:\n                # Case 2: The source peak is assigned already, but not the destination\n                # peak. We'll assign the destination peak to the same instance as the\n                # source.\n                instance_assignments[dst_id] = src_instance\n\n            elif src_instance is not None and dst_instance is not None:\n                # Case 3: Both peaks have been assigned. We'll update the destination\n                # peak to be a part of the source peak instance.\n                instance_assignments[dst_id] = src_instance\n\n                # We'll also check if they form disconnected subgraphs, in which case\n                # we'll merge them by assigning all peaks belonging to the destination\n                # peak's instance to the source peak's instance.\n                src_instance_nodes = set(\n                    peak_id.node_ind\n                    for peak_id, instance in instance_assignments.items()\n                    if instance == src_instance\n                )\n                dst_instance_nodes = set(\n                    peak_id.node_ind\n                    for peak_id, instance in instance_assignments.items()\n                    if instance == dst_instance\n                )\n\n                if len(src_instance_nodes.intersection(dst_instance_nodes)) == 0:\n                    for peak_id in instance_assignments:\n                        if instance_assignments[peak_id] == dst_instance:\n                            instance_assignments[peak_id] = src_instance\n\n    if min_instance_peaks &gt; 0:\n        if isinstance(min_instance_peaks, float):\n            if n_nodes is None:\n                # Infer number of nodes if not specified.\n                all_node_types = set()\n                for edge_type in connections:\n                    all_node_types.add(edge_type.src_node_ind)\n                    all_node_types.add(edge_type.dst_node_ind)\n                n_nodes = len(all_node_types)\n\n            # Calculate minimum threshold.\n            min_instance_peaks = int(min_instance_peaks * n_nodes)\n\n        # Compute instance peak counts.\n        instance_ids, instance_peak_counts = np.unique(\n            list(instance_assignments.values()), return_counts=True\n        )\n        instance_peak_counts = {\n            instance: peaks_count\n            for instance, peaks_count in zip(instance_ids, instance_peak_counts)\n        }\n\n        # Filter out small instances.\n        instance_assignments = {\n            peak_id: instance\n            for peak_id, instance in instance_assignments.items()\n            if instance_peak_counts[instance] &gt;= min_instance_peaks\n        }\n\n    return instance_assignments\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.compute_distance_penalty","title":"<code>compute_distance_penalty(spatial_vec_lengths, max_edge_length, dist_penalty_weight=1.0)</code>","text":"<p>Compute the distance penalty component of the PAF line integral score.</p> <p>Parameters:</p> Name Type Description Default <code>spatial_vec_lengths</code> <code>Tensor</code> <p>Euclidean distance between candidate source and destination points as a <code>torch.float32</code> tensor of any shape (typically <code>(n_candidates, 1)</code>).</p> required <code>max_edge_length</code> <code>float</code> <p>Maximum length expected for any connection as a scalar <code>float</code> in units of pixels (corresponding to <code>peaks_sample</code>). Scores of lines longer than this will be penalized. Useful for ignoring spurious connections that are far apart in space.</p> required <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The distance penalty for each candidate as a <code>torch.float32</code> tensor of the same shape as <code>spatial_vec_lengths</code>.</p> <p>The penalty will be 0 (when below the threshold) and -1 as the distance approaches infinity. This is then scaled by the <code>dist_penalty_weight</code>.</p> Notes <p>The penalty is computed from the distances scaled by the max length:</p> <pre><code>if distance &lt;= max_edge_length:\n    penalty = 0\nelse:\n    penalty = (max_edge_length / distance) - 1\n</code></pre> <p>For example, if the max length is 10 and the distance is 20, then the penalty will be: <code>(10 / 20) - 1 == 0.5 - 1 == -0.5</code>.</p> <p>See also: score_paf_lines</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def compute_distance_penalty(\n    spatial_vec_lengths: torch.Tensor,\n    max_edge_length: float,\n    dist_penalty_weight: float = 1.0,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the distance penalty component of the PAF line integral score.\n\n    Args:\n        spatial_vec_lengths: Euclidean distance between candidate source and\n            destination points as a `torch.float32` tensor of any shape (typically\n            `(n_candidates, 1)`).\n        max_edge_length: Maximum length expected for any connection as a scalar `float`\n            in units of pixels (corresponding to `peaks_sample`). Scores of lines\n            longer than this will be penalized. Useful for ignoring spurious\n            connections that are far apart in space.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n\n    Returns:\n        The distance penalty for each candidate as a `torch.float32` tensor of the same\n        shape as `spatial_vec_lengths`.\n\n        The penalty will be 0 (when below the threshold) and -1 as the distance\n        approaches infinity. This is then scaled by the `dist_penalty_weight`.\n\n    Notes:\n        The penalty is computed from the distances scaled by the max length:\n\n        ```\n        if distance &lt;= max_edge_length:\n            penalty = 0\n        else:\n            penalty = (max_edge_length / distance) - 1\n        ```\n\n        For example, if the max length is 10 and the distance is 20, then the penalty\n        will be: `(10 / 20) - 1 == 0.5 - 1 == -0.5`.\n\n    See also: score_paf_lines\n    \"\"\"\n    penalty = torch.clamp((max_edge_length / spatial_vec_lengths) - 1, max=0)\n    return penalty * dist_penalty_weight\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.get_connection_candidates","title":"<code>get_connection_candidates(peak_channel_inds_sample, skeleton_edges, n_nodes)</code>","text":"<p>Find the indices of all the possible connections formed by the detected peaks.</p> <p>Parameters:</p> Name Type Description Default <code>peak_channel_inds_sample</code> <code>Tensor</code> <p>The channel indices of the peaks found in a sample. This is a <code>torch.Tensor</code> of shape <code>(n_peaks,)</code> and dtype <code>torch.int32</code> that is used to represent a detected peak by its channel/node index in the skeleton.</p> required <code>skeleton_edges</code> <code>Tensor</code> <p>The indices of the nodes that form the skeleton graph as a <code>torch.Tensor</code> of shape <code>(n_edges, 2)</code> and dtype <code>torch.int32</code> where each row corresponds to the source and destination node indices.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton as a scalar integer.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of <code>(edge_inds, edge_peak_inds)</code>.</p> <p><code>edge_inds</code> is a <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> indicating the indices of the edge that each of the candidate connections belongs to.</p> <p><code>edge_peak_inds</code> is a <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peak_channel_inds_sample</code>.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def get_connection_candidates(\n    peak_channel_inds_sample: torch.Tensor, skeleton_edges: torch.Tensor, n_nodes: int\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of all the possible connections formed by the detected peaks.\n\n    Args:\n        peak_channel_inds_sample: The channel indices of the peaks found in a sample.\n            This is a `torch.Tensor` of shape `(n_peaks,)` and dtype `torch.int32` that is\n            used to represent a detected peak by its channel/node index in the skeleton.\n        skeleton_edges: The indices of the nodes that form the skeleton graph as a\n            `torch.Tensor` of shape `(n_edges, 2)` and dtype `torch.int32` where each row\n            corresponds to the source and destination node indices.\n        n_nodes: The total number of nodes in the skeleton as a scalar integer.\n\n    Returns:\n        A tuple of `(edge_inds, edge_peak_inds)`.\n\n        `edge_inds` is a `torch.Tensor` of shape `(n_candidates,)` indicating the indices\n        of the edge that each of the candidate connections belongs to.\n\n        `edge_peak_inds` is a `torch.Tensor` of shape `(n_candidates, 2)` with the indices\n        of the peaks that form the source and destination of each candidate connection.\n        This indexes into the input `peak_channel_inds_sample`.\n    \"\"\"\n    peak_inds = torch.argsort(peak_channel_inds_sample)\n    node_inds = torch.gather(peak_channel_inds_sample, 0, peak_inds)\n    node_grouped_peak_inds = [\n        peak_inds[node_inds == k] for k in range(n_nodes)\n    ]  # (n_nodes, (n_peaks_k))\n    edge_grouped_peak_inds = [\n        (node_grouped_peak_inds[src], node_grouped_peak_inds[dst])\n        for src, dst in skeleton_edges\n    ]  # (n_edges, (n_src_peaks), (n_dst_peaks))\n\n    edge_inds = []  # (n_edges, (n_src * n_dst))\n    edge_peak_inds = []  # (n_edges, (n_src * n_dst), 2)\n    for k, (src_peaks, dst_peaks) in enumerate(edge_grouped_peak_inds):\n        grid_src, grid_dst = torch.meshgrid(src_peaks, dst_peaks, indexing=\"ij\")\n        grid_src_dst = torch.stack([grid_src.flatten(), grid_dst.flatten()], dim=1)\n\n        edge_inds.append(torch.full((grid_src_dst.size(0),), k, dtype=torch.int32))\n        edge_peak_inds.append(grid_src_dst)\n\n    edge_inds = torch.cat(edge_inds)  # (n_candidates,)\n    edge_peak_inds = torch.cat(edge_peak_inds)  # (n_candidates, 2)\n\n    return edge_inds, edge_peak_inds\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.get_paf_lines","title":"<code>get_paf_lines(pafs_sample, peaks_sample, edge_peak_inds, edge_inds, n_line_points, pafs_stride)</code>","text":"<p>Get the PAF values at the lines formed between all detected peaks in a sample.</p> <p>Parameters:</p> Name Type Description Default <code>pafs_sample</code> <code>Tensor</code> <p>The PAFs for the sample as a <code>torch.Tensor</code> of shape <code>(height, width, 2 * n_edges)</code>.</p> required <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale (they will be scaled by the <code>pafs_stride</code>).</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peaks_sample</code>. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>edge_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>n_line_points</code> <code>int</code> <p>The number of points to interpolate between source and destination peaks in each connection candidate as a scalar integer. Values ranging from 5 to 10 are pretty reasonable.</p> required <code>pafs_stride</code> <code>int</code> <p>The stride (1/scale) of the PAFs that these lines will need to index into relative to the image. Coordinates in <code>peaks_sample</code> will be divided by this value to adjust the indexing into the PAFs tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The PAF vectors at all of the line points as a <code>torch.Tensor</code> of shape <code>(n_candidates, n_line_points, 2, 3)</code> and dtype <code>torch.int32</code>.</p> <p>The last dimension of the line subscripts correspond to the full <code>[row, col, channel]</code> subscripts of each element of the lines. Axis -2 contains the same <code>[row, col]</code> for each line but <code>channel</code> is adjusted to match the channels in the PAFs tensor.</p> Notes <p>If only the subscripts are needed, use <code>make_line_subs()</code> to generate the lines without retrieving the PAF vector at the line points.</p> <p>See also: get_connection_candidates, make_line_subs, score_paf_lines</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def get_paf_lines(\n    pafs_sample: torch.Tensor,\n    peaks_sample: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    edge_inds: torch.Tensor,\n    n_line_points: int,\n    pafs_stride: int,\n) -&gt; torch.Tensor:\n    \"\"\"Get the PAF values at the lines formed between all detected peaks in a sample.\n\n    Args:\n        pafs_sample: The PAFs for the sample as a `torch.Tensor` of shape\n            `(height, width, 2 * n_edges)`.\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale (they will be scaled by the `pafs_stride`).\n        edge_peak_inds: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype `torch.int32`\n            with the indices of the peaks that form the source and destination of each\n            candidate connection. This indexes into the input `peaks_sample`. Can be\n            generated using `get_connection_candidates()`.\n        edge_inds: A `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.int32`\n            indicating the indices of the edge that each of the candidate connections\n            belongs to. Can be generated using `get_connection_candidates()`.\n        n_line_points: The number of points to interpolate between source and\n            destination peaks in each connection candidate as a scalar integer. Values\n            ranging from 5 to 10 are pretty reasonable.\n        pafs_stride: The stride (1/scale) of the PAFs that these lines will need to\n            index into relative to the image. Coordinates in `peaks_sample` will be\n            divided by this value to adjust the indexing into the PAFs tensor.\n\n    Returns:\n        The PAF vectors at all of the line points as a `torch.Tensor` of shape\n        `(n_candidates, n_line_points, 2, 3)` and dtype `torch.int32`.\n\n        The last dimension of the line subscripts correspond to the full\n        `[row, col, channel]` subscripts of each element of the lines. Axis -2 contains\n        the same `[row, col]` for each line but `channel` is adjusted to match the\n        channels in the PAFs tensor.\n\n    Notes:\n        If only the subscripts are needed, use `make_line_subs()` to generate the lines\n        without retrieving the PAF vector at the line points.\n\n    See also: get_connection_candidates, make_line_subs, score_paf_lines\n    \"\"\"\n    pafs_hw = pafs_sample.shape[:2]\n    line_subs = make_line_subs(\n        peaks_sample, edge_peak_inds, edge_inds, n_line_points, pafs_stride, pafs_hw\n    )\n    lines = pafs_sample[line_subs[..., 0], line_subs[..., 1], line_subs[..., 2]]\n    return lines\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.group_instances_batch","title":"<code>group_instances_batch(peaks, peak_vals, peak_channel_inds, match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores, n_nodes, sorted_edge_inds, edge_types, min_instance_peaks, min_line_scores=0.25)</code>","text":"<p>Group matched connections into full instances for a batch.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <code>Tensor</code> <p>The sample-grouped detected peaks in a batch as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks), 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>peak_vals</code> <code>Tensor</code> <p>The sample-grouped scores of the detected peaks in a batch as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>The sample-grouped indices of the channel (node) that each detected peak is associated with as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.int32</code>.</p> required <code>match_edge_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the skeleton edge that each connection corresponds to as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>match_src_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the source peaks that form each connection as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in each sample. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>match_dst_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the destination peaks that form each connection as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>match_line_scores</code> <code>Tensor</code> <p>Sample-grouped PAF line scores of the matched connections as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton as a scalar integer.</p> required <code>sorted_edge_inds</code> <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> required <code>edge_types</code> <code>List[EdgeType]</code> <p>A torch nested <code>EdgeType</code>s associated with the skeleton.</p> required <code>min_instance_peaks</code> <code>int</code> <p>If this is greater than 0, grouped instances with fewer assigned peaks than this threshold will be excluded. If a <code>float</code> in the range <code>(0., 1.]</code> is provided, this is interpreted as a fraction of the total number of nodes in the skeleton. If an <code>int</code> is provided, this is the absolute minimum number of peaks.</p> required <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of <code>torch.Tensor</code> with the grouped instances for the whole batch grouped by sample:</p> <p><code>predicted_instances</code>: The sample- and instance-grouped coordinates for each instance as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes, 2)</code> and dtype <code>torch.float32</code>. Missing peaks are represented by <code>NaN</code>s.</p> <p><code>predicted_peak_scores</code>: The sample- and instance-grouped confidence map values for each peak as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes)</code> and dtype <code>torch.float32</code>.</p> <p><code>predicted_instance_scores</code>: The sample-grouped instance grouping score for each instance as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances))</code> and dtype <code>torch.float32</code>.</p> <p>See also: match_candidates_batch, group_instances_sample</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def group_instances_batch(\n    peaks: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    match_edge_inds: torch.Tensor,\n    match_src_peak_inds: torch.Tensor,\n    match_dst_peak_inds: torch.Tensor,\n    match_line_scores: torch.Tensor,\n    n_nodes: int,\n    sorted_edge_inds: Tuple[int],\n    edge_types: List[EdgeType],\n    min_instance_peaks: int,\n    min_line_scores: float = 0.25,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Group matched connections into full instances for a batch.\n\n    Args:\n        peaks: The sample-grouped detected peaks in a batch as a torch nested `torch.Tensor` of\n            shape `(n_samples, (n_peaks), 2)` and dtype `torch.float32`. These should be\n            `(x, y)` coordinates of each peak in the image scale.\n        peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n            torch nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype `torch.float32`.\n        peak_channel_inds: The sample-grouped indices of the channel (node) that each\n            detected peak is associated with as a torch nested `torch.Tensor` of shape\n            `(n_samples, (n_peaks))` and dtype `torch.int32`.\n        match_edge_inds: Sample-grouped indices of the skeleton edge that each\n            connection corresponds to as a torch nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. This can be generated\n            by `match_candidates_batch()`.\n        match_src_peak_inds: Sample-grouped indices of the source peaks that form each\n            connection as a torch nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n            and dtype `torch.int32`. Important: These indices correspond to the\n            edge-grouped peaks, not the set of all peaks in each sample. This can be\n            generated by `match_candidates_batch()`.\n        match_dst_peak_inds: Sample-grouped indices of the destination peaks that form\n            each connection as a torch nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks in\n            the sample. This can be generated by `match_candidates_batch()`.\n        match_line_scores: Sample-grouped PAF line scores of the matched connections as\n            a torch nested `torch.Tensor` of shape `(n_samples, (n_connections))` and dtype\n            `torch.float32`. This can be generated by `match_candidates_batch()`.\n        n_nodes: The total number of nodes in the skeleton as a scalar integer.\n        sorted_edge_inds: A tuple of indices specifying the topological order that the\n            edge types should be accessed in during instance assembly\n            (`assign_connections_to_instances`).\n        edge_types: A torch nested `EdgeType`s associated with the skeleton.\n        min_instance_peaks: If this is greater than 0, grouped instances with fewer\n            assigned peaks than this threshold will be excluded. If a `float` in the\n            range `(0., 1.]` is provided, this is interpreted as a fraction of the total\n            number of nodes in the skeleton. If an `int` is provided, this is the\n            absolute minimum number of peaks.\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs.\n\n    Returns:\n        A tuple of `torch.Tensor` with the grouped instances for the whole batch grouped by\n        sample:\n\n        `predicted_instances`: The sample- and instance-grouped coordinates for each\n        instance as a torch nested `torch.Tensor` of shape `(n_samples, (n_instances), n_nodes, 2)`\n        and dtype `torch.float32`. Missing peaks are represented by `NaN`s.\n\n        `predicted_peak_scores`: The sample- and instance-grouped confidence map values\n        for each peak as a torch nested `torch.Tensor` of shape `(n_samples, (n_instances), n_nodes)` and dtype\n        `torch.float32`.\n\n        `predicted_instance_scores`: The sample-grouped instance grouping score for each\n        instance as a torch nested `torch.Tensor` of shape `(n_samples, (n_instances))` and dtype\n        `torch.float32`.\n\n    See also: match_candidates_batch, group_instances_sample\n    \"\"\"\n    n_samples = len(peaks)\n    predicted_instances_batch = []\n    predicted_peak_scores_batch = []\n    predicted_instance_scores_batch = []\n\n    for sample in range(n_samples):\n        (\n            predicted_instances_sample,\n            predicted_peak_scores_sample,\n            predicted_instance_scores_sample,\n        ) = group_instances_sample(\n            peaks[sample],\n            peak_vals[sample],\n            peak_channel_inds[sample],\n            match_edge_inds[sample],\n            match_src_peak_inds[sample],\n            match_dst_peak_inds[sample],\n            match_line_scores[sample],\n            n_nodes,\n            sorted_edge_inds,\n            edge_types,\n            min_instance_peaks,\n            min_line_scores,\n        )\n\n        predicted_instances_batch.append(torch.tensor(predicted_instances_sample))\n        predicted_peak_scores_batch.append(torch.tensor(predicted_peak_scores_sample))\n        predicted_instance_scores_batch.append(\n            torch.tensor(predicted_instance_scores_sample)\n        )\n\n    return (\n        predicted_instances_batch,\n        predicted_peak_scores_batch,\n        predicted_instance_scores_batch,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.group_instances_sample","title":"<code>group_instances_sample(peaks_sample, peak_scores_sample, peak_channel_inds_sample, match_edge_inds_sample, match_src_peak_inds_sample, match_dst_peak_inds_sample, match_line_scores_sample, n_nodes, sorted_edge_inds, edge_types, min_instance_peaks, min_line_scores=0.25)</code>","text":"<p>Group matched connections into full instances for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>peak_scores_sample</code> <code>Tensor</code> <p>The scores of the detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks,)</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds_sample</code> <code>Tensor</code> <p>The indices of the channel (node) that each detected peak is associated with as a <code>torch.Tensor</code> of shape <code>(n_peaks,)</code> and dtype <code>torch.int32</code>.</p> required <code>match_edge_inds_sample</code> <code>Tensor</code> <p>Indices of the skeleton edge that each connection corresponds to as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>match_src_peak_inds_sample</code> <code>Tensor</code> <p>Indices of the source peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>match_dst_peak_inds_sample</code> <code>Tensor</code> <p>Indices of the destination peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>match_line_scores_sample</code> <code>Tensor</code> <p>PAF line scores of the matched connections as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton as a scalar integer.</p> required <code>sorted_edge_inds</code> <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> required <code>edge_types</code> <code>List[EdgeType]</code> <p>A list of <code>EdgeType</code>s associated with the skeleton.</p> required <code>min_instance_peaks</code> <code>int</code> <p>If this is greater than 0, grouped instances with fewer assigned peaks than this threshold will be excluded. If a <code>float</code> in the range <code>(0., 1.]</code> is provided, this is interpreted as a fraction of the total number of nodes in the skeleton. If an <code>int</code> is provided, this is the absolute minimum number of peaks.</p> required <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>A tuple of arrays with the grouped instances:</p> <p><code>predicted_instances</code>: The grouped coordinates for each instance as an array of shape <code>(n_instances, n_nodes, 2)</code> and dtype <code>float32</code>. Missing peaks are represented by <code>np.nan</code>s.</p> <p><code>predicted_peak_scores</code>: The confidence map values for each peak as an array of <code>(n_instances, n_nodes)</code> and dtype <code>float32</code>.</p> <p><code>predicted_instance_scores</code>: The grouping score for each instance as an array of shape <code>(n_instances,)</code> and dtype <code>float32</code>.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def group_instances_sample(\n    peaks_sample: torch.Tensor,\n    peak_scores_sample: torch.Tensor,\n    peak_channel_inds_sample: torch.Tensor,\n    match_edge_inds_sample: torch.Tensor,\n    match_src_peak_inds_sample: torch.Tensor,\n    match_dst_peak_inds_sample: torch.Tensor,\n    match_line_scores_sample: torch.Tensor,\n    n_nodes: int,\n    sorted_edge_inds: Tuple[int],\n    edge_types: List[EdgeType],\n    min_instance_peaks: int,\n    min_line_scores: float = 0.25,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Group matched connections into full instances for a single sample.\n\n    Args:\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale.\n        peak_scores_sample: The scores of the detected peaks in a sample as a\n            `torch.Tensor` of shape `(n_peaks,)` and dtype `torch.float32`.\n        peak_channel_inds_sample: The indices of the channel (node) that each detected\n            peak is associated with as a `torch.Tensor` of shape `(n_peaks,)` and dtype\n            `torch.int32`.\n        match_edge_inds_sample: Indices of the skeleton edge that each connection\n            corresponds to as a `torch.Tensor` of shape `(n_connections,)` and dtype\n            `torch.int32`. This can be generated by `match_candidates_sample()`.\n        match_src_peak_inds_sample: Indices of the source peaks that form each\n            connection as a `torch.Tensor` of shape `(n_connections,)` and dtype\n            `torch.int32`. Important: These indices correspond to the edge-grouped peaks,\n            not the set of all peaks in the sample. This can be generated by\n            `match_candidates_sample()`.\n        match_dst_peak_inds_sample: Indices of the destination peaks that form each\n            connection as a `torch.Tensor` of shape `(n_connections,)` and dtype\n            `torch.int32`. Important: These indices correspond to the edge-grouped peaks,\n            not the set of all peaks in the sample. This can be generated by\n            `match_candidates_sample()`.\n        match_line_scores_sample: PAF line scores of the matched connections as a\n            `torch.Tensor` of shape `(n_connections,)` and dtype `torch.float32`. This can be\n            generated by `match_candidates_sample()`.\n        n_nodes: The total number of nodes in the skeleton as a scalar integer.\n        sorted_edge_inds: A tuple of indices specifying the topological order that the\n            edge types should be accessed in during instance assembly\n            (`assign_connections_to_instances`).\n        edge_types: A list of `EdgeType`s associated with the skeleton.\n        min_instance_peaks: If this is greater than 0, grouped instances with fewer\n            assigned peaks than this threshold will be excluded. If a `float` in the\n            range `(0., 1.]` is provided, this is interpreted as a fraction of the total\n            number of nodes in the skeleton. If an `int` is provided, this is the\n            absolute minimum number of peaks.\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs.\n\n    Returns:\n        A tuple of arrays with the grouped instances:\n\n        `predicted_instances`: The grouped coordinates for each instance as an array of\n        shape `(n_instances, n_nodes, 2)` and dtype `float32`. Missing peaks are\n        represented by `np.nan`s.\n\n        `predicted_peak_scores`: The confidence map values for each peak as an array of\n        `(n_instances, n_nodes)` and dtype `float32`.\n\n        `predicted_instance_scores`: The grouping score for each instance as an array of\n        shape `(n_instances,)` and dtype `float32`.\n    \"\"\"\n    # Convert PyTorch tensors to NumPy arrays for non-tensor computations\n    if isinstance(peaks_sample, torch.Tensor):\n        peaks_sample = peaks_sample.cpu().numpy()\n        peak_scores_sample = peak_scores_sample.cpu().numpy()\n        peak_channel_inds_sample = peak_channel_inds_sample.cpu().numpy()\n        match_edge_inds_sample = match_edge_inds_sample.cpu().numpy()\n        match_src_peak_inds_sample = match_src_peak_inds_sample.cpu().numpy()\n        match_dst_peak_inds_sample = match_dst_peak_inds_sample.cpu().numpy()\n        match_line_scores_sample = match_line_scores_sample.cpu().numpy()\n\n    # Filter out low scoring matches.\n    is_valid_match = match_line_scores_sample &gt;= min_line_scores\n    match_edge_inds_sample = match_edge_inds_sample[is_valid_match]\n    match_src_peak_inds_sample = match_src_peak_inds_sample[is_valid_match]\n    match_dst_peak_inds_sample = match_dst_peak_inds_sample[is_valid_match]\n    match_line_scores_sample = match_line_scores_sample[is_valid_match]\n\n    # Group peaks by channel.\n    peaks = []\n    peak_scores = []\n    for i in range(n_nodes):\n        in_channel = peak_channel_inds_sample == i\n        peaks.append(peaks_sample[in_channel])\n        peak_scores.append(peak_scores_sample[in_channel])\n\n    # Group connection data by edge in sorted order.\n    # Note: This step is crucial since the instance assembly depends on the ordering\n    # of the edges.\n    connections = {}\n    for edge_ind in sorted_edge_inds:\n        in_edge = match_edge_inds_sample == edge_ind\n        edge_type = edge_types[edge_ind]\n\n        src_peak_inds = match_src_peak_inds_sample[in_edge]\n        dst_peak_inds = match_dst_peak_inds_sample[in_edge]\n        line_scores = match_line_scores_sample[in_edge]\n\n        connections[edge_type] = [\n            EdgeConnection(src, dst, score)\n            for src, dst, score in zip(src_peak_inds, dst_peak_inds, line_scores)\n        ]\n\n    # Bipartite graph partitioning to group connections into instances.\n    instance_assignments = assign_connections_to_instances(\n        connections,\n        min_instance_peaks=min_instance_peaks,\n        n_nodes=n_nodes,\n    )\n\n    # Gather the data by instance.\n    (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n    ) = make_predicted_instances(peaks, peak_scores, connections, instance_assignments)\n\n    return predicted_instances, predicted_peak_scores, predicted_instance_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.make_line_subs","title":"<code>make_line_subs(peaks_sample, edge_peak_inds, edge_inds, n_line_points, pafs_stride, pafs_hw)</code>","text":"<p>Create the lines between candidate connections for evaluating the PAFs.</p> <p>Parameters:</p> Name Type Description Default <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale (they will be scaled by the <code>pafs_stride</code>).</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peaks_sample</code>. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>edge_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>n_line_points</code> <code>int</code> <p>The number of points to interpolate between source and destination peaks in each connection candidate as a scalar integer. Values ranging from 5 to 10 are pretty reasonable.</p> required <code>pafs_stride</code> <code>int</code> <p>The stride (1/scale) of the PAFs that these lines will need to index into relative to the image. Coordinates in <code>peaks_sample</code> will be divided by this value to adjust the indexing into the PAFs tensor.</p> required <code>pafs_hw</code> <code>tuple</code> <p>Tuple (height, width) with the dimension of PAFs tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The line subscripts as a <code>torch.Tensor</code> of shape <code>(n_candidates, n_line_points, 2, 3)</code> and dtype <code>torch.int32</code>.</p> <p>The last dimension of the line subscripts correspond to the full <code>[row, col, channel]</code> subscripts of each element of the lines. Axis -2 contains the same <code>[row, col]</code> for each line but <code>channel</code> is adjusted to match the channels in the PAFs tensor.</p> Notes <p>The subscripts are interpolated via nearest neighbor, so multiple fractional coordinates may map on to the same pixel if the line is short.</p> <p>See also: get_connection_candidates</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def make_line_subs(\n    peaks_sample: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    edge_inds: torch.Tensor,\n    n_line_points: int,\n    pafs_stride: int,\n    pafs_hw: tuple,\n) -&gt; torch.Tensor:\n    \"\"\"Create the lines between candidate connections for evaluating the PAFs.\n\n    Args:\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale (they will be scaled by the `pafs_stride`).\n        edge_peak_inds: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype `torch.int32`\n            with the indices of the peaks that form the source and destination of each\n            candidate connection. This indexes into the input `peaks_sample`. Can be\n            generated using `get_connection_candidates()`.\n        edge_inds: A `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.int32`\n            indicating the indices of the edge that each of the candidate connections\n            belongs to. Can be generated using `get_connection_candidates()`.\n        n_line_points: The number of points to interpolate between source and\n            destination peaks in each connection candidate as a scalar integer. Values\n            ranging from 5 to 10 are pretty reasonable.\n        pafs_stride: The stride (1/scale) of the PAFs that these lines will need to\n            index into relative to the image. Coordinates in `peaks_sample` will be\n            divided by this value to adjust the indexing into the PAFs tensor.\n        pafs_hw: Tuple (height, width) with the dimension of PAFs tensor.\n\n    Returns:\n        The line subscripts as a `torch.Tensor` of shape\n        `(n_candidates, n_line_points, 2, 3)` and dtype `torch.int32`.\n\n        The last dimension of the line subscripts correspond to the full\n        `[row, col, channel]` subscripts of each element of the lines. Axis -2 contains\n        the same `[row, col]` for each line but `channel` is adjusted to match the\n        channels in the PAFs tensor.\n\n    Notes:\n        The subscripts are interpolated via nearest neighbor, so multiple fractional\n        coordinates may map on to the same pixel if the line is short.\n\n    See also: get_connection_candidates\n    \"\"\"\n    src_peaks = torch.index_select(peaks_sample, 0, edge_peak_inds[:, 0])\n    dst_peaks = torch.index_select(peaks_sample, 0, edge_peak_inds[:, 1])\n    n_candidates = torch.tensor(src_peaks.shape[0], device=peaks_sample.device)\n\n    X = torch.cat(\n        (src_peaks[:, 0].unsqueeze(dim=-1), dst_peaks[:, 0].unsqueeze(dim=-1)), dim=-1\n    ).to(torch.float32)\n    Y = torch.cat(\n        (src_peaks[:, 1].unsqueeze(dim=-1), dst_peaks[:, 1].unsqueeze(dim=-1)), dim=-1\n    ).to(torch.float32)\n    samples = torch.tensor([0, 1], device=X.device).repeat(n_candidates, 1)\n    samples_new = torch.linspace(0, 1, steps=n_line_points, device=X.device).repeat(\n        n_candidates, 1\n    )\n\n    X = interp1d(samples, X, samples_new).unsqueeze(\n        dim=1\n    )  # (n_candidates, 1, n_line_points)\n    Y = interp1d(samples, Y, samples_new).unsqueeze(\n        dim=1\n    )  # (n_candidates, 1, n_line_points)\n    XY = torch.concat([X, Y], dim=1)\n\n    XY = (\n        (XY / pafs_stride).round().int()\n    )  # (n_candidates, 2, n_line_points)  # dim 1 is [x, y]\n    XY = XY[:, [1, 0], :]  # dim 1 is [row, col]\n\n    # clip coordinates for size of pafs tensor.\n    height, width = pafs_hw\n    XY[:, 0] = torch.clip(XY[:, 0], min=0, max=height - 1)\n    XY[:, 1] = torch.clip(XY[:, 1], min=0, max=width - 1)\n\n    edge_inds_expanded = (\n        edge_inds.view(-1, 1, 1)\n        .expand(-1, 1, n_line_points)\n        .to(device=peaks_sample.device)\n    )\n    line_subs = torch.cat((XY, edge_inds_expanded), dim=1)\n    line_subs = line_subs.permute(\n        0, 2, 1\n    )  # (n_candidates, n_line_points, 3) -- last dim is [row, col, edge_ind]\n\n    multiplier = torch.tensor(\n        [1, 1, 2], dtype=torch.int32, device=line_subs.device\n    ).view(1, 1, 3)\n    adder = torch.tensor([0, 0, 1], dtype=torch.int32, device=line_subs.device).view(\n        1, 1, 3\n    )\n\n    line_subs_first = line_subs * multiplier\n    line_subs_second = line_subs * multiplier + adder\n    line_subs = torch.stack(\n        (line_subs_first, line_subs_second), dim=2\n    )  # (n_candidates, n_line_points, 2, 3)\n    # The last dim is [row, col, edge_ind], but for both PAF (x and y) edge channels.\n\n    return line_subs\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.make_predicted_instances","title":"<code>make_predicted_instances(peaks, peak_scores, connections, instance_assignments)</code>","text":"<p>Group peaks by assignments and accumulate scores.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <code>array</code> <p>Node-grouped peaks</p> required <code>peak_scores</code> <code>array</code> <p>Node-grouped peak scores</p> required <code>connections</code> <code>List[EdgeConnection]</code> <p><code>EdgeConnection</code>s grouped by edge type</p> required <code>instance_assignments</code> <code>Dict[PeakID, int]</code> <p><code>PeakID</code> to instance ID mapping</p> required <p>Returns:</p> Type Description <code>Tuple[array, array, array]</code> <p>Tuple of (predicted_instances, predicted_peak_scores, predicted_instance_scores)</p> <p>predicted_instances: (n_instances, n_nodes, 2) array predicted_peak_scores: (n_instances, n_nodes) array predicted_instance_scores: (n_instances,) array</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def make_predicted_instances(\n    peaks: np.array,\n    peak_scores: np.array,\n    connections: List[EdgeConnection],\n    instance_assignments: Dict[PeakID, int],\n) -&gt; Tuple[np.array, np.array, np.array]:\n    \"\"\"Group peaks by assignments and accumulate scores.\n\n    Args:\n        peaks: Node-grouped peaks\n        peak_scores: Node-grouped peak scores\n        connections: `EdgeConnection`s grouped by edge type\n        instance_assignments: `PeakID` to instance ID mapping\n\n    Returns:\n        Tuple of (predicted_instances, predicted_peak_scores, predicted_instance_scores)\n\n        predicted_instances: (n_instances, n_nodes, 2) array\n        predicted_peak_scores: (n_instances, n_nodes) array\n        predicted_instance_scores: (n_instances,) array\n    \"\"\"\n    # Ensure instance IDs are contiguous.\n    instance_ids, instance_inds = np.unique(\n        list(instance_assignments.values()), return_inverse=True\n    )\n    for peak_id, instance_ind in zip(instance_assignments.keys(), instance_inds):\n        instance_assignments[peak_id] = instance_ind\n    n_instances = len(instance_ids)\n\n    # Compute instance scores as the sum of all edge scores.\n    predicted_instance_scores = np.full((n_instances,), 0.0, dtype=\"float32\")\n\n    for edge_type, edge_connections in connections.items():\n        # Loop over all connections for this edge type.\n        for edge_connection in edge_connections:\n            # Look up the source peak.\n            src_peak_id = PeakID(\n                node_ind=edge_type.src_node_ind, peak_ind=edge_connection.src_peak_ind\n            )\n            if src_peak_id in instance_assignments:\n                # Add to the total instance score.\n                instance_ind = instance_assignments[src_peak_id]\n                predicted_instance_scores[instance_ind] += edge_connection.score\n\n                # Sanity check: both peaks in the edge should have been assigned to the\n                # same instance.\n                dst_peak_id = PeakID(\n                    node_ind=edge_type.dst_node_ind,\n                    peak_ind=edge_connection.dst_peak_ind,\n                )\n                assert instance_ind == instance_assignments[dst_peak_id]\n\n    # Fill out instances and peak scores.\n    n_nodes = len(peaks)\n    predicted_instances = np.full((n_instances, n_nodes, 2), np.nan, dtype=\"float32\")\n    predicted_peak_scores = np.full((n_instances, n_nodes), np.nan, dtype=\"float32\")\n    for peak_id, instance_ind in instance_assignments.items():\n        predicted_instances[instance_ind, peak_id.node_ind, :] = peaks[\n            peak_id.node_ind\n        ][peak_id.peak_ind]\n        predicted_peak_scores[instance_ind, peak_id.node_ind] = peak_scores[\n            peak_id.node_ind\n        ][peak_id.peak_ind]\n\n    return predicted_instances, predicted_peak_scores, predicted_instance_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.match_candidates_batch","title":"<code>match_candidates_batch(edge_inds, edge_peak_inds, line_scores, n_edges)</code>","text":"<p>Match candidate connections for a batch based on PAF scores.</p> <p>Parameters:</p> Name Type Description Default <code>edge_inds</code> <code>Tensor</code> <p>Sample-grouped edge indices as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>score_paf_lines_batch()</code>.</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the peaks that form the source and destination of each candidate connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_candidates), 2)</code> and dtype <code>torch.int32</code>. Can be generated using <code>score_paf_lines_batch()</code>.</p> required <code>line_scores</code> <code>Tensor</code> <p>Sample-grouped scores for each candidate connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.float32</code>. Can be generated using <code>score_paf_lines_batch()</code>.</p> required <code>n_edges</code> <code>int</code> <p>A scalar <code>int</code> denoting the number of edges in the skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>The connection peaks for each edge matched based on score as tuple of <code>(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code></p> <p><code>match_edge_inds</code>: Sample-grouped indices of the skeleton edge for each connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>.</p> <p><code>match_src_peak_inds</code>: Sample-grouped indices of the source peaks that form each connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_dst_peak_inds</code>: Sample-grouped indices of the destination peaks that form each connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_line_scores</code>: Sample-grouped PAF line scores of the matched connections as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>.</p> Notes <p>The matching is performed using the Munkres algorithm implemented in <code>scipy.optimize.linear_sum_assignment()</code>.</p> <p>See also: match_candidates_sample, score_paf_lines_batch, group_instances_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def match_candidates_batch(\n    edge_inds: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    line_scores: torch.Tensor,\n    n_edges: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Match candidate connections for a batch based on PAF scores.\n\n    Args:\n        edge_inds: Sample-grouped edge indices as a torch nested `torch.Tensor`s of shape\n            `(n_samples, (n_candidates))` and dtype `torch.int32` indicating the indices\n            of the edge that each of the candidate connections belongs to. Can be\n            generated using `score_paf_lines_batch()`.\n        edge_peak_inds: Sample-grouped indices of the peaks that form the source and\n            destination of each candidate connection as a torch nested `torch.Tensor`s of shape\n            `(n_samples, (n_candidates), 2)` and dtype `torch.int32`. Can be generated\n            using `score_paf_lines_batch()`.\n        line_scores: Sample-grouped scores for each candidate connection as a\n            torch nested `torch.Tensor`s of shape `(n_samples, (n_candidates))` and dtype\n            `torch.float32`. Can be generated using `score_paf_lines_batch()`.\n        n_edges: A scalar `int` denoting the number of edges in the skeleton.\n\n    Returns:\n        The connection peaks for each edge matched based on score as tuple of\n        `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n        `match_edge_inds`: Sample-grouped indices of the skeleton edge for each\n        connection as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))` and\n        dtype `torch.int32`.\n\n        `match_src_peak_inds`: Sample-grouped indices of the source peaks that form each\n        connection as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))` and\n        dtype `torch.int32`. Important: These indices correspond to the edge-grouped peaks,\n        not the set of all peaks in the sample.\n\n        `match_dst_peak_inds`: Sample-grouped indices of the destination peaks that form\n        each connection as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))`\n        and dtype `torch.int32`. Important: These indices correspond to the edge-grouped\n        peaks, not the set of all peaks in the sample.\n\n        `match_line_scores`: Sample-grouped PAF line scores of the matched connections\n        as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))` and dtype\n        `torch.float32`.\n\n    Notes:\n        The matching is performed using the Munkres algorithm implemented in\n        `scipy.optimize.linear_sum_assignment()`.\n\n    See also: match_candidates_sample, score_paf_lines_batch, group_instances_batch\n    \"\"\"\n    match_sample_inds = []\n    match_edge_inds = []\n    match_src_peak_inds = []\n    match_dst_peak_inds = []\n    match_line_scores = []\n\n    for sample in range(len(edge_inds)):\n        edge_inds_sample = edge_inds[sample]\n        edge_peak_inds_sample = edge_peak_inds[sample]\n        line_scores_sample = line_scores[sample]\n\n        matched_sample = match_candidates_sample(\n            edge_inds_sample, edge_peak_inds_sample, line_scores_sample, n_edges\n        )\n\n        (\n            match_edge_inds_sample,\n            match_src_peak_inds_sample,\n            match_dst_peak_inds_sample,\n            match_line_scores_sample,\n        ) = matched_sample\n\n        match_sample_inds.append(\n            torch.full_like(match_edge_inds_sample, sample, dtype=torch.int32)\n        )\n        match_edge_inds.append(match_edge_inds_sample)\n        match_src_peak_inds.append(match_src_peak_inds_sample)\n        match_dst_peak_inds.append(match_dst_peak_inds_sample)\n        match_line_scores.append(match_line_scores_sample)\n\n    return match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.match_candidates_sample","title":"<code>match_candidates_sample(edge_inds_sample, edge_peak_inds_sample, line_scores_sample, n_edges)</code>","text":"<p>Match candidate connections for a sample based on PAF scores.</p> <p>Parameters:</p> Name Type Description Default <code>edge_inds_sample</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to for the sample. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>edge_peak_inds_sample</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>line_scores_sample</code> <code>Tensor</code> <p>Scores for each candidate connection in the sample as a <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.float32</code>. Can be generated using <code>score_paf_lines()</code>.</p> required <code>n_edges</code> <code>int</code> <p>A scalar <code>int</code> denoting the number of edges in the skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>The connection peaks for each edge matched based on score as tuple of <code>(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code></p> <p><code>match_edge_inds</code>: Indices of the skeleton edge that each connection corresponds to as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>.</p> <p><code>match_src_peak_inds</code>: Indices of the source peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_dst_peak_inds</code>: Indices of the destination peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_line_scores</code>: PAF line scores of the matched connections as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.float32</code>.</p> Notes <p>The matching is performed using the Munkres algorithm implemented in <code>scipy.optimize.linear_sum_assignment()</code>.</p> <p>See also: match_candidates_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def match_candidates_sample(\n    edge_inds_sample: torch.Tensor,\n    edge_peak_inds_sample: torch.Tensor,\n    line_scores_sample: torch.Tensor,\n    n_edges: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Match candidate connections for a sample based on PAF scores.\n\n    Args:\n        edge_inds_sample: A `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.int32`\n            indicating the indices of the edge that each of the candidate connections\n            belongs to for the sample. Can be generated using\n            `get_connection_candidates()`.\n        edge_peak_inds_sample: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype\n            `torch.int32` with the indices of the peaks that form the source and\n            destination of each candidate connection. Can be generated using\n            `get_connection_candidates()`.\n        line_scores_sample: Scores for each candidate connection in the sample as a\n            `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.float32`. Can be\n            generated using `score_paf_lines()`.\n        n_edges: A scalar `int` denoting the number of edges in the skeleton.\n\n    Returns:\n        The connection peaks for each edge matched based on score as tuple of\n        `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n        `match_edge_inds`: Indices of the skeleton edge that each connection corresponds\n        to as a `torch.Tensor` of shape `(n_connections,)` and dtype `torch.int32`.\n\n        `match_src_peak_inds`: Indices of the source peaks that form each connection\n        as a `torch.Tensor` of shape `(n_connections,)` and dtype `torch.int32`. Important:\n        These indices correspond to the edge-grouped peaks, not the set of all peaks in\n        the sample.\n\n        `match_dst_peak_inds`: Indices of the destination peaks that form each\n        connection as a `torch.Tensor` of shape `(n_connections,)` and dtype `torch.int32`.\n        Important: These indices correspond to the edge-grouped peaks, not the set of\n        all peaks in the sample.\n\n        `match_line_scores`: PAF line scores of the matched connections as a `torch.Tensor`\n        of shape `(n_connections,)` and dtype `torch.float32`.\n\n    Notes:\n        The matching is performed using the Munkres algorithm implemented in\n        `scipy.optimize.linear_sum_assignment()`.\n\n    See also: match_candidates_batch\n    \"\"\"\n    match_edge_inds = []\n    match_src_peak_inds = []\n    match_dst_peak_inds = []\n    match_line_scores = []\n\n    for k in range(n_edges):\n        is_edge_k = (edge_inds_sample == k).nonzero(as_tuple=True)[0]\n        edge_peak_inds_k = edge_peak_inds_sample[is_edge_k]\n        line_scores_k = line_scores_sample[is_edge_k]\n\n        # Get the unique peak indices.\n        src_peak_inds_k = torch.unique(edge_peak_inds_k[:, 0])\n        dst_peak_inds_k = torch.unique(edge_peak_inds_k[:, 1])\n\n        n_src = src_peak_inds_k.size(0)\n        n_dst = dst_peak_inds_k.size(0)\n\n        # Initialize cost matrix with infinite cost.\n        cost_matrix = torch.full((n_src, n_dst), np.inf)\n\n        # Update cost matrix with line scores.\n        for i, src_ind in enumerate(src_peak_inds_k):\n            for j, dst_ind in enumerate(dst_peak_inds_k):\n                mask = (edge_peak_inds_k[:, 0] == src_ind) &amp; (\n                    edge_peak_inds_k[:, 1] == dst_ind\n                )\n                if mask.any():\n                    cost_matrix[i, j] = -line_scores_k[\n                        mask\n                    ].item()  # Flip sign for maximization.\n\n        # Convert cost matrix to numpy for use with scipy's linear_sum_assignment.\n        cost_matrix_np = cost_matrix.numpy()\n        cost_matrix_np[np.isnan(cost_matrix_np)] = np.inf\n\n        # Match.\n        match_src_inds, match_dst_inds = linear_sum_assignment(cost_matrix_np)\n\n        # Pull out matched scores from the numpy cost matrix.\n        match_line_scores_k = -cost_matrix_np[\n            match_src_inds, match_dst_inds\n        ]  # Flip sign back.\n\n        # Get the peak indices for the matched points (these index into peaks_sample).\n        # These index into the edge-grouped peaks.\n        match_src_peak_inds_k = match_src_inds\n        match_dst_peak_inds_k = match_dst_inds\n\n        # Save.\n        match_edge_inds.append(\n            torch.full((match_src_peak_inds_k.size,), k, dtype=torch.int32)\n        )\n        match_src_peak_inds.append(\n            torch.tensor(match_src_peak_inds_k, dtype=torch.int32)\n        )\n        match_dst_peak_inds.append(\n            torch.tensor(match_dst_peak_inds_k, dtype=torch.int32)\n        )\n        match_line_scores.append(torch.tensor(match_line_scores_k, dtype=torch.float32))\n\n    # Convert lists to tensors.\n    match_edge_inds = torch.cat(match_edge_inds)\n    match_src_peak_inds = torch.cat(match_src_peak_inds)\n    match_dst_peak_inds = torch.cat(match_dst_peak_inds)\n    match_line_scores = torch.cat(match_line_scores)\n\n    return match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.score_paf_lines","title":"<code>score_paf_lines(paf_lines_sample, peaks_sample, edge_peak_inds_sample, max_edge_length, dist_penalty_weight=1.0)</code>","text":"<p>Compute the connectivity score for each PAF line in a sample.</p> <p>Parameters:</p> Name Type Description Default <code>paf_lines_sample</code> <code>Tensor</code> <p>The PAF vectors evaluated at the lines formed between candidate connections as a <code>torch.Tensor</code> of shape <code>(n_candidates, n_line_points, 2, 3)</code> dtype <code>torch.int32</code>. This can be generated by <code>get_paf_lines()</code>.</p> required <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>edge_peak_inds_sample</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peaks_sample</code>. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>max_edge_length</code> <code>float</code> <p>Maximum length expected for any connection as a scalar <code>float</code> in units of pixels (corresponding to <code>peaks_sample</code>). Scores of lines longer than this will be penalized. Useful for ignoring spurious connections that are far apart in space.</p> required <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The line scores as a <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.float32</code>. Each score value is the average dot product between the PAFs and the normalized displacement vector between source and destination peaks.</p> <p>Scores range from roughly -1.5 to 1.0, where larger values indicate a better connectivity score for the candidate. Values can be larger or smaller due to prediction error.</p> Notes <p>This function operates on a single sample (frame). For batches of multiple frames, use <code>score_paf_lines_batch()</code>.</p> <p>See also: get_paf_lines, score_paf_lines_batch, compute_distance_penalty</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def score_paf_lines(\n    paf_lines_sample: torch.Tensor,\n    peaks_sample: torch.Tensor,\n    edge_peak_inds_sample: torch.Tensor,\n    max_edge_length: float,\n    dist_penalty_weight: float = 1.0,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the connectivity score for each PAF line in a sample.\n\n    Args:\n        paf_lines_sample: The PAF vectors evaluated at the lines formed between\n            candidate connections as a `torch.Tensor` of shape\n            `(n_candidates, n_line_points, 2, 3)` dtype `torch.int32`. This can be\n            generated by `get_paf_lines()`.\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale.\n        edge_peak_inds_sample: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype\n            `torch.int32` with the indices of the peaks that form the source and\n            destination of each candidate connection. This indexes into the input\n            `peaks_sample`. Can be generated using `get_connection_candidates()`.\n        max_edge_length: Maximum length expected for any connection as a scalar `float`\n            in units of pixels (corresponding to `peaks_sample`). Scores of lines\n            longer than this will be penalized. Useful for ignoring spurious\n            connections that are far apart in space.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n\n    Returns:\n        The line scores as a `torch.Tensor` of shape `(n_candidates,)` and dtype\n        `torch.float32`. Each score value is the average dot product between the PAFs and\n        the normalized displacement vector between source and destination peaks.\n\n        Scores range from roughly -1.5 to 1.0, where larger values indicate a better\n        connectivity score for the candidate. Values can be larger or smaller due to\n        prediction error.\n\n    Notes:\n        This function operates on a single sample (frame). For batches of multiple\n        frames, use `score_paf_lines_batch()`.\n\n    See also: get_paf_lines, score_paf_lines_batch, compute_distance_penalty\n    \"\"\"\n    # Pull out points using advanced indexing\n    src_peaks = peaks_sample[edge_peak_inds_sample[:, 0]]  # (n_candidates, 2)\n    dst_peaks = peaks_sample[edge_peak_inds_sample[:, 1]]  # (n_candidates, 2)\n\n    # Compute normalized spatial displacement vector\n    spatial_vecs = dst_peaks - src_peaks\n    spatial_vec_lengths = torch.norm(\n        spatial_vecs, dim=1, keepdim=True\n    )  # (n_candidates, 1)\n    spatial_vecs = spatial_vecs / spatial_vec_lengths  # Normalize\n\n    # Compute similarity scores\n    spatial_vecs = spatial_vecs.unsqueeze(2)  # Add dimension for matrix multiplication\n    line_scores = torch.squeeze(\n        paf_lines_sample @ spatial_vecs, dim=-1\n    )  # (n_candidates, n_line_points)\n\n    # Compute distance penalties\n    dist_penalties = torch.squeeze(\n        compute_distance_penalty(\n            spatial_vec_lengths,\n            max_edge_length,\n            dist_penalty_weight=dist_penalty_weight,\n        ),\n        dim=1,\n    )  # (n_candidates,)\n\n    # Compute average line scores with distance penalty.\n    mean_line_scores = torch.mean(line_scores, dim=1)\n    penalized_line_scores = mean_line_scores + dist_penalties  # (n_candidates,)\n\n    return penalized_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.score_paf_lines_batch","title":"<code>score_paf_lines_batch(pafs, peaks, peak_channel_inds, skeleton_edges, n_line_points, pafs_stride, max_edge_length_ratio, dist_penalty_weight, n_nodes)</code>","text":"<p>Process a batch of images to score the Part Affinity Fields (PAFs) lines formed between connection candidates for each sample.</p> <p>This function loops over each sample in the batch and applies the process of getting connection candidates, retrieving PAF vectors for each line, and computing the connectivity score for each candidate based on the PAF lines.</p> <p>Parameters:</p> Name Type Description Default <code>pafs</code> <code>Tensor</code> <p>A tensor of shape <code>(n_samples, height, width, 2 * n_edges)</code> containing the part affinity fields for each sample in the batch.</p> required <code>peaks</code> <code>Tensor</code> <p>A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_peaks), 2)</code> containing the (x, y) coordinates of the detected peaks for each sample.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_peaks))</code> indicating the channel (node) index that each peak corresponds to.</p> required <code>skeleton_edges</code> <code>Tensor</code> <p>A tensor of shape <code>(n_edges, 2)</code> indicating the indices of the nodes that form each edge of the skeleton.</p> required <code>n_line_points</code> <code>int</code> <p>The number of points used to interpolate between source and destination peaks in each connection candidate.</p> required <code>pafs_stride</code> <code>int</code> <p>The stride (1/scale) of the PAFs relative to the image scale.</p> required <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points relative to the image dimensions.</p> required <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale the weight of the distance penalty applied to the score of each line.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple containing three lists for each sample in the batch:     - A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_connections,))</code> indicating the indices       of the edges that each connection corresponds to.     - A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_connections, 2))</code> containing the indices       of the source and destination peaks forming each connection.     - A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_connections,))</code> containing the scores       for each connection based on the PAFs.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def score_paf_lines_batch(\n    pafs: torch.Tensor,\n    peaks: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    skeleton_edges: torch.Tensor,\n    n_line_points: int,\n    pafs_stride: int,\n    max_edge_length_ratio: float,\n    dist_penalty_weight: float,\n    n_nodes: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Process a batch of images to score the Part Affinity Fields (PAFs) lines formed between connection candidates for each sample.\n\n    This function loops over each sample in the batch and applies the process of\n    getting connection candidates, retrieving PAF vectors for each line, and\n    computing the connectivity score for each candidate based on the PAF lines.\n\n    Args:\n        pafs: A tensor of shape `(n_samples, height, width, 2 * n_edges)`\n            containing the part affinity fields for each sample in the batch.\n        peaks: A list of tensors (torch nested tensors) of shape `(n_samples, (n_peaks), 2)` containing the\n            (x, y) coordinates of the detected peaks for each sample.\n        peak_channel_inds: A list of tensors (torch nested tensors) of shape `(n_samples, (n_peaks))` indicating\n            the channel (node) index that each peak corresponds to.\n        skeleton_edges: A tensor of shape `(n_edges, 2)` indicating the indices\n            of the nodes that form each edge of the skeleton.\n        n_line_points: The number of points used to interpolate between source\n            and destination peaks in each connection candidate.\n        pafs_stride: The stride (1/scale) of the PAFs relative to the image scale.\n        max_edge_length_ratio: The maximum expected length of a connected pair\n            of points relative to the image dimensions.\n        dist_penalty_weight: A coefficient to scale the weight of the distance\n            penalty applied to the score of each line.\n        n_nodes: The total number of nodes in the skeleton.\n\n    Returns:\n        A tuple containing three lists for each sample in the batch:\n            - A list of tensors (torch nested tensors) of shape `(n_samples, (n_connections,))` indicating the indices\n              of the edges that each connection corresponds to.\n            - A list of tensors (torch nested tensors) of shape `(n_samples, (n_connections, 2))` containing the indices\n              of the source and destination peaks forming each connection.\n            - A list of tensors (torch nested tensors) of shape `(n_samples, (n_connections,))` containing the scores\n              for each connection based on the PAFs.\n    \"\"\"\n    max_edge_length = (\n        max_edge_length_ratio\n        * max(pafs.shape[-1], pafs.shape[-2], pafs.shape[-3])\n        * pafs_stride\n    )\n\n    n_samples = pafs.shape[0]\n    batch_edge_inds = []\n    batch_edge_peak_inds = []\n    batch_line_scores = []\n\n    for sample in range(n_samples):\n        pafs_sample = pafs[sample]\n        peaks_sample = peaks[sample]\n        peak_channel_inds_sample = peak_channel_inds[sample]\n\n        edge_inds_sample, edge_peak_inds_sample = get_connection_candidates(\n            peak_channel_inds_sample, skeleton_edges, n_nodes\n        )\n        paf_lines_sample = get_paf_lines(\n            pafs_sample,\n            peaks_sample,\n            edge_peak_inds_sample,\n            edge_inds_sample,\n            n_line_points,\n            pafs_stride,\n        )\n        line_scores_sample = score_paf_lines(\n            paf_lines_sample,\n            peaks_sample,\n            edge_peak_inds_sample,\n            max_edge_length,\n            dist_penalty_weight=dist_penalty_weight,\n        )\n\n        # Appending as lists to maintain the nested structure.\n        batch_edge_inds.append(edge_inds_sample)\n        batch_edge_peak_inds.append(edge_peak_inds_sample)\n        batch_line_scores.append(line_scores_sample)\n\n    return batch_edge_inds, batch_edge_peak_inds, batch_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.toposort_edges","title":"<code>toposort_edges(edge_types)</code>","text":"<p>Find a topological ordering for a list of edge types.</p> <p>Parameters:</p> Name Type Description Default <code>edge_types</code> <code>List[EdgeType]</code> <p>A list of <code>EdgeType</code> instances describing a skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> <p>This is important to ensure that instances are assembled starting at the root of the skeleton and moving down.</p> <p>See also: assign_connections_to_instances</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def toposort_edges(edge_types: List[EdgeType]) -&gt; Tuple[int]:\n    \"\"\"Find a topological ordering for a list of edge types.\n\n    Args:\n        edge_types: A list of `EdgeType` instances describing a skeleton.\n\n    Returns:\n        A tuple of indices specifying the topological order that the edge types should\n        be accessed in during instance assembly (`assign_connections_to_instances`).\n\n        This is important to ensure that instances are assembled starting at the root\n        of the skeleton and moving down.\n\n    See also: assign_connections_to_instances\n    \"\"\"\n    edges = [\n        (edge_type.src_node_ind, edge_type.dst_node_ind) for edge_type in edge_types\n    ]\n    dg = nx.DiGraph(edges)\n    root_ind = next(nx.topological_sort(dg))\n    sorted_edges = nx.bfs_edges(dg, root_ind)\n    sorted_edge_inds = tuple([edges.index(edge) for edge in sorted_edges])\n    return sorted_edge_inds\n</code></pre>"},{"location":"api/inference/peak_finding/","title":"peak_finding","text":""},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding","title":"<code>sleap_nn.inference.peak_finding</code>","text":"<p>Peak finding for inference.</p> <p>Functions:</p> Name Description <code>crop_bboxes</code> <p>Crop bounding boxes from a batch of images.</p> <code>find_global_peaks</code> <p>Find global peaks with optional refinement.</p> <code>find_global_peaks_rough</code> <p>Find the global maximum for each sample and channel.</p> <code>find_local_peaks</code> <p>Find local peaks with optional refinement.</p> <code>find_local_peaks_rough</code> <p>Find local maxima via non-maximum suppression.</p> <code>integral_regression</code> <p>Compute regression by integrating over the confidence maps on a grid.</p>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.crop_bboxes","title":"<code>crop_bboxes(images, bboxes, sample_inds)</code>","text":"<p>Crop bounding boxes from a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Tensor</code> <p>Tensor of shape (samples, channels, height, width) of a batch of images.</p> required <code>bboxes</code> <code>Tensor</code> <p>Tensor of shape (n_bboxes, 4, 2) and dtype torch.float32, where n_bboxes is the number of centroids, and the second dimension represents the four corner points of the bounding boxes, each with x and y coordinates. The order of the corners follows a clockwise arrangement: top-left, top-right, bottom-right, and bottom-left. This can be generated from centroids using <code>make_centered_bboxes</code>.</p> required <code>sample_inds</code> <code>Tensor</code> <p>Tensor of shape (n_bboxes,) specifying which samples each bounding box should be cropped from.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (n_bboxes, crop_height, crop_width, channels) of the same dtype as the input image. The crop size is inferred from the bounding box coordinates.</p> Notes <p>This function expects bounding boxes with coordinates at the centers of the pixels in the box limits. Technically, the box will span (x1 - 0.5, x2 + 0.5) and (y1 - 0.5, y2 + 0.5).</p> <p>For example, a 3x3 patch centered at (1, 1) would be specified by (y1, x1, y2, x2) = (0, 0, 2, 2). This would be exactly equivalent to indexing the image with <code>image[:, :, 0:3, 0:3]</code>.</p> <p>See also: <code>make_centered_bboxes</code></p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def crop_bboxes(\n    images: torch.Tensor, bboxes: torch.Tensor, sample_inds: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Crop bounding boxes from a batch of images.\n\n    Args:\n        images: Tensor of shape (samples, channels, height, width) of a batch of images.\n        bboxes: Tensor of shape (n_bboxes, 4, 2) and dtype torch.float32, where n_bboxes\n            is the number of centroids, and the second dimension represents the four\n            corner points of the bounding boxes, each with x and y coordinates.\n            The order of the corners follows a clockwise arrangement: top-left,\n            top-right, bottom-right, and bottom-left. This can be generated from\n            centroids using `make_centered_bboxes`.\n        sample_inds: Tensor of shape (n_bboxes,) specifying which samples each bounding\n            box should be cropped from.\n\n    Returns:\n        A tensor of shape (n_bboxes, crop_height, crop_width, channels) of the same\n        dtype as the input image. The crop size is inferred from the bounding box\n        coordinates.\n\n    Notes:\n        This function expects bounding boxes with coordinates at the centers of the\n        pixels in the box limits. Technically, the box will span (x1 - 0.5, x2 + 0.5)\n        and (y1 - 0.5, y2 + 0.5).\n\n        For example, a 3x3 patch centered at (1, 1) would be specified by\n        (y1, x1, y2, x2) = (0, 0, 2, 2). This would be exactly equivalent to indexing\n        the image with `image[:, :, 0:3, 0:3]`.\n\n    See also: `make_centered_bboxes`\n    \"\"\"\n    # Compute bounding box size to use for crops.\n    height = abs(bboxes[0, 3, 1] - bboxes[0, 0, 1])\n    width = abs(bboxes[0, 1, 0] - bboxes[0, 0, 0])\n    box_size = tuple(torch.round(torch.Tensor((height + 1, width + 1))).to(torch.int32))\n\n    # Crop.\n    crops = crop_and_resize(\n        images[sample_inds],  # (n_boxes, channels, height, width)\n        boxes=bboxes,\n        size=box_size,\n    )\n\n    # Cast back to original dtype and return.\n    crops = crops.to(images.dtype)\n    return crops\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_global_peaks","title":"<code>find_global_peaks(cms, threshold=0.2, refinement=None, integral_patch_size=5)</code>","text":"<p>Find global peaks with optional refinement.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Confidence maps. Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence threshold. Peaks with values below this will ignored.</p> <code>0.2</code> <code>refinement</code> <code>Optional[str]</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression.</p> <code>None</code> <code>integral_patch_size</code> <code>int</code> <p>Size of patches to crop around each rough peak as an integer scalar.</p> <code>5</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals).</p> <p>peak_points: float32 tensor of shape (samples, channels, 2), where the last axis indicates peak locations in xy order.</p> <p>peak_vals: float32 tensor of shape (samples, channels) containing the values at the peak points.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_global_peaks(\n    cms: torch.Tensor,\n    threshold: float = 0.2,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find global peaks with optional refinement.\n\n    Args:\n        cms: Confidence maps. Tensor of shape (samples, channels, height, width).\n        threshold: Minimum confidence threshold. Peaks with values below this will\n            ignored.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression.\n        integral_patch_size: Size of patches to crop around each rough peak as an\n            integer scalar.\n\n    Returns:\n        A tuple of (peak_points, peak_vals).\n\n        peak_points: float32 tensor of shape (samples, channels, 2), where the last axis\n        indicates peak locations in xy order.\n\n        peak_vals: float32 tensor of shape (samples, channels) containing the values at\n        the peak points.\n    \"\"\"\n    # Find grid aligned peaks.\n    rough_peaks, peak_vals = find_global_peaks_rough(\n        cms, threshold=threshold\n    )  # (samples, channels, 2)\n\n    # Return early if not refining or no rough peaks found.\n    if refinement is None or torch.isnan(rough_peaks).all():\n        return rough_peaks, peak_vals\n\n    if refinement == \"integral\":\n        crop_size = integral_patch_size\n    else:\n        return rough_peaks, peak_vals\n\n    # Flatten samples and channels to (n_peaks, 2).\n    samples = cms.size(0)\n    channels = cms.size(1)\n    rough_peaks = rough_peaks.view(samples * channels, 2)\n\n    # Keep only peaks that are not NaNs.\n    valid_idx = torch.where(~torch.isnan(rough_peaks[:, 0]))[0]\n    valid_peaks = rough_peaks[valid_idx]\n\n    # Make bounding boxes for cropping around peaks.\n    bboxes = make_centered_bboxes(\n        valid_peaks, box_height=crop_size, box_width=crop_size\n    )\n\n    # Crop patch around each grid-aligned peak.\n    cms = torch.reshape(\n        cms,\n        [samples * channels, 1, cms.size(2), cms.size(3)],\n    )\n    cm_crops = crop_bboxes(cms, bboxes, valid_idx)\n\n    # Compute offsets via integral regression on a local patch.\n    if refinement == \"integral\":\n        gv = torch.arange(crop_size, dtype=torch.float32) - ((crop_size - 1) / 2)\n        dx_hat, dy_hat = integral_regression(cm_crops, xv=gv, yv=gv)\n        offsets = torch.cat([dx_hat, dy_hat], dim=1)\n\n    # Apply offsets.\n    refined_peaks = rough_peaks.clone()\n    refined_peaks[valid_idx] += offsets\n\n    # Reshape to (samples, channels, 2).\n    refined_peaks = refined_peaks.reshape(samples, channels, 2)\n\n    return refined_peaks, peak_vals\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_global_peaks_rough","title":"<code>find_global_peaks_rough(cms, threshold=0.1)</code>","text":"<p>Find the global maximum for each sample and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Scalar float specifying the minimum confidence value for peaks. Peaks with values below this threshold will be replaced with NaNs.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals). peak_points: float32 tensor of shape (samples, channels, 2), where the last axis indicates peak locations in xy order. peak_vals: float32 tensor of shape (samples, channels) containing the values at the peak points.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_global_peaks_rough(\n    cms: torch.Tensor, threshold: float = 0.1\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the global maximum for each sample and channel.\n\n    Args:\n        cms: Tensor of shape (samples, channels, height, width).\n        threshold: Scalar float specifying the minimum confidence value for peaks. Peaks\n            with values below this threshold will be replaced with NaNs.\n\n    Returns:\n        A tuple of (peak_points, peak_vals).\n        peak_points: float32 tensor of shape (samples, channels, 2), where the last axis\n        indicates peak locations in xy order.\n        peak_vals: float32 tensor of shape (samples, channels) containing the values at\n        the peak points.\n\n    \"\"\"\n    # Find the maximum values and their indices along the height and width axes.\n    max_values, max_indices_y = torch.max(cms, dim=2, keepdim=True)\n    max_values, max_indices_x = torch.max(max_values, dim=3, keepdim=True)\n    max_indices_x = max_indices_x.squeeze(dim=(2, 3))  # (samples, channels)\n    # Find the maximum values and their indices along the height and width axes.\n    amax_values, amax_indices_x = torch.max(cms, dim=3, keepdim=True)\n    amax_values, amax_indices_y = torch.max(amax_values, dim=2, keepdim=True)\n    amax_indices_y = amax_indices_y.squeeze(dim=(2, 3))\n    peak_points = torch.cat(\n        [max_indices_x.unsqueeze(-1), amax_indices_y.unsqueeze(-1)], dim=-1\n    ).to(torch.float32)\n    max_values = max_values.squeeze(-1).squeeze(-1)\n    # Create masks for values below the threshold.\n    below_threshold_mask = max_values &lt; threshold\n    # Replace values below the threshold with NaN.\n    peak_points[below_threshold_mask] = float(\"nan\")\n    max_values[below_threshold_mask] = float(0)\n    return peak_points, max_values\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_local_peaks","title":"<code>find_local_peaks(cms, threshold=0.2, refinement=None, integral_patch_size=5)</code>","text":"<p>Find local peaks with optional refinement.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Confidence maps. Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence threshold. Peaks with values below this will ignored.</p> <code>0.2</code> <code>refinement</code> <code>Optional[str]</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression.</p> <code>None</code> <code>integral_patch_size</code> <code>int</code> <p>Size of patches to crop around each rough peak as an integer scalar.</p> <code>5</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds).</p> <p>peak_points: float32 tensor of shape (n_peaks, 2), where the last axis indicates peak locations in xy order.</p> <p>peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak points.</p> <p>peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the sample each peak belongs to.</p> <p>peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of the channel each peak belongs to.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_local_peaks(\n    cms: torch.Tensor,\n    threshold: float = 0.2,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Find local peaks with optional refinement.\n\n    Args:\n        cms: Confidence maps. Tensor of shape (samples, channels, height, width).\n        threshold: Minimum confidence threshold. Peaks with values below this will\n            ignored.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression.\n        integral_patch_size: Size of patches to crop around each rough peak as an\n            integer scalar.\n\n    Returns:\n        A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds).\n\n        peak_points: float32 tensor of shape (n_peaks, 2), where the last axis\n        indicates peak locations in xy order.\n\n        peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak\n        points.\n\n        peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the\n        sample each peak belongs to.\n\n        peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of\n        the channel each peak belongs to.\n    \"\"\"\n    # Find grid aligned peaks.\n    (\n        rough_peaks,\n        peak_vals,\n        peak_sample_inds,\n        peak_channel_inds,\n    ) = find_local_peaks_rough(cms, threshold=threshold)\n\n    # Return early if no rough peaks found.\n    if rough_peaks.size(0) == 0 or refinement is None:\n        return rough_peaks, peak_vals, peak_sample_inds, peak_channel_inds\n\n    if refinement == \"integral\":\n        crop_size = integral_patch_size\n    else:\n        return rough_peaks, peak_vals, peak_sample_inds, peak_channel_inds\n\n    # Make bounding boxes for cropping around peaks.\n    bboxes = make_centered_bboxes(\n        rough_peaks, box_height=crop_size, box_width=crop_size\n    )\n\n    # Reshape to (samples * channels, height, width, 1).\n    samples = cms.size(0)\n    channels = cms.size(1)\n    cms = torch.reshape(\n        cms,\n        [samples * channels, 1, cms.size(2), cms.size(3)],\n    )\n    box_sample_inds = (peak_sample_inds * channels) + peak_channel_inds\n\n    # Crop patch around each grid-aligned peak.\n    cm_crops = crop_bboxes(cms, bboxes, sample_inds=box_sample_inds)\n\n    # Compute offsets via integral regression on a local patch.\n    if refinement == \"integral\":\n        gv = torch.arange(crop_size, dtype=torch.float32) - ((crop_size - 1) / 2)\n        dx_hat, dy_hat = integral_regression(cm_crops, xv=gv, yv=gv)\n        offsets = torch.cat([dx_hat, dy_hat], dim=1)\n\n    # Apply offsets.\n    refined_peaks = rough_peaks + offsets\n\n    return refined_peaks, peak_vals, peak_sample_inds, peak_channel_inds\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_local_peaks_rough","title":"<code>find_local_peaks_rough(cms, threshold=0.2)</code>","text":"<p>Find local maxima via non-maximum suppression.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Scalar float specifying the minimum confidence value for peaks. Peaks with values below this threshold will not be returned.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds). peak_points: float32 tensor of shape (n_peaks, 2), where the last axis indicates peak locations in xy order.</p> <p>peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak points.</p> <p>peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the sample each peak belongs to.</p> <p>peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of the channel each peak belongs to.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_local_peaks_rough(\n    cms: torch.Tensor, threshold: float = 0.2\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Find local maxima via non-maximum suppression.\n\n    Args:\n        cms: Tensor of shape (samples, channels, height, width).\n        threshold: Scalar float specifying the minimum confidence value for peaks. Peaks\n            with values below this threshold will not be returned.\n\n    Returns:\n        A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds).\n        peak_points: float32 tensor of shape (n_peaks, 2), where the last axis\n        indicates peak locations in xy order.\n\n        peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak\n        points.\n\n        peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the\n        sample each peak belongs to.\n\n        peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of\n        the channel each peak belongs to.\n    \"\"\"\n    # Build custom local NMS kernel.\n    kernel = torch.tensor([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=torch.float32)\n\n    # Reshape to have singleton channels.\n    height = cms.size(2)\n    width = cms.size(3)\n    channels = cms.size(1)\n    flat_img = cms.reshape(-1, 1, height, width)\n\n    # Perform dilation filtering to find local maxima per channel and reshape back.\n    max_img = K.morphology.dilation(flat_img, kernel.to(flat_img.device))\n    max_img = max_img.reshape(-1, channels, height, width)\n\n    # Filter for maxima and threshold.\n    argmax_and_thresh_img = (cms &gt; max_img) &amp; (cms &gt; threshold)\n\n    # Convert to subscripts.\n    peak_subs = torch.stack(\n        torch.where(argmax_and_thresh_img.permute(0, 2, 3, 1)), axis=-1\n    )\n\n    # Get peak values.\n    peak_vals = cms[peak_subs[:, 0], peak_subs[:, 3], peak_subs[:, 1], peak_subs[:, 2]]\n\n    # Convert to points format.\n    peak_points = peak_subs[:, [2, 1]].to(torch.float32)\n\n    # Pull out indexing vectors.\n    peak_sample_inds = peak_subs[:, 0].to(torch.int32)\n    peak_channel_inds = peak_subs[:, 3].to(torch.int32)\n\n    return peak_points, peak_vals, peak_sample_inds, peak_channel_inds\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.integral_regression","title":"<code>integral_regression(cms, xv, yv)</code>","text":"<p>Compute regression by integrating over the confidence maps on a grid.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Confidence maps with shape (samples, channels, height, width).</p> required <code>xv</code> <code>Tensor</code> <p>X grid vector torch.float32 of grid coordinates to sample.</p> required <code>yv</code> <code>Tensor</code> <p>Y grid vector torch.float32 of grid coordinates to sample.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (x_hat, y_hat) with the regressed x- and y-coordinates for each channel of the confidence maps.</p> <p>x_hat and y_hat are of shape (samples, channels)</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def integral_regression(\n    cms: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute regression by integrating over the confidence maps on a grid.\n\n    Args:\n        cms: Confidence maps with shape (samples, channels, height, width).\n        xv: X grid vector torch.float32 of grid coordinates to sample.\n        yv: Y grid vector torch.float32 of grid coordinates to sample.\n\n    Returns:\n        A tuple of (x_hat, y_hat) with the regressed x- and y-coordinates for each\n        channel of the confidence maps.\n\n        x_hat and y_hat are of shape (samples, channels)\n    \"\"\"\n    # Compute normalizing factor.\n    z = torch.sum(cms, dim=[2, 3]).to(cms.device)\n    xv = xv.to(cms.device)\n    yv = yv.to(cms.device)\n\n    # Regress to expectation.\n    x_hat = torch.sum(xv.view(1, 1, 1, -1) * cms, dim=[2, 3]) / z\n    y_hat = torch.sum(yv.view(1, 1, -1, 1) * cms, dim=[2, 3]) / z\n\n    return x_hat, y_hat\n</code></pre>"},{"location":"api/inference/predictors/","title":"predictors","text":""},{"location":"api/inference/predictors/#sleap_nn.inference.predictors","title":"<code>sleap_nn.inference.predictors</code>","text":"<p>Predictors for running inference.</p> <p>Classes:</p> Name Description <code>BottomUpMultiClassPredictor</code> <p>BottomUp ID model predictor.</p> <code>BottomUpPredictor</code> <p>BottomUp model predictor.</p> <code>Predictor</code> <p>Base interface class for predictors.</p> <code>RateColumn</code> <p>Renders the progress rate.</p> <code>SingleInstancePredictor</code> <p>Single-Instance predictor.</p> <code>TopDownMultiClassPredictor</code> <p>Top-down multi-class predictor.</p> <code>TopDownPredictor</code> <p>Top-down multi-instance predictor.</p>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpMultiClassPredictor","title":"<code>BottomUpMultiClassPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>BottomUp ID model predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained BottomUp SLEAP-NN model.</p> <p>This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>bottomup_config</code> <code>Optional[OmegaConf]</code> <p>A OmegaConfig dictionary with the configs used for training the             multi_class_bottomup model.</p> <code>bottomup_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights for            multi_class_bottomup model.</p> <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\".</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters     in the <code>data_config.preprocessing</code> section.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass BottomUpMultiClassPredictor(Predictor):\n    \"\"\"BottomUp ID model predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained BottomUp SLEAP-NN model.\n\n    This should be initialized using the `from_trained_models()` constructor.\n\n    Attributes:\n        bottomup_config: A OmegaConfig dictionary with the configs used for training the\n                        multi_class_bottomup model.\n        bottomup_model: A LightningModule instance created from the trained weights for\n                       multi_class_bottomup model.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\".\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    bottomup_config: Optional[OmegaConf] = attrs.field(default=None)\n    bottomup_model: Optional[L.LightningModule] = attrs.field(default=None)\n    backbone_type: str = \"unet\"\n    videos: Optional[List[sio.Video]] = attrs.field(default=None)\n    skeletons: Optional[List[sio.Skeleton]] = attrs.field(default=None)\n    peak_threshold: float = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # initialize the BottomUpMultiClassInferenceModel\n        self.inference_model = BottomUpMultiClassInferenceModel(\n            torch_model=self.bottomup_model,\n            peak_threshold=self.peak_threshold,\n            cms_output_stride=self.bottomup_config.model_config.head_configs.multi_class_bottomup.confmaps.output_stride,\n            class_maps_output_stride=self.bottomup_config.model_config.head_configs.multi_class_bottomup.class_maps.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            input_scale=self.bottomup_config.data_config.preprocessing.scale,\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        bottomup_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        max_stride: int = 16,\n    ) -&gt; \"BottomUpMultiClassPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            bottomup_ckpt_path: Path to a multi-class bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                    are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                    from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `BottomUpPredictor` with the loaded models.\n\n        \"\"\"\n        is_sleap_ckpt = False\n        if (\n            Path(bottomup_ckpt_path) / \"training_config.yaml\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            bottomup_config = OmegaConf.load(\n                (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(bottomup_ckpt_path) / \"training_config.json\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            bottomup_config = TrainingJobConfig.load_sleap_config(\n                (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in bottomup_config.model_config.backbone_config.items():\n            if v is not None:\n                backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n            bottomup_model = BottomUpMultiClassLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                backbone_type=backbone_type,\n                model_type=\"multi_class_bottomup\",\n                map_location=device,\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n                init_weights=bottomup_config.model_config.init_weights,\n                trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            )\n        else:\n            bottomup_converted_model = load_legacy_model(\n                model_dir=f\"{bottomup_ckpt_path}\"\n            )\n            bottomup_model = BottomUpMultiClassLightningModule(\n                backbone_type=backbone_type,\n                model_type=\"multi_class_bottomup\",\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n                init_weights=bottomup_config.model_config.init_weights,\n                trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            )\n            bottomup_model.eval()\n            bottomup_model.model = bottomup_converted_model\n            bottomup_model.to(device)\n\n        bottomup_model.eval()\n        skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n        bottomup_model.to(device)\n\n        for k, v in preprocess_config.items():\n            if v is None:\n                preprocess_config[k] = (\n                    bottomup_config.data_config.preprocessing[k]\n                    if k in bottomup_config.data_config.preprocessing\n                    else None\n                )\n\n        # create an instance of SingleInstancePredictor class\n        obj = cls(\n            bottomup_config=bottomup_config,\n            backbone_type=backbone_type,\n            bottomup_model=bottomup_model,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            preprocess_config=preprocess_config,\n            max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        data_path: str,\n        queue_maxsize: int = 8,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n            frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        # LabelsReader provider\n        if data_path.endswith(\".slp\") and video_index is None:\n            provider = LabelsReader\n\n            max_stride = self.bottomup_config.model_config.backbone_config[\n                f\"{self.backbone_type}\"\n            ][\"max_stride\"]\n\n            self.preprocess = False\n\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n            self.preprocess = True\n\n            if data_path.endswith(\".slp\") and video_index is not None:\n                labels = sio.load_slp(data_path)\n                self.pipeline = provider.from_video(\n                    video=labels.videos[video_index],\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                self.pipeline = provider.from_filename(\n                    filename=data_path,\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        predicted_frames = []\n        tracks = [\n            sio.Track(name=x)\n            for x in self.bottomup_config.model_config.head_configs.multi_class_bottomup.class_maps.classes\n        ]\n\n        skeleton_idx = 0\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                pred_instances,\n                pred_values,\n                instance_score,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"instance_scores\"],\n            ):\n\n                # Loop over instances.\n                predicted_instances = []\n                for i, (pts, confs, score) in enumerate(\n                    zip(pred_instances, pred_values, instance_score)\n                ):\n                    if np.isnan(pts).all():\n                        continue\n\n                    track = None\n                    if tracks is not None and len(tracks) &gt;= (i - 1):\n                        track = tracks[i]\n\n                    predicted_instances.append(\n                        sio.PredictedInstance.from_numpy(\n                            points_data=pts,\n                            point_scores=confs,\n                            score=np.nanmean(confs),\n                            skeleton=self.skeletons[skeleton_idx],\n                            track=track,\n                            tracking_score=np.nanmean(score),\n                        )\n                    )\n\n                max_instances = (\n                    self.max_instances if self.max_instances is not None else None\n                )\n                if max_instances is not None:\n                    # Filter by score.\n                    predicted_instances = sorted(\n                        predicted_instances, key=lambda x: x.score, reverse=True\n                    )\n                    predicted_instances = predicted_instances[\n                        : min(max_instances, len(predicted_instances))\n                    ]\n\n                lf = sio.LabeledFrame(\n                    video=self.videos[video_idx],\n                    frame_idx=frame_idx,\n                    instances=predicted_instances,\n                )\n\n                predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpMultiClassPredictor.from_trained_models","title":"<code>from_trained_models(bottomup_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>bottomup_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a multi-class bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights     are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt     from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>BottomUpMultiClassPredictor</code> <p>An instance of <code>BottomUpPredictor</code> with the loaded models.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    bottomup_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    max_stride: int = 16,\n) -&gt; \"BottomUpMultiClassPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        bottomup_ckpt_path: Path to a multi-class bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `BottomUpPredictor` with the loaded models.\n\n    \"\"\"\n    is_sleap_ckpt = False\n    if (\n        Path(bottomup_ckpt_path) / \"training_config.yaml\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        bottomup_config = OmegaConf.load(\n            (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n        )\n    elif (\n        Path(bottomup_ckpt_path) / \"training_config.json\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        is_sleap_ckpt = True\n        bottomup_config = TrainingJobConfig.load_sleap_config(\n            (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n        )\n\n    # check which backbone architecture\n    for k, v in bottomup_config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n\n    if not is_sleap_ckpt:\n        ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n        bottomup_model = BottomUpMultiClassLightningModule.load_from_checkpoint(\n            checkpoint_path=ckpt_path,\n            backbone_type=backbone_type,\n            model_type=\"multi_class_bottomup\",\n            map_location=device,\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n            init_weights=bottomup_config.model_config.init_weights,\n            trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n        )\n    else:\n        bottomup_converted_model = load_legacy_model(\n            model_dir=f\"{bottomup_ckpt_path}\"\n        )\n        bottomup_model = BottomUpMultiClassLightningModule(\n            backbone_type=backbone_type,\n            model_type=\"multi_class_bottomup\",\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n            init_weights=bottomup_config.model_config.init_weights,\n            trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n        )\n        bottomup_model.eval()\n        bottomup_model.model = bottomup_converted_model\n        bottomup_model.to(device)\n\n    bottomup_model.eval()\n    skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n    if backbone_ckpt_path is not None and head_ckpt_path is not None:\n        logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path,\n            map_location=device,\n            weights_only=False,\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".backbone\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    elif backbone_ckpt_path is not None:\n        logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path,\n            map_location=device,\n            weights_only=False,\n        )\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    if head_ckpt_path is not None:\n        logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n        ckpt = torch.load(\n            head_ckpt_path,\n            map_location=device,\n            weights_only=False,\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".head_layers\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n    bottomup_model.to(device)\n\n    for k, v in preprocess_config.items():\n        if v is None:\n            preprocess_config[k] = (\n                bottomup_config.data_config.preprocessing[k]\n                if k in bottomup_config.data_config.preprocessing\n                else None\n            )\n\n    # create an instance of SingleInstancePredictor class\n    obj = cls(\n        bottomup_config=bottomup_config,\n        backbone_type=backbone_type,\n        bottomup_model=bottomup_model,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        preprocess_config=preprocess_config,\n        max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n            \"max_stride\"\n        ],\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpMultiClassPredictor.make_pipeline","title":"<code>make_pipeline(data_path, queue_maxsize=8, frames=None, only_labeled_frames=False, only_suggested_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 8.</p> <code>8</code> <code>frames</code> <code>Optional[list]</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    data_path: str,\n    queue_maxsize: int = 8,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n        frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    # LabelsReader provider\n    if data_path.endswith(\".slp\") and video_index is None:\n        provider = LabelsReader\n\n        max_stride = self.bottomup_config.model_config.backbone_config[\n            f\"{self.backbone_type}\"\n        ][\"max_stride\"]\n\n        self.preprocess = False\n\n        self.pipeline = provider.from_filename(\n            filename=data_path,\n            queue_maxsize=queue_maxsize,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n        self.preprocess = True\n\n        if data_path.endswith(\".slp\") and video_index is not None:\n            labels = sio.load_slp(data_path)\n            self.pipeline = provider.from_video(\n                video=labels.videos[video_index],\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpPredictor","title":"<code>BottomUpPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>BottomUp model predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained BottomUp SLEAP-NN model.</p> <p>This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>bottomup_config</code> <code>Optional[OmegaConf]</code> <p>A OmegaConfig dictionary with the configs used for training the             bottom-up model.</p> <code>bottomup_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights for            bottom-up model.</p> <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching.</p> <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>n_points</code> <code>int</code> <p>Number of points to sample along the line integral.</p> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Minimum number of peaks the instance should have to be     considered a real instance. Instances with fewer peaks than this will be     discarded (useful for filtering spurious detections).</p> <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\".</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters     in the <code>data_config.preprocessing</code> section.</p> <code>tracker</code> <code>Optional[Tracker]</code> <p>A <code>sleap.nn.tracking.Tracker</code> that will be called to associate detections over time. Predicted instances will not be assigned to tracks if if this is <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass BottomUpPredictor(Predictor):\n    \"\"\"BottomUp model predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained BottomUp SLEAP-NN model.\n\n    This should be initialized using the `from_trained_models()` constructor.\n\n    Attributes:\n        bottomup_config: A OmegaConfig dictionary with the configs used for training the\n                        bottom-up model.\n        bottomup_model: A LightningModule instance created from the trained weights for\n                       bottom-up model.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        max_edge_length_ratio: The maximum expected length of a connected pair of points\n            as a fraction of the image size. Candidate connections longer than this\n            length will be penalized during matching.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n        n_points: Number of points to sample along the line integral.\n        min_instance_peaks: Minimum number of peaks the instance should have to be\n                considered a real instance. Instances with fewer peaks than this will be\n                discarded (useful for filtering spurious detections).\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs. Useful for rejecting spurious detections when\n            there are no better ones.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\".\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n        tracker: A `sleap.nn.tracking.Tracker` that will be called to associate\n            detections over time. Predicted instances will not be assigned to tracks if\n            if this is `None`.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    bottomup_config: Optional[OmegaConf] = attrs.field(default=None)\n    bottomup_model: Optional[L.LightningModule] = attrs.field(default=None)\n    backbone_type: str = \"unet\"\n    max_edge_length_ratio: float = 0.25\n    dist_penalty_weight: float = 1.0\n    n_points: int = 10\n    min_instance_peaks: Union[int, float] = 0\n    min_line_scores: float = 0.25\n    videos: Optional[List[sio.Video]] = attrs.field(default=None)\n    skeletons: Optional[List[sio.Skeleton]] = attrs.field(default=None)\n    peak_threshold: float = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    tracker: Optional[Tracker] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # initialize the paf scorer\n        paf_scorer = PAFScorer.from_config(\n            config=OmegaConf.create(\n                {\n                    \"confmaps\": self.bottomup_config.model_config.head_configs.bottomup[\n                        \"confmaps\"\n                    ],\n                    \"pafs\": self.bottomup_config.model_config.head_configs.bottomup[\n                        \"pafs\"\n                    ],\n                }\n            ),\n            max_edge_length_ratio=self.max_edge_length_ratio,\n            dist_penalty_weight=self.dist_penalty_weight,\n            n_points=self.n_points,\n            min_instance_peaks=self.min_instance_peaks,\n            min_line_scores=self.min_line_scores,\n        )\n\n        # initialize the BottomUpInferenceModel\n        self.inference_model = BottomUpInferenceModel(\n            torch_model=self.bottomup_model,\n            paf_scorer=paf_scorer,\n            peak_threshold=self.peak_threshold,\n            cms_output_stride=self.bottomup_config.model_config.head_configs.bottomup.confmaps.output_stride,\n            pafs_output_stride=self.bottomup_config.model_config.head_configs.bottomup.pafs.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            input_scale=self.bottomup_config.data_config.preprocessing.scale,\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        bottomup_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        max_stride: int = 16,\n    ) -&gt; \"BottomUpPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            bottomup_ckpt_path: Path to a bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                    are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                    from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `BottomUpPredictor` with the loaded models.\n\n        \"\"\"\n        is_sleap_ckpt = False\n        if (\n            Path(bottomup_ckpt_path) / \"training_config.yaml\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            bottomup_config = OmegaConf.load(\n                (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(bottomup_ckpt_path) / \"training_config.json\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            bottomup_config = TrainingJobConfig.load_sleap_config(\n                (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in bottomup_config.model_config.backbone_config.items():\n            if v is not None:\n                backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n            bottomup_model = BottomUpLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n                init_weights=bottomup_config.model_config.init_weights,\n                trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n                backbone_type=backbone_type,\n                model_type=\"bottomup\",\n                map_location=device,\n            )\n        else:\n            bottomup_converted_model = load_legacy_model(\n                model_dir=f\"{bottomup_ckpt_path}\"\n            )\n            bottomup_model = BottomUpLightningModule(\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n                init_weights=bottomup_config.model_config.init_weights,\n                trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                backbone_type=backbone_type,\n                model_type=\"bottomup\",\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            )\n            bottomup_model.eval()\n            bottomup_model.model = bottomup_converted_model\n            bottomup_model.to(device)\n\n        bottomup_model.eval()\n        skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n        bottomup_model.to(device)\n\n        for k, v in preprocess_config.items():\n            if v is None:\n                preprocess_config[k] = (\n                    bottomup_config.data_config.preprocessing[k]\n                    if k in bottomup_config.data_config.preprocessing\n                    else None\n                )\n\n        # create an instance of BottomUpPredictor class\n        obj = cls(\n            bottomup_config=bottomup_config,\n            backbone_type=backbone_type,\n            bottomup_model=bottomup_model,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            preprocess_config=preprocess_config,\n            max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        data_path: str,\n        queue_maxsize: int = 8,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n            frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        # LabelsReader provider\n        if data_path.endswith(\".slp\") and video_index is None:\n            provider = LabelsReader\n\n            self.preprocess = False\n\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n            self.preprocess = True\n\n            if data_path.endswith(\".slp\") and video_index is not None:\n                labels = sio.load_slp(data_path)\n                self.pipeline = provider.from_video(\n                    video=labels.videos[video_index],\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                self.pipeline = provider.from_filename(\n                    filename=data_path,\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        predicted_frames = []\n\n        skeleton_idx = 0\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                pred_instances,\n                pred_values,\n                instance_score,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"instance_scores\"],\n            ):\n\n                # Loop over instances.\n                predicted_instances = []\n                for pts, confs, score in zip(\n                    pred_instances, pred_values, instance_score\n                ):\n                    if np.isnan(pts).all():\n                        continue\n\n                    predicted_instances.append(\n                        sio.PredictedInstance.from_numpy(\n                            points_data=pts,\n                            point_scores=confs,\n                            score=score,\n                            skeleton=self.skeletons[skeleton_idx],\n                        )\n                    )\n\n                max_instances = (\n                    self.max_instances if self.max_instances is not None else None\n                )\n                if max_instances is not None:\n                    # Filter by score.\n                    predicted_instances = sorted(\n                        predicted_instances, key=lambda x: x.score, reverse=True\n                    )\n                    predicted_instances = predicted_instances[\n                        : min(max_instances, len(predicted_instances))\n                    ]\n\n                lf = sio.LabeledFrame(\n                    video=self.videos[video_idx],\n                    frame_idx=frame_idx,\n                    instances=predicted_instances,\n                )\n\n                if self.tracker:\n                    lf.instances = self.tracker.track(\n                        untracked_instances=predicted_instances,\n                        frame_idx=frame_idx,\n                        image=lf.image,\n                    )\n\n                predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpPredictor.from_trained_models","title":"<code>from_trained_models(bottomup_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>bottomup_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights     are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt     from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>BottomUpPredictor</code> <p>An instance of <code>BottomUpPredictor</code> with the loaded models.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    bottomup_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    max_stride: int = 16,\n) -&gt; \"BottomUpPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        bottomup_ckpt_path: Path to a bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `BottomUpPredictor` with the loaded models.\n\n    \"\"\"\n    is_sleap_ckpt = False\n    if (\n        Path(bottomup_ckpt_path) / \"training_config.yaml\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        bottomup_config = OmegaConf.load(\n            (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n        )\n    elif (\n        Path(bottomup_ckpt_path) / \"training_config.json\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        is_sleap_ckpt = True\n        bottomup_config = TrainingJobConfig.load_sleap_config(\n            (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n        )\n\n    # check which backbone architecture\n    for k, v in bottomup_config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n\n    if not is_sleap_ckpt:\n        ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n        bottomup_model = BottomUpLightningModule.load_from_checkpoint(\n            checkpoint_path=ckpt_path,\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n            init_weights=bottomup_config.model_config.init_weights,\n            trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            backbone_type=backbone_type,\n            model_type=\"bottomup\",\n            map_location=device,\n        )\n    else:\n        bottomup_converted_model = load_legacy_model(\n            model_dir=f\"{bottomup_ckpt_path}\"\n        )\n        bottomup_model = BottomUpLightningModule(\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=bottomup_config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=bottomup_config.model_config.pretrained_head_weights,\n            init_weights=bottomup_config.model_config.init_weights,\n            trainer_accelerator=bottomup_config.trainer_config.trainer_accelerator,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            backbone_type=backbone_type,\n            model_type=\"bottomup\",\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n        )\n        bottomup_model.eval()\n        bottomup_model.model = bottomup_converted_model\n        bottomup_model.to(device)\n\n    bottomup_model.eval()\n    skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n    if backbone_ckpt_path is not None and head_ckpt_path is not None:\n        logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".backbone\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    elif backbone_ckpt_path is not None:\n        logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    if head_ckpt_path is not None:\n        logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n        ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".head_layers\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n    bottomup_model.to(device)\n\n    for k, v in preprocess_config.items():\n        if v is None:\n            preprocess_config[k] = (\n                bottomup_config.data_config.preprocessing[k]\n                if k in bottomup_config.data_config.preprocessing\n                else None\n            )\n\n    # create an instance of BottomUpPredictor class\n    obj = cls(\n        bottomup_config=bottomup_config,\n        backbone_type=backbone_type,\n        bottomup_model=bottomup_model,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        preprocess_config=preprocess_config,\n        max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n            \"max_stride\"\n        ],\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpPredictor.make_pipeline","title":"<code>make_pipeline(data_path, queue_maxsize=8, frames=None, only_labeled_frames=False, only_suggested_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 8.</p> <code>8</code> <code>frames</code> <code>Optional[list]</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    data_path: str,\n    queue_maxsize: int = 8,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n        frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    # LabelsReader provider\n    if data_path.endswith(\".slp\") and video_index is None:\n        provider = LabelsReader\n\n        self.preprocess = False\n\n        self.pipeline = provider.from_filename(\n            filename=data_path,\n            queue_maxsize=queue_maxsize,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n        self.preprocess = True\n\n        if data_path.endswith(\".slp\") and video_index is not None:\n            labels = sio.load_slp(data_path)\n            self.pipeline = provider.from_video(\n                video=labels.videos[video_index],\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor","title":"<code>Predictor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base interface class for predictors.</p> <p>This is the base predictor class for different types of models.</p> <p>Attributes:</p> Name Type Description <code>preprocess</code> <code>bool</code> <p>True if preprocessing (resizing and apply_pad_to_stride) should be applied on the frames read in the video reader. Default: True.</p> <code>preprocess_config</code> <code>dict</code> <p>Preprocessing config with keys: [<code>scale</code>, <code>ensure_rgb</code>, <code>ensure_grayscale</code>, <code>scale</code>, <code>max_height</code>, <code>max_width</code>, <code>crop_hw</code>]. Default: {\"scale\": 1.0, \"ensure_rgb\": False, \"ensure_grayscale\": False, \"max_height\": None, \"max_width\": None, \"crop_hw\": None}</p> <code>pipeline</code> <code>Optional[Union[LabelsReader, VideoReader]]</code> <p>If provider is LabelsReader, pipeline is a <code>DataLoader</code> object. If provider is VideoReader, pipeline is an instance of <code>sleap_nn.data.providers.VideoReader</code> class. Default: None.</p> <code>inference_model</code> <code>Optional[Union[TopDownInferenceModel, SingleInstanceInferenceModel, BottomUpInferenceModel]]</code> <p>Instance of one of the inference models [\"TopDownInferenceModel\", \"SingleInstanceInferenceModel\", \"BottomUpInferenceModel\"]. Default: None.</p> <code>instances_key</code> <code>bool</code> <p>If <code>True</code>, then instances are appended to the data samples.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_model_paths</code> <p>Create the appropriate <code>Predictor</code> subclass from from the ckpt path.</p> <code>from_trained_models</code> <p>Initialize the Predictor class for certain type of model.</p> <code>make_pipeline</code> <p>Create the data pipeline.</p> <code>predict</code> <p>Run inference on a data source.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass Predictor(ABC):\n    \"\"\"Base interface class for predictors.\n\n    This is the base predictor class for different types of models.\n\n    Attributes:\n        preprocess: True if preprocessing (resizing and\n            apply_pad_to_stride) should be applied on the frames read in the video reader.\n            Default: True.\n        preprocess_config: Preprocessing config with keys: [`scale`,\n            `ensure_rgb`, `ensure_grayscale`, `scale`, `max_height`, `max_width`, `crop_hw`]. Default: {\"scale\": 1.0,\n            \"ensure_rgb\": False, \"ensure_grayscale\": False, \"max_height\": None, \"max_width\": None, \"crop_hw\": None}\n        pipeline: If provider is LabelsReader, pipeline is a `DataLoader` object. If provider\n            is VideoReader, pipeline is an instance of `sleap_nn.data.providers.VideoReader`\n            class. Default: None.\n        inference_model: Instance of one of the inference models [\"TopDownInferenceModel\",\n            \"SingleInstanceInferenceModel\", \"BottomUpInferenceModel\"]. Default: None.\n        instances_key: If `True`, then instances are appended to the data samples.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n    \"\"\"\n\n    preprocess: bool = True\n    preprocess_config: dict = {\n        \"scale\": 1.0,\n        \"ensure_rgb\": False,\n        \"ensure_grayscale\": False,\n        \"crop_hw\": None,\n        \"max_height\": None,\n        \"max_width\": None,\n    }\n    pipeline: Optional[Union[LabelsReader, VideoReader]] = None\n    inference_model: Optional[\n        Union[\n            TopDownInferenceModel, SingleInstanceInferenceModel, BottomUpInferenceModel\n        ]\n    ] = None\n    instances_key: bool = False\n    max_stride: int = 16\n\n    @classmethod\n    def from_model_paths(\n        cls,\n        model_paths: List[Text],\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: Union[float, List[float]] = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        anchor_part: Optional[str] = None,\n    ) -&gt; \"Predictor\":\n        \"\"\"Create the appropriate `Predictor` subclass from from the ckpt path.\n\n        Args:\n            model_paths: (List[str]) List of paths to the directory where the best.ckpt (or from SLEAP &lt;=1.4 best_model.h5)\n                and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json) are saved.\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n        Returns:\n            A subclass of `Predictor`.\n\n        See also: `SingleInstancePredictor`, `TopDownPredictor`, `BottomUpPredictor`,\n            `MoveNetPredictor`, `TopDownMultiClassPredictor`,\n            `BottomUpMultiClassPredictor`.\n        \"\"\"\n        model_configs = []\n        for model_path in model_paths:\n            path = Path(model_path)\n            if path / \"training_config.yaml\" in path.iterdir():\n                model_configs.append(\n                    OmegaConf.load((path / \"training_config.yaml\").as_posix())\n                )\n            elif path / \"training_config.json\" in path.iterdir():\n                model_configs.append(\n                    TrainingJobConfig.load_sleap_config(\n                        (path / \"training_config.json\").as_posix()\n                    )\n                )\n            else:\n                raise ValueError(\n                    f\"Could not find training_config.yaml or training_config.json in {model_path}\"\n                )\n\n        model_names = []\n        for config in model_configs:\n            model_names.append(get_model_type_from_cfg(config=config))\n\n        if \"single_instance\" in model_names:\n            confmap_ckpt_path = model_paths[model_names.index(\"single_instance\")]\n            predictor = SingleInstancePredictor.from_trained_models(\n                confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n            )\n\n        elif (\n            \"centroid\" in model_names\n            or \"centered_instance\" in model_names\n            or \"multi_class_topdown\" in model_names\n        ):\n            centroid_ckpt_path = None\n            confmap_ckpt_path = None\n            if \"centroid\" in model_names:\n                centroid_ckpt_path = model_paths[model_names.index(\"centroid\")]\n                predictor = TopDownPredictor.from_trained_models(\n                    centroid_ckpt_path=centroid_ckpt_path,\n                    confmap_ckpt_path=confmap_ckpt_path,\n                    backbone_ckpt_path=backbone_ckpt_path,\n                    head_ckpt_path=head_ckpt_path,\n                    peak_threshold=peak_threshold,\n                    integral_refinement=integral_refinement,\n                    integral_patch_size=integral_patch_size,\n                    batch_size=batch_size,\n                    max_instances=max_instances,\n                    return_confmaps=return_confmaps,\n                    device=device,\n                    preprocess_config=preprocess_config,\n                    anchor_part=anchor_part,\n                )\n            if \"centered_instance\" in model_names:\n                confmap_ckpt_path = model_paths[model_names.index(\"centered_instance\")]\n                # create an instance of the TopDown predictor class\n                predictor = TopDownPredictor.from_trained_models(\n                    centroid_ckpt_path=centroid_ckpt_path,\n                    confmap_ckpt_path=confmap_ckpt_path,\n                    backbone_ckpt_path=backbone_ckpt_path,\n                    head_ckpt_path=head_ckpt_path,\n                    peak_threshold=peak_threshold,\n                    integral_refinement=integral_refinement,\n                    integral_patch_size=integral_patch_size,\n                    batch_size=batch_size,\n                    max_instances=max_instances,\n                    return_confmaps=return_confmaps,\n                    device=device,\n                    preprocess_config=preprocess_config,\n                    anchor_part=anchor_part,\n                )\n            elif \"multi_class_topdown\" in model_names:\n                confmap_ckpt_path = model_paths[\n                    model_names.index(\"multi_class_topdown\")\n                ]\n                # create an instance of the TopDown predictor class\n                predictor = TopDownMultiClassPredictor.from_trained_models(\n                    centroid_ckpt_path=centroid_ckpt_path,\n                    confmap_ckpt_path=confmap_ckpt_path,\n                    backbone_ckpt_path=backbone_ckpt_path,\n                    head_ckpt_path=head_ckpt_path,\n                    peak_threshold=peak_threshold,\n                    integral_refinement=integral_refinement,\n                    integral_patch_size=integral_patch_size,\n                    batch_size=batch_size,\n                    max_instances=max_instances,\n                    return_confmaps=return_confmaps,\n                    device=device,\n                    preprocess_config=preprocess_config,\n                    anchor_part=anchor_part,\n                )\n\n        elif \"bottomup\" in model_names:\n            bottomup_ckpt_path = model_paths[model_names.index(\"bottomup\")]\n            predictor = BottomUpPredictor.from_trained_models(\n                bottomup_ckpt_path=bottomup_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n            )\n\n        elif \"multi_class_bottomup\" in model_names:\n            bottomup_ckpt_path = model_paths[model_names.index(\"multi_class_bottomup\")]\n            predictor = BottomUpMultiClassPredictor.from_trained_models(\n                bottomup_ckpt_path=bottomup_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n            )\n\n        else:\n            message = f\"Could not create predictor from model paths:\\n{model_paths}\"\n            logger.error(message)\n            raise ValueError(message)\n        return predictor\n\n    @classmethod\n    @abstractmethod\n    def from_trained_models(cls, *args, **kwargs):\n        \"\"\"Initialize the Predictor class for certain type of model.\"\"\"\n\n    @abstractmethod\n    def make_pipeline(\n        self,\n        data_path: str,\n        queue_maxsize: int = 8,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Create the data pipeline.\"\"\"\n\n    @abstractmethod\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the Inference model.\"\"\"\n\n    def _convert_tensors_to_numpy(self, output):\n        \"\"\"Convert tensors in output dictionary to numpy arrays.\"\"\"\n        for k, v in output.items():\n            if isinstance(v, torch.Tensor):\n                output[k] = output[k].cpu().numpy()\n            if isinstance(v, list) and isinstance(v[0], torch.Tensor):\n                for n in range(len(v)):\n                    v[n] = v[n].cpu().numpy()\n        return output\n\n    def _predict_generator(self) -&gt; Iterator[Dict[str, np.ndarray]]:\n        \"\"\"Create a generator that yields batches of inference results.\n\n        This method handles creating a pipeline object depending on the model type and\n        provider for loading the data, as well as looping over the batches and\n        running inference.\n\n        Returns:\n            A generator yielding batches predicted results as dictionaries of numpy\n            arrays.\n        \"\"\"\n        # Initialize inference model if needed.\n\n        if self.inference_model is None:\n            self._initialize_inference_model()\n\n        # Loop over data batches.\n        self.pipeline.start()\n        total_frames = self.pipeline.total_len()\n        done = False\n\n        with Progress(\n            \"{task.description}\",\n            BarColumn(),\n            \"[progress.percentage]{task.percentage:&gt;3.0f}%\",\n            MofNCompleteColumn(),\n            \"ETA:\",\n            TimeRemainingColumn(),\n            \"Elapsed:\",\n            TimeElapsedColumn(),\n            RateColumn(),\n            auto_refresh=False,\n            refresh_per_second=4,  # Change to self.report_rate if needed\n            speed_estimate_period=5,\n        ) as progress:\n\n            task = progress.add_task(\"Predicting...\", total=total_frames)\n            last_report = time()\n\n            done = False\n            while not done:\n                imgs = []\n                fidxs = []\n                vidxs = []\n                org_szs = []\n                instances = []\n                eff_scales = []\n                for _ in range(self.batch_size):\n                    frame = self.pipeline.frame_buffer.get()\n                    if frame[\"image\"] is None:\n                        done = True\n                        break\n                    frame[\"image\"] = apply_normalization(frame[\"image\"])\n                    frame[\"image\"], eff_scale = apply_sizematcher(\n                        frame[\"image\"],\n                        self.preprocess_config[\"max_height\"],\n                        self.preprocess_config[\"max_width\"],\n                    )\n                    if self.instances_key:\n                        frame[\"instances\"] = frame[\"instances\"] * eff_scale\n                    if (\n                        self.preprocess_config[\"ensure_rgb\"]\n                        and frame[\"image\"].shape[-3] != 3\n                    ):\n                        frame[\"image\"] = frame[\"image\"].repeat(1, 3, 1, 1)\n                    elif (\n                        self.preprocess_config[\"ensure_grayscale\"]\n                        and frame[\"image\"].shape[-3] != 1\n                    ):\n                        frame[\"image\"] = F.rgb_to_grayscale(\n                            frame[\"image\"], num_output_channels=1\n                        )\n\n                    eff_scales.append(torch.tensor(eff_scale))\n                    imgs.append(frame[\"image\"].unsqueeze(dim=0))\n                    fidxs.append(frame[\"frame_idx\"])\n                    vidxs.append(frame[\"video_idx\"])\n                    org_szs.append(frame[\"orig_size\"].unsqueeze(dim=0))\n                    if self.instances_key:\n                        instances.append(frame[\"instances\"].unsqueeze(dim=0))\n                if imgs:\n                    # TODO: all preprocessing should be moved into InferenceModels to be exportable.\n                    imgs = torch.concatenate(imgs, dim=0)\n                    fidxs = torch.tensor(fidxs, dtype=torch.int32)\n                    vidxs = torch.tensor(vidxs, dtype=torch.int32)\n                    org_szs = torch.concatenate(org_szs, dim=0)\n                    eff_scales = torch.tensor(eff_scales, dtype=torch.float32)\n                    if self.instances_key:\n                        instances = torch.concatenate(instances, dim=0)\n                    ex = {\n                        \"image\": imgs,\n                        \"frame_idx\": fidxs,\n                        \"video_idx\": vidxs,\n                        \"orig_size\": org_szs,\n                        \"eff_scale\": eff_scales,\n                    }\n                    if self.instances_key:\n                        ex[\"instances\"] = instances\n                    if self.preprocess:\n                        scale = self.preprocess_config[\"scale\"]\n                        if scale != 1.0:\n                            if self.instances_key:\n                                ex[\"image\"], ex[\"instances\"] = apply_resizer(\n                                    ex[\"image\"], ex[\"instances\"]\n                                )\n                            else:\n                                ex[\"image\"] = resize_image(ex[\"image\"], scale)\n                        ex[\"image\"] = apply_pad_to_stride(ex[\"image\"], self.max_stride)\n                    outputs_list = self.inference_model(ex)\n                    if outputs_list is not None:\n                        for output in outputs_list:\n                            output = self._convert_tensors_to_numpy(output)\n                            yield output\n\n                    # Advance progress\n                    num_frames = (\n                        len(ex[\"frame_idx\"]) if \"frame_idx\" in ex else self.batch_size\n                    )\n                    progress.update(task, advance=num_frames)\n\n                # Manually refresh progress bar\n                if time() - last_report &gt; 0.25:\n                    progress.refresh()\n                    last_report = time()\n\n        self.pipeline.join()\n\n    def predict(\n        self,\n        make_labels: bool = True,\n    ) -&gt; Union[List[Dict[str, np.ndarray]], sio.Labels]:\n        \"\"\"Run inference on a data source.\n\n        Args:\n            make_labels: If `True` (the default), returns a `sio.Labels` instance with\n                `sio.PredictedInstance`s. If `False`, just return a list of\n                dictionaries containing the raw arrays returned by the inference model.\n\n        Returns:\n            A `sio.Labels` with `sio.PredictedInstance`s if `make_labels` is `True`,\n            otherwise a list of dictionaries containing batches of numpy arrays with the\n            raw results.\n        \"\"\"\n        # Initialize inference loop generator.\n        generator = self._predict_generator()\n\n        if make_labels:\n            # Create SLEAP data structures from the predictions.\n            pred_labels = self._make_labeled_frames_from_generator(generator)\n            return pred_labels\n\n        else:\n            # Just return the raw results.\n            return list(generator)\n\n    @abstractmethod\n    def _make_labeled_frames_from_generator(self, generator) -&gt; sio.Labels:\n        \"\"\"Create `sio.Labels` object from the predictions.\"\"\"\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.from_model_paths","title":"<code>from_model_paths(model_paths, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, anchor_part=None)</code>  <code>classmethod</code>","text":"<p>Create the appropriate <code>Predictor</code> subclass from from the ckpt path.</p> <p>Parameters:</p> Name Type Description Default <code>model_paths</code> <code>List[Text]</code> <p>(List[str]) List of paths to the directory where the best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json) are saved.</p> required <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown centroid and centered-instance model, where the first element corresponds to centroid model peak finding threshold and the second element is for centered-instance model peak finding.</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Predictor</code> <p>A subclass of <code>Predictor</code>.</p> <code>SingleInstancePredictor</code>, <code>TopDownPredictor</code>, <code>BottomUpPredictor</code>, <p><code>MoveNetPredictor</code>, <code>TopDownMultiClassPredictor</code>, <code>BottomUpMultiClassPredictor</code>.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_model_paths(\n    cls,\n    model_paths: List[Text],\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: Union[float, List[float]] = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    anchor_part: Optional[str] = None,\n) -&gt; \"Predictor\":\n    \"\"\"Create the appropriate `Predictor` subclass from from the ckpt path.\n\n    Args:\n        model_paths: (List[str]) List of paths to the directory where the best.ckpt (or from SLEAP &lt;=1.4 best_model.h5)\n            and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json) are saved.\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n            centroid and centered-instance model, where the first element corresponds\n            to centroid model peak finding threshold and the second element is for\n            centered-instance model peak finding.\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n    Returns:\n        A subclass of `Predictor`.\n\n    See also: `SingleInstancePredictor`, `TopDownPredictor`, `BottomUpPredictor`,\n        `MoveNetPredictor`, `TopDownMultiClassPredictor`,\n        `BottomUpMultiClassPredictor`.\n    \"\"\"\n    model_configs = []\n    for model_path in model_paths:\n        path = Path(model_path)\n        if path / \"training_config.yaml\" in path.iterdir():\n            model_configs.append(\n                OmegaConf.load((path / \"training_config.yaml\").as_posix())\n            )\n        elif path / \"training_config.json\" in path.iterdir():\n            model_configs.append(\n                TrainingJobConfig.load_sleap_config(\n                    (path / \"training_config.json\").as_posix()\n                )\n            )\n        else:\n            raise ValueError(\n                f\"Could not find training_config.yaml or training_config.json in {model_path}\"\n            )\n\n    model_names = []\n    for config in model_configs:\n        model_names.append(get_model_type_from_cfg(config=config))\n\n    if \"single_instance\" in model_names:\n        confmap_ckpt_path = model_paths[model_names.index(\"single_instance\")]\n        predictor = SingleInstancePredictor.from_trained_models(\n            confmap_ckpt_path,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n        )\n\n    elif (\n        \"centroid\" in model_names\n        or \"centered_instance\" in model_names\n        or \"multi_class_topdown\" in model_names\n    ):\n        centroid_ckpt_path = None\n        confmap_ckpt_path = None\n        if \"centroid\" in model_names:\n            centroid_ckpt_path = model_paths[model_names.index(\"centroid\")]\n            predictor = TopDownPredictor.from_trained_models(\n                centroid_ckpt_path=centroid_ckpt_path,\n                confmap_ckpt_path=confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n                anchor_part=anchor_part,\n            )\n        if \"centered_instance\" in model_names:\n            confmap_ckpt_path = model_paths[model_names.index(\"centered_instance\")]\n            # create an instance of the TopDown predictor class\n            predictor = TopDownPredictor.from_trained_models(\n                centroid_ckpt_path=centroid_ckpt_path,\n                confmap_ckpt_path=confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n                anchor_part=anchor_part,\n            )\n        elif \"multi_class_topdown\" in model_names:\n            confmap_ckpt_path = model_paths[\n                model_names.index(\"multi_class_topdown\")\n            ]\n            # create an instance of the TopDown predictor class\n            predictor = TopDownMultiClassPredictor.from_trained_models(\n                centroid_ckpt_path=centroid_ckpt_path,\n                confmap_ckpt_path=confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n                anchor_part=anchor_part,\n            )\n\n    elif \"bottomup\" in model_names:\n        bottomup_ckpt_path = model_paths[model_names.index(\"bottomup\")]\n        predictor = BottomUpPredictor.from_trained_models(\n            bottomup_ckpt_path=bottomup_ckpt_path,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n        )\n\n    elif \"multi_class_bottomup\" in model_names:\n        bottomup_ckpt_path = model_paths[model_names.index(\"multi_class_bottomup\")]\n        predictor = BottomUpMultiClassPredictor.from_trained_models(\n            bottomup_ckpt_path=bottomup_ckpt_path,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n        )\n\n    else:\n        message = f\"Could not create predictor from model paths:\\n{model_paths}\"\n        logger.error(message)\n        raise ValueError(message)\n    return predictor\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.from_trained_models","title":"<code>from_trained_models(*args, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Initialize the Predictor class for certain type of model.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_trained_models(cls, *args, **kwargs):\n    \"\"\"Initialize the Predictor class for certain type of model.\"\"\"\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.make_pipeline","title":"<code>make_pipeline(data_path, queue_maxsize=8, frames=None, only_labeled_frames=False, only_suggested_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>  <code>abstractmethod</code>","text":"<p>Create the data pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@abstractmethod\ndef make_pipeline(\n    self,\n    data_path: str,\n    queue_maxsize: int = 8,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Create the data pipeline.\"\"\"\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.predict","title":"<code>predict(make_labels=True)</code>","text":"<p>Run inference on a data source.</p> <p>Parameters:</p> Name Type Description Default <code>make_labels</code> <code>bool</code> <p>If <code>True</code> (the default), returns a <code>sio.Labels</code> instance with <code>sio.PredictedInstance</code>s. If <code>False</code>, just return a list of dictionaries containing the raw arrays returned by the inference model.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[List[Dict[str, ndarray]], Labels]</code> <p>A <code>sio.Labels</code> with <code>sio.PredictedInstance</code>s if <code>make_labels</code> is <code>True</code>, otherwise a list of dictionaries containing batches of numpy arrays with the raw results.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def predict(\n    self,\n    make_labels: bool = True,\n) -&gt; Union[List[Dict[str, np.ndarray]], sio.Labels]:\n    \"\"\"Run inference on a data source.\n\n    Args:\n        make_labels: If `True` (the default), returns a `sio.Labels` instance with\n            `sio.PredictedInstance`s. If `False`, just return a list of\n            dictionaries containing the raw arrays returned by the inference model.\n\n    Returns:\n        A `sio.Labels` with `sio.PredictedInstance`s if `make_labels` is `True`,\n        otherwise a list of dictionaries containing batches of numpy arrays with the\n        raw results.\n    \"\"\"\n    # Initialize inference loop generator.\n    generator = self._predict_generator()\n\n    if make_labels:\n        # Create SLEAP data structures from the predictions.\n        pred_labels = self._make_labeled_frames_from_generator(generator)\n        return pred_labels\n\n    else:\n        # Just return the raw results.\n        return list(generator)\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.RateColumn","title":"<code>RateColumn</code>","text":"<p>               Bases: <code>ProgressColumn</code></p> <p>Renders the progress rate.</p> <p>Methods:</p> Name Description <code>render</code> <p>Show progress rate.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>class RateColumn(rich.progress.ProgressColumn):\n    \"\"\"Renders the progress rate.\"\"\"\n\n    def render(self, task: \"Task\") -&gt; rich.progress.Text:\n        \"\"\"Show progress rate.\"\"\"\n        speed = task.speed\n        if speed is None:\n            return rich.progress.Text(\"?\", style=\"progress.data.speed\")\n        return rich.progress.Text(f\"{speed:.1f} FPS\", style=\"progress.data.speed\")\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.RateColumn.render","title":"<code>render(task)</code>","text":"<p>Show progress rate.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def render(self, task: \"Task\") -&gt; rich.progress.Text:\n    \"\"\"Show progress rate.\"\"\"\n    speed = task.speed\n    if speed is None:\n        return rich.progress.Text(\"?\", style=\"progress.data.speed\")\n    return rich.progress.Text(f\"{speed:.1f} FPS\", style=\"progress.data.speed\")\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.SingleInstancePredictor","title":"<code>SingleInstancePredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>Single-Instance predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained single instance SLEAP-NN model.</p> <p>This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>confmap_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the             single-instance model.</p> <code>confmap_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights for            single-instance model.</p> <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters     in the <code>data_config.preprocessing</code> section.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass SingleInstancePredictor(Predictor):\n    \"\"\"Single-Instance predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained single instance SLEAP-NN model.\n\n    This should be initialized using the `from_trained_models()` constructor.\n\n    Attributes:\n        confmap_config: A Dictionary with the configs used for training the\n                        single-instance model.\n        confmap_model: A LightningModule instance created from the trained weights for\n                       single-instance model.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    confmap_config: Optional[OmegaConf] = attrs.field(default=None)\n    confmap_model: Optional[L.LightningModule] = attrs.field(default=None)\n    backbone_type: str = \"unet\"\n    videos: Optional[List[sio.Video]] = attrs.field(default=None)\n    skeletons: Optional[List[sio.Skeleton]] = attrs.field(default=None)\n    peak_threshold: float = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        self.inference_model = SingleInstanceInferenceModel(\n            torch_model=self.confmap_model,\n            peak_threshold=self.peak_threshold,\n            output_stride=self.confmap_config.model_config.head_configs.single_instance.confmaps.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            input_scale=self.confmap_config.data_config.preprocessing.scale,\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        confmap_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        max_stride: int = 16,\n    ) -&gt; \"SingleInstancePredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            confmap_ckpt_path: Path to a single instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `SingleInstancePredictor` with the loaded models.\n\n        \"\"\"\n        is_sleap_ckpt = False\n        if (\n            Path(confmap_ckpt_path) / \"training_config.yaml\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            confmap_config = OmegaConf.load(\n                (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(confmap_ckpt_path) / \"training_config.json\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            confmap_config = TrainingJobConfig.load_sleap_config(\n                (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in confmap_config.model_config.backbone_config.items():\n            if v is not None:\n                backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n            confmap_model = SingleInstanceLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"single_instance\",\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                init_weights=confmap_config.model_config.init_weights,\n                trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                backbone_type=backbone_type,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n            )\n        else:\n            confmap_converted_model = load_legacy_model(\n                model_dir=f\"{confmap_ckpt_path}\"\n            )\n            confmap_model = SingleInstanceLightningModule(\n                backbone_type=backbone_type,\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                init_weights=confmap_config.model_config.init_weights,\n                trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                model_type=\"single_instance\",\n            )\n            confmap_model.eval()\n            confmap_model.model = confmap_converted_model\n            confmap_model.to(device)\n\n        confmap_model.eval()\n\n        skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n        confmap_model.to(device)\n\n        for k, v in preprocess_config.items():\n            if v is None:\n                preprocess_config[k] = (\n                    confmap_config.data_config.preprocessing[k]\n                    if k in confmap_config.data_config.preprocessing\n                    else None\n                )\n\n        # create an instance of SingleInstancePredictor class\n        obj = cls(\n            confmap_config=confmap_config,\n            confmap_model=confmap_model,\n            backbone_type=backbone_type,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n            max_stride=confmap_config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        data_path: str,\n        queue_maxsize: int = 8,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n            frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n\n        \"\"\"\n        # LabelsReader provider\n        if data_path.endswith(\".slp\") and video_index is None:\n            provider = LabelsReader\n\n            self.preprocess = False\n\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n            self.preprocess = True\n\n            if data_path.endswith(\".slp\") and video_index is not None:\n                labels = sio.load_slp(data_path)\n                self.pipeline = provider.from_video(\n                    video=labels.videos[video_index],\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                self.pipeline = provider.from_filename(\n                    filename=data_path,\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`.\n                This can be created using the `_predict_generator()` method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        predicted_frames = []\n\n        skeleton_idx = 0\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                pred_instances,\n                pred_values,\n                org_size,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"orig_size\"],\n            ):\n\n                if np.isnan(pred_instances).all():\n                    continue\n                inst = sio.PredictedInstance.from_numpy(\n                    points_data=pred_instances,\n                    skeleton=self.skeletons[skeleton_idx],\n                    score=np.nansum(pred_values),\n                    point_scores=pred_values,\n                )\n                predicted_frames.append(\n                    sio.LabeledFrame(\n                        video=self.videos[video_idx],\n                        frame_idx=frame_idx,\n                        instances=[inst],\n                    )\n                )\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.SingleInstancePredictor.from_trained_models","title":"<code>from_trained_models(confmap_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, return_confmaps=False, device='cpu', preprocess_config=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>confmap_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a single instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>SingleInstancePredictor</code> <p>An instance of <code>SingleInstancePredictor</code> with the loaded models.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    confmap_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    max_stride: int = 16,\n) -&gt; \"SingleInstancePredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        confmap_ckpt_path: Path to a single instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `SingleInstancePredictor` with the loaded models.\n\n    \"\"\"\n    is_sleap_ckpt = False\n    if (\n        Path(confmap_ckpt_path) / \"training_config.yaml\"\n        in Path(confmap_ckpt_path).iterdir()\n    ):\n        confmap_config = OmegaConf.load(\n            (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n        )\n    elif (\n        Path(confmap_ckpt_path) / \"training_config.json\"\n        in Path(confmap_ckpt_path).iterdir()\n    ):\n        is_sleap_ckpt = True\n        confmap_config = TrainingJobConfig.load_sleap_config(\n            (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n        )\n\n    # check which backbone architecture\n    for k, v in confmap_config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n\n    if not is_sleap_ckpt:\n        ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n        confmap_model = SingleInstanceLightningModule.load_from_checkpoint(\n            checkpoint_path=ckpt_path,\n            model_type=\"single_instance\",\n            backbone_config=confmap_config.model_config.backbone_config,\n            head_configs=confmap_config.model_config.head_configs,\n            pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n            init_weights=confmap_config.model_config.init_weights,\n            trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n            lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n            backbone_type=backbone_type,\n            online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=confmap_config.trainer_config.optimizer_name,\n            learning_rate=confmap_config.trainer_config.optimizer.lr,\n            amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            map_location=device,\n        )\n    else:\n        confmap_converted_model = load_legacy_model(\n            model_dir=f\"{confmap_ckpt_path}\"\n        )\n        confmap_model = SingleInstanceLightningModule(\n            backbone_type=backbone_type,\n            backbone_config=confmap_config.model_config.backbone_config,\n            head_configs=confmap_config.model_config.head_configs,\n            pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n            init_weights=confmap_config.model_config.init_weights,\n            trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n            lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n            online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=confmap_config.trainer_config.optimizer_name,\n            learning_rate=confmap_config.trainer_config.optimizer.lr,\n            amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            model_type=\"single_instance\",\n        )\n        confmap_model.eval()\n        confmap_model.model = confmap_converted_model\n        confmap_model.to(device)\n\n    confmap_model.eval()\n\n    skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n    if backbone_ckpt_path is not None and head_ckpt_path is not None:\n        logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".backbone\" in k\n        }\n        confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    elif backbone_ckpt_path is not None:\n        logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    if head_ckpt_path is not None:\n        logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n        ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".head_layers\" in k\n        }\n        confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n    confmap_model.to(device)\n\n    for k, v in preprocess_config.items():\n        if v is None:\n            preprocess_config[k] = (\n                confmap_config.data_config.preprocessing[k]\n                if k in confmap_config.data_config.preprocessing\n                else None\n            )\n\n    # create an instance of SingleInstancePredictor class\n    obj = cls(\n        confmap_config=confmap_config,\n        confmap_model=confmap_model,\n        backbone_type=backbone_type,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        return_confmaps=return_confmaps,\n        device=device,\n        preprocess_config=preprocess_config,\n        max_stride=confmap_config.model_config.backbone_config[f\"{backbone_type}\"][\n            \"max_stride\"\n        ],\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.SingleInstancePredictor.make_pipeline","title":"<code>make_pipeline(data_path, queue_maxsize=8, frames=None, only_labeled_frames=False, only_suggested_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 8.</p> <code>8</code> <code>frames</code> <code>Optional[list]</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    data_path: str,\n    queue_maxsize: int = 8,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n        frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n\n    \"\"\"\n    # LabelsReader provider\n    if data_path.endswith(\".slp\") and video_index is None:\n        provider = LabelsReader\n\n        self.preprocess = False\n\n        self.pipeline = provider.from_filename(\n            filename=data_path,\n            queue_maxsize=queue_maxsize,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n        self.preprocess = True\n\n        if data_path.endswith(\".slp\") and video_index is not None:\n            labels = sio.load_slp(data_path)\n            self.pipeline = provider.from_video(\n                video=labels.videos[video_index],\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownMultiClassPredictor","title":"<code>TopDownMultiClassPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>Top-down multi-class predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained TopDown SLEAP-NN model. This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>centroid_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the centroid model.</p> <code>confmap_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the             centered-instance model</p> <code>centroid_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights             for centroid model.</p> <code>confmap_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights            for centered-instance model.</p> <code>centroid_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>centered_instance_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below     this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown     centroid and centered-instance model, where the first element corresponds     to centroid model peak finding threshold and the second element is for     centered-instance model peak finding.</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass TopDownMultiClassPredictor(Predictor):\n    \"\"\"Top-down multi-class predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained TopDown SLEAP-NN model. This should be initialized using the\n    `from_trained_models()` constructor.\n\n    Attributes:\n        centroid_config: A Dictionary with the configs used for training the centroid model.\n        confmap_config: A Dictionary with the configs used for training the\n                        centered-instance model\n        centroid_model: A LightningModule instance created from the trained weights\n                        for centroid model.\n        confmap_model: A LightningModule instance created from the trained weights\n                       for centered-instance model.\n        centroid_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        centered_instance_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    centroid_config: Optional[OmegaConf] = None\n    confmap_config: Optional[OmegaConf] = None\n    centroid_model: Optional[L.LightningModule] = None\n    confmap_model: Optional[L.LightningModule] = None\n    centroid_backbone_type: Optional[str] = None\n    centered_instance_backbone_type: Optional[str] = None\n    videos: Optional[List[sio.Video]] = None\n    skeletons: Optional[List[sio.Skeleton]] = None\n    peak_threshold: Union[float, List[float]] = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    anchor_part: Optional[str] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # Create an instance of CentroidLayer if centroid_config is not None\n        return_crops = False\n        # if both centroid and centered-instance model are provided, set return crops to True\n        if self.confmap_model:\n            return_crops = True\n        if isinstance(self.peak_threshold, list):\n            centroid_peak_threshold = self.peak_threshold[0]\n            centered_instance_peak_threshold = self.peak_threshold[1]\n        else:\n            centroid_peak_threshold = self.peak_threshold\n            centered_instance_peak_threshold = self.peak_threshold\n\n        if self.anchor_part is not None:\n            anchor_ind = self.skeletons[0].node_names.index(self.anchor_part)\n        else:\n            anch_pt = None\n            if self.centroid_config is not None:\n                anch_pt = (\n                    self.centroid_config.model_config.head_configs.centroid.confmaps.anchor_part\n                )\n            if self.confmap_config is not None:\n                anch_pt = (\n                    self.confmap_config.model_config.head_configs.multi_class_topdown.confmaps.anchor_part\n                )\n            anchor_ind = (\n                self.skeletons[0].node_names.index(anch_pt)\n                if anch_pt is not None\n                else None\n            )\n\n        if self.centroid_config is None:\n            centroid_crop_layer = CentroidCrop(\n                use_gt_centroids=True,\n                crop_hw=self.preprocess_config.crop_hw,\n                anchor_ind=anchor_ind,\n                return_crops=return_crops,\n            )\n\n        else:\n            max_stride = self.centroid_config.model_config.backbone_config[\n                f\"{self.centroid_backbone_type}\"\n            ][\"max_stride\"]\n            # initialize centroid crop layer\n            centroid_crop_layer = CentroidCrop(\n                torch_model=self.centroid_model,\n                peak_threshold=centroid_peak_threshold,\n                output_stride=self.centroid_config.model_config.head_configs.centroid.confmaps.output_stride,\n                refinement=self.integral_refinement,\n                integral_patch_size=self.integral_patch_size,\n                return_confmaps=self.return_confmaps,\n                return_crops=return_crops,\n                max_instances=self.max_instances,\n                max_stride=max_stride,\n                input_scale=self.centroid_config.data_config.preprocessing.scale,\n                crop_hw=self.preprocess_config.crop_hw,\n                use_gt_centroids=False,\n            )\n\n        max_stride = self.confmap_config.model_config.backbone_config[\n            f\"{self.centered_instance_backbone_type}\"\n        ][\"max_stride\"]\n        instance_peaks_layer = TopDownMultiClassFindInstancePeaks(\n            torch_model=self.confmap_model,\n            peak_threshold=centered_instance_peak_threshold,\n            output_stride=self.confmap_config.model_config.head_configs.multi_class_topdown.confmaps.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            max_stride=max_stride,\n            input_scale=self.confmap_config.data_config.preprocessing.scale,\n        )\n        centroid_crop_layer.precrop_resize = (\n            self.confmap_config.data_config.preprocessing.scale\n        )\n\n        if self.centroid_config is None:\n            self.instances_key = (\n                True  # we need `instances` to get ground-truth centroids\n            )\n\n        # Initialize the inference model with centroid and instance peak layers\n        self.inference_model = TopDownInferenceModel(\n            centroid_crop=centroid_crop_layer, instance_peaks=instance_peaks_layer\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        centroid_ckpt_path: Optional[Text] = None,\n        confmap_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        anchor_part: Optional[str] = None,\n        max_stride: int = 16,\n    ) -&gt; \"TopDownPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n            confmap_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `TopDownPredictor` with the loaded models.\n\n            One of the two models can be left as `None` to perform inference with ground\n            truth data. This will only work with `LabelsReader` as the provider.\n\n        \"\"\"\n        centered_instance_backbone_type = None\n        centroid_backbone_type = None\n        if centroid_ckpt_path is not None:\n            is_sleap_ckpt = False\n            if (\n                Path(centroid_ckpt_path) / \"training_config.yaml\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                centroid_config = OmegaConf.load(\n                    (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(centroid_ckpt_path) / \"training_config.json\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                centroid_config = TrainingJobConfig.load_sleap_config(\n                    (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            # Load centroid model.\n            skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n            # check which backbone architecture\n            for k, v in centroid_config.model_config.backbone_config.items():\n                if v is not None:\n                    centroid_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n\n                centroid_model = CentroidLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"centroid\",\n                    backbone_type=centroid_backbone_type,\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                    init_weights=centroid_config.model_config.init_weights,\n                    trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                    map_location=device,\n                )\n\n            else:\n                centroid_converted_model = load_legacy_model(\n                    model_dir=f\"{centroid_ckpt_path}\"\n                )\n                centroid_model = CentroidLightningModule(\n                    model_type=\"centroid\",\n                    backbone_type=centroid_backbone_type,\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                    init_weights=centroid_config.model_config.init_weights,\n                    trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                )\n                centroid_model.eval()\n                centroid_model.model = centroid_converted_model\n                centroid_model.to(device)\n\n            centroid_model.eval()\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                centroid_model.load_state_dict(\n                    ckpt[\"state_dict\"],\n                    strict=False,\n                    weights_only=False,\n                )\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            centroid_model.to(device)\n\n        else:\n            centroid_config = None\n            centroid_model = None\n\n        if confmap_ckpt_path is not None:\n            # Load confmap model.\n            is_sleap_ckpt = False\n            if (\n                Path(confmap_ckpt_path) / \"training_config.yaml\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                confmap_config = OmegaConf.load(\n                    (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(confmap_ckpt_path) / \"training_config.json\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                confmap_config = TrainingJobConfig.load_sleap_config(\n                    (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            # check which backbone architecture\n            for k, v in confmap_config.model_config.backbone_config.items():\n                if v is not None:\n                    centered_instance_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n\n                confmap_model = TopDownCenteredInstanceMultiClassLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"multi_class_topdown\",\n                    backbone_type=centered_instance_backbone_type,\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                    init_weights=confmap_config.model_config.init_weights,\n                    trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                    map_location=device,\n                )\n            else:\n                confmap_converted_model = load_legacy_model(\n                    model_dir=f\"{confmap_ckpt_path}\"\n                )\n                confmap_model = TopDownCenteredInstanceMultiClassLightningModule(\n                    model_type=\"multi_class_topdown\",\n                    backbone_type=centered_instance_backbone_type,\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                    init_weights=confmap_config.model_config.init_weights,\n                    trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                )\n                confmap_model.eval()\n                confmap_model.model = confmap_converted_model\n                confmap_model.to(device)\n\n            confmap_model.eval()\n            skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            confmap_model.to(device)\n\n        else:\n            confmap_config = None\n            confmap_model = None\n\n        if centroid_config is None and confmap_config is None:\n            message = (\n                \"Both a centroid and a confidence map model must be provided to \"\n                \"initialize a TopDownMultiClassPredictor.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        if centroid_config is not None:\n            preprocess_config[\"scale\"] = (\n                centroid_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                centroid_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                centroid_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                centroid_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                centroid_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        else:\n            preprocess_config[\"scale\"] = (\n                confmap_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                confmap_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                confmap_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                confmap_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                confmap_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        preprocess_config[\"crop_hw\"] = (\n            confmap_config.data_config.preprocessing.crop_hw\n            if preprocess_config[\"crop_hw\"] is None and confmap_config is not None\n            else preprocess_config[\"crop_hw\"]\n        )\n\n        # create an instance of TopDownPredictor class\n        obj = cls(\n            centroid_config=centroid_config,\n            centroid_model=centroid_model,\n            confmap_config=confmap_config,\n            confmap_model=confmap_model,\n            centroid_backbone_type=centroid_backbone_type,\n            centered_instance_backbone_type=centered_instance_backbone_type,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n            anchor_part=anchor_part,\n            max_stride=(\n                centroid_config.model_config.backbone_config[\n                    f\"{centroid_backbone_type}\"\n                ][\"max_stride\"]\n                if centroid_config is not None\n                else confmap_config.model_config.backbone_config[\n                    f\"{centered_instance_backbone_type}\"\n                ][\"max_stride\"]\n            ),\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        data_path: str,\n        queue_maxsize: int = 8,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n            frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        # LabelsReader provider\n        if data_path.endswith(\".slp\") and video_index is None:\n            provider = LabelsReader\n\n            self.preprocess = False\n\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                instances_key=self.instances_key,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n            if self.centroid_config is None:\n                message = (\n                    \"Ground truth data was not detected... \"\n                    \"Please load both models when predicting on non-ground-truth data.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n\n            self.preprocess = False\n\n            if data_path.endswith(\".slp\") and video_index is not None:\n                labels = sio.load_slp(data_path)\n                self.pipeline = provider.from_video(\n                    video=labels.videos[video_index],\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                self.pipeline = provider.from_filename(\n                    filename=data_path,\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        preds = defaultdict(list)\n        predicted_frames = []\n        skeleton_idx = 0\n\n        tracks = [\n            sio.Track(name=x)\n            for x in self.confmap_config.model_config.head_configs.multi_class_topdown.class_vectors.classes\n        ]\n\n        # Loop through each predicted instance.\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                bbox,\n                pred_instances,\n                pred_values,\n                centroid_val,\n                org_size,\n                class_ind,\n                instance_score,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"instance_bbox\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"centroid_val\"],\n                ex[\"orig_size\"],\n                ex[\"pred_class_inds\"],\n                ex[\"instance_scores\"],\n            ):\n                if np.isnan(pred_instances).all():\n                    continue\n                pred_instances = pred_instances + bbox.squeeze(axis=0)[0, :]\n\n                track = None\n                if tracks is not None:\n                    track = tracks[class_ind]\n\n                preds[(int(video_idx), int(frame_idx))].append(\n                    sio.PredictedInstance.from_numpy(\n                        points_data=pred_instances,\n                        skeleton=self.skeletons[skeleton_idx],\n                        point_scores=pred_values,\n                        score=centroid_val,\n                        track=track,\n                        tracking_score=instance_score,\n                    )\n                )\n        for key, inst in preds.items():\n            # Create list of LabeledFrames.\n            video_idx, frame_idx = key\n            lf = sio.LabeledFrame(\n                video=self.videos[video_idx],\n                frame_idx=frame_idx,\n                instances=inst,\n            )\n\n            predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownMultiClassPredictor.from_trained_models","title":"<code>from_trained_models(centroid_ckpt_path=None, confmap_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, anchor_part=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).</p> <code>None</code> <code>confmap_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>TopDownPredictor</code> <p>An instance of <code>TopDownPredictor</code> with the loaded models.</p> <p>One of the two models can be left as <code>None</code> to perform inference with ground truth data. This will only work with <code>LabelsReader</code> as the provider.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    centroid_ckpt_path: Optional[Text] = None,\n    confmap_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    anchor_part: Optional[str] = None,\n    max_stride: int = 16,\n) -&gt; \"TopDownPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n        confmap_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `TopDownPredictor` with the loaded models.\n\n        One of the two models can be left as `None` to perform inference with ground\n        truth data. This will only work with `LabelsReader` as the provider.\n\n    \"\"\"\n    centered_instance_backbone_type = None\n    centroid_backbone_type = None\n    if centroid_ckpt_path is not None:\n        is_sleap_ckpt = False\n        if (\n            Path(centroid_ckpt_path) / \"training_config.yaml\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            centroid_config = OmegaConf.load(\n                (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(centroid_ckpt_path) / \"training_config.json\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            centroid_config = TrainingJobConfig.load_sleap_config(\n                (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # Load centroid model.\n        skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n        # check which backbone architecture\n        for k, v in centroid_config.model_config.backbone_config.items():\n            if v is not None:\n                centroid_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n\n            centroid_model = CentroidLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"centroid\",\n                backbone_type=centroid_backbone_type,\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                init_weights=centroid_config.model_config.init_weights,\n                trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n            )\n\n        else:\n            centroid_converted_model = load_legacy_model(\n                model_dir=f\"{centroid_ckpt_path}\"\n            )\n            centroid_model = CentroidLightningModule(\n                model_type=\"centroid\",\n                backbone_type=centroid_backbone_type,\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                init_weights=centroid_config.model_config.init_weights,\n                trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n            )\n            centroid_model.eval()\n            centroid_model.model = centroid_converted_model\n            centroid_model.to(device)\n\n        centroid_model.eval()\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            centroid_model.load_state_dict(\n                ckpt[\"state_dict\"],\n                strict=False,\n                weights_only=False,\n            )\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        centroid_model.to(device)\n\n    else:\n        centroid_config = None\n        centroid_model = None\n\n    if confmap_ckpt_path is not None:\n        # Load confmap model.\n        is_sleap_ckpt = False\n        if (\n            Path(confmap_ckpt_path) / \"training_config.yaml\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            confmap_config = OmegaConf.load(\n                (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(confmap_ckpt_path) / \"training_config.json\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            confmap_config = TrainingJobConfig.load_sleap_config(\n                (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in confmap_config.model_config.backbone_config.items():\n            if v is not None:\n                centered_instance_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n\n            confmap_model = TopDownCenteredInstanceMultiClassLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"multi_class_topdown\",\n                backbone_type=centered_instance_backbone_type,\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                init_weights=confmap_config.model_config.init_weights,\n                trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n            )\n        else:\n            confmap_converted_model = load_legacy_model(\n                model_dir=f\"{confmap_ckpt_path}\"\n            )\n            confmap_model = TopDownCenteredInstanceMultiClassLightningModule(\n                model_type=\"multi_class_topdown\",\n                backbone_type=centered_instance_backbone_type,\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                init_weights=confmap_config.model_config.init_weights,\n                trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            )\n            confmap_model.eval()\n            confmap_model.model = confmap_converted_model\n            confmap_model.to(device)\n\n        confmap_model.eval()\n        skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        confmap_model.to(device)\n\n    else:\n        confmap_config = None\n        confmap_model = None\n\n    if centroid_config is None and confmap_config is None:\n        message = (\n            \"Both a centroid and a confidence map model must be provided to \"\n            \"initialize a TopDownMultiClassPredictor.\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n\n    if centroid_config is not None:\n        preprocess_config[\"scale\"] = (\n            centroid_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            centroid_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            centroid_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            centroid_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            centroid_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    else:\n        preprocess_config[\"scale\"] = (\n            confmap_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            confmap_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            confmap_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            confmap_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            confmap_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    preprocess_config[\"crop_hw\"] = (\n        confmap_config.data_config.preprocessing.crop_hw\n        if preprocess_config[\"crop_hw\"] is None and confmap_config is not None\n        else preprocess_config[\"crop_hw\"]\n    )\n\n    # create an instance of TopDownPredictor class\n    obj = cls(\n        centroid_config=centroid_config,\n        centroid_model=centroid_model,\n        confmap_config=confmap_config,\n        confmap_model=confmap_model,\n        centroid_backbone_type=centroid_backbone_type,\n        centered_instance_backbone_type=centered_instance_backbone_type,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        device=device,\n        preprocess_config=preprocess_config,\n        anchor_part=anchor_part,\n        max_stride=(\n            centroid_config.model_config.backbone_config[\n                f\"{centroid_backbone_type}\"\n            ][\"max_stride\"]\n            if centroid_config is not None\n            else confmap_config.model_config.backbone_config[\n                f\"{centered_instance_backbone_type}\"\n            ][\"max_stride\"]\n        ),\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownMultiClassPredictor.make_pipeline","title":"<code>make_pipeline(data_path, queue_maxsize=8, frames=None, only_labeled_frames=False, only_suggested_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 8.</p> <code>8</code> <code>frames</code> <code>Optional[list]</code> <p>(list) List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    data_path: str,\n    queue_maxsize: int = 8,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n        frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    # LabelsReader provider\n    if data_path.endswith(\".slp\") and video_index is None:\n        provider = LabelsReader\n\n        self.preprocess = False\n\n        self.pipeline = provider.from_filename(\n            filename=data_path,\n            queue_maxsize=queue_maxsize,\n            instances_key=self.instances_key,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n        if self.centroid_config is None:\n            message = (\n                \"Ground truth data was not detected... \"\n                \"Please load both models when predicting on non-ground-truth data.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        self.preprocess = False\n\n        if data_path.endswith(\".slp\") and video_index is not None:\n            labels = sio.load_slp(data_path)\n            self.pipeline = provider.from_video(\n                video=labels.videos[video_index],\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownPredictor","title":"<code>TopDownPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>Top-down multi-instance predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained TopDown SLEAP-NN model. This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>centroid_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the centroid model.</p> <code>confmap_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the             centered-instance model</p> <code>centroid_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights             for centroid model.</p> <code>confmap_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights            for centered-instance model.</p> <code>centroid_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>centered_instance_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below     this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown     centroid and centered-instance model, where the first element corresponds     to centroid model peak finding threshold and the second element is for     centered-instance model peak finding.</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>tracker</code> <code>Optional[Tracker]</code> <p>A <code>sleap_nn.tracking.Tracker</code> that will be called to associate detections over time. Predicted instances will not be assigned to tracks if if this is <code>None</code>.</p> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass TopDownPredictor(Predictor):\n    \"\"\"Top-down multi-instance predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained TopDown SLEAP-NN model. This should be initialized using the\n    `from_trained_models()` constructor.\n\n    Attributes:\n        centroid_config: A Dictionary with the configs used for training the centroid model.\n        confmap_config: A Dictionary with the configs used for training the\n                        centered-instance model\n        centroid_model: A LightningModule instance created from the trained weights\n                        for centroid model.\n        confmap_model: A LightningModule instance created from the trained weights\n                       for centered-instance model.\n        centroid_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        centered_instance_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        tracker: A `sleap_nn.tracking.Tracker` that will be called to associate\n            detections over time. Predicted instances will not be assigned to tracks if\n            if this is `None`.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    centroid_config: Optional[OmegaConf] = None\n    confmap_config: Optional[OmegaConf] = None\n    centroid_model: Optional[L.LightningModule] = None\n    confmap_model: Optional[L.LightningModule] = None\n    centroid_backbone_type: Optional[str] = None\n    centered_instance_backbone_type: Optional[str] = None\n    videos: Optional[List[sio.Video]] = None\n    skeletons: Optional[List[sio.Skeleton]] = None\n    peak_threshold: Union[float, List[float]] = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    tracker: Optional[Tracker] = None\n    anchor_part: Optional[str] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # Create an instance of CentroidLayer if centroid_config is not None\n        return_crops = False\n        # if both centroid and centered-instance model are provided, set return crops to True\n        if self.confmap_model:\n            return_crops = True\n        if isinstance(self.peak_threshold, list):\n            centroid_peak_threshold = self.peak_threshold[0]\n            centered_instance_peak_threshold = self.peak_threshold[1]\n        else:\n            centroid_peak_threshold = self.peak_threshold\n            centered_instance_peak_threshold = self.peak_threshold\n\n        if self.anchor_part is not None:\n            anchor_ind = self.skeletons[0].node_names.index(self.anchor_part)\n        else:\n            anch_pt = None\n            if self.centroid_config is not None:\n                anch_pt = (\n                    self.centroid_config.model_config.head_configs.centroid.confmaps.anchor_part\n                )\n            if self.confmap_config is not None:\n                anch_pt = (\n                    self.confmap_config.model_config.head_configs.centered_instance.confmaps.anchor_part\n                )\n            anchor_ind = (\n                self.skeletons[0].node_names.index(anch_pt)\n                if anch_pt is not None\n                else None\n            )\n\n        if self.centroid_config is None:\n            centroid_crop_layer = CentroidCrop(\n                use_gt_centroids=True,\n                crop_hw=self.preprocess_config.crop_hw,\n                anchor_ind=anchor_ind,\n                return_crops=return_crops,\n            )\n\n        else:\n            max_stride = self.centroid_config.model_config.backbone_config[\n                f\"{self.centroid_backbone_type}\"\n            ][\"max_stride\"]\n            # initialize centroid crop layer\n            centroid_crop_layer = CentroidCrop(\n                torch_model=self.centroid_model,\n                peak_threshold=centroid_peak_threshold,\n                output_stride=self.centroid_config.model_config.head_configs.centroid.confmaps.output_stride,\n                refinement=self.integral_refinement,\n                integral_patch_size=self.integral_patch_size,\n                return_confmaps=self.return_confmaps,\n                return_crops=return_crops,\n                max_instances=self.max_instances,\n                max_stride=max_stride,\n                input_scale=self.centroid_config.data_config.preprocessing.scale,\n                crop_hw=self.preprocess_config.crop_hw,\n                use_gt_centroids=False,\n            )\n\n        # Create an instance of FindInstancePeaks layer if confmap_config is not None\n        if self.confmap_config is None:\n            instance_peaks_layer = FindInstancePeaksGroundTruth()\n            self.instances_key = True\n        else:\n\n            max_stride = self.confmap_config.model_config.backbone_config[\n                f\"{self.centered_instance_backbone_type}\"\n            ][\"max_stride\"]\n            instance_peaks_layer = FindInstancePeaks(\n                torch_model=self.confmap_model,\n                peak_threshold=centered_instance_peak_threshold,\n                output_stride=self.confmap_config.model_config.head_configs.centered_instance.confmaps.output_stride,\n                refinement=self.integral_refinement,\n                integral_patch_size=self.integral_patch_size,\n                return_confmaps=self.return_confmaps,\n                max_stride=max_stride,\n                input_scale=self.confmap_config.data_config.preprocessing.scale,\n            )\n            centroid_crop_layer.precrop_resize = (\n                self.confmap_config.data_config.preprocessing.scale\n            )\n\n        if self.centroid_config is None and self.confmap_config is not None:\n            self.instances_key = (\n                True  # we need `instances` to get ground-truth centroids\n            )\n\n        # Initialize the inference model with centroid and instance peak layers\n        self.inference_model = TopDownInferenceModel(\n            centroid_crop=centroid_crop_layer, instance_peaks=instance_peaks_layer\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        centroid_ckpt_path: Optional[Text] = None,\n        confmap_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        anchor_part: Optional[str] = None,\n    ) -&gt; \"TopDownPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5)  and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n            confmap_ckpt_path: Path to a centered-instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n        Returns:\n            An instance of `TopDownPredictor` with the loaded models.\n\n            One of the two models can be left as `None` to perform inference with ground\n            truth data. This will only work with `LabelsReader` as the provider.\n\n        \"\"\"\n        centered_instance_backbone_type = None\n        centroid_backbone_type = None\n        if centroid_ckpt_path is not None:\n            is_sleap_ckpt = False\n            # Load centroid model.\n            if (\n                Path(centroid_ckpt_path) / \"training_config.yaml\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                centroid_config = OmegaConf.load(\n                    (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(centroid_ckpt_path) / \"training_config.json\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                centroid_config = TrainingJobConfig.load_sleap_config(\n                    (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n            # check which backbone architecture\n            for k, v in centroid_config.model_config.backbone_config.items():\n                if v is not None:\n                    centroid_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n                centroid_model = CentroidLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"centroid\",\n                    backbone_type=centroid_backbone_type,\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                    init_weights=centroid_config.model_config.init_weights,\n                    trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                    map_location=device,\n                )\n            else:\n                # Load the converted model\n                centroid_converted_model = load_legacy_model(\n                    model_dir=f\"{centroid_ckpt_path}\"\n                )\n                centroid_model = CentroidLightningModule(\n                    backbone_type=centroid_backbone_type,\n                    model_type=\"centroid\",\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                    init_weights=centroid_config.model_config.init_weights,\n                    trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                )\n\n                centroid_model.eval()\n                centroid_model.model = centroid_converted_model\n                centroid_model.to(device)\n\n            centroid_model.eval()\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            centroid_model.to(device)\n\n        else:\n            centroid_config = None\n            centroid_model = None\n\n        if confmap_ckpt_path is not None:\n            is_sleap_ckpt = False\n            # Load confmap model.\n            if (\n                Path(confmap_ckpt_path) / \"training_config.yaml\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                confmap_config = OmegaConf.load(\n                    (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(confmap_ckpt_path) / \"training_config.json\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                confmap_config = TrainingJobConfig.load_sleap_config(\n                    (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n            # check which backbone architecture\n            for k, v in confmap_config.model_config.backbone_config.items():\n                if v is not None:\n                    centered_instance_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n                confmap_model = TopDownCenteredInstanceLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"centered_instance\",\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                    init_weights=confmap_config.model_config.init_weights,\n                    trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                    backbone_type=centered_instance_backbone_type,\n                    map_location=device,\n                )\n            else:\n                # Load the converted model\n                confmap_converted_model = load_legacy_model(\n                    model_dir=f\"{confmap_ckpt_path}\"\n                )\n\n                # Create a new LightningModule with the converted model\n                confmap_model = TopDownCenteredInstanceLightningModule(\n                    backbone_type=centered_instance_backbone_type,\n                    model_type=\"centered_instance\",\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                    pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                    init_weights=confmap_config.model_config.init_weights,\n                    trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                )\n\n                confmap_model.eval()\n                confmap_model.model = confmap_converted_model\n                confmap_model.to(device)\n\n            confmap_model.eval()\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            confmap_model.to(device)\n\n        else:\n            confmap_config = None\n            confmap_model = None\n\n        if centroid_config is not None:\n            preprocess_config[\"scale\"] = (\n                centroid_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                centroid_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                centroid_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                centroid_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                centroid_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        else:\n            preprocess_config[\"scale\"] = (\n                confmap_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                confmap_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                confmap_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                confmap_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                confmap_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        preprocess_config[\"crop_hw\"] = (\n            confmap_config.data_config.preprocessing.crop_hw\n            if preprocess_config[\"crop_hw\"] is None and confmap_config is not None\n            else preprocess_config[\"crop_hw\"]\n        )\n\n        # create an instance of TopDownPredictor class\n        obj = cls(\n            centroid_config=centroid_config,\n            centroid_model=centroid_model,\n            confmap_config=confmap_config,\n            confmap_model=confmap_model,\n            centroid_backbone_type=centroid_backbone_type,\n            centered_instance_backbone_type=centered_instance_backbone_type,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n            anchor_part=anchor_part,\n            max_stride=(\n                centroid_config.model_config.backbone_config[\n                    f\"{centroid_backbone_type}\"\n                ][\"max_stride\"]\n                if centroid_config is not None\n                else confmap_config.model_config.backbone_config[\n                    f\"{centered_instance_backbone_type}\"\n                ][\"max_stride\"]\n            ),\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        data_path: str,\n        queue_maxsize: int = 8,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n            frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        # LabelsReader provider\n        if data_path.endswith(\".slp\") and video_index is None:\n            provider = LabelsReader\n\n            self.preprocess = False\n\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                instances_key=self.instances_key,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n            if self.centroid_config is None:\n                message = (\n                    \"Ground truth data was not detected... \"\n                    \"Please load both models when predicting on non-ground-truth data.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n\n            self.preprocess = False\n\n            if data_path.endswith(\".slp\") and video_index is not None:\n                labels = sio.load_slp(data_path)\n                self.pipeline = provider.from_video(\n                    video=labels.videos[video_index],\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                self.pipeline = provider.from_filename(\n                    filename=data_path,\n                    queue_maxsize=queue_maxsize,\n                    frames=frames,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        preds = defaultdict(list)\n        predicted_frames = []\n        skeleton_idx = 0\n        # Loop through each predicted instance.\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                bbox,\n                pred_instances,\n                pred_values,\n                instance_score,\n                org_size,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"instance_bbox\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"centroid_val\"],\n                ex[\"orig_size\"],\n            ):\n                if np.isnan(pred_instances).all():\n                    continue\n                pred_instances = pred_instances + bbox.squeeze(axis=0)[0, :]\n                preds[(int(video_idx), int(frame_idx))].append(\n                    sio.PredictedInstance.from_numpy(\n                        points_data=pred_instances,\n                        skeleton=self.skeletons[skeleton_idx],\n                        point_scores=pred_values,\n                        score=instance_score,\n                    )\n                )\n        for key, inst in preds.items():\n            # Create list of LabeledFrames.\n            video_idx, frame_idx = key\n            lf = sio.LabeledFrame(\n                video=self.videos[video_idx],\n                frame_idx=frame_idx,\n                instances=inst,\n            )\n\n            if self.tracker:\n                lf.instances = self.tracker.track(\n                    untracked_instances=inst, frame_idx=frame_idx, image=lf.image\n                )\n\n            predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownPredictor.from_trained_models","title":"<code>from_trained_models(centroid_ckpt_path=None, confmap_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, anchor_part=None)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5)  and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).</p> <code>None</code> <code>confmap_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centered-instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>None</code> <p>Returns:</p> Type Description <code>TopDownPredictor</code> <p>An instance of <code>TopDownPredictor</code> with the loaded models.</p> <p>One of the two models can be left as <code>None</code> to perform inference with ground truth data. This will only work with <code>LabelsReader</code> as the provider.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    centroid_ckpt_path: Optional[Text] = None,\n    confmap_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    anchor_part: Optional[str] = None,\n) -&gt; \"TopDownPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5)  and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n        confmap_ckpt_path: Path to a centered-instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mkldnn\", \"opengl\", \"opencl\", \"ideep\", \"hip\", \"msnpu\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n    Returns:\n        An instance of `TopDownPredictor` with the loaded models.\n\n        One of the two models can be left as `None` to perform inference with ground\n        truth data. This will only work with `LabelsReader` as the provider.\n\n    \"\"\"\n    centered_instance_backbone_type = None\n    centroid_backbone_type = None\n    if centroid_ckpt_path is not None:\n        is_sleap_ckpt = False\n        # Load centroid model.\n        if (\n            Path(centroid_ckpt_path) / \"training_config.yaml\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            centroid_config = OmegaConf.load(\n                (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(centroid_ckpt_path) / \"training_config.json\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            centroid_config = TrainingJobConfig.load_sleap_config(\n                (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n        # check which backbone architecture\n        for k, v in centroid_config.model_config.backbone_config.items():\n            if v is not None:\n                centroid_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n            centroid_model = CentroidLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"centroid\",\n                backbone_type=centroid_backbone_type,\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                init_weights=centroid_config.model_config.init_weights,\n                trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n            )\n        else:\n            # Load the converted model\n            centroid_converted_model = load_legacy_model(\n                model_dir=f\"{centroid_ckpt_path}\"\n            )\n            centroid_model = CentroidLightningModule(\n                backbone_type=centroid_backbone_type,\n                model_type=\"centroid\",\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=centroid_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=centroid_config.model_config.pretrained_head_weights,\n                init_weights=centroid_config.model_config.init_weights,\n                trainer_accelerator=centroid_config.trainer_config.trainer_accelerator,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n            )\n\n            centroid_model.eval()\n            centroid_model.model = centroid_converted_model\n            centroid_model.to(device)\n\n        centroid_model.eval()\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        centroid_model.to(device)\n\n    else:\n        centroid_config = None\n        centroid_model = None\n\n    if confmap_ckpt_path is not None:\n        is_sleap_ckpt = False\n        # Load confmap model.\n        if (\n            Path(confmap_ckpt_path) / \"training_config.yaml\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            confmap_config = OmegaConf.load(\n                (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(confmap_ckpt_path) / \"training_config.json\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            confmap_config = TrainingJobConfig.load_sleap_config(\n                (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n        # check which backbone architecture\n        for k, v in confmap_config.model_config.backbone_config.items():\n            if v is not None:\n                centered_instance_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n            confmap_model = TopDownCenteredInstanceLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"centered_instance\",\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                init_weights=confmap_config.model_config.init_weights,\n                trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                backbone_type=centered_instance_backbone_type,\n                map_location=device,\n            )\n        else:\n            # Load the converted model\n            confmap_converted_model = load_legacy_model(\n                model_dir=f\"{confmap_ckpt_path}\"\n            )\n\n            # Create a new LightningModule with the converted model\n            confmap_model = TopDownCenteredInstanceLightningModule(\n                backbone_type=centered_instance_backbone_type,\n                model_type=\"centered_instance\",\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=confmap_config.model_config.pretrained_backbone_weights,\n                pretrained_head_weights=confmap_config.model_config.pretrained_head_weights,\n                init_weights=confmap_config.model_config.init_weights,\n                trainer_accelerator=confmap_config.trainer_config.trainer_accelerator,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            )\n\n            confmap_model.eval()\n            confmap_model.model = confmap_converted_model\n            confmap_model.to(device)\n\n        confmap_model.eval()\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        confmap_model.to(device)\n\n    else:\n        confmap_config = None\n        confmap_model = None\n\n    if centroid_config is not None:\n        preprocess_config[\"scale\"] = (\n            centroid_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            centroid_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            centroid_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            centroid_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            centroid_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    else:\n        preprocess_config[\"scale\"] = (\n            confmap_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            confmap_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            confmap_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            confmap_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            confmap_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    preprocess_config[\"crop_hw\"] = (\n        confmap_config.data_config.preprocessing.crop_hw\n        if preprocess_config[\"crop_hw\"] is None and confmap_config is not None\n        else preprocess_config[\"crop_hw\"]\n    )\n\n    # create an instance of TopDownPredictor class\n    obj = cls(\n        centroid_config=centroid_config,\n        centroid_model=centroid_model,\n        confmap_config=confmap_config,\n        confmap_model=confmap_model,\n        centroid_backbone_type=centroid_backbone_type,\n        centered_instance_backbone_type=centered_instance_backbone_type,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        device=device,\n        preprocess_config=preprocess_config,\n        anchor_part=anchor_part,\n        max_stride=(\n            centroid_config.model_config.backbone_config[\n                f\"{centroid_backbone_type}\"\n            ][\"max_stride\"]\n            if centroid_config is not None\n            else confmap_config.model_config.backbone_config[\n                f\"{centered_instance_backbone_type}\"\n            ][\"max_stride\"]\n        ),\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownPredictor.make_pipeline","title":"<code>make_pipeline(data_path, queue_maxsize=8, frames=None, only_labeled_frames=False, only_suggested_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 8.</p> <code>8</code> <code>frames</code> <code>Optional[list]</code> <p>(list) List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    data_path: str,\n    queue_maxsize: int = 8,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 8.\n        frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    # LabelsReader provider\n    if data_path.endswith(\".slp\") and video_index is None:\n        provider = LabelsReader\n\n        self.preprocess = False\n\n        self.pipeline = provider.from_filename(\n            filename=data_path,\n            queue_maxsize=queue_maxsize,\n            instances_key=self.instances_key,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n        if self.centroid_config is None:\n            message = (\n                \"Ground truth data was not detected... \"\n                \"Please load both models when predicting on non-ground-truth data.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        self.preprocess = False\n\n        if data_path.endswith(\".slp\") and video_index is not None:\n            labels = sio.load_slp(data_path)\n            self.pipeline = provider.from_video(\n                video=labels.videos[video_index],\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            self.pipeline = provider.from_filename(\n                filename=data_path,\n                queue_maxsize=queue_maxsize,\n                frames=frames,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/single_instance/","title":"single_instance","text":""},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance","title":"<code>sleap_nn.inference.single_instance</code>","text":"<p>Inference modules for SingleInstance models.</p> <p>Classes:</p> Name Description <code>SingleInstanceInferenceModel</code> <p>Single instance prediction model.</p>"},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance.SingleInstanceInferenceModel","title":"<code>SingleInstanceInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Single instance prediction model.</p> <p>This model encapsulates the basic single instance approach where it is assumed that there is only one instance in the frame. The images are passed to a peak detector which is trained to detect all body parts for the instance assuming a single peak per body part.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/single_instance.py</code> <pre><code>class SingleInstanceInferenceModel(L.LightningModule):\n    \"\"\"Single instance prediction model.\n\n    This model encapsulates the basic single instance approach where it is assumed that\n    there is only one instance in the frame. The images are passed to a peak detector\n    which is trained to detect all body parts for the instance assuming a single peak\n    per body part.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = None,\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__()\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.input_scale = input_scale\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        Args:\n            inputs: Dictionary with \"image\" as one of the keys.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n                as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n        \"\"\"\n        # Network forward pass.\n        cms = self.torch_model(inputs[\"image\"])\n\n        peak_points, peak_vals = find_global_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n\n        # Adjust for stride and scale.\n        peak_points = peak_points * self.output_stride\n        if self.input_scale != 1.0:\n            peak_points = peak_points / self.input_scale\n        peak_points = peak_points / (\n            inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2)\n        ).to(peak_points.device)\n\n        # Build outputs.\n        outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n        if self.return_confmaps:\n            outputs[\"pred_confmaps\"] = cms.detach()\n        inputs.update(outputs)\n        return [inputs]\n</code></pre>"},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance.SingleInstanceInferenceModel.__init__","title":"<code>__init__(torch_model, output_stride=None, peak_threshold=0.0, refinement=None, integral_patch_size=5, return_confmaps=False, input_scale=1.0)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/single_instance.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__()\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance.SingleInstanceInferenceModel.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with \"image\" as one of the keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch     as a <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> Source code in <code>sleap_nn/inference/single_instance.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    Args:\n        inputs: Dictionary with \"image\" as one of the keys.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n            as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n    \"\"\"\n    # Network forward pass.\n    cms = self.torch_model(inputs[\"image\"])\n\n    peak_points, peak_vals = find_global_peaks(\n        cms.detach(),\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n\n    # Adjust for stride and scale.\n    peak_points = peak_points * self.output_stride\n    if self.input_scale != 1.0:\n        peak_points = peak_points / self.input_scale\n    peak_points = peak_points / (\n        inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2)\n    ).to(peak_points.device)\n\n    # Build outputs.\n    outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n    if self.return_confmaps:\n        outputs[\"pred_confmaps\"] = cms.detach()\n    inputs.update(outputs)\n    return [inputs]\n</code></pre>"},{"location":"api/inference/topdown/","title":"topdown","text":""},{"location":"api/inference/topdown/#sleap_nn.inference.topdown","title":"<code>sleap_nn.inference.topdown</code>","text":"<p>Inference modules for TopDown centroid and centered-instance models.</p> <p>Classes:</p> Name Description <code>CentroidCrop</code> <p>Lightning Module for running inference for a centroid model.</p> <code>FindInstancePeaks</code> <p>Lightning Module that predicts instance peaks from images using a trained model.</p> <code>FindInstancePeaksGroundTruth</code> <p>LightningModule that simulates a centered instance peaks model.</p> <code>TopDownInferenceModel</code> <p>Top-down instance prediction model.</p> <code>TopDownMultiClassFindInstancePeaks</code> <p>Lightning Module that predicts instance peaks from images using a trained model.</p>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.CentroidCrop","title":"<code>CentroidCrop</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning Module for running inference for a centroid model.</p> <p>This layer encapsulates all of the inference operations requires for generating predictions from a centroid confidence map model. This includes model forward pass, generating crops for cenetered instance model, peak finding, coordinate adjustment and cropping.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>max_instances</code> <p>Max number of instances to consider during centroid predictions.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>return_crops</code> <p>If <code>True</code>, the output dictionary will also contain <code>instance_image</code> which is the cropped image of size (batch, 1, chn, crop_height, crop_width).</p> <code>crop_hw</code> <p>Tuple (height, width) representing the crop size.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <code>precrop_resize</code> <p>Float indicating the factor by which the original images (not images resized for centroid model) should be resized before cropping. Note: This resize happens only after getting the predictions for centroid model.</p> <code>max_stride</code> <p>Maximum stride in a model that the images must be divisible by. If &gt; 1, this will pad the bottom and right of the images to ensure they meet this divisibility criteria. Padding is applied after the scaling specified in the <code>scale</code> attribute.</p> <code>use_gt_centroids</code> <p>If <code>True</code>, then the crops are generated using ground-truth centroids. If <code>False</code>, then centroids are predicted using a trained centroid model.</p> <code>anchor_ind</code> <p>The index of the node to use as the anchor for the centroid. If not provided or if not present in the instance, the midpoint of the bounding box is used instead.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict centroid confidence maps and crop around peaks.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class CentroidCrop(L.LightningModule):\n    \"\"\"Lightning Module for running inference for a centroid model.\n\n    This layer encapsulates all of the inference operations requires for generating\n    predictions from a centroid confidence map model. This includes model forward pass,\n    generating crops for cenetered instance model, peak finding, coordinate adjustment\n    and cropping.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        max_instances: Max number of instances to consider during centroid predictions.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        return_crops: If `True`, the output dictionary will also contain `instance_image`\n            which is the cropped image of size (batch, 1, chn, crop_height, crop_width).\n        crop_hw: Tuple (height, width) representing the crop size.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n        precrop_resize: Float indicating the factor by which the original images\n            (not images resized for centroid model) should be resized before cropping.\n            Note: This resize happens only after getting the predictions for centroid model.\n        max_stride: Maximum stride in a model that the images must be divisible by.\n            If &gt; 1, this will pad the bottom and right of the images to ensure they meet\n            this divisibility criteria. Padding is applied after the scaling specified\n            in the `scale` attribute.\n        use_gt_centroids: If `True`, then the crops are generated using ground-truth centroids.\n            If `False`, then centroids are predicted using a trained centroid model.\n        anchor_ind: The index of the node to use as the anchor for the centroid. If not\n            provided or if not present in the instance, the midpoint of the bounding box\n            is used instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: Optional[L.LightningModule] = None,\n        output_stride: int = 1,\n        peak_threshold: float = 0.0,\n        max_instances: Optional[int] = None,\n        refinement: Optional[str] = None,\n        integral_patch_size: int = 5,\n        return_confmaps: bool = False,\n        return_crops: bool = False,\n        crop_hw: Optional[List[int]] = None,\n        input_scale: float = 1.0,\n        precrop_resize: float = 1.0,\n        max_stride: int = 1,\n        use_gt_centroids: bool = False,\n        anchor_ind: Optional[int] = None,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.max_instances = max_instances\n        self.return_crops = return_crops\n        self.crop_hw = crop_hw\n        self.input_scale = input_scale\n        self.precrop_resize = precrop_resize\n        self.max_stride = max_stride\n        self.use_gt_centroids = use_gt_centroids\n        self.anchor_ind = anchor_ind\n\n    def _generate_crops(self, inputs):\n        \"\"\"Generate Crops from the predicted centroids.\"\"\"\n        crops_dict = []\n        for centroid, centroid_val, image, fidx, vidx, sz, eff_sc in zip(\n            self.refined_peaks_batched,\n            self.peak_vals_batched,\n            inputs[\"image\"],\n            inputs[\"frame_idx\"],\n            inputs[\"video_idx\"],\n            inputs[\"orig_size\"],\n            inputs[\"eff_scale\"],\n        ):\n            if torch.any(torch.isnan(centroid)):\n                if torch.all(torch.isnan(centroid)):\n                    continue\n                else:\n                    non_nans = ~torch.any(centroid.isnan(), dim=-1)\n                    centroid = centroid[non_nans]\n                    if len(centroid.shape) == 1:\n                        centroid = centroid.unsqueeze(dim=0)\n                    centroid_val = centroid_val[non_nans]\n            n = centroid.shape[0]\n            box_size = (\n                self.crop_hw[0],\n                self.crop_hw[1],\n            )\n            instance_bbox = torch.unsqueeze(\n                make_centered_bboxes(centroid, box_size[0], box_size[1]), 0\n            )  # (1, n, 4, 2)\n\n            # Generate cropped image of shape (n, C, crop_H, crop_W)\n            instance_image = crop_bboxes(\n                image,\n                bboxes=instance_bbox.squeeze(dim=0),\n                sample_inds=[0] * n,\n            )\n\n            # Access top left point (x,y) of bounding box and subtract this offset from\n            # position of nodes.\n            point = instance_bbox[0, :, 0]\n            centered_centroid = centroid - point\n\n            ex = {}\n            ex[\"image\"] = torch.cat([image] * n)\n            ex[\"centroid\"] = centered_centroid\n            ex[\"centroid_val\"] = centroid_val\n            ex[\"frame_idx\"] = torch.Tensor([fidx] * n)\n            ex[\"video_idx\"] = torch.Tensor([vidx] * n)\n            ex[\"instance_bbox\"] = instance_bbox.squeeze(dim=0).unsqueeze(dim=1)\n            ex[\"instance_image\"] = instance_image.unsqueeze(dim=1)\n            ex[\"orig_size\"] = torch.cat([torch.Tensor(sz)] * n)\n            ex[\"eff_scale\"] = torch.Tensor([eff_sc] * n)\n            crops_dict.append(ex)\n\n        return crops_dict\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict centroid confidence maps and crop around peaks.\n\n        This layer can be chained with a `FindInstancePeaks` layer to create a top-down\n        inference function from full images.\n\n        Args:\n            inputs: Dictionary with key `\"image\"`. Other keys will be passed down the pipeline.\n\n        Returns:\n            A list of dictionaries (size = batch size) where each dictionary has cropped\n            images with key `instance_image` and `centroid_val` batched based on the\n            number of centroids predicted for each image in the original batch if\n            return_crops is True.\n            If return_crops is not True, this module returns the dictionary with\n            `centroids` and `centroid_val` keys with shapes (batch, 1, max_instances, 2)\n            and (batch, max_instances) respectively which could then to passed to\n            FindInstancePeaksGroundTruth class.\n        \"\"\"\n        if self.use_gt_centroids:\n            batch = inputs[\"video_idx\"].shape[0]\n            centroids = generate_centroids(\n                inputs[\"instances\"], anchor_ind=self.anchor_ind\n            )\n            centroid_vals = torch.ones(centroids.shape)[..., 0]\n            self.refined_peaks_batched = [x[0] for x in centroids]\n            self.peak_vals_batched = [x[0] for x in centroid_vals]\n\n            max_instances = (\n                self.max_instances\n                if self.max_instances is not None\n                else inputs[\"instances\"].shape[-3]\n            )\n\n            refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n            peak_vals_with_nans = torch.zeros((batch, max_instances))\n            for ind, (r, p) in enumerate(\n                zip(self.refined_peaks_batched, self.peak_vals_batched)\n            ):\n                refined_peaks_with_nans[ind] = r\n                peak_vals_with_nans[ind] = p\n\n            inputs.update(\n                {\n                    \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                    \"centroid_vals\": peak_vals_with_nans,\n                }\n            )\n\n            if self.return_crops:\n                crops_dict = self._generate_crops(inputs)\n                inputs[\"image\"] = resize_image(inputs[\"image\"], self.precrop_resize)\n                inputs[\"centroids\"] *= self.precrop_resize\n                scaled_refined_peaks = []\n                for ref_peak in self.refined_peaks_batched:\n                    scaled_refined_peaks.append(ref_peak * self.precrop_resize)\n                self.refined_peaks_batched = scaled_refined_peaks\n                return crops_dict\n            else:\n                return inputs\n\n        # Network forward pass.\n        orig_image = inputs[\"image\"]\n        scaled_image = resize_image(orig_image, self.input_scale)\n        if self.max_stride != 1:\n            scaled_image = apply_pad_to_stride(scaled_image, self.max_stride)\n\n        cms = self.torch_model(scaled_image)\n\n        refined_peaks, peak_vals, peak_sample_inds, _ = find_local_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n        # Adjust for stride and scale.\n        refined_peaks = refined_peaks * self.output_stride  # (n_centroids, 2)\n        refined_peaks = refined_peaks / self.input_scale\n\n        batch = cms.shape[0]\n\n        # if max instances is not provided, find the max_instances for this batch\n        num_instances = defaultdict(int)\n        for p in peak_sample_inds:\n            num_instances[int(p)] += 1\n\n        if num_instances:\n            max_instances = max(num_instances.values()) if num_instances else None\n            if self.max_instances is not None:\n                max_instances = self.max_instances\n\n            self.refined_peaks_batched = []\n            self.peak_vals_batched = []\n\n            for b in range(batch):\n                indices = (peak_sample_inds == b).nonzero()\n                # list for predicted centroids and corresponding peak values for current batch.\n                current_peaks = refined_peaks[indices].squeeze(dim=-2)\n                current_peak_vals = peak_vals[indices].squeeze(dim=-1)\n                # Choose top k centroids if max_instances is provided.\n                if len(current_peaks) &gt; max_instances:\n                    current_peak_vals, indices = torch.topk(\n                        current_peak_vals, max_instances\n                    )\n                    current_peaks = current_peaks[indices]\n                    num_nans = 0\n                else:\n                    num_nans = max_instances - len(current_peaks)\n                nans = torch.full((num_nans, 2), torch.nan)\n                current_peaks = torch.cat(\n                    [current_peaks, nans.to(current_peaks.device)], dim=0\n                )\n                nans = torch.full((num_nans,), torch.nan)\n                current_peak_vals = torch.cat(\n                    [current_peak_vals, nans.to(current_peak_vals.device)], dim=0\n                )\n                self.refined_peaks_batched.append(current_peaks)\n                self.peak_vals_batched.append(current_peak_vals)\n\n            # Generate crops if return_crops=True to pass the crops to CenteredInstance model.\n            if self.return_crops:\n                inputs[\"image\"] = resize_image(inputs[\"image\"], self.precrop_resize)\n                scaled_refined_peaks = []\n                for ref_peak in self.refined_peaks_batched:\n                    scaled_refined_peaks.append(ref_peak * self.precrop_resize)\n                self.refined_peaks_batched = scaled_refined_peaks\n\n                inputs.update(\n                    {\n                        \"centroids\": self.refined_peaks_batched,\n                        \"centroid_vals\": self.peak_vals_batched,\n                    }\n                )\n                crops_dict = self._generate_crops(inputs)\n                return crops_dict\n            else:\n                # batch the peaks to pass it to FindInstancePeaksGroundTruth class.\n                refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n                peak_vals_with_nans = torch.zeros((batch, max_instances))\n                for ind, (r, p) in enumerate(\n                    zip(self.refined_peaks_batched, self.peak_vals_batched)\n                ):\n                    refined_peaks_with_nans[ind] = r\n                    peak_vals_with_nans[ind] = p\n                refined_peaks_with_nans = refined_peaks_with_nans / (\n                    inputs[\"eff_scale\"]\n                    .unsqueeze(dim=1)\n                    .unsqueeze(dim=2)\n                    .to(refined_peaks_with_nans.device)\n                )\n                inputs.update(\n                    {\n                        \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                        \"centroid_vals\": peak_vals_with_nans,\n                    }\n                )\n                if self.return_confmaps:\n                    inputs.update(\n                        {\n                            \"pred_centroid_confmaps\": cms.detach(),\n                        }\n                    )\n\n                return inputs\n\n        else:\n            # if there are no peak detections\n            max_instances = 1 if self.max_instances is None else self.max_instances\n            if self.return_crops:\n                return None\n            refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n            peak_vals_with_nans = torch.zeros((batch, max_instances))\n            for b in range(batch):\n                refined_peaks_with_nans[b] = torch.full((1, 2), torch.nan)\n                peak_vals_with_nans[b] = torch.nan\n\n            inputs.update(\n                {\n                    \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                    \"centroid_vals\": peak_vals_with_nans,\n                }\n            )\n            if self.return_confmaps:\n                inputs.update(\n                    {\n                        \"pred_centroid_confmaps\": cms.detach(),\n                    }\n                )\n            return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.CentroidCrop.__init__","title":"<code>__init__(torch_model=None, output_stride=1, peak_threshold=0.0, max_instances=None, refinement=None, integral_patch_size=5, return_confmaps=False, return_crops=False, crop_hw=None, input_scale=1.0, precrop_resize=1.0, max_stride=1, use_gt_centroids=False, anchor_ind=None, **kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    torch_model: Optional[L.LightningModule] = None,\n    output_stride: int = 1,\n    peak_threshold: float = 0.0,\n    max_instances: Optional[int] = None,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n    return_confmaps: bool = False,\n    return_crops: bool = False,\n    crop_hw: Optional[List[int]] = None,\n    input_scale: float = 1.0,\n    precrop_resize: float = 1.0,\n    max_stride: int = 1,\n    use_gt_centroids: bool = False,\n    anchor_ind: Optional[int] = None,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.max_instances = max_instances\n    self.return_crops = return_crops\n    self.crop_hw = crop_hw\n    self.input_scale = input_scale\n    self.precrop_resize = precrop_resize\n    self.max_stride = max_stride\n    self.use_gt_centroids = use_gt_centroids\n    self.anchor_ind = anchor_ind\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.CentroidCrop.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict centroid confidence maps and crop around peaks.</p> <p>This layer can be chained with a <code>FindInstancePeaks</code> layer to create a top-down inference function from full images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with key <code>\"image\"</code>. Other keys will be passed down the pipeline.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A list of dictionaries (size = batch size) where each dictionary has cropped images with key <code>instance_image</code> and <code>centroid_val</code> batched based on the number of centroids predicted for each image in the original batch if return_crops is True. If return_crops is not True, this module returns the dictionary with <code>centroids</code> and <code>centroid_val</code> keys with shapes (batch, 1, max_instances, 2) and (batch, max_instances) respectively which could then to passed to FindInstancePeaksGroundTruth class.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict centroid confidence maps and crop around peaks.\n\n    This layer can be chained with a `FindInstancePeaks` layer to create a top-down\n    inference function from full images.\n\n    Args:\n        inputs: Dictionary with key `\"image\"`. Other keys will be passed down the pipeline.\n\n    Returns:\n        A list of dictionaries (size = batch size) where each dictionary has cropped\n        images with key `instance_image` and `centroid_val` batched based on the\n        number of centroids predicted for each image in the original batch if\n        return_crops is True.\n        If return_crops is not True, this module returns the dictionary with\n        `centroids` and `centroid_val` keys with shapes (batch, 1, max_instances, 2)\n        and (batch, max_instances) respectively which could then to passed to\n        FindInstancePeaksGroundTruth class.\n    \"\"\"\n    if self.use_gt_centroids:\n        batch = inputs[\"video_idx\"].shape[0]\n        centroids = generate_centroids(\n            inputs[\"instances\"], anchor_ind=self.anchor_ind\n        )\n        centroid_vals = torch.ones(centroids.shape)[..., 0]\n        self.refined_peaks_batched = [x[0] for x in centroids]\n        self.peak_vals_batched = [x[0] for x in centroid_vals]\n\n        max_instances = (\n            self.max_instances\n            if self.max_instances is not None\n            else inputs[\"instances\"].shape[-3]\n        )\n\n        refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n        peak_vals_with_nans = torch.zeros((batch, max_instances))\n        for ind, (r, p) in enumerate(\n            zip(self.refined_peaks_batched, self.peak_vals_batched)\n        ):\n            refined_peaks_with_nans[ind] = r\n            peak_vals_with_nans[ind] = p\n\n        inputs.update(\n            {\n                \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                \"centroid_vals\": peak_vals_with_nans,\n            }\n        )\n\n        if self.return_crops:\n            crops_dict = self._generate_crops(inputs)\n            inputs[\"image\"] = resize_image(inputs[\"image\"], self.precrop_resize)\n            inputs[\"centroids\"] *= self.precrop_resize\n            scaled_refined_peaks = []\n            for ref_peak in self.refined_peaks_batched:\n                scaled_refined_peaks.append(ref_peak * self.precrop_resize)\n            self.refined_peaks_batched = scaled_refined_peaks\n            return crops_dict\n        else:\n            return inputs\n\n    # Network forward pass.\n    orig_image = inputs[\"image\"]\n    scaled_image = resize_image(orig_image, self.input_scale)\n    if self.max_stride != 1:\n        scaled_image = apply_pad_to_stride(scaled_image, self.max_stride)\n\n    cms = self.torch_model(scaled_image)\n\n    refined_peaks, peak_vals, peak_sample_inds, _ = find_local_peaks(\n        cms.detach(),\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n    # Adjust for stride and scale.\n    refined_peaks = refined_peaks * self.output_stride  # (n_centroids, 2)\n    refined_peaks = refined_peaks / self.input_scale\n\n    batch = cms.shape[0]\n\n    # if max instances is not provided, find the max_instances for this batch\n    num_instances = defaultdict(int)\n    for p in peak_sample_inds:\n        num_instances[int(p)] += 1\n\n    if num_instances:\n        max_instances = max(num_instances.values()) if num_instances else None\n        if self.max_instances is not None:\n            max_instances = self.max_instances\n\n        self.refined_peaks_batched = []\n        self.peak_vals_batched = []\n\n        for b in range(batch):\n            indices = (peak_sample_inds == b).nonzero()\n            # list for predicted centroids and corresponding peak values for current batch.\n            current_peaks = refined_peaks[indices].squeeze(dim=-2)\n            current_peak_vals = peak_vals[indices].squeeze(dim=-1)\n            # Choose top k centroids if max_instances is provided.\n            if len(current_peaks) &gt; max_instances:\n                current_peak_vals, indices = torch.topk(\n                    current_peak_vals, max_instances\n                )\n                current_peaks = current_peaks[indices]\n                num_nans = 0\n            else:\n                num_nans = max_instances - len(current_peaks)\n            nans = torch.full((num_nans, 2), torch.nan)\n            current_peaks = torch.cat(\n                [current_peaks, nans.to(current_peaks.device)], dim=0\n            )\n            nans = torch.full((num_nans,), torch.nan)\n            current_peak_vals = torch.cat(\n                [current_peak_vals, nans.to(current_peak_vals.device)], dim=0\n            )\n            self.refined_peaks_batched.append(current_peaks)\n            self.peak_vals_batched.append(current_peak_vals)\n\n        # Generate crops if return_crops=True to pass the crops to CenteredInstance model.\n        if self.return_crops:\n            inputs[\"image\"] = resize_image(inputs[\"image\"], self.precrop_resize)\n            scaled_refined_peaks = []\n            for ref_peak in self.refined_peaks_batched:\n                scaled_refined_peaks.append(ref_peak * self.precrop_resize)\n            self.refined_peaks_batched = scaled_refined_peaks\n\n            inputs.update(\n                {\n                    \"centroids\": self.refined_peaks_batched,\n                    \"centroid_vals\": self.peak_vals_batched,\n                }\n            )\n            crops_dict = self._generate_crops(inputs)\n            return crops_dict\n        else:\n            # batch the peaks to pass it to FindInstancePeaksGroundTruth class.\n            refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n            peak_vals_with_nans = torch.zeros((batch, max_instances))\n            for ind, (r, p) in enumerate(\n                zip(self.refined_peaks_batched, self.peak_vals_batched)\n            ):\n                refined_peaks_with_nans[ind] = r\n                peak_vals_with_nans[ind] = p\n            refined_peaks_with_nans = refined_peaks_with_nans / (\n                inputs[\"eff_scale\"]\n                .unsqueeze(dim=1)\n                .unsqueeze(dim=2)\n                .to(refined_peaks_with_nans.device)\n            )\n            inputs.update(\n                {\n                    \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                    \"centroid_vals\": peak_vals_with_nans,\n                }\n            )\n            if self.return_confmaps:\n                inputs.update(\n                    {\n                        \"pred_centroid_confmaps\": cms.detach(),\n                    }\n                )\n\n            return inputs\n\n    else:\n        # if there are no peak detections\n        max_instances = 1 if self.max_instances is None else self.max_instances\n        if self.return_crops:\n            return None\n        refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n        peak_vals_with_nans = torch.zeros((batch, max_instances))\n        for b in range(batch):\n            refined_peaks_with_nans[b] = torch.full((1, 2), torch.nan)\n            peak_vals_with_nans[b] = torch.nan\n\n        inputs.update(\n            {\n                \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                \"centroid_vals\": peak_vals_with_nans,\n            }\n        )\n        if self.return_confmaps:\n            inputs.update(\n                {\n                    \"pred_centroid_confmaps\": cms.detach(),\n                }\n            )\n        return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaks","title":"<code>FindInstancePeaks</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning Module that predicts instance peaks from images using a trained model.</p> <p>This layer encapsulates all of the inference operations required for generating predictions from a centered instance confidence map model. This includes model forward pass, peak finding and coordinate adjustment.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>input_scale</code> <p>Float indicating the scale with which the images were scaled before cropping.</p> <code>max_stride</code> <p>Maximum stride in a model that the images must be divisible by. If &gt; 1, this will pad the bottom and right of the images to ensure they meet this divisibility criteria. Padding is applied after the scaling specified in the <code>scale</code> attribute.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class FindInstancePeaks(L.LightningModule):\n    \"\"\"Lightning Module that predicts instance peaks from images using a trained model.\n\n    This layer encapsulates all of the inference operations required for generating\n    predictions from a centered instance confidence map model. This includes\n    model forward pass, peak finding and coordinate adjustment.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        input_scale: Float indicating the scale with which the images were scaled before\n            cropping.\n        max_stride: Maximum stride in a model that the images must be divisible by.\n            If &gt; 1, this will pad the bottom and right of the images to ensure they meet\n            this divisibility criteria. Padding is applied after the scaling specified\n            in the `scale` attribute.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = None,\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        input_scale: float = 1.0,\n        max_stride: int = 1,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.input_scale = input_scale\n        self.max_stride = max_stride\n\n    def forward(\n        self,\n        inputs: Dict[str, torch.Tensor],\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        This layer can be chained with a `CentroidCrop` layer to create a top-down\n        inference function from full images.\n\n        Args:\n            inputs: Dictionary with keys:\n                `\"instance_image\"`: Cropped images.\n                Other keys will be passed down the pipeline.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n                `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n            If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n            generated the crops will also be included in the keys `\"centroid\"` and\n            `\"centroid_val\"`.\n\n        \"\"\"\n        # Network forward pass.\n        # resize and pad the input image\n        input_image = inputs[\"instance_image\"]\n        if self.max_stride != 1:\n            input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n        cms = self.torch_model(input_image)\n\n        peak_points, peak_vals = find_global_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n\n        # Adjust for stride and scale.\n        peak_points = peak_points * self.output_stride\n        if self.input_scale != 1.0:\n            peak_points = peak_points / self.input_scale\n\n        peak_points = peak_points / (\n            inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n        )\n\n        inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / self.input_scale\n\n        inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n            inputs[\"eff_scale\"]\n            .unsqueeze(dim=1)\n            .unsqueeze(dim=2)\n            .unsqueeze(dim=3)\n            .to(inputs[\"instance_bbox\"].device)\n        )\n\n        # Build outputs.\n        outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n        if self.return_confmaps:\n            outputs[\"pred_confmaps\"] = cms.detach()\n        inputs.update(outputs)\n        return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaks.__init__","title":"<code>__init__(torch_model, output_stride=None, peak_threshold=0.0, refinement=None, integral_patch_size=5, return_confmaps=False, input_scale=1.0, max_stride=1, **kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    input_scale: float = 1.0,\n    max_stride: int = 1,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.input_scale = input_scale\n    self.max_stride = max_stride\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaks.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>This layer can be chained with a <code>CentroidCrop</code> layer to create a top-down inference function from full images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with keys: <code>\"instance_image\"</code>: Cropped images. Other keys will be passed down the pipeline.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch as a     <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> <p>If provided (e.g., from an input <code>CentroidCrop</code> layer), the centroids that generated the crops will also be included in the keys <code>\"centroid\"</code> and <code>\"centroid_val\"</code>.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(\n    self,\n    inputs: Dict[str, torch.Tensor],\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    This layer can be chained with a `CentroidCrop` layer to create a top-down\n    inference function from full images.\n\n    Args:\n        inputs: Dictionary with keys:\n            `\"instance_image\"`: Cropped images.\n            Other keys will be passed down the pipeline.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n            `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n        If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n        generated the crops will also be included in the keys `\"centroid\"` and\n        `\"centroid_val\"`.\n\n    \"\"\"\n    # Network forward pass.\n    # resize and pad the input image\n    input_image = inputs[\"instance_image\"]\n    if self.max_stride != 1:\n        input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n    cms = self.torch_model(input_image)\n\n    peak_points, peak_vals = find_global_peaks(\n        cms.detach(),\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n\n    # Adjust for stride and scale.\n    peak_points = peak_points * self.output_stride\n    if self.input_scale != 1.0:\n        peak_points = peak_points / self.input_scale\n\n    peak_points = peak_points / (\n        inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n    )\n\n    inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / self.input_scale\n\n    inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n        inputs[\"eff_scale\"]\n        .unsqueeze(dim=1)\n        .unsqueeze(dim=2)\n        .unsqueeze(dim=3)\n        .to(inputs[\"instance_bbox\"].device)\n    )\n\n    # Build outputs.\n    outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n    if self.return_confmaps:\n        outputs[\"pred_confmaps\"] = cms.detach()\n    inputs.update(outputs)\n    return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaksGroundTruth","title":"<code>FindInstancePeaksGroundTruth</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>LightningModule that simulates a centered instance peaks model.</p> <p>This layer is useful for testing and evaluating centroid models.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Return the ground truth instance peaks given a set of crops.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class FindInstancePeaksGroundTruth(L.LightningModule):\n    \"\"\"LightningModule that simulates a centered instance peaks model.\n\n    This layer is useful for testing and evaluating centroid models.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n\n    def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, np.array]:\n        \"\"\"Return the ground truth instance peaks given a set of crops.\"\"\"\n        b, _, max_inst, nodes, _ = batch[\"instances\"].shape\n        inst = (\n            batch[\"instances\"].unsqueeze(dim=-4).float()\n        )  # (batch, 1, 1, n_inst, nodes, 2)\n        cent = (\n            batch[\"centroids\"].unsqueeze(dim=-2).unsqueeze(dim=-3).float()\n        )  # (batch, 1, n_centroids, 1, 1, 2)\n        dists = torch.sum(\n            (inst - cent) ** 2, dim=-1\n        )  # (batch, 1, n_centroids, n_inst, nodes)\n        dists = torch.sqrt(dists)\n\n        dists = torch.where(torch.isnan(dists), torch.inf, dists)\n        dists = torch.min(dists, dim=-1).values  # (batch, 1, n_centroids, n_inst)\n\n        # find nearest gt instance\n        matches = torch.argmin(dists, dim=-1)  # (batch, 1, n_centroids)\n\n        # filter matches without NaNs (nans have been converted to inf)\n        subs = torch.argwhere(\n            ~torch.all(dists == torch.inf, dim=-1)\n        )  # each element represents an index with (batch, 1, n_centroids)\n        valid_matches = matches[subs[:, 0], 0, subs[:, 2]]\n        matched_batch_inds = subs[:, 0]\n\n        counts = torch.bincount(matched_batch_inds.detach())\n        peaks_list = batch[\"instances\"][matched_batch_inds, 0, valid_matches, :, :]\n        parsed = 0\n        for i in range(b):\n            if i not in matched_batch_inds:\n                batch_peaks = torch.full((max_inst, nodes, 2), torch.nan)\n                vals = torch.full((max_inst, nodes), torch.nan)\n            else:\n                c = counts[i]\n                batch_peaks = peaks_list[parsed : parsed + c]\n                num_inst = len(batch_peaks)\n                vals = torch.ones((num_inst, nodes))\n                if c &lt; max_inst:\n                    batch_peaks = torch.cat(\n                        [\n                            batch_peaks,\n                            torch.full((max_inst - num_inst, nodes, 2), torch.nan),\n                        ]\n                    )\n                    vals = torch.cat(\n                        [vals, torch.full((max_inst - num_inst, nodes), torch.nan)]\n                    )\n                else:\n                    batch_peaks = batch_peaks[:max_inst]\n                    vals = vals[:max_inst]\n                parsed += c\n\n            batch_peaks = batch_peaks.unsqueeze(dim=0)\n\n            if i != 0:\n                peaks = torch.cat([peaks, batch_peaks])\n                peaks_vals = torch.cat([peaks_vals, vals])\n            else:\n                peaks = batch_peaks\n                peaks_vals = vals\n\n        peaks_output = batch\n        if peaks.size(0) != 0:\n            peaks = peaks / (\n                batch[\"eff_scale\"]\n                .unsqueeze(dim=1)\n                .unsqueeze(dim=2)\n                .unsqueeze(dim=3)\n                .to(peaks.device)\n            )\n        peaks_output[\"pred_instance_peaks\"] = peaks\n        peaks_output[\"pred_peak_values\"] = peaks_vals\n\n        batch_size, num_centroids = (\n            batch[\"centroids\"].shape[0],\n            batch[\"centroids\"].shape[2],\n        )\n        output_dict = {}\n        output_dict[\"centroid\"] = batch[\"centroids\"].squeeze(dim=1).reshape(-1, 1, 2)\n        output_dict[\"centroid_val\"] = batch[\"centroid_vals\"].reshape(-1)\n        output_dict[\"pred_instance_peaks\"] = batch[\"pred_instance_peaks\"].reshape(\n            -1, nodes, 2\n        )\n        output_dict[\"pred_peak_values\"] = batch[\"pred_peak_values\"].reshape(-1, nodes)\n        output_dict[\"instance_bbox\"] = torch.zeros(\n            (batch_size * num_centroids, 1, 4, 2)\n        )\n        frame_inds = []\n        video_inds = []\n        orig_szs = []\n        for b_idx in range(b):\n            curr_batch_size = len(batch[\"centroids\"][b_idx][0])\n            frame_inds.extend([batch[\"frame_idx\"][b_idx]] * curr_batch_size)\n            video_inds.extend([batch[\"video_idx\"][b_idx]] * curr_batch_size)\n            orig_szs.append(torch.cat([batch[\"orig_size\"][b_idx]] * curr_batch_size))\n\n        output_dict[\"frame_idx\"] = torch.tensor(frame_inds)\n        output_dict[\"video_idx\"] = torch.tensor(video_inds)\n        output_dict[\"orig_size\"] = torch.concatenate(orig_szs, dim=0)\n\n        return output_dict\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaksGroundTruth.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaksGroundTruth.forward","title":"<code>forward(batch)</code>","text":"<p>Return the ground truth instance peaks given a set of crops.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, np.array]:\n    \"\"\"Return the ground truth instance peaks given a set of crops.\"\"\"\n    b, _, max_inst, nodes, _ = batch[\"instances\"].shape\n    inst = (\n        batch[\"instances\"].unsqueeze(dim=-4).float()\n    )  # (batch, 1, 1, n_inst, nodes, 2)\n    cent = (\n        batch[\"centroids\"].unsqueeze(dim=-2).unsqueeze(dim=-3).float()\n    )  # (batch, 1, n_centroids, 1, 1, 2)\n    dists = torch.sum(\n        (inst - cent) ** 2, dim=-1\n    )  # (batch, 1, n_centroids, n_inst, nodes)\n    dists = torch.sqrt(dists)\n\n    dists = torch.where(torch.isnan(dists), torch.inf, dists)\n    dists = torch.min(dists, dim=-1).values  # (batch, 1, n_centroids, n_inst)\n\n    # find nearest gt instance\n    matches = torch.argmin(dists, dim=-1)  # (batch, 1, n_centroids)\n\n    # filter matches without NaNs (nans have been converted to inf)\n    subs = torch.argwhere(\n        ~torch.all(dists == torch.inf, dim=-1)\n    )  # each element represents an index with (batch, 1, n_centroids)\n    valid_matches = matches[subs[:, 0], 0, subs[:, 2]]\n    matched_batch_inds = subs[:, 0]\n\n    counts = torch.bincount(matched_batch_inds.detach())\n    peaks_list = batch[\"instances\"][matched_batch_inds, 0, valid_matches, :, :]\n    parsed = 0\n    for i in range(b):\n        if i not in matched_batch_inds:\n            batch_peaks = torch.full((max_inst, nodes, 2), torch.nan)\n            vals = torch.full((max_inst, nodes), torch.nan)\n        else:\n            c = counts[i]\n            batch_peaks = peaks_list[parsed : parsed + c]\n            num_inst = len(batch_peaks)\n            vals = torch.ones((num_inst, nodes))\n            if c &lt; max_inst:\n                batch_peaks = torch.cat(\n                    [\n                        batch_peaks,\n                        torch.full((max_inst - num_inst, nodes, 2), torch.nan),\n                    ]\n                )\n                vals = torch.cat(\n                    [vals, torch.full((max_inst - num_inst, nodes), torch.nan)]\n                )\n            else:\n                batch_peaks = batch_peaks[:max_inst]\n                vals = vals[:max_inst]\n            parsed += c\n\n        batch_peaks = batch_peaks.unsqueeze(dim=0)\n\n        if i != 0:\n            peaks = torch.cat([peaks, batch_peaks])\n            peaks_vals = torch.cat([peaks_vals, vals])\n        else:\n            peaks = batch_peaks\n            peaks_vals = vals\n\n    peaks_output = batch\n    if peaks.size(0) != 0:\n        peaks = peaks / (\n            batch[\"eff_scale\"]\n            .unsqueeze(dim=1)\n            .unsqueeze(dim=2)\n            .unsqueeze(dim=3)\n            .to(peaks.device)\n        )\n    peaks_output[\"pred_instance_peaks\"] = peaks\n    peaks_output[\"pred_peak_values\"] = peaks_vals\n\n    batch_size, num_centroids = (\n        batch[\"centroids\"].shape[0],\n        batch[\"centroids\"].shape[2],\n    )\n    output_dict = {}\n    output_dict[\"centroid\"] = batch[\"centroids\"].squeeze(dim=1).reshape(-1, 1, 2)\n    output_dict[\"centroid_val\"] = batch[\"centroid_vals\"].reshape(-1)\n    output_dict[\"pred_instance_peaks\"] = batch[\"pred_instance_peaks\"].reshape(\n        -1, nodes, 2\n    )\n    output_dict[\"pred_peak_values\"] = batch[\"pred_peak_values\"].reshape(-1, nodes)\n    output_dict[\"instance_bbox\"] = torch.zeros(\n        (batch_size * num_centroids, 1, 4, 2)\n    )\n    frame_inds = []\n    video_inds = []\n    orig_szs = []\n    for b_idx in range(b):\n        curr_batch_size = len(batch[\"centroids\"][b_idx][0])\n        frame_inds.extend([batch[\"frame_idx\"][b_idx]] * curr_batch_size)\n        video_inds.extend([batch[\"video_idx\"][b_idx]] * curr_batch_size)\n        orig_szs.append(torch.cat([batch[\"orig_size\"][b_idx]] * curr_batch_size))\n\n    output_dict[\"frame_idx\"] = torch.tensor(frame_inds)\n    output_dict[\"video_idx\"] = torch.tensor(video_inds)\n    output_dict[\"orig_size\"] = torch.concatenate(orig_szs, dim=0)\n\n    return output_dict\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownInferenceModel","title":"<code>TopDownInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Top-down instance prediction model.</p> <p>This model encapsulates the top-down approach where instances are first detected by local peak detection of an anchor point and then cropped. These instance-centered crops are then passed to an instance peak detector which is trained to detect all remaining body parts for the instance that is centered within the crop.</p> <p>Attributes:</p> Name Type Description <code>centroid_crop</code> <p>A centroid cropping layer. This can be either <code>CentroidCrop</code> or <code>None</code>. This layer takes the full image as input and outputs a set of centroids and cropped boxes. If <code>None</code>, the centroids are calculated with the provided anchor index using InstanceCentroid module and the centroid vals are set as 1.</p> <code>instance_peaks</code> <p>A instance peak detection layer. This can be either <code>FindInstancePeaks</code> or <code>FindInstancePeaksGroundTruth</code> or <code>TopDownMultiClassFindInstancePeaks</code>. This layer takes as input the output of the centroid cropper (if CentroidCrop not None else the image is cropped with the InstanceCropper module) and outputs the detected peaks for the instances within each crop.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class with Inference models.</p> <code>forward</code> <p>Predict instances for one batch of images.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class TopDownInferenceModel(L.LightningModule):\n    \"\"\"Top-down instance prediction model.\n\n    This model encapsulates the top-down approach where instances are first detected by\n    local peak detection of an anchor point and then cropped. These instance-centered\n    crops are then passed to an instance peak detector which is trained to detect all\n    remaining body parts for the instance that is centered within the crop.\n\n    Attributes:\n        centroid_crop: A centroid cropping layer. This can be either `CentroidCrop` or\n            `None`. This layer takes the full image as input and outputs a set of centroids\n            and cropped boxes. If `None`, the centroids are calculated with the provided anchor index\n            using InstanceCentroid module and the centroid vals are set as 1.\n        instance_peaks: A instance peak detection layer. This can be either `FindInstancePeaks`\n            or `FindInstancePeaksGroundTruth` or `TopDownMultiClassFindInstancePeaks`. This layer takes as input the output of the centroid cropper\n            (if CentroidCrop not None else the image is cropped with the InstanceCropper module)\n            and outputs the detected peaks for the instances within each crop.\n    \"\"\"\n\n    def __init__(\n        self,\n        centroid_crop: Union[CentroidCrop, None],\n        instance_peaks: Union[\n            FindInstancePeaks,\n            FindInstancePeaksGroundTruth,\n            TopDownMultiClassFindInstancePeaks,\n        ],\n    ):\n        \"\"\"Initialize the class with Inference models.\"\"\"\n        super().__init__()\n        self.centroid_crop = centroid_crop\n        self.instance_peaks = instance_peaks\n\n    def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict instances for one batch of images.\n\n        Args:\n            batch: This is a dictionary containing the image batch in the `image` key.\n                   If centroid model is not provided, the dictionary should have\n                   `instance_image` key.\n\n        Returns:\n            The predicted instances as a list of dictionaries of tensors with the\n            entries in example along with the below keys:\n\n            `\"centroids\": (batch_size, 1, 2)`: Instance centroids.\n            `\"centroid_val\": (batch_size, 1)`: Instance centroid confidence\n                values.\n            `\"pred_instance_peaks\": (batch_size, n_nodes, 2)`: Instance skeleton\n                points.\n            `\"pred_peak_vals\": (batch_size, n_nodes)`: Confidence\n                values for the instance skeleton points.\n        \"\"\"\n        if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n            if \"instances\" not in batch:\n                message = (\n                    \"Ground truth data was not detected... \"\n                    \"Please load both models when predicting on non-ground-truth data.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n        self.centroid_crop.eval()\n        peaks_output = []\n        batch = self.centroid_crop(batch)\n\n        if batch is not None:\n\n            if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n                peaks_output.append(self.instance_peaks(batch))\n            else:\n                for i in batch:\n                    self.instance_peaks.eval()\n                    peaks_output.append(\n                        self.instance_peaks(\n                            i,\n                        )\n                    )\n            return peaks_output\n        return batch\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownInferenceModel.__init__","title":"<code>__init__(centroid_crop, instance_peaks)</code>","text":"<p>Initialize the class with Inference models.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    centroid_crop: Union[CentroidCrop, None],\n    instance_peaks: Union[\n        FindInstancePeaks,\n        FindInstancePeaksGroundTruth,\n        TopDownMultiClassFindInstancePeaks,\n    ],\n):\n    \"\"\"Initialize the class with Inference models.\"\"\"\n    super().__init__()\n    self.centroid_crop = centroid_crop\n    self.instance_peaks = instance_peaks\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownInferenceModel.forward","title":"<code>forward(batch)</code>","text":"<p>Predict instances for one batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>This is a dictionary containing the image batch in the <code>image</code> key.    If centroid model is not provided, the dictionary should have    <code>instance_image</code> key.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>The predicted instances as a list of dictionaries of tensors with the entries in example along with the below keys:</p> <p><code>\"centroids\": (batch_size, 1, 2)</code>: Instance centroids. <code>\"centroid_val\": (batch_size, 1)</code>: Instance centroid confidence     values. <code>\"pred_instance_peaks\": (batch_size, n_nodes, 2)</code>: Instance skeleton     points. <code>\"pred_peak_vals\": (batch_size, n_nodes)</code>: Confidence     values for the instance skeleton points.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict instances for one batch of images.\n\n    Args:\n        batch: This is a dictionary containing the image batch in the `image` key.\n               If centroid model is not provided, the dictionary should have\n               `instance_image` key.\n\n    Returns:\n        The predicted instances as a list of dictionaries of tensors with the\n        entries in example along with the below keys:\n\n        `\"centroids\": (batch_size, 1, 2)`: Instance centroids.\n        `\"centroid_val\": (batch_size, 1)`: Instance centroid confidence\n            values.\n        `\"pred_instance_peaks\": (batch_size, n_nodes, 2)`: Instance skeleton\n            points.\n        `\"pred_peak_vals\": (batch_size, n_nodes)`: Confidence\n            values for the instance skeleton points.\n    \"\"\"\n    if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n        if \"instances\" not in batch:\n            message = (\n                \"Ground truth data was not detected... \"\n                \"Please load both models when predicting on non-ground-truth data.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n    self.centroid_crop.eval()\n    peaks_output = []\n    batch = self.centroid_crop(batch)\n\n    if batch is not None:\n\n        if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n            peaks_output.append(self.instance_peaks(batch))\n        else:\n            for i in batch:\n                self.instance_peaks.eval()\n                peaks_output.append(\n                    self.instance_peaks(\n                        i,\n                    )\n                )\n        return peaks_output\n    return batch\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownMultiClassFindInstancePeaks","title":"<code>TopDownMultiClassFindInstancePeaks</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning Module that predicts instance peaks from images using a trained model.</p> <p>This layer encapsulates all of the inference operations required for generating predictions from a centered instance confidence map model. This includes model forward pass, peak finding and coordinate adjustment.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>return_class_vectors</code> <p>If <code>True</code>, the classification probabilities will be returned together with the predicted peaks. This will not line up with the grouped instances, for which the associtated class probabilities will always be returned in <code>\"instance_scores\"</code>.</p> <code>input_scale</code> <p>Float indicating the scale with which the images were scaled before cropping.</p> <code>max_stride</code> <p>Maximum stride in a model that the images must be divisible by. If &gt; 1, this will pad the bottom and right of the images to ensure they meet this divisibility criteria. Padding is applied after the scaling specified in the <code>scale</code> attribute.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class TopDownMultiClassFindInstancePeaks(L.LightningModule):\n    \"\"\"Lightning Module that predicts instance peaks from images using a trained model.\n\n    This layer encapsulates all of the inference operations required for generating\n    predictions from a centered instance confidence map model. This includes\n    model forward pass, peak finding and coordinate adjustment.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        return_class_vectors: If `True`, the classification probabilities will be\n            returned together with the predicted peaks. This will not line up with the\n            grouped instances, for which the associtated class probabilities will always\n            be returned in `\"instance_scores\"`.\n        input_scale: Float indicating the scale with which the images were scaled before\n            cropping.\n        max_stride: Maximum stride in a model that the images must be divisible by.\n            If &gt; 1, this will pad the bottom and right of the images to ensure they meet\n            this divisibility criteria. Padding is applied after the scaling specified\n            in the `scale` attribute.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = \"integral\",\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        return_class_vectors: bool = False,\n        input_scale: float = 1.0,\n        max_stride: int = 1,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.return_class_vectors = return_class_vectors\n        self.input_scale = input_scale\n        self.max_stride = max_stride\n\n    def forward(\n        self,\n        inputs: Dict[str, torch.Tensor],\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        This layer can be chained with a `CentroidCrop` layer to create a top-down\n        inference function from full images.\n\n        Args:\n            inputs: Dictionary with keys:\n                `\"instance_image\"`: Cropped images.\n                Other keys will be passed down the pipeline.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n                `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n            If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n            generated the crops will also be included in the keys `\"centroid\"` and\n            `\"centroid_val\"`.\n\n        \"\"\"\n        # Network forward pass.\n        # resize and pad the input image\n        input_image = inputs[\"instance_image\"]\n        if self.max_stride != 1:\n            input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n        out = self.torch_model(input_image)\n        cms = out[\"CenteredInstanceConfmapsHead\"].detach()\n        peak_class_probs = out[\"ClassVectorsHead\"].detach()\n\n        peak_points, peak_vals = find_global_peaks(\n            cms,\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n\n        # Adjust for stride and scale.\n        peak_points = peak_points * self.output_stride\n        if self.input_scale != 1.0:\n            peak_points = peak_points / self.input_scale\n\n        peak_points = peak_points / (\n            inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n        )\n\n        inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / self.input_scale\n\n        inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n            inputs[\"eff_scale\"]\n            .unsqueeze(dim=1)\n            .unsqueeze(dim=2)\n            .unsqueeze(dim=3)\n            .to(inputs[\"instance_bbox\"].device)\n        )\n\n        (\n            class_inds,\n            class_probs,\n        ) = get_class_inds_from_vectors(peak_class_probs)\n\n        # Build outputs.\n        outputs = {\n            \"pred_instance_peaks\": peak_points,\n            \"pred_peak_values\": peak_vals,\n            \"instance_scores\": class_probs,\n            \"pred_class_inds\": class_inds,\n        }\n\n        if self.return_confmaps:\n            outputs[\"pred_confmaps\"] = cms\n        if self.return_class_vectors:\n            outputs[\"pred_class_vectors\"] = peak_class_probs\n        inputs.update(outputs)\n        return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownMultiClassFindInstancePeaks.__init__","title":"<code>__init__(torch_model, output_stride=None, peak_threshold=0.0, refinement='integral', integral_patch_size=5, return_confmaps=False, return_class_vectors=False, input_scale=1.0, max_stride=1, **kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    return_class_vectors: bool = False,\n    input_scale: float = 1.0,\n    max_stride: int = 1,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.return_class_vectors = return_class_vectors\n    self.input_scale = input_scale\n    self.max_stride = max_stride\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownMultiClassFindInstancePeaks.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>This layer can be chained with a <code>CentroidCrop</code> layer to create a top-down inference function from full images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with keys: <code>\"instance_image\"</code>: Cropped images. Other keys will be passed down the pipeline.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch as a     <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> <p>If provided (e.g., from an input <code>CentroidCrop</code> layer), the centroids that generated the crops will also be included in the keys <code>\"centroid\"</code> and <code>\"centroid_val\"</code>.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(\n    self,\n    inputs: Dict[str, torch.Tensor],\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    This layer can be chained with a `CentroidCrop` layer to create a top-down\n    inference function from full images.\n\n    Args:\n        inputs: Dictionary with keys:\n            `\"instance_image\"`: Cropped images.\n            Other keys will be passed down the pipeline.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n            `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n        If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n        generated the crops will also be included in the keys `\"centroid\"` and\n        `\"centroid_val\"`.\n\n    \"\"\"\n    # Network forward pass.\n    # resize and pad the input image\n    input_image = inputs[\"instance_image\"]\n    if self.max_stride != 1:\n        input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n    out = self.torch_model(input_image)\n    cms = out[\"CenteredInstanceConfmapsHead\"].detach()\n    peak_class_probs = out[\"ClassVectorsHead\"].detach()\n\n    peak_points, peak_vals = find_global_peaks(\n        cms,\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n\n    # Adjust for stride and scale.\n    peak_points = peak_points * self.output_stride\n    if self.input_scale != 1.0:\n        peak_points = peak_points / self.input_scale\n\n    peak_points = peak_points / (\n        inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n    )\n\n    inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / self.input_scale\n\n    inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n        inputs[\"eff_scale\"]\n        .unsqueeze(dim=1)\n        .unsqueeze(dim=2)\n        .unsqueeze(dim=3)\n        .to(inputs[\"instance_bbox\"].device)\n    )\n\n    (\n        class_inds,\n        class_probs,\n    ) = get_class_inds_from_vectors(peak_class_probs)\n\n    # Build outputs.\n    outputs = {\n        \"pred_instance_peaks\": peak_points,\n        \"pred_peak_values\": peak_vals,\n        \"instance_scores\": class_probs,\n        \"pred_class_inds\": class_inds,\n    }\n\n    if self.return_confmaps:\n        outputs[\"pred_confmaps\"] = cms\n    if self.return_class_vectors:\n        outputs[\"pred_class_vectors\"] = peak_class_probs\n    inputs.update(outputs)\n    return inputs\n</code></pre>"},{"location":"api/inference/utils/","title":"utils","text":""},{"location":"api/inference/utils/#sleap_nn.inference.utils","title":"<code>sleap_nn.inference.utils</code>","text":"<p>Miscellaneous utility functions for Inference modules.</p> <p>Functions:</p> Name Description <code>get_skeleton_from_config</code> <p>Create Sleap-io Skeleton objects from config.</p> <code>interp1d</code> <p>Linear 1-D interpolation.</p>"},{"location":"api/inference/utils/#sleap_nn.inference.utils.get_skeleton_from_config","title":"<code>get_skeleton_from_config(skeleton_config)</code>","text":"<p>Create Sleap-io Skeleton objects from config.</p> <p>Parameters:</p> Name Type Description Default <code>skeleton_config</code> <code>OmegaConf</code> <p>OmegaConf object containing the skeleton config.</p> required <p>Returns:</p> Type Description <p>Returns a list of <code>sio.Skeleton</code> objects created from the skeleton config stored in the <code>training_config.yaml</code>.</p> Source code in <code>sleap_nn/inference/utils.py</code> <pre><code>def get_skeleton_from_config(skeleton_config: OmegaConf):\n    \"\"\"Create Sleap-io Skeleton objects from config.\n\n    Args:\n        skeleton_config: OmegaConf object containing the skeleton config.\n\n    Returns:\n        Returns a list of `sio.Skeleton` objects created from the skeleton config\n        stored in the `training_config.yaml`.\n\n    \"\"\"\n    skeletons = []\n    for skel_cfg in skeleton_config:\n\n        skel = sio.Skeleton(\n            nodes=[n[\"name\"] for n in skel_cfg.nodes], name=skel_cfg.name\n        )\n        skel.add_edges(\n            [(e[\"source\"][\"name\"], e[\"destination\"][\"name\"]) for e in skel_cfg.edges]\n        )\n        if skel_cfg.symmetries:\n            for n1, n2 in skel_cfg.symmetries:\n                skel.add_symmetry(n1[\"name\"], n2[\"name\"])\n\n        skeletons.append(skel)\n\n    return skeletons\n</code></pre>"},{"location":"api/inference/utils/#sleap_nn.inference.utils.interp1d","title":"<code>interp1d(x, y, xnew)</code>","text":"<p>Linear 1-D interpolation.</p> <p>Src: https://github.com/aliutkus/torchinterp1d/blob/master/torchinterp1d/interp1d.py</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>(N, ) or (D, N) Tensor.</p> required <code>y</code> <p>(N,) or (D, N) float Tensor. The length of <code>y</code> along its last dimension must be the same as that of <code>x</code></p> required <code>xnew</code> <p>(P,) or (D, P) Tensor. <code>xnew</code> can only be 1-D if both <code>x</code> and <code>y</code> are 1-D. Otherwise, its length along the first dimension must be the same as that of whichever <code>x</code> and <code>y</code> is 2-D.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>(P, ) or (D, P) Tensor.</p> Source code in <code>sleap_nn/inference/utils.py</code> <pre><code>def interp1d(x: torch.Tensor, y: torch.Tensor, xnew: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Linear 1-D interpolation.\n\n    Src: https://github.com/aliutkus/torchinterp1d/blob/master/torchinterp1d/interp1d.py\n\n    Args:\n        x : (N, ) or (D, N) Tensor.\n        y : (N,) or (D, N) float Tensor. The length of `y` along its\n            last dimension must be the same as that of `x`\n        xnew : (P,) or (D, P) Tensor. `xnew` can only be 1-D if\n            _both_ `x` and `y` are 1-D. Otherwise, its length along the first\n            dimension must be the same as that of whichever `x` and `y` is 2-D.\n\n    Returns:\n        (P, ) or (D, P) Tensor.\n    \"\"\"\n    # making the vectors at least 2D\n    is_flat = {}\n    v = {}\n    eps = torch.finfo(y.dtype).eps\n    for name, vec in {\"x\": x, \"y\": y, \"xnew\": xnew}.items():\n        assert len(vec.shape) &lt;= 2, \"interp1d: all inputs must be \" \"at most 2-D.\"\n        if len(vec.shape) == 1:\n            v[name] = vec[None, :]\n        else:\n            v[name] = vec\n        is_flat[name] = v[name].shape[0] == 1\n    device = y.device\n\n    # Checking for the dimensions\n    assert v[\"x\"].shape[1] == v[\"y\"].shape[1] and (\n        v[\"x\"].shape[0] == v[\"y\"].shape[0]\n        or v[\"x\"].shape[0] == 1\n        or v[\"y\"].shape[0] == 1\n    ), (\n        \"x and y must have the same number of columns, and either \"\n        \"the same number of row or one of them having only one \"\n        \"row.\"\n    )\n\n    if (v[\"x\"].shape[0] == 1) and (v[\"y\"].shape[0] == 1) and (v[\"xnew\"].shape[0] &gt; 1):\n        # if there is only one row for both x and y, there is no need to\n        # loop over the rows of xnew because they will all have to face the\n        # same interpolation problem. We should just stack them together to\n        # call interp1d and put them back in place afterwards.\n        v[\"xnew\"] = v[\"xnew\"].contiguous().view(1, -1)\n\n    # identify the dimensions of output\n    D = max(v[\"x\"].shape[0], v[\"xnew\"].shape[0])\n    shape_ynew = (D, v[\"xnew\"].shape[-1])\n    ynew = torch.zeros(*shape_ynew, device=device)\n\n    # moving everything to the desired device in case it was not there\n    # already (not handling the case things do not fit entirely, user will\n    # do it if required.)\n    for name in v:\n        v[name] = v[name].to(device)\n\n    # calling searchsorted on the x values.\n    ind = ynew.long()\n\n    # expanding xnew to match the number of rows of x in case only one xnew is\n    # provided\n    if v[\"xnew\"].shape[0] == 1:\n        v[\"xnew\"] = v[\"xnew\"].expand(v[\"x\"].shape[0], -1)\n\n    # the squeeze is because torch.searchsorted does accept either an n-d tensor with\n    # matching shapes for x and xnew or a 1d vector for x. Here we would\n    # have (1,len) for x sometimes\n    torch.searchsorted(v[\"x\"].contiguous().squeeze(), v[\"xnew\"].contiguous(), out=ind)\n\n    # the `-1` is because searchsorted looks for the index where the values\n    # must be inserted to preserve order. And we want the index of the\n    # preceding value.\n    ind -= 1\n    # we clamp the index, because the number of intervals is x.shape-1,\n    # and the left neighbour should hence be at most number of intervals\n    # -1, i.e. number of columns in x -2\n    ind = torch.clamp(ind, 0, v[\"x\"].shape[1] - 1 - 1)\n\n    # helper function to select stuff according to the found indices.\n    def sel(name):\n        if is_flat[name]:\n            return v[name].contiguous().view(-1)[ind]\n        return torch.gather(v[name], 1, ind)\n\n    # assuming x are sorted in the dimension 1, computing the slopes for\n    # the segments\n    is_flat[\"slopes\"] = is_flat[\"x\"]\n    # now we have found the indices of the neighbors, we start building the\n    # output.\n    v[\"slopes\"] = (v[\"y\"][:, 1:] - v[\"y\"][:, :-1]) / (\n        eps + (v[\"x\"][:, 1:] - v[\"x\"][:, :-1])\n    )\n\n    # now build the linear interpolation\n    ynew = sel(\"y\") + sel(\"slopes\") * (v[\"xnew\"] - sel(\"x\"))\n\n    if len(y.shape) == 1:\n        ynew = ynew.view(-1)\n\n    return ynew\n</code></pre>"},{"location":"api/tracking/","title":"tracking","text":""},{"location":"api/tracking/#sleap_nn.tracking","title":"<code>sleap_nn.tracking</code>","text":"<p>Tracker related modules.</p> <p>Modules:</p> Name Description <code>candidates</code> <p>Candidate generation modules for tracking.</p> <code>track_instance</code> <p>TrackInstance Data structure for Tracker queue.</p> <code>tracker</code> <p>Module for tracking.</p> <code>utils</code> <p>Helper functions for Tracker module.</p>"},{"location":"api/tracking/track_instance/","title":"track_instance","text":""},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance","title":"<code>sleap_nn.tracking.track_instance</code>","text":"<p>TrackInstance Data structure for Tracker queue.</p> <p>Classes:</p> Name Description <code>TrackInstanceLocalQueue</code> <p>Data structure for instances in tracker queue for Local Queue method.</p> <code>TrackInstances</code> <p>Data structure for instances in tracker queue for fixed window method.</p> <code>TrackedInstanceFeature</code> <p>Data structure for tracked instances.</p>"},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance.TrackInstanceLocalQueue","title":"<code>TrackInstanceLocalQueue</code>","text":"<p>Data structure for instances in tracker queue for Local Queue method.</p> Source code in <code>sleap_nn/tracking/track_instance.py</code> <pre><code>@attrs.define\nclass TrackInstanceLocalQueue:\n    \"\"\"Data structure for instances in tracker queue for Local Queue method.\"\"\"\n\n    src_instance: sio.PredictedInstance\n    src_instance_idx: int\n    feature: np.array\n    track_id: Optional[int] = None\n    tracking_score: Optional[float] = None\n    frame_idx: Optional[float] = None\n    image: Optional[np.array] = None\n</code></pre>"},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance.TrackInstances","title":"<code>TrackInstances</code>","text":"<p>Data structure for instances in tracker queue for fixed window method.</p> Source code in <code>sleap_nn/tracking/track_instance.py</code> <pre><code>@attrs.define\nclass TrackInstances:\n    \"\"\"Data structure for instances in tracker queue for fixed window method.\"\"\"\n\n    src_instances: List[sio.PredictedInstance]\n    features: List[np.array]\n    track_ids: Optional[List[int]] = None\n    tracking_scores: Optional[List[float]] = None\n    frame_idx: Optional[float] = None\n    image: Optional[np.array] = None\n</code></pre>"},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance.TrackedInstanceFeature","title":"<code>TrackedInstanceFeature</code>","text":"<p>Data structure for tracked instances.</p> <p>This data structure is used for updating the previous tracked instances and get the features of the tracked instances. <code>shifted_keypoints</code> is used only for the <code>FlowShiftTracker</code> to store the optical flow shifted instances.</p> Source code in <code>sleap_nn/tracking/track_instance.py</code> <pre><code>@attrs.define\nclass TrackedInstanceFeature:\n    \"\"\"Data structure for tracked instances.\n\n    This data structure is used for updating the previous tracked instances and get the\n    features of the tracked instances. `shifted_keypoints` is used only for the `FlowShiftTracker`\n    to store the optical flow shifted instances.\n    \"\"\"\n\n    feature: np.ndarray\n    src_predicted_instance: sio.PredictedInstance\n    frame_idx: int\n    tracking_score: float\n    shifted_keypoints: np.ndarray = None\n</code></pre>"},{"location":"api/tracking/tracker/","title":"tracker","text":""},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker","title":"<code>sleap_nn.tracking.tracker</code>","text":"<p>Module for tracking.</p> <p>Classes:</p> Name Description <code>FlowShiftTracker</code> <p>Module for tracking using optical flow shift matching.</p> <code>Tracker</code> <p>Simple Pose Tracker.</p> <p>Functions:</p> Name Description <code>connect_single_breaks</code> <p>Merge single-frame breaks in tracks by connecting single lost track with single new track.</p> <code>run_tracker</code> <p>Run tracking on a given set of frames.</p>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.FlowShiftTracker","title":"<code>FlowShiftTracker</code>","text":"<p>               Bases: <code>Tracker</code></p> <p>Module for tracking using optical flow shift matching.</p> <p>This module handles tracking instances across frames by creating new track IDs (or) assigning track IDs to each instance when the <code>.track()</code> is called using optical flow based track matching. This is a sub-class of the <code>Tracker</code> module, which configures the <code>update_candidates()</code> method specific to optical flow shift matching. This class is initialized in the <code>Tracker.from_config()</code> method.</p> <p>Attributes:</p> Name Type Description <code>candidates</code> <p>Either <code>FixedWindowCandidates</code> or <code>LocalQueueCandidates</code> object.</p> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>features</code> <code>str</code> <p>One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1     (excluded), use a robust quantile similarity score for the     track. If the value is 1, use the max similarity (non-robust).     For selecting a robust score, 0.95 is a good value.</p> <code>track_matching_method</code> <code>str</code> <p>track matching algorithm. One of <code>hungarian</code>, <code>greedy.     Default:</code>hungarian`.</p> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using optical flow. Default: <code>False</code>.</p> <code>is_local_queue</code> <code>bool</code> <p><code>True</code> if <code>LocalQueueCandidates</code> is used else <code>False</code>.</p> <code>img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0.</p> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21.</p> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling. Default: 3</p> <p>Methods:</p> Name Description <code>get_shifted_instances_from_prv_frames</code> <p>Generate shifted instances onto the new frame by applying optical flow.</p> <code>update_candidates</code> <p>Return dictionary with the features of tracked instances.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>@attrs.define\nclass FlowShiftTracker(Tracker):\n    \"\"\"Module for tracking using optical flow shift matching.\n\n    This module handles tracking instances across frames by creating new track IDs (or)\n    assigning track IDs to each instance when the `.track()` is called using optical flow\n    based track matching. This is a sub-class of the `Tracker` module, which configures\n    the `update_candidates()` method specific to optical flow shift matching. This class is\n    initialized in the `Tracker.from_config()` method.\n\n    Attributes:\n        candidates: Either `FixedWindowCandidates` or `LocalQueueCandidates` object.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: One of [`keypoints`, `centroids`, `bboxes`, `image`].\n            Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n                (excluded), use a robust quantile similarity score for the\n                track. If the value is 1, use the max similarity (non-robust).\n                For selecting a robust score, 0.95 is a good value.\n        track_matching_method: track matching algorithm. One of `hungarian`, `greedy.\n                Default: `hungarian`.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n            optical flow. Default: `False`.\n        is_local_queue: `True` if `LocalQueueCandidates` is used else `False`.\n        img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0.\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21.\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n            Default: 3\n\n    \"\"\"\n\n    img_scale: float = 1.0\n    of_window_size: int = 21\n    of_max_levels: int = 3\n\n    def _compute_optical_flow(\n        self, ref_pts: np.ndarray, ref_img: np.ndarray, new_img: np.ndarray\n    ):\n        \"\"\"Compute instances on new frame using optical flow displacements.\"\"\"\n        ref_img, new_img = self._preprocess_imgs(ref_img, new_img)\n        shifted_pts, status, errs = cv2.calcOpticalFlowPyrLK(\n            ref_img,\n            new_img,\n            (np.concatenate(ref_pts, axis=0)).astype(\"float32\") * self.img_scale,\n            None,\n            winSize=(self.of_window_size, self.of_window_size),\n            maxLevel=self.of_max_levels,\n            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01),\n        )\n        shifted_pts /= self.img_scale\n        return shifted_pts, status, errs\n\n    def _preprocess_imgs(self, ref_img: np.ndarray, new_img: np.ndarray):\n        \"\"\"Pre-process images for optical flow.\"\"\"\n        # Convert to uint8 for cv2.calcOpticalFlowPyrLK\n        if np.issubdtype(ref_img.dtype, np.floating):\n            ref_img = ref_img.astype(\"uint8\")\n        if np.issubdtype(new_img.dtype, np.floating):\n            new_img = new_img.astype(\"uint8\")\n\n        # Ensure images are rank 2 in case there is a singleton channel dimension.\n        if ref_img.ndim &gt; 3:\n            ref_img = np.squeeze(ref_img)\n            new_img = np.squeeze(new_img)\n\n        # Convert RGB to grayscale.\n        if ref_img.ndim &gt; 2 and ref_img.shape[0] == 3:\n            ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)\n            new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n\n        # Input image scaling.\n        if self.img_scale != 1:\n            ref_img = cv2.resize(ref_img, None, None, self.img_scale, self.img_scale)\n            new_img = cv2.resize(new_img, None, None, self.img_scale, self.img_scale)\n\n        return ref_img, new_img\n\n    def get_shifted_instances_from_prv_frames(\n        self,\n        candidates_list: Union[Deque, DefaultDict[int, Deque]],\n        new_img: np.ndarray,\n        feature_method,\n    ) -&gt; Dict[int, List[TrackedInstanceFeature]]:\n        \"\"\"Generate shifted instances onto the new frame by applying optical flow.\"\"\"\n        shifted_instances_prv_frames = defaultdict(list)\n\n        if self.is_local_queue:\n            # for local queue\n            ref_candidates = self.candidate.get_instances_groupby_frame_idx(\n                candidates_list\n            )\n            for fidx, ref_candidate_list in ref_candidates.items():\n                ref_pts = [x.src_instance.numpy() for x in ref_candidate_list]\n                shifted_pts, status, errs = self._compute_optical_flow(\n                    ref_pts=ref_pts,\n                    ref_img=ref_candidate_list[0].image,\n                    new_img=new_img,\n                )\n\n                sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n                shifted_pts = np.split(shifted_pts, sections, axis=0)\n                status = np.split(status, sections, axis=0)\n                errs = np.split(errs, sections, axis=0)\n\n                # Create shifted instances.\n                for idx, (ref_candidate, pts, found) in enumerate(\n                    zip(ref_candidate_list, shifted_pts, status)\n                ):\n                    # Exclude points that weren't found by optical flow.\n                    found = found.squeeze().astype(bool)\n                    pts[~found] = np.nan\n\n                    # Create a shifted instance.\n                    shifted_instances_prv_frames[ref_candidate.track_id].append(\n                        TrackedInstanceFeature(\n                            feature=feature_method(pts),\n                            src_predicted_instance=ref_candidate.src_instance,\n                            frame_idx=fidx,\n                            tracking_score=ref_candidate.tracking_score,\n                            shifted_keypoints=pts,\n                        )\n                    )\n\n        else:\n            # for fixed window\n            candidates_list = (\n                candidates_list\n                if candidates_list is not None\n                else self.candidate.tracker_queue\n            )\n            for ref_candidate in candidates_list:\n                ref_pts = [x.numpy() for x in ref_candidate.src_instances]\n                shifted_pts, status, errs = self._compute_optical_flow(\n                    ref_pts=ref_pts, ref_img=ref_candidate.image, new_img=new_img\n                )\n\n                sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n                shifted_pts = np.split(shifted_pts, sections, axis=0)\n                status = np.split(status, sections, axis=0)\n                errs = np.split(errs, sections, axis=0)\n\n                # Create shifted instances.\n                for idx, (pts, found) in enumerate(zip(shifted_pts, status)):\n                    # Exclude points that weren't found by optical flow.\n                    found = found.squeeze().astype(bool)\n                    pts[~found] = np.nan\n\n                    # Create a shifted instance.\n                    shifted_instances_prv_frames[ref_candidate.track_ids[idx]].append(\n                        TrackedInstanceFeature(\n                            feature=feature_method(pts),\n                            src_predicted_instance=ref_candidate.src_instances[idx],\n                            frame_idx=ref_candidate.frame_idx,\n                            tracking_score=ref_candidate.tracking_scores[idx],\n                            shifted_keypoints=pts,\n                        )\n                    )\n\n        return shifted_instances_prv_frames\n\n    def update_candidates(\n        self,\n        candidates_list: Union[Deque, DefaultDict[int, Deque]],\n        image: np.ndarray,\n    ) -&gt; Dict[int, TrackedInstanceFeature]:\n        \"\"\"Return dictionary with the features of tracked instances.\n\n        In this method, the tracked instances in the tracker queue are shifted on to the\n        current frame using optical flow. The features are then computed from the shifted\n        instances.\n\n        Args:\n            candidates_list: Tracker queue from the candidate class.\n            image: Image of the current untracked frame. (used for flow shift tracker)\n\n        Returns:\n            Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n        \"\"\"\n        # get feature method for the shifted instances\n        if self.features not in self._feature_methods:\n            message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n            logger.error(message)\n            raise ValueError(message)\n        feature_method = self._feature_methods[self.features]\n\n        # get shifted instances from optical flow\n        shifted_instances_prv_frames = self.get_shifted_instances_from_prv_frames(\n            candidates_list=candidates_list,\n            new_img=image,\n            feature_method=feature_method,\n        )\n\n        return shifted_instances_prv_frames\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.FlowShiftTracker.get_shifted_instances_from_prv_frames","title":"<code>get_shifted_instances_from_prv_frames(candidates_list, new_img, feature_method)</code>","text":"<p>Generate shifted instances onto the new frame by applying optical flow.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def get_shifted_instances_from_prv_frames(\n    self,\n    candidates_list: Union[Deque, DefaultDict[int, Deque]],\n    new_img: np.ndarray,\n    feature_method,\n) -&gt; Dict[int, List[TrackedInstanceFeature]]:\n    \"\"\"Generate shifted instances onto the new frame by applying optical flow.\"\"\"\n    shifted_instances_prv_frames = defaultdict(list)\n\n    if self.is_local_queue:\n        # for local queue\n        ref_candidates = self.candidate.get_instances_groupby_frame_idx(\n            candidates_list\n        )\n        for fidx, ref_candidate_list in ref_candidates.items():\n            ref_pts = [x.src_instance.numpy() for x in ref_candidate_list]\n            shifted_pts, status, errs = self._compute_optical_flow(\n                ref_pts=ref_pts,\n                ref_img=ref_candidate_list[0].image,\n                new_img=new_img,\n            )\n\n            sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n            shifted_pts = np.split(shifted_pts, sections, axis=0)\n            status = np.split(status, sections, axis=0)\n            errs = np.split(errs, sections, axis=0)\n\n            # Create shifted instances.\n            for idx, (ref_candidate, pts, found) in enumerate(\n                zip(ref_candidate_list, shifted_pts, status)\n            ):\n                # Exclude points that weren't found by optical flow.\n                found = found.squeeze().astype(bool)\n                pts[~found] = np.nan\n\n                # Create a shifted instance.\n                shifted_instances_prv_frames[ref_candidate.track_id].append(\n                    TrackedInstanceFeature(\n                        feature=feature_method(pts),\n                        src_predicted_instance=ref_candidate.src_instance,\n                        frame_idx=fidx,\n                        tracking_score=ref_candidate.tracking_score,\n                        shifted_keypoints=pts,\n                    )\n                )\n\n    else:\n        # for fixed window\n        candidates_list = (\n            candidates_list\n            if candidates_list is not None\n            else self.candidate.tracker_queue\n        )\n        for ref_candidate in candidates_list:\n            ref_pts = [x.numpy() for x in ref_candidate.src_instances]\n            shifted_pts, status, errs = self._compute_optical_flow(\n                ref_pts=ref_pts, ref_img=ref_candidate.image, new_img=new_img\n            )\n\n            sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n            shifted_pts = np.split(shifted_pts, sections, axis=0)\n            status = np.split(status, sections, axis=0)\n            errs = np.split(errs, sections, axis=0)\n\n            # Create shifted instances.\n            for idx, (pts, found) in enumerate(zip(shifted_pts, status)):\n                # Exclude points that weren't found by optical flow.\n                found = found.squeeze().astype(bool)\n                pts[~found] = np.nan\n\n                # Create a shifted instance.\n                shifted_instances_prv_frames[ref_candidate.track_ids[idx]].append(\n                    TrackedInstanceFeature(\n                        feature=feature_method(pts),\n                        src_predicted_instance=ref_candidate.src_instances[idx],\n                        frame_idx=ref_candidate.frame_idx,\n                        tracking_score=ref_candidate.tracking_scores[idx],\n                        shifted_keypoints=pts,\n                    )\n                )\n\n    return shifted_instances_prv_frames\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.FlowShiftTracker.update_candidates","title":"<code>update_candidates(candidates_list, image)</code>","text":"<p>Return dictionary with the features of tracked instances.</p> <p>In this method, the tracked instances in the tracker queue are shifted on to the current frame using optical flow. The features are then computed from the shifted instances.</p> <p>Parameters:</p> Name Type Description Default <code>candidates_list</code> <code>Union[Deque, DefaultDict[int, Deque]]</code> <p>Tracker queue from the candidate class.</p> required <code>image</code> <code>ndarray</code> <p>Image of the current untracked frame. (used for flow shift tracker)</p> required <p>Returns:</p> Type Description <code>Dict[int, TrackedInstanceFeature]</code> <p>Dictionary with keys as track IDs and values as the list of <code>TrackedInstanceFeature</code>.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def update_candidates(\n    self,\n    candidates_list: Union[Deque, DefaultDict[int, Deque]],\n    image: np.ndarray,\n) -&gt; Dict[int, TrackedInstanceFeature]:\n    \"\"\"Return dictionary with the features of tracked instances.\n\n    In this method, the tracked instances in the tracker queue are shifted on to the\n    current frame using optical flow. The features are then computed from the shifted\n    instances.\n\n    Args:\n        candidates_list: Tracker queue from the candidate class.\n        image: Image of the current untracked frame. (used for flow shift tracker)\n\n    Returns:\n        Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n    \"\"\"\n    # get feature method for the shifted instances\n    if self.features not in self._feature_methods:\n        message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n        logger.error(message)\n        raise ValueError(message)\n    feature_method = self._feature_methods[self.features]\n\n    # get shifted instances from optical flow\n    shifted_instances_prv_frames = self.get_shifted_instances_from_prv_frames(\n        candidates_list=candidates_list,\n        new_img=image,\n        feature_method=feature_method,\n    )\n\n    return shifted_instances_prv_frames\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker","title":"<code>Tracker</code>","text":"<p>Simple Pose Tracker.</p> <p>This is the base class for all Trackers. This module handles tracking instances across frames by creating new track IDs (or) assigning track IDs to each predicted instance when the <code>.track()</code> is called. This class is initialized in the <code>Predictor</code> classes.</p> <p>Attributes:</p> Name Type Description <code>candidate</code> <code>Union[FixedWindowCandidates, LocalQueueCandidates]</code> <p>Instance of either <code>FixedWindowCandidates</code> or <code>LocalQueueCandidates</code>.</p> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using optical flow shifts. Default: <code>False</code>.</p> <code>is_local_queue</code> <code>bool</code> <p><code>True</code> if <code>LocalQueueCandidates</code> is used else <code>False</code>.</p> <p>Methods:</p> Name Description <code>assign_tracks</code> <p>Assign track IDs using Hungarian method.</p> <code>from_config</code> <p>Create <code>Tracker</code> from config.</p> <code>generate_candidates</code> <p>Get the tracked instances from tracker queue.</p> <code>get_features</code> <p>Get features for the current untracked instances.</p> <code>get_scores</code> <p>Compute association score between untracked and tracked instances.</p> <code>scores_to_cost_matrix</code> <p>Converts <code>scores</code> matrix to cost matrix for track assignments.</p> <code>track</code> <p>Assign track IDs to the untracked list of <code>sio.PredictedInstance</code> objects.</p> <code>update_candidates</code> <p>Return dictionary with the features of tracked instances.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>@attrs.define\nclass Tracker:\n    \"\"\"Simple Pose Tracker.\n\n    This is the base class for all Trackers. This module handles tracking instances\n    across frames by creating new track IDs (or) assigning track IDs to each predicted\n    instance when the `.track()` is called. This class is initialized in the `Predictor`\n    classes.\n\n    Attributes:\n        candidate: Instance of either `FixedWindowCandidates` or `LocalQueueCandidates`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n            optical flow shifts. Default: `False`.\n        is_local_queue: `True` if `LocalQueueCandidates` is used else `False`.\n\n    \"\"\"\n\n    candidate: Union[FixedWindowCandidates, LocalQueueCandidates] = (\n        FixedWindowCandidates()\n    )\n    min_match_points: int = 0\n    features: str = \"keypoints\"\n    scoring_method: str = \"oks\"\n    scoring_reduction: str = \"mean\"\n    track_matching_method: str = \"hungarian\"\n    robust_best_instance: float = 1.0\n    use_flow: bool = False\n    is_local_queue: bool = False\n    _scoring_functions: Dict[str, Any] = {\n        \"oks\": compute_oks,\n        \"iou\": compute_iou,\n        \"cosine_sim\": compute_cosine_sim,\n        \"euclidean_dist\": compute_euclidean_distance,\n    }\n    _quantile_method = functools.partial(np.quantile, q=robust_best_instance)\n    _scoring_reduction_methods: Dict[str, Any] = {\n        \"mean\": np.nanmean,\n        \"max\": np.nanmax,\n        \"robust_quantile\": _quantile_method,\n    }\n    _feature_methods: Dict[str, Any] = {\n        \"keypoints\": get_keypoints,\n        \"centroids\": get_centroid,\n        \"bboxes\": get_bbox,\n    }\n    _track_matching_methods: Dict[str, Any] = {\n        \"hungarian\": hungarian_matching,\n        \"greedy\": greedy_matching,\n    }\n    _track_objects: Dict[int, sio.Track] = {}\n\n    @classmethod\n    def from_config(\n        cls,\n        window_size: int = 5,\n        min_new_track_points: int = 0,\n        candidates_method: str = \"fixed_window\",\n        min_match_points: int = 0,\n        features: str = \"keypoints\",\n        scoring_method: str = \"oks\",\n        scoring_reduction: str = \"mean\",\n        robust_best_instance: float = 1.0,\n        track_matching_method: str = \"hungarian\",\n        max_tracks: Optional[int] = None,\n        use_flow: bool = False,\n        of_img_scale: float = 1.0,\n        of_window_size: int = 21,\n        of_max_levels: int = 3,\n    ):\n        \"\"\"Create `Tracker` from config.\n\n        Args:\n            window_size: Number of frames to look for in the candidate instances to match\n                with the current detections. Default: 5.\n            min_new_track_points: We won't spawn a new track for an instance with\n                fewer than this many non-nan points. Default: 0.\n            candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n                method, candidates from the last `window_size` frames. In local queues,\n                last `window_size` instances for each track ID is considered for matching\n                against the current detection. Default: `fixed_window`.\n            min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n            features: Feature representation for the candidates to update current detections.\n                One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n            scoring_method: Method to compute association score between features from the\n                current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n                `euclidean_dist`]. Default: `oks`.\n            scoring_reduction: Method to aggregate and reduce multiple scores if there are\n                several detections associated with the same track. One of [`mean`, `max`,\n                `robust_quantile`]. Default: `mean`.\n            robust_best_instance: If the value is between 0 and 1\n                (excluded), use a robust quantile similarity score for the\n                track. If the value is 1, use the max similarity (non-robust).\n                For selecting a robust score, 0.95 is a good value.\n            track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n                Default: `hungarian`.\n            max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n                (only for local queues candidate) Default: None.\n            use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n            optical flow shifts. Default: `False`.\n            of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n                this to increase performance at the cost of finer accuracy. Sometimes\n                decreasing the image scale can improve performance with fast movements.\n                Default: 1.0. (only if `use_flow` is True)\n            of_window_size: Optical flow window size to consider at each pyramid scale\n                level. Default: 21. (only if `use_flow` is True)\n            of_max_levels: Number of pyramid scale levels to consider. This is different\n                from the scale parameter, which determines the initial image scaling.\n                Default: 3. (only if `use_flow` is True)\n\n        \"\"\"\n        if candidates_method == \"fixed_window\":\n            candidate = FixedWindowCandidates(\n                window_size=window_size,\n                min_new_track_points=min_new_track_points,\n            )\n            is_local_queue = False\n\n        elif candidates_method == \"local_queues\":\n            candidate = LocalQueueCandidates(\n                window_size=window_size,\n                max_tracks=max_tracks,\n                min_new_track_points=min_new_track_points,\n            )\n            is_local_queue = True\n\n        else:\n            message = f\"{candidates_method} is not a valid method. Please choose one of [`fixed_window`, `local_queues`]\"\n            logger.error(message)\n            raise ValueError(message)\n\n        if use_flow:\n            return FlowShiftTracker(\n                candidate=candidate,\n                min_match_points=min_match_points,\n                features=features,\n                scoring_method=scoring_method,\n                scoring_reduction=scoring_reduction,\n                robust_best_instance=robust_best_instance,\n                track_matching_method=track_matching_method,\n                img_scale=of_img_scale,\n                of_window_size=of_window_size,\n                of_max_levels=of_max_levels,\n                is_local_queue=is_local_queue,\n            )\n\n        tracker = cls(\n            candidate=candidate,\n            min_match_points=min_match_points,\n            features=features,\n            scoring_method=scoring_method,\n            scoring_reduction=scoring_reduction,\n            robust_best_instance=robust_best_instance,\n            track_matching_method=track_matching_method,\n            use_flow=use_flow,\n            is_local_queue=is_local_queue,\n        )\n        return tracker\n\n    def track(\n        self,\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.ndarray = None,\n    ) -&gt; List[sio.PredictedInstance]:\n        \"\"\"Assign track IDs to the untracked list of `sio.PredictedInstance` objects.\n\n        Args:\n            untracked_instances: List of untracked `sio.PredictedInstance` objects.\n            frame_idx: Frame index of the predicted instances.\n            image: Source image if visual features are to be used (also when using flow).\n\n        Returns:\n            List of `sio.PredictedInstance` objects, each having an assigned track.\n        \"\"\"\n        # get features for the untracked instances.\n        current_instances = self.get_features(untracked_instances, frame_idx, image)\n\n        candidates_list = (\n            self.generate_candidates()\n        )  # either Deque/ DefaultDict for FixedWindow/ LocalQueue candidate.\n\n        if candidates_list:\n            # if track queue is not empty\n\n            # update candidates if needed and get the features from previous tracked instances.\n            candidates_feature_dict = self.update_candidates(candidates_list, image)\n\n            # scoring function\n            scores = self.get_scores(current_instances, candidates_feature_dict)\n            cost_matrix = self.scores_to_cost_matrix(scores)\n\n            # track assignment\n            current_tracked_instances = self.assign_tracks(\n                current_instances, cost_matrix\n            )\n\n        else:\n            # Initialize the tracker queue if empty.\n            current_tracked_instances = self.candidate.add_new_tracks(current_instances)\n\n        # convert the `current_instances` back to `List[sio.PredictedInstance]` objects.\n        if self.is_local_queue:\n            new_pred_instances = []\n            for instance in current_tracked_instances:\n                if instance.track_id is not None:\n                    if instance.track_id not in self._track_objects:\n                        self._track_objects[instance.track_id] = sio.Track(\n                            f\"track_{instance.track_id}\"\n                        )\n                    instance.src_instance.track = self._track_objects[instance.track_id]\n                    instance.src_instance.tracking_score = instance.tracking_score\n                new_pred_instances.append(instance.src_instance)\n\n        else:\n            new_pred_instances = []\n            for idx, inst in enumerate(current_tracked_instances.src_instances):\n                track_id = current_tracked_instances.track_ids[idx]\n                if track_id is not None:\n                    if track_id not in self._track_objects:\n                        self._track_objects[track_id] = sio.Track(f\"track_{track_id}\")\n                    inst.track = self._track_objects[track_id]\n                    inst.tracking_score = current_tracked_instances.tracking_scores[idx]\n                    new_pred_instances.append(inst)\n\n        return new_pred_instances\n\n    def get_features(\n        self,\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.ndarray = None,\n    ) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n        \"\"\"Get features for the current untracked instances.\n\n        The feature can either be an embedding of cropped image around each instance (visual feature),\n        the bounding box coordinates, or centroids, or the poses as a feature.\n\n        Args:\n            untracked_instances: List of untracked `sio.PredictedInstance` objects.\n            frame_idx: Frame index of the current untracked instances.\n            image: Image of the current frame if visual features are to be used.\n\n        Returns:\n            `TrackInstances` object or `List[TrackInstanceLocalQueue]` with the features\n            assigned for the untracked instances and track_id set as `None`.\n        \"\"\"\n        if self.features not in self._feature_methods:\n            message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n            logger.error(message)\n            raise ValueError(message)\n\n        feature_method = self._feature_methods[self.features]\n        feature_list = []\n        for pred_instance in untracked_instances:\n            feature_list.append(feature_method(pred_instance))\n\n        current_instances = self.candidate.get_track_instances(\n            feature_list, untracked_instances, frame_idx=frame_idx, image=image\n        )\n\n        return current_instances\n\n    def generate_candidates(self):\n        \"\"\"Get the tracked instances from tracker queue.\"\"\"\n        return self.candidate.tracker_queue\n\n    def update_candidates(\n        self, candidates_list: Union[Deque, DefaultDict[int, Deque]], image: np.ndarray\n    ) -&gt; Dict[int, TrackedInstanceFeature]:\n        \"\"\"Return dictionary with the features of tracked instances.\n\n        Args:\n            candidates_list: List of tracked instances from tracker queue to consider.\n            image: Image of the current untracked frame. (used for flow shift tracker)\n\n        Returns:\n            Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n        \"\"\"\n        candidates_feature_dict = defaultdict(list)\n        for track_id in self.candidate.current_tracks:\n            candidates_feature_dict[track_id].extend(\n                self.candidate.get_features_from_track_id(track_id, candidates_list)\n            )\n        return candidates_feature_dict\n\n    def get_scores(\n        self,\n        current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n        candidates_feature_dict: Dict[int, TrackedInstanceFeature],\n    ):\n        \"\"\"Compute association score between untracked and tracked instances.\n\n        For visual feature vectors, this can be `cosine_sim`, for bounding boxes\n        it could be `iou`, for centroids it could be `euclidean_dist`, and for poses it\n        could be `oks`.\n\n        Args:\n            current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n                with features and unassigned tracks.\n            candidates_feature_dict: Dictionary with keys as track IDs and values as the\n                list of `TrackedInstanceFeature`.\n\n        Returns:\n            scores: Score matrix of shape (num_new_instances, num_existing_tracks)\n        \"\"\"\n        if self.scoring_method not in self._scoring_functions:\n            message = \"Invalid `scoring_method` argument. Please provide one of `oks`, `cosine_sim`, `iou`, and `euclidean_dist`.\"\n            logger.error(message)\n            raise ValueError(message)\n\n        if self.scoring_reduction not in self._scoring_reduction_methods:\n            message = \"Invalid `scoring_reduction` argument. Please provide one of `mean`, `max`, and `robust_quantile`.\"\n            logger.error(message)\n            raise ValueError(message)\n\n        scoring_method = self._scoring_functions[self.scoring_method]\n        scoring_reduction = self._scoring_reduction_methods[self.scoring_reduction]\n\n        # Get list of features for the `current_instances`.\n        if self.is_local_queue:\n            current_instances_features = [x.feature for x in current_instances]\n        else:\n            current_instances_features = [x for x in current_instances.features]\n\n        scores = np.zeros(\n            (len(current_instances_features), len(self.candidate.current_tracks))\n        )\n\n        for f_idx, f in enumerate(current_instances_features):\n            for t_idx, track_id in enumerate(self.candidate.current_tracks):\n                scores_trackid = [\n                    scoring_method(f, x.feature)\n                    for x in candidates_feature_dict[track_id]\n                    if (~np.isnan(x.src_predicted_instance.numpy()).any(axis=1)).sum()\n                    &gt; self.min_match_points  # only if the candidates have min non-nan points\n                ]\n                score_trackid = scoring_reduction(scores_trackid)  # scoring reduction\n                scores[f_idx][t_idx] = score_trackid\n\n        return scores\n\n    def scores_to_cost_matrix(self, scores: np.ndarray):\n        \"\"\"Converts `scores` matrix to cost matrix for track assignments.\"\"\"\n        cost_matrix = -scores\n        cost_matrix[np.isnan(cost_matrix)] = np.inf\n        return cost_matrix\n\n    def assign_tracks(\n        self,\n        current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n        cost_matrix: np.ndarray,\n    ) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n        \"\"\"Assign track IDs using Hungarian method.\n\n        Args:\n            current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n                with features and unassigned tracks.\n            cost_matrix: Cost matrix of shape (num_new_instances, num_existing_tracks).\n\n        Returns:\n            `TrackInstances` object or `List[TrackInstanceLocalQueue]`objects with\n                track IDs assigned.\n        \"\"\"\n        if self.track_matching_method not in self._track_matching_methods:\n            message = \"Invalid `track_matching_method` argument. Please provide one of `hungarian`, and `greedy`.\"\n            logger.error(message)\n            raise ValueError(message)\n\n        matching_method = self._track_matching_methods[self.track_matching_method]\n\n        row_inds, col_inds = matching_method(cost_matrix)\n        tracking_scores = [\n            -cost_matrix[row, col] for row, col in zip(row_inds, col_inds)\n        ]\n\n        # update the candidates tracker queue with the newly tracked instances and assign\n        # track IDs to `current_instances`.\n        current_tracked_instances = self.candidate.update_tracks(\n            current_instances, row_inds, col_inds, tracking_scores\n        )\n\n        return current_tracked_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.assign_tracks","title":"<code>assign_tracks(current_instances, cost_matrix)</code>","text":"<p>Assign track IDs using Hungarian method.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code> with features and unassigned tracks.</p> required <code>cost_matrix</code> <code>ndarray</code> <p>Cost matrix of shape (num_new_instances, num_existing_tracks).</p> required <p>Returns:</p> Type Description <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code>objects with     track IDs assigned.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def assign_tracks(\n    self,\n    current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n    cost_matrix: np.ndarray,\n) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n    \"\"\"Assign track IDs using Hungarian method.\n\n    Args:\n        current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n            with features and unassigned tracks.\n        cost_matrix: Cost matrix of shape (num_new_instances, num_existing_tracks).\n\n    Returns:\n        `TrackInstances` object or `List[TrackInstanceLocalQueue]`objects with\n            track IDs assigned.\n    \"\"\"\n    if self.track_matching_method not in self._track_matching_methods:\n        message = \"Invalid `track_matching_method` argument. Please provide one of `hungarian`, and `greedy`.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    matching_method = self._track_matching_methods[self.track_matching_method]\n\n    row_inds, col_inds = matching_method(cost_matrix)\n    tracking_scores = [\n        -cost_matrix[row, col] for row, col in zip(row_inds, col_inds)\n    ]\n\n    # update the candidates tracker queue with the newly tracked instances and assign\n    # track IDs to `current_instances`.\n    current_tracked_instances = self.candidate.update_tracks(\n        current_instances, row_inds, col_inds, tracking_scores\n    )\n\n    return current_tracked_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.from_config","title":"<code>from_config(window_size=5, min_new_track_points=0, candidates_method='fixed_window', min_match_points=0, features='keypoints', scoring_method='oks', scoring_reduction='mean', robust_best_instance=1.0, track_matching_method='hungarian', max_tracks=None, use_flow=False, of_img_scale=1.0, of_window_size=21, of_max_levels=3)</code>  <code>classmethod</code>","text":"<p>Create <code>Tracker</code> from config.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Number of frames to look for in the candidate instances to match with the current detections. Default: 5.</p> <code>5</code> <code>min_new_track_points</code> <code>int</code> <p>We won't spawn a new track for an instance with fewer than this many non-nan points. Default: 0.</p> <code>0</code> <code>candidates_method</code> <code>str</code> <p>Either of <code>fixed_window</code> or <code>local_queues</code>. In fixed window method, candidates from the last <code>window_size</code> frames. In local queues, last <code>window_size</code> instances for each track ID is considered for matching against the current detection. Default: <code>fixed_window</code>.</p> <code>'fixed_window'</code> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>0</code> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>'keypoints'</code> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>'oks'</code> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>'mean'</code> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>1.0</code> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>'hungarian'</code> <code>max_tracks</code> <code>Optional[int]</code> <p>Meaximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate) Default: None.</p> <code>None</code> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using</p> <code>False</code> <code>optical</code> <code>flow shifts. Default</code> <p><code>False</code>.</p> required <code>of_img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0. (only if <code>use_flow</code> is True)</p> <code>1.0</code> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21. (only if <code>use_flow</code> is True)</p> <code>21</code> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling. Default: 3. (only if <code>use_flow</code> is True)</p> <code>3</code> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    window_size: int = 5,\n    min_new_track_points: int = 0,\n    candidates_method: str = \"fixed_window\",\n    min_match_points: int = 0,\n    features: str = \"keypoints\",\n    scoring_method: str = \"oks\",\n    scoring_reduction: str = \"mean\",\n    robust_best_instance: float = 1.0,\n    track_matching_method: str = \"hungarian\",\n    max_tracks: Optional[int] = None,\n    use_flow: bool = False,\n    of_img_scale: float = 1.0,\n    of_window_size: int = 21,\n    of_max_levels: int = 3,\n):\n    \"\"\"Create `Tracker` from config.\n\n    Args:\n        window_size: Number of frames to look for in the candidate instances to match\n            with the current detections. Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many non-nan points. Default: 0.\n        candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n            method, candidates from the last `window_size` frames. In local queues,\n            last `window_size` instances for each track ID is considered for matching\n            against the current detection. Default: `fixed_window`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n            (only for local queues candidate) Default: None.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n        optical flow shifts. Default: `False`.\n        of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0. (only if `use_flow` is True)\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21. (only if `use_flow` is True)\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n            Default: 3. (only if `use_flow` is True)\n\n    \"\"\"\n    if candidates_method == \"fixed_window\":\n        candidate = FixedWindowCandidates(\n            window_size=window_size,\n            min_new_track_points=min_new_track_points,\n        )\n        is_local_queue = False\n\n    elif candidates_method == \"local_queues\":\n        candidate = LocalQueueCandidates(\n            window_size=window_size,\n            max_tracks=max_tracks,\n            min_new_track_points=min_new_track_points,\n        )\n        is_local_queue = True\n\n    else:\n        message = f\"{candidates_method} is not a valid method. Please choose one of [`fixed_window`, `local_queues`]\"\n        logger.error(message)\n        raise ValueError(message)\n\n    if use_flow:\n        return FlowShiftTracker(\n            candidate=candidate,\n            min_match_points=min_match_points,\n            features=features,\n            scoring_method=scoring_method,\n            scoring_reduction=scoring_reduction,\n            robust_best_instance=robust_best_instance,\n            track_matching_method=track_matching_method,\n            img_scale=of_img_scale,\n            of_window_size=of_window_size,\n            of_max_levels=of_max_levels,\n            is_local_queue=is_local_queue,\n        )\n\n    tracker = cls(\n        candidate=candidate,\n        min_match_points=min_match_points,\n        features=features,\n        scoring_method=scoring_method,\n        scoring_reduction=scoring_reduction,\n        robust_best_instance=robust_best_instance,\n        track_matching_method=track_matching_method,\n        use_flow=use_flow,\n        is_local_queue=is_local_queue,\n    )\n    return tracker\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.generate_candidates","title":"<code>generate_candidates()</code>","text":"<p>Get the tracked instances from tracker queue.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def generate_candidates(self):\n    \"\"\"Get the tracked instances from tracker queue.\"\"\"\n    return self.candidate.tracker_queue\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.get_features","title":"<code>get_features(untracked_instances, frame_idx, image=None)</code>","text":"<p>Get features for the current untracked instances.</p> <p>The feature can either be an embedding of cropped image around each instance (visual feature), the bounding box coordinates, or centroids, or the poses as a feature.</p> <p>Parameters:</p> Name Type Description Default <code>untracked_instances</code> <code>List[PredictedInstance]</code> <p>List of untracked <code>sio.PredictedInstance</code> objects.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index of the current untracked instances.</p> required <code>image</code> <code>ndarray</code> <p>Image of the current frame if visual features are to be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code> with the features assigned for the untracked instances and track_id set as <code>None</code>.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def get_features(\n    self,\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.ndarray = None,\n) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n    \"\"\"Get features for the current untracked instances.\n\n    The feature can either be an embedding of cropped image around each instance (visual feature),\n    the bounding box coordinates, or centroids, or the poses as a feature.\n\n    Args:\n        untracked_instances: List of untracked `sio.PredictedInstance` objects.\n        frame_idx: Frame index of the current untracked instances.\n        image: Image of the current frame if visual features are to be used.\n\n    Returns:\n        `TrackInstances` object or `List[TrackInstanceLocalQueue]` with the features\n        assigned for the untracked instances and track_id set as `None`.\n    \"\"\"\n    if self.features not in self._feature_methods:\n        message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n        logger.error(message)\n        raise ValueError(message)\n\n    feature_method = self._feature_methods[self.features]\n    feature_list = []\n    for pred_instance in untracked_instances:\n        feature_list.append(feature_method(pred_instance))\n\n    current_instances = self.candidate.get_track_instances(\n        feature_list, untracked_instances, frame_idx=frame_idx, image=image\n    )\n\n    return current_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.get_scores","title":"<code>get_scores(current_instances, candidates_feature_dict)</code>","text":"<p>Compute association score between untracked and tracked instances.</p> <p>For visual feature vectors, this can be <code>cosine_sim</code>, for bounding boxes it could be <code>iou</code>, for centroids it could be <code>euclidean_dist</code>, and for poses it could be <code>oks</code>.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code> with features and unassigned tracks.</p> required <code>candidates_feature_dict</code> <code>Dict[int, TrackedInstanceFeature]</code> <p>Dictionary with keys as track IDs and values as the list of <code>TrackedInstanceFeature</code>.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <p>Score matrix of shape (num_new_instances, num_existing_tracks)</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def get_scores(\n    self,\n    current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n    candidates_feature_dict: Dict[int, TrackedInstanceFeature],\n):\n    \"\"\"Compute association score between untracked and tracked instances.\n\n    For visual feature vectors, this can be `cosine_sim`, for bounding boxes\n    it could be `iou`, for centroids it could be `euclidean_dist`, and for poses it\n    could be `oks`.\n\n    Args:\n        current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n            with features and unassigned tracks.\n        candidates_feature_dict: Dictionary with keys as track IDs and values as the\n            list of `TrackedInstanceFeature`.\n\n    Returns:\n        scores: Score matrix of shape (num_new_instances, num_existing_tracks)\n    \"\"\"\n    if self.scoring_method not in self._scoring_functions:\n        message = \"Invalid `scoring_method` argument. Please provide one of `oks`, `cosine_sim`, `iou`, and `euclidean_dist`.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    if self.scoring_reduction not in self._scoring_reduction_methods:\n        message = \"Invalid `scoring_reduction` argument. Please provide one of `mean`, `max`, and `robust_quantile`.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    scoring_method = self._scoring_functions[self.scoring_method]\n    scoring_reduction = self._scoring_reduction_methods[self.scoring_reduction]\n\n    # Get list of features for the `current_instances`.\n    if self.is_local_queue:\n        current_instances_features = [x.feature for x in current_instances]\n    else:\n        current_instances_features = [x for x in current_instances.features]\n\n    scores = np.zeros(\n        (len(current_instances_features), len(self.candidate.current_tracks))\n    )\n\n    for f_idx, f in enumerate(current_instances_features):\n        for t_idx, track_id in enumerate(self.candidate.current_tracks):\n            scores_trackid = [\n                scoring_method(f, x.feature)\n                for x in candidates_feature_dict[track_id]\n                if (~np.isnan(x.src_predicted_instance.numpy()).any(axis=1)).sum()\n                &gt; self.min_match_points  # only if the candidates have min non-nan points\n            ]\n            score_trackid = scoring_reduction(scores_trackid)  # scoring reduction\n            scores[f_idx][t_idx] = score_trackid\n\n    return scores\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.scores_to_cost_matrix","title":"<code>scores_to_cost_matrix(scores)</code>","text":"<p>Converts <code>scores</code> matrix to cost matrix for track assignments.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def scores_to_cost_matrix(self, scores: np.ndarray):\n    \"\"\"Converts `scores` matrix to cost matrix for track assignments.\"\"\"\n    cost_matrix = -scores\n    cost_matrix[np.isnan(cost_matrix)] = np.inf\n    return cost_matrix\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.track","title":"<code>track(untracked_instances, frame_idx, image=None)</code>","text":"<p>Assign track IDs to the untracked list of <code>sio.PredictedInstance</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>untracked_instances</code> <code>List[PredictedInstance]</code> <p>List of untracked <code>sio.PredictedInstance</code> objects.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index of the predicted instances.</p> required <code>image</code> <code>ndarray</code> <p>Source image if visual features are to be used (also when using flow).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[PredictedInstance]</code> <p>List of <code>sio.PredictedInstance</code> objects, each having an assigned track.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def track(\n    self,\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.ndarray = None,\n) -&gt; List[sio.PredictedInstance]:\n    \"\"\"Assign track IDs to the untracked list of `sio.PredictedInstance` objects.\n\n    Args:\n        untracked_instances: List of untracked `sio.PredictedInstance` objects.\n        frame_idx: Frame index of the predicted instances.\n        image: Source image if visual features are to be used (also when using flow).\n\n    Returns:\n        List of `sio.PredictedInstance` objects, each having an assigned track.\n    \"\"\"\n    # get features for the untracked instances.\n    current_instances = self.get_features(untracked_instances, frame_idx, image)\n\n    candidates_list = (\n        self.generate_candidates()\n    )  # either Deque/ DefaultDict for FixedWindow/ LocalQueue candidate.\n\n    if candidates_list:\n        # if track queue is not empty\n\n        # update candidates if needed and get the features from previous tracked instances.\n        candidates_feature_dict = self.update_candidates(candidates_list, image)\n\n        # scoring function\n        scores = self.get_scores(current_instances, candidates_feature_dict)\n        cost_matrix = self.scores_to_cost_matrix(scores)\n\n        # track assignment\n        current_tracked_instances = self.assign_tracks(\n            current_instances, cost_matrix\n        )\n\n    else:\n        # Initialize the tracker queue if empty.\n        current_tracked_instances = self.candidate.add_new_tracks(current_instances)\n\n    # convert the `current_instances` back to `List[sio.PredictedInstance]` objects.\n    if self.is_local_queue:\n        new_pred_instances = []\n        for instance in current_tracked_instances:\n            if instance.track_id is not None:\n                if instance.track_id not in self._track_objects:\n                    self._track_objects[instance.track_id] = sio.Track(\n                        f\"track_{instance.track_id}\"\n                    )\n                instance.src_instance.track = self._track_objects[instance.track_id]\n                instance.src_instance.tracking_score = instance.tracking_score\n            new_pred_instances.append(instance.src_instance)\n\n    else:\n        new_pred_instances = []\n        for idx, inst in enumerate(current_tracked_instances.src_instances):\n            track_id = current_tracked_instances.track_ids[idx]\n            if track_id is not None:\n                if track_id not in self._track_objects:\n                    self._track_objects[track_id] = sio.Track(f\"track_{track_id}\")\n                inst.track = self._track_objects[track_id]\n                inst.tracking_score = current_tracked_instances.tracking_scores[idx]\n                new_pred_instances.append(inst)\n\n    return new_pred_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.update_candidates","title":"<code>update_candidates(candidates_list, image)</code>","text":"<p>Return dictionary with the features of tracked instances.</p> <p>Parameters:</p> Name Type Description Default <code>candidates_list</code> <code>Union[Deque, DefaultDict[int, Deque]]</code> <p>List of tracked instances from tracker queue to consider.</p> required <code>image</code> <code>ndarray</code> <p>Image of the current untracked frame. (used for flow shift tracker)</p> required <p>Returns:</p> Type Description <code>Dict[int, TrackedInstanceFeature]</code> <p>Dictionary with keys as track IDs and values as the list of <code>TrackedInstanceFeature</code>.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def update_candidates(\n    self, candidates_list: Union[Deque, DefaultDict[int, Deque]], image: np.ndarray\n) -&gt; Dict[int, TrackedInstanceFeature]:\n    \"\"\"Return dictionary with the features of tracked instances.\n\n    Args:\n        candidates_list: List of tracked instances from tracker queue to consider.\n        image: Image of the current untracked frame. (used for flow shift tracker)\n\n    Returns:\n        Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n    \"\"\"\n    candidates_feature_dict = defaultdict(list)\n    for track_id in self.candidate.current_tracks:\n        candidates_feature_dict[track_id].extend(\n            self.candidate.get_features_from_track_id(track_id, candidates_list)\n        )\n    return candidates_feature_dict\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.connect_single_breaks","title":"<code>connect_single_breaks(lfs, max_instances)</code>","text":"<p>Merge single-frame breaks in tracks by connecting single lost track with single new track.</p> <p>Parameters:</p> Name Type Description Default <code>lfs</code> <code>List[LabeledFrame]</code> <p>List of <code>LabeledFrame</code> objects with predicted instances.</p> required <code>max_instances</code> <code>int</code> <p>The maximum number of instances we want per frame.</p> required <p>Returns:</p> Type Description <code>List[LabeledFrame]</code> <p>Updated list of labeled frames with modified track IDs.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def connect_single_breaks(\n    lfs: List[sio.LabeledFrame], max_instances: int\n) -&gt; List[sio.LabeledFrame]:\n    \"\"\"Merge single-frame breaks in tracks by connecting single lost track with single new track.\n\n    Args:\n        lfs: List of `LabeledFrame` objects with predicted instances.\n        max_instances: The maximum number of instances we want per frame.\n\n    Returns:\n        Updated list of labeled frames with modified track IDs.\n    \"\"\"\n    if not lfs:\n        return lfs\n\n    # Move instances in new tracks into tracks that disappeared on previous frame\n    fix_track_map = dict()\n    last_good_frame_tracks = {inst.track for inst in lfs[0].instances}\n    for lf in lfs:\n        frame_tracks = {inst.track for inst in lf.instances}\n\n        tracks_fixed_before = frame_tracks.intersection(set(fix_track_map.keys()))\n        if tracks_fixed_before:\n            for inst in lf.instances:\n                if (\n                    inst.track in fix_track_map\n                    and fix_track_map[inst.track] not in frame_tracks\n                ):\n                    inst.track = fix_track_map[inst.track]\n                    frame_tracks = {inst.track for inst in lf.instances}\n\n        extra_tracks = frame_tracks - last_good_frame_tracks\n        missing_tracks = last_good_frame_tracks - frame_tracks\n\n        if len(extra_tracks) == 1 and len(missing_tracks) == 1:\n            for inst in lf.instances:\n                if inst.track in extra_tracks:\n                    old_track = inst.track\n                    new_track = missing_tracks.pop()\n                    fix_track_map[old_track] = new_track\n                    inst.track = new_track\n\n                    break\n        else:\n            if len(frame_tracks) == max_instances:\n                last_good_frame_tracks = frame_tracks\n\n    return lfs\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.run_tracker","title":"<code>run_tracker(untracked_frames, window_size=5, min_new_track_points=0, candidates_method='fixed_window', min_match_points=0, features='keypoints', scoring_method='oks', scoring_reduction='mean', robust_best_instance=1.0, track_matching_method='hungarian', max_tracks=None, use_flow=False, of_img_scale=1.0, of_window_size=21, of_max_levels=3, post_connect_single_breaks=False)</code>","text":"<p>Run tracking on a given set of frames.</p> <p>Parameters:</p> Name Type Description Default <code>untracked_frames</code> <code>List[LabeledFrame]</code> <p>List of labeled frames with predicted instances to be tracked.</p> required <code>window_size</code> <code>int</code> <p>Number of frames to look for in the candidate instances to match     with the current detections. Default: 5.</p> <code>5</code> <code>min_new_track_points</code> <code>int</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>0</code> <code>candidates_method</code> <code>str</code> <p>Either of <code>fixed_window</code> or <code>local_queues</code>. In fixed window method, candidates from the last <code>window_size</code> frames. In local queues, last <code>window_size</code> instances for each track ID is considered for matching against the current detection. Default: <code>fixed_window</code>.</p> <code>'fixed_window'</code> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>0</code> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>'keypoints'</code> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>'oks'</code> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>'mean'</code> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>1.0</code> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>'hungarian'</code> <code>max_tracks</code> <code>Optional[int]</code> <p>Meaximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate) Default: None.</p> <code>None</code> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using</p> <code>False</code> <code>optical</code> <code>flow shifts. Default</code> <p><code>False</code>.</p> required <code>of_img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0. (only if <code>use_flow</code> is True)</p> <code>1.0</code> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21. (only if <code>use_flow</code> is True)</p> <code>21</code> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling.     Default: 3. (only if <code>use_flow</code> is True).</p> <code>3</code> <code>post_connect_single_breaks</code> <code>bool</code> <p>If True and <code>max_tracks</code> is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[LabeledFrame]</code> <p><code>sio.Labels</code> object with tracked instances.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def run_tracker(\n    untracked_frames: List[sio.LabeledFrame],\n    window_size: int = 5,\n    min_new_track_points: int = 0,\n    candidates_method: str = \"fixed_window\",\n    min_match_points: int = 0,\n    features: str = \"keypoints\",\n    scoring_method: str = \"oks\",\n    scoring_reduction: str = \"mean\",\n    robust_best_instance: float = 1.0,\n    track_matching_method: str = \"hungarian\",\n    max_tracks: Optional[int] = None,\n    use_flow: bool = False,\n    of_img_scale: float = 1.0,\n    of_window_size: int = 21,\n    of_max_levels: int = 3,\n    post_connect_single_breaks: bool = False,\n) -&gt; List[sio.LabeledFrame]:\n    \"\"\"Run tracking on a given set of frames.\n\n    Args:\n        untracked_frames: List of labeled frames with predicted instances to be tracked.\n        window_size: Number of frames to look for in the candidate instances to match\n                with the current detections. Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n            method, candidates from the last `window_size` frames. In local queues,\n            last `window_size` instances for each track ID is considered for matching\n            against the current detection. Default: `fixed_window`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n            (only for local queues candidate) Default: None.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n        optical flow shifts. Default: `False`.\n        of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0. (only if `use_flow` is True)\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21. (only if `use_flow` is True)\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n                Default: 3. (only if `use_flow` is True).\n        post_connect_single_breaks: If True and `max_tracks` is not None with local queues candidate method,\n            connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.\n\n    Returns:\n        `sio.Labels` object with tracked instances.\n\n    \"\"\"\n    tracker = Tracker.from_config(\n        window_size=window_size,\n        min_new_track_points=min_new_track_points,\n        candidates_method=candidates_method,\n        min_match_points=min_match_points,\n        features=features,\n        scoring_method=scoring_method,\n        scoring_reduction=scoring_reduction,\n        robust_best_instance=robust_best_instance,\n        track_matching_method=track_matching_method,\n        max_tracks=max_tracks,\n        use_flow=use_flow,\n        of_img_scale=of_img_scale,\n        of_window_size=of_window_size,\n        of_max_levels=of_max_levels,\n    )\n    tracked_lfs = []\n    for lf in untracked_frames:\n        # prefer user instances over predicted instance\n        instances = []\n        if lf.has_user_instances:\n            instances_to_track = lf.user_instances\n            if lf.has_predicted_instances:\n                instances = lf.predicted_instances\n        else:\n            instances_to_track = lf.predicted_instances\n\n        instances.extend(\n            tracker.track(\n                untracked_instances=instances_to_track,\n                frame_idx=lf.frame_idx,\n                image=lf.image,\n            )\n        )\n        tracked_lfs.append(\n            sio.LabeledFrame(\n                video=lf.video, frame_idx=lf.frame_idx, instances=instances\n            )\n        )\n\n    if post_connect_single_breaks:\n        if max_tracks is None:\n            message = \"Max_tracks is None. To connect single breaks, max_tracks should be set to an integer.\"\n            logger.error(message)\n            raise ValueError(message)\n        start_final_pass_time = time()\n        start_fp_timestamp = str(datetime.now())\n        logger.info(\n            f\"Started final-pass (connecting single breaks) at: {start_fp_timestamp}\"\n        )\n        tracked_lfs = connect_single_breaks(tracked_lfs, max_instances=max_tracks)\n        finish_fp_timestamp = str(datetime.now())\n        total_fp_elapsed = time() - start_final_pass_time\n        logger.info(\n            f\"Finished final-pass (connecting single breaks) at: {finish_fp_timestamp}\"\n        )\n        logger.info(f\"Total runtime: {total_fp_elapsed} secs\")\n\n    return tracked_lfs\n</code></pre>"},{"location":"api/tracking/utils/","title":"utils","text":""},{"location":"api/tracking/utils/#sleap_nn.tracking.utils","title":"<code>sleap_nn.tracking.utils</code>","text":"<p>Helper functions for Tracker module.</p> <p>Functions:</p> Name Description <code>compute_cosine_sim</code> <p>Return cosine simalirity between a and b vectors.</p> <code>compute_euclidean_distance</code> <p>Return the negative euclidean distance between a and b points.</p> <code>compute_iou</code> <p>Return the intersection over union for given a and b bounding boxes [xmin, ymin, xmax, ymax].</p> <code>get_bbox</code> <p>Return the bounding box coordinates for the <code>PredictedInstance</code> object.</p> <code>get_centroid</code> <p>Return the centroid of the <code>PredictedInstance</code> object.</p> <code>get_keypoints</code> <p>Return keypoints as np.array from the <code>PredictedInstance</code> object.</p> <code>greedy_matching</code> <p>Match new instances to existing tracks using greedy bipartite matching.</p> <code>hungarian_matching</code> <p>Match new instances to existing tracks using Hungarian matching.</p>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.compute_cosine_sim","title":"<code>compute_cosine_sim(a, b)</code>","text":"<p>Return cosine simalirity between a and b vectors.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def compute_cosine_sim(a, b):\n    \"\"\"Return cosine simalirity between a and b vectors.\"\"\"\n    number = np.dot(a, b)\n    denom = np.linalg.norm(a) * np.linalg.norm(b)\n    cosine_sim = number / denom\n    return cosine_sim\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.compute_euclidean_distance","title":"<code>compute_euclidean_distance(a, b)</code>","text":"<p>Return the negative euclidean distance between a and b points.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def compute_euclidean_distance(a, b):\n    \"\"\"Return the negative euclidean distance between a and b points.\"\"\"\n    return -np.linalg.norm(a - b)\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.compute_iou","title":"<code>compute_iou(a, b)</code>","text":"<p>Return the intersection over union for given a and b bounding boxes [xmin, ymin, xmax, ymax].</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def compute_iou(a, b):\n    \"\"\"Return the intersection over union for given a and b bounding boxes [xmin, ymin, xmax, ymax].\"\"\"\n    (xmin1, ymin1, xmax1, ymax1), (xmin2, ymin2, xmax2, ymax2) = a, b\n\n    xmin_intersection = max(xmin1, xmin2)\n    ymin_intersection = max(ymin1, ymin2)\n    xmax_intersection = min(xmax1, xmax2)\n    ymax_intersection = min(ymax1, ymax2)\n\n    intersection_area = max(0, xmax_intersection - xmin_intersection + 1) * max(\n        0, ymax_intersection - ymin_intersection + 1\n    )\n    bbox1_area = (xmax1 - xmin1 + 1) * (ymax1 - ymin1 + 1)\n    bbox2_area = (xmax2 - xmin2 + 1) * (ymax2 - ymin2 + 1)\n    union_area = bbox1_area + bbox2_area - intersection_area\n\n    iou = intersection_area / union_area\n    return iou\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.get_bbox","title":"<code>get_bbox(pred_instance)</code>","text":"<p>Return the bounding box coordinates for the <code>PredictedInstance</code> object.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def get_bbox(pred_instance: Union[sio.PredictedInstance, np.ndarray]):\n    \"\"\"Return the bounding box coordinates for the `PredictedInstance` object.\"\"\"\n    points = (\n        pred_instance.numpy()\n        if not isinstance(pred_instance, np.ndarray)\n        else pred_instance\n    )\n    bbox = np.concatenate(\n        [\n            np.nanmin(points, axis=0),\n            np.nanmax(points, axis=0),\n        ]  # [xmin, ymin, xmax, ymax]\n    )\n    return bbox\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.get_centroid","title":"<code>get_centroid(pred_instance)</code>","text":"<p>Return the centroid of the <code>PredictedInstance</code> object.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def get_centroid(pred_instance: Union[sio.PredictedInstance, np.ndarray]):\n    \"\"\"Return the centroid of the `PredictedInstance` object.\"\"\"\n    pts = pred_instance\n    if not isinstance(pred_instance, np.ndarray):\n        pts = pred_instance.numpy()\n    centroid = np.nanmedian(pts, axis=0)\n    return centroid\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.get_keypoints","title":"<code>get_keypoints(pred_instance)</code>","text":"<p>Return keypoints as np.array from the <code>PredictedInstance</code> object.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def get_keypoints(pred_instance: Union[sio.PredictedInstance, np.ndarray]):\n    \"\"\"Return keypoints as np.array from the `PredictedInstance` object.\"\"\"\n    if isinstance(pred_instance, np.ndarray):\n        return pred_instance\n    return pred_instance.numpy()\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.greedy_matching","title":"<code>greedy_matching(cost_matrix)</code>","text":"<p>Match new instances to existing tracks using greedy bipartite matching.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def greedy_matching(cost_matrix: np.ndarray) -&gt; List[Tuple[int, int]]:\n    \"\"\"Match new instances to existing tracks using greedy bipartite matching.\"\"\"\n    # Sort edges by ascending cost.\n    rows, cols = np.unravel_index(np.argsort(cost_matrix, axis=None), cost_matrix.shape)\n    unassigned_edges = list(zip(rows, cols))\n\n    # Greedily assign edges.\n    row_inds, col_inds = [], []\n    while len(unassigned_edges) &gt; 0:\n        # Assign the lowest cost edge.\n        row_ind, col_ind = unassigned_edges.pop(0)\n        row_inds.append(row_ind)\n        col_inds.append(col_ind)\n\n        # Remove all other edges that contain either node (in reverse order).\n        for i in range(len(unassigned_edges) - 1, -1, -1):\n            if unassigned_edges[i][0] == row_ind or unassigned_edges[i][1] == col_ind:\n                del unassigned_edges[i]\n\n    return row_inds, col_inds\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.hungarian_matching","title":"<code>hungarian_matching(cost_matrix)</code>","text":"<p>Match new instances to existing tracks using Hungarian matching.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def hungarian_matching(cost_matrix: np.ndarray) -&gt; List[Tuple[int, int]]:\n    \"\"\"Match new instances to existing tracks using Hungarian matching.\"\"\"\n    row_ids, col_ids = linear_sum_assignment(cost_matrix)\n    return row_ids, col_ids\n</code></pre>"},{"location":"api/tracking/candidates/","title":"candidates","text":""},{"location":"api/tracking/candidates/#sleap_nn.tracking.candidates","title":"<code>sleap_nn.tracking.candidates</code>","text":"<p>Candidate generation modules for tracking.</p> <p>Modules:</p> Name Description <code>fixed_window</code> <p>Module to generate Fixed window candidates.</p> <code>local_queues</code> <p>Module to generate Tracking local queue candidates.</p>"},{"location":"api/tracking/candidates/fixed_window/","title":"fixed_window","text":""},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window","title":"<code>sleap_nn.tracking.candidates.fixed_window</code>","text":"<p>Module to generate Fixed window candidates.</p> <p>Classes:</p> Name Description <code>FixedWindowCandidates</code> <p>Fixed-window method for candidate generation.</p>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates","title":"<code>FixedWindowCandidates</code>","text":"<p>Fixed-window method for candidate generation.</p> <p>This module handles <code>tracker_queue</code> using the fixed window method, where track assignments are determined based on the last <code>window_size</code> frames.</p> <p>Attributes:</p> Name Type Description <code>window_size</code> <p>Number of previous frames to compare the current predicted instance with. Default: 5.</p> <code>min_new_track_points</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>tracker_queue</code> <p>Deque object that stores the past <code>window_size</code> tracked instances.</p> <code>all_tracks</code> <p>List of track IDs that are created.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize class variables.</p> <code>add_new_tracks</code> <p>Add new track IDs to the <code>TrackInstances</code> object and to the tracker queue.</p> <code>get_features_from_track_id</code> <p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> <code>get_new_track_id</code> <p>Return a new track_id.</p> <code>get_track_instances</code> <p>Return an instance of <code>TrackInstances</code> object for the <code>untracked_instances</code>.</p> <code>update_tracks</code> <p>Assign tracks to <code>TrackInstances</code> based on the output of track matching algorithm.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>class FixedWindowCandidates:\n    \"\"\"Fixed-window method for candidate generation.\n\n    This module handles `tracker_queue` using the fixed window method, where track assignments\n    are determined based on the last `window_size` frames.\n\n    Attributes:\n        window_size: Number of previous frames to compare the current predicted instance with.\n            Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        tracker_queue: Deque object that stores the past `window_size` tracked instances.\n        all_tracks: List of track IDs that are created.\n    \"\"\"\n\n    def __init__(self, window_size: int = 5, min_new_track_points: int = 0):\n        \"\"\"Initialize class variables.\"\"\"\n        self.window_size = window_size\n        self.min_new_track_points = min_new_track_points\n        self.tracker_queue = deque(maxlen=self.window_size)\n        self.all_tracks = []\n\n    @property\n    def current_tracks(self):\n        \"\"\"Get track IDs of items currently in tracker queue.\"\"\"\n        if not len(self.tracker_queue):\n            return []\n        else:\n            curr_tracks = set()\n            for item in self.tracker_queue:\n                curr_tracks.update(item.track_ids)\n            return list(curr_tracks)\n\n    def get_track_instances(\n        self,\n        feature_list: List[Union[np.array]],\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.array,\n    ) -&gt; TrackInstances:\n        \"\"\"Return an instance of `TrackInstances` object for the `untracked_instances`.\"\"\"\n        track_instance = TrackInstances(\n            src_instances=untracked_instances,\n            track_ids=[None] * len(untracked_instances),\n            tracking_scores=[None] * len(untracked_instances),\n            features=feature_list,\n            frame_idx=frame_idx,\n            image=image,\n        )\n        return track_instance\n\n    def get_features_from_track_id(\n        self, track_id: int, candidates_list: Optional[Deque] = None\n    ) -&gt; List[TrackedInstanceFeature]:\n        \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n        Note: If `candidates_list` is `None`, then features of all the instances in the\n            tracker queue are returned by default. Else, only the features from the given\n            candidates_list are returned.\n        \"\"\"\n        output = []\n        tracked_candidates = (\n            candidates_list if candidates_list is not None else self.tracker_queue\n        )\n        for t in tracked_candidates:\n            if track_id in t.track_ids:\n                track_idx = t.track_ids.index(track_id)\n                tracked_instance_feature = TrackedInstanceFeature(\n                    feature=t.features[track_idx],\n                    src_predicted_instance=t.src_instances[track_idx],\n                    frame_idx=t.frame_idx,\n                    tracking_score=t.tracking_scores[track_idx],\n                    shifted_keypoints=None,\n                )\n                output.append(tracked_instance_feature)\n        return output\n\n    def get_new_track_id(self) -&gt; int:\n        \"\"\"Return a new track_id.\"\"\"\n        if not self.all_tracks:\n            new_track_id = 0\n        else:\n            new_track_id = max(self.all_tracks) + 1\n        return new_track_id\n\n    def add_new_tracks(\n        self, current_instances: TrackInstances, add_to_queue: bool = True\n    ) -&gt; TrackInstances:\n        \"\"\"Add new track IDs to the `TrackInstances` object and to the tracker queue.\"\"\"\n        is_new_track = False\n        for i, src_instance in enumerate(current_instances.src_instances):\n            # Spawning a new track only if num visbile points is more than the threshold\n            num_visible_keypoints = (~np.isnan(src_instance.numpy()).any(axis=1)).sum()\n            if (\n                num_visible_keypoints &gt; self.min_new_track_points\n                and current_instances.track_ids[i] is None\n            ):\n                is_new_track = True\n                new_tracks_id = self.get_new_track_id()\n                current_instances.track_ids[i] = new_tracks_id\n                current_instances.tracking_scores[i] = 1.0\n                self.all_tracks.append(new_tracks_id)\n\n        if add_to_queue and is_new_track:\n            self.tracker_queue.append(current_instances)\n\n        return current_instances\n\n    def update_tracks(\n        self,\n        current_instances: TrackInstances,\n        row_inds: np.array,\n        col_inds: np.array,\n        tracking_scores: List[float],\n    ) -&gt; TrackInstances:\n        \"\"\"Assign tracks to `TrackInstances` based on the output of track matching algorithm.\n\n        Args:\n            current_instances: `TrackInstances` instance with features and unassigned tracks.\n            row_inds: List of indices for the  `current_instances` object that has an assigned\n                track.\n            col_inds: List of track IDs that have been assigned a new instance.\n            tracking_scores: List of tracking scores from the cost matrix.\n\n        \"\"\"\n        add_to_queue = True\n        if row_inds is not None and col_inds is not None:\n\n            for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n                current_instances.track_ids[row] = self.current_tracks[col]\n                current_instances.tracking_scores[row] = tracking_scores[idx]\n\n            # update tracks to queue\n            self.tracker_queue.append(current_instances)\n            add_to_queue = False\n\n            # Create new tracks for instances with unassigned tracks from track matching\n            new_current_instances_inds = [\n                x for x in range(len(current_instances.features)) if x not in row_inds\n            ]\n            if new_current_instances_inds:\n                current_instances = self.add_new_tracks(\n                    current_instances, add_to_queue=add_to_queue\n                )\n        return current_instances\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.current_tracks","title":"<code>current_tracks</code>  <code>property</code>","text":"<p>Get track IDs of items currently in tracker queue.</p>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.__init__","title":"<code>__init__(window_size=5, min_new_track_points=0)</code>","text":"<p>Initialize class variables.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def __init__(self, window_size: int = 5, min_new_track_points: int = 0):\n    \"\"\"Initialize class variables.\"\"\"\n    self.window_size = window_size\n    self.min_new_track_points = min_new_track_points\n    self.tracker_queue = deque(maxlen=self.window_size)\n    self.all_tracks = []\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.add_new_tracks","title":"<code>add_new_tracks(current_instances, add_to_queue=True)</code>","text":"<p>Add new track IDs to the <code>TrackInstances</code> object and to the tracker queue.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def add_new_tracks(\n    self, current_instances: TrackInstances, add_to_queue: bool = True\n) -&gt; TrackInstances:\n    \"\"\"Add new track IDs to the `TrackInstances` object and to the tracker queue.\"\"\"\n    is_new_track = False\n    for i, src_instance in enumerate(current_instances.src_instances):\n        # Spawning a new track only if num visbile points is more than the threshold\n        num_visible_keypoints = (~np.isnan(src_instance.numpy()).any(axis=1)).sum()\n        if (\n            num_visible_keypoints &gt; self.min_new_track_points\n            and current_instances.track_ids[i] is None\n        ):\n            is_new_track = True\n            new_tracks_id = self.get_new_track_id()\n            current_instances.track_ids[i] = new_tracks_id\n            current_instances.tracking_scores[i] = 1.0\n            self.all_tracks.append(new_tracks_id)\n\n    if add_to_queue and is_new_track:\n        self.tracker_queue.append(current_instances)\n\n    return current_instances\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.get_features_from_track_id","title":"<code>get_features_from_track_id(track_id, candidates_list=None)</code>","text":"<p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> If <code>candidates_list</code> is <code>None</code>, then features of all the instances in the <p>tracker queue are returned by default. Else, only the features from the given candidates_list are returned.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def get_features_from_track_id(\n    self, track_id: int, candidates_list: Optional[Deque] = None\n) -&gt; List[TrackedInstanceFeature]:\n    \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n    Note: If `candidates_list` is `None`, then features of all the instances in the\n        tracker queue are returned by default. Else, only the features from the given\n        candidates_list are returned.\n    \"\"\"\n    output = []\n    tracked_candidates = (\n        candidates_list if candidates_list is not None else self.tracker_queue\n    )\n    for t in tracked_candidates:\n        if track_id in t.track_ids:\n            track_idx = t.track_ids.index(track_id)\n            tracked_instance_feature = TrackedInstanceFeature(\n                feature=t.features[track_idx],\n                src_predicted_instance=t.src_instances[track_idx],\n                frame_idx=t.frame_idx,\n                tracking_score=t.tracking_scores[track_idx],\n                shifted_keypoints=None,\n            )\n            output.append(tracked_instance_feature)\n    return output\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.get_new_track_id","title":"<code>get_new_track_id()</code>","text":"<p>Return a new track_id.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def get_new_track_id(self) -&gt; int:\n    \"\"\"Return a new track_id.\"\"\"\n    if not self.all_tracks:\n        new_track_id = 0\n    else:\n        new_track_id = max(self.all_tracks) + 1\n    return new_track_id\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.get_track_instances","title":"<code>get_track_instances(feature_list, untracked_instances, frame_idx, image)</code>","text":"<p>Return an instance of <code>TrackInstances</code> object for the <code>untracked_instances</code>.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def get_track_instances(\n    self,\n    feature_list: List[Union[np.array]],\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.array,\n) -&gt; TrackInstances:\n    \"\"\"Return an instance of `TrackInstances` object for the `untracked_instances`.\"\"\"\n    track_instance = TrackInstances(\n        src_instances=untracked_instances,\n        track_ids=[None] * len(untracked_instances),\n        tracking_scores=[None] * len(untracked_instances),\n        features=feature_list,\n        frame_idx=frame_idx,\n        image=image,\n    )\n    return track_instance\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.update_tracks","title":"<code>update_tracks(current_instances, row_inds, col_inds, tracking_scores)</code>","text":"<p>Assign tracks to <code>TrackInstances</code> based on the output of track matching algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>TrackInstances</code> <p><code>TrackInstances</code> instance with features and unassigned tracks.</p> required <code>row_inds</code> <code>array</code> <p>List of indices for the  <code>current_instances</code> object that has an assigned track.</p> required <code>col_inds</code> <code>array</code> <p>List of track IDs that have been assigned a new instance.</p> required <code>tracking_scores</code> <code>List[float]</code> <p>List of tracking scores from the cost matrix.</p> required Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def update_tracks(\n    self,\n    current_instances: TrackInstances,\n    row_inds: np.array,\n    col_inds: np.array,\n    tracking_scores: List[float],\n) -&gt; TrackInstances:\n    \"\"\"Assign tracks to `TrackInstances` based on the output of track matching algorithm.\n\n    Args:\n        current_instances: `TrackInstances` instance with features and unassigned tracks.\n        row_inds: List of indices for the  `current_instances` object that has an assigned\n            track.\n        col_inds: List of track IDs that have been assigned a new instance.\n        tracking_scores: List of tracking scores from the cost matrix.\n\n    \"\"\"\n    add_to_queue = True\n    if row_inds is not None and col_inds is not None:\n\n        for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n            current_instances.track_ids[row] = self.current_tracks[col]\n            current_instances.tracking_scores[row] = tracking_scores[idx]\n\n        # update tracks to queue\n        self.tracker_queue.append(current_instances)\n        add_to_queue = False\n\n        # Create new tracks for instances with unassigned tracks from track matching\n        new_current_instances_inds = [\n            x for x in range(len(current_instances.features)) if x not in row_inds\n        ]\n        if new_current_instances_inds:\n            current_instances = self.add_new_tracks(\n                current_instances, add_to_queue=add_to_queue\n            )\n    return current_instances\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/","title":"local_queues","text":""},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues","title":"<code>sleap_nn.tracking.candidates.local_queues</code>","text":"<p>Module to generate Tracking local queue candidates.</p> <p>Classes:</p> Name Description <code>LocalQueueCandidates</code> <p>Track local queues method for candidate generation.</p>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates","title":"<code>LocalQueueCandidates</code>","text":"<p>Track local queues method for candidate generation.</p> <p>This module handles <code>tracker_queue</code> using the local queues method, where track assignments are determined based on the last <code>window_size</code> instances for each track.</p> <p>Attributes:</p> Name Type Description <code>window_size</code> <p>Number of previous frames to compare the current predicted instance with. Default: 5.</p> <code>max_tracks</code> <p>Maximum number of new tracks that can be created. Default: None.</p> <code>min_new_track_points</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>tracker_queue</code> <p>Dictionary that stores the past frames of all the tracks identified so far as <code>deque</code>.</p> <code>current_tracks</code> <p>List of track IDs that are being tracked.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize class variables.</p> <code>add_new_tracks</code> <p>Add new track IDs to the <code>TrackInstanceLocalQueue</code> objects and to the tracker queue.</p> <code>get_features_from_track_id</code> <p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> <code>get_instances_groupby_frame_idx</code> <p>Return dictionary with list of <code>TrackInstanceLocalQueue</code> objects grouped by frame index.</p> <code>get_new_track_id</code> <p>Return a new track_id.</p> <code>get_track_instances</code> <p>Return a list of <code>TrackInstanceLocalQueue</code> instances for the <code>untracked_instances</code>.</p> <code>update_tracks</code> <p>Assign tracks to <code>TrackInstanceLocalQueue</code> objects based on the output of track matching algorithm.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>class LocalQueueCandidates:\n    \"\"\"Track local queues method for candidate generation.\n\n    This module handles `tracker_queue` using the local queues method, where track assignments\n    are determined based on the last `window_size` instances for each track.\n\n    Attributes:\n        window_size: Number of previous frames to compare the current predicted instance with.\n            Default: 5.\n        max_tracks: Maximum number of new tracks that can be created. Default: None.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        tracker_queue: Dictionary that stores the past frames of all the tracks identified\n            so far as `deque`.\n        current_tracks: List of track IDs that are being tracked.\n    \"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 5,\n        max_tracks: Optional[int] = None,\n        min_new_track_points: int = 0,\n    ):\n        \"\"\"Initialize class variables.\"\"\"\n        self.window_size = window_size\n        self.max_tracks = max_tracks\n        self.min_new_track_points = min_new_track_points\n        self.tracker_queue = defaultdict(Deque)\n        self.current_tracks = []\n\n    def get_track_instances(\n        self,\n        feature_list: List[Union[np.array]],\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.array,\n    ) -&gt; List[TrackInstanceLocalQueue]:\n        \"\"\"Return a list of `TrackInstanceLocalQueue` instances for the `untracked_instances`.\"\"\"\n        track_instances = []\n        for ind, (feat, instance) in enumerate(zip(feature_list, untracked_instances)):\n            track_instance = TrackInstanceLocalQueue(\n                src_instance=instance,\n                src_instance_idx=ind,\n                track_id=None,\n                feature=feat,\n                frame_idx=frame_idx,\n                image=image,\n            )\n            track_instances.append(track_instance)\n        return track_instances\n\n    def get_features_from_track_id(\n        self, track_id: int, candidates_list: Optional[DefaultDict[int, Deque]] = None\n    ) -&gt; List[TrackedInstanceFeature]:\n        \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n        Note: If `candidates_list` is `None`, then features of all the instances in the\n            tracker queue are returned by default. Else, only the features from the given\n            candidates_list are returned.\n        \"\"\"\n        tracked_instances = (\n            candidates_list if candidates_list is not None else self.tracker_queue\n        )\n        output = []\n        for t in tracked_instances[track_id]:\n            tracked_instance_feature = TrackedInstanceFeature(\n                feature=t.feature,\n                src_predicted_instance=t.src_instance,\n                frame_idx=t.frame_idx,\n                tracking_score=t.tracking_score,\n                shifted_keypoints=None,\n            )\n            output.append(tracked_instance_feature)\n        return output\n\n    def get_new_track_id(self) -&gt; int:\n        \"\"\"Return a new track_id.\"\"\"\n        if not self.current_tracks:\n            new_track_id = 0\n        else:\n            new_track_id = max(self.current_tracks) + 1\n            if self.max_tracks is not None and new_track_id &gt;= self.max_tracks:\n                return None\n        self.tracker_queue[new_track_id] = deque(maxlen=self.window_size)\n        return new_track_id\n\n    def add_new_tracks(\n        self, current_instances: List[TrackInstanceLocalQueue]\n    ) -&gt; List[TrackInstanceLocalQueue]:\n        \"\"\"Add new track IDs to the `TrackInstanceLocalQueue` objects and to the tracker queue.\"\"\"\n        track_instances = []\n        for t in current_instances:\n            # Spawning a new track only if num visbile points is more than the threshold\n            num_visible_keypoints = (\n                ~np.isnan(t.src_instance.numpy()).any(axis=1)\n            ).sum()\n            if num_visible_keypoints &gt; self.min_new_track_points:\n                new_track_id = self.get_new_track_id()\n                if new_track_id is not None:\n                    t.track_id = new_track_id\n                    t.tracking_score = 1.0\n                    self.current_tracks.append(new_track_id)\n                    self.tracker_queue[new_track_id].append(t)\n                else:\n                    continue\n                # if new_track_id = `None`, max tracks is reached and we skip this instance\n            track_instances.append(t)\n\n        return track_instances\n\n    def update_tracks(\n        self,\n        current_instances: List[TrackInstanceLocalQueue],\n        row_inds: np.array,\n        col_inds: np.array,\n        tracking_scores: List[float],\n    ) -&gt; List[TrackInstanceLocalQueue]:\n        \"\"\"Assign tracks to `TrackInstanceLocalQueue` objects based on the output of track matching algorithm.\n\n        Args:\n            current_instances: List of TrackInstanceLocalQueue objects with features and unassigned tracks.\n            row_inds: List of indices for the  `current_instances` object that has an assigned\n                track.\n            col_inds: List of track IDs that have been assigned a new instance.\n            tracking_scores: List of tracking scores from the cost matrix.\n\n        \"\"\"\n        res = []\n        if row_inds is not None and col_inds is not None:\n            for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n                current_instances[row].track_id = col\n                current_instances[row].tracking_score = tracking_scores[idx]\n                res.append(current_instances[row])\n\n            for track_instance in current_instances:\n                if track_instance.track_id is not None:\n                    self.tracker_queue[track_instance.track_id].append(track_instance)\n\n            # Create new tracks for instances with unassigned tracks from track matching\n            new_current_instances_inds = [\n                x for x in range(len(current_instances)) if x not in row_inds\n            ]\n            if new_current_instances_inds:\n                for ind in new_current_instances_inds:\n                    res.extend(self.add_new_tracks([current_instances[ind]]))\n\n        return [x for x in res if x.track_id is not None]\n\n    def get_instances_groupby_frame_idx(\n        self, candidates_list: Optional[DefaultDict[int, Deque]]\n    ) -&gt; Dict[int, List[TrackInstanceLocalQueue]]:\n        \"\"\"Return dictionary with list of `TrackInstanceLocalQueue` objects grouped by frame index.\"\"\"\n        instances_dict = defaultdict(list)\n        tracked_instances = (\n            candidates_list if candidates_list is not None else self.tracker_queue\n        )\n        for _, instances in tracked_instances.items():\n            for instance in instances:\n                instances_dict[instance.frame_idx].append(instance)\n        return instances_dict\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.__init__","title":"<code>__init__(window_size=5, max_tracks=None, min_new_track_points=0)</code>","text":"<p>Initialize class variables.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 5,\n    max_tracks: Optional[int] = None,\n    min_new_track_points: int = 0,\n):\n    \"\"\"Initialize class variables.\"\"\"\n    self.window_size = window_size\n    self.max_tracks = max_tracks\n    self.min_new_track_points = min_new_track_points\n    self.tracker_queue = defaultdict(Deque)\n    self.current_tracks = []\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.add_new_tracks","title":"<code>add_new_tracks(current_instances)</code>","text":"<p>Add new track IDs to the <code>TrackInstanceLocalQueue</code> objects and to the tracker queue.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def add_new_tracks(\n    self, current_instances: List[TrackInstanceLocalQueue]\n) -&gt; List[TrackInstanceLocalQueue]:\n    \"\"\"Add new track IDs to the `TrackInstanceLocalQueue` objects and to the tracker queue.\"\"\"\n    track_instances = []\n    for t in current_instances:\n        # Spawning a new track only if num visbile points is more than the threshold\n        num_visible_keypoints = (\n            ~np.isnan(t.src_instance.numpy()).any(axis=1)\n        ).sum()\n        if num_visible_keypoints &gt; self.min_new_track_points:\n            new_track_id = self.get_new_track_id()\n            if new_track_id is not None:\n                t.track_id = new_track_id\n                t.tracking_score = 1.0\n                self.current_tracks.append(new_track_id)\n                self.tracker_queue[new_track_id].append(t)\n            else:\n                continue\n            # if new_track_id = `None`, max tracks is reached and we skip this instance\n        track_instances.append(t)\n\n    return track_instances\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_features_from_track_id","title":"<code>get_features_from_track_id(track_id, candidates_list=None)</code>","text":"<p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> If <code>candidates_list</code> is <code>None</code>, then features of all the instances in the <p>tracker queue are returned by default. Else, only the features from the given candidates_list are returned.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_features_from_track_id(\n    self, track_id: int, candidates_list: Optional[DefaultDict[int, Deque]] = None\n) -&gt; List[TrackedInstanceFeature]:\n    \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n    Note: If `candidates_list` is `None`, then features of all the instances in the\n        tracker queue are returned by default. Else, only the features from the given\n        candidates_list are returned.\n    \"\"\"\n    tracked_instances = (\n        candidates_list if candidates_list is not None else self.tracker_queue\n    )\n    output = []\n    for t in tracked_instances[track_id]:\n        tracked_instance_feature = TrackedInstanceFeature(\n            feature=t.feature,\n            src_predicted_instance=t.src_instance,\n            frame_idx=t.frame_idx,\n            tracking_score=t.tracking_score,\n            shifted_keypoints=None,\n        )\n        output.append(tracked_instance_feature)\n    return output\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_instances_groupby_frame_idx","title":"<code>get_instances_groupby_frame_idx(candidates_list)</code>","text":"<p>Return dictionary with list of <code>TrackInstanceLocalQueue</code> objects grouped by frame index.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_instances_groupby_frame_idx(\n    self, candidates_list: Optional[DefaultDict[int, Deque]]\n) -&gt; Dict[int, List[TrackInstanceLocalQueue]]:\n    \"\"\"Return dictionary with list of `TrackInstanceLocalQueue` objects grouped by frame index.\"\"\"\n    instances_dict = defaultdict(list)\n    tracked_instances = (\n        candidates_list if candidates_list is not None else self.tracker_queue\n    )\n    for _, instances in tracked_instances.items():\n        for instance in instances:\n            instances_dict[instance.frame_idx].append(instance)\n    return instances_dict\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_new_track_id","title":"<code>get_new_track_id()</code>","text":"<p>Return a new track_id.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_new_track_id(self) -&gt; int:\n    \"\"\"Return a new track_id.\"\"\"\n    if not self.current_tracks:\n        new_track_id = 0\n    else:\n        new_track_id = max(self.current_tracks) + 1\n        if self.max_tracks is not None and new_track_id &gt;= self.max_tracks:\n            return None\n    self.tracker_queue[new_track_id] = deque(maxlen=self.window_size)\n    return new_track_id\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_track_instances","title":"<code>get_track_instances(feature_list, untracked_instances, frame_idx, image)</code>","text":"<p>Return a list of <code>TrackInstanceLocalQueue</code> instances for the <code>untracked_instances</code>.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_track_instances(\n    self,\n    feature_list: List[Union[np.array]],\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.array,\n) -&gt; List[TrackInstanceLocalQueue]:\n    \"\"\"Return a list of `TrackInstanceLocalQueue` instances for the `untracked_instances`.\"\"\"\n    track_instances = []\n    for ind, (feat, instance) in enumerate(zip(feature_list, untracked_instances)):\n        track_instance = TrackInstanceLocalQueue(\n            src_instance=instance,\n            src_instance_idx=ind,\n            track_id=None,\n            feature=feat,\n            frame_idx=frame_idx,\n            image=image,\n        )\n        track_instances.append(track_instance)\n    return track_instances\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.update_tracks","title":"<code>update_tracks(current_instances, row_inds, col_inds, tracking_scores)</code>","text":"<p>Assign tracks to <code>TrackInstanceLocalQueue</code> objects based on the output of track matching algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>List[TrackInstanceLocalQueue]</code> <p>List of TrackInstanceLocalQueue objects with features and unassigned tracks.</p> required <code>row_inds</code> <code>array</code> <p>List of indices for the  <code>current_instances</code> object that has an assigned track.</p> required <code>col_inds</code> <code>array</code> <p>List of track IDs that have been assigned a new instance.</p> required <code>tracking_scores</code> <code>List[float]</code> <p>List of tracking scores from the cost matrix.</p> required Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def update_tracks(\n    self,\n    current_instances: List[TrackInstanceLocalQueue],\n    row_inds: np.array,\n    col_inds: np.array,\n    tracking_scores: List[float],\n) -&gt; List[TrackInstanceLocalQueue]:\n    \"\"\"Assign tracks to `TrackInstanceLocalQueue` objects based on the output of track matching algorithm.\n\n    Args:\n        current_instances: List of TrackInstanceLocalQueue objects with features and unassigned tracks.\n        row_inds: List of indices for the  `current_instances` object that has an assigned\n            track.\n        col_inds: List of track IDs that have been assigned a new instance.\n        tracking_scores: List of tracking scores from the cost matrix.\n\n    \"\"\"\n    res = []\n    if row_inds is not None and col_inds is not None:\n        for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n            current_instances[row].track_id = col\n            current_instances[row].tracking_score = tracking_scores[idx]\n            res.append(current_instances[row])\n\n        for track_instance in current_instances:\n            if track_instance.track_id is not None:\n                self.tracker_queue[track_instance.track_id].append(track_instance)\n\n        # Create new tracks for instances with unassigned tracks from track matching\n        new_current_instances_inds = [\n            x for x in range(len(current_instances)) if x not in row_inds\n        ]\n        if new_current_instances_inds:\n            for ind in new_current_instances_inds:\n                res.extend(self.add_new_tracks([current_instances[ind]]))\n\n    return [x for x in res if x.track_id is not None]\n</code></pre>"},{"location":"api/training/","title":"training","text":""},{"location":"api/training/#sleap_nn.training","title":"<code>sleap_nn.training</code>","text":"<p>Training-related modules.</p> <p>Modules:</p> Name Description <code>callbacks</code> <p>Custom Callback modules for Lightning Trainer.</p> <code>lightning_modules</code> <p>This module has the LightningModule classes for all model types.</p> <code>losses</code> <p>Custom loss functions.</p> <code>model_trainer</code> <p>This module is to train a sleap-nn model using Lightning.</p> <code>utils</code> <p>Miscellaneous utility functions for training.</p>"},{"location":"api/training/callbacks/","title":"callbacks","text":""},{"location":"api/training/callbacks/#sleap_nn.training.callbacks","title":"<code>sleap_nn.training.callbacks</code>","text":"<p>Custom Callback modules for Lightning Trainer.</p> <p>Classes:</p> Name Description <code>CSVLoggerCallback</code> <p>Callback for logging metrics to csv.</p> <code>MatplotlibSaver</code> <p>Callback for saving images rendered with matplotlib during training.</p> <code>ProgressReporterZMQ</code> <p>Callback to publish training progress events to a ZMQ PUB socket.</p> <code>TrainingControllerZMQ</code> <p>Lightning callback to receive control commands during training via ZMQ.</p> <code>WandBPredImageLogger</code> <p>Callback for writing image predictions to wandb.</p>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CSVLoggerCallback","title":"<code>CSVLoggerCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for logging metrics to csv.</p> <p>Attributes:</p> Name Type Description <code>filepath</code> <p>Path to save the csv file.</p> <code>keys</code> <p>List of field names to be logged in the csv.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attributes.</p> <code>on_validation_epoch_end</code> <p>Log metrics to csv at the end of validation epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class CSVLoggerCallback(Callback):\n    \"\"\"Callback for logging metrics to csv.\n\n    Attributes:\n        filepath: Path to save the csv file.\n        keys: List of field names to be logged in the csv.\n    \"\"\"\n\n    def __init__(\n        self,\n        filepath: Path,\n        keys: list = [\"epoch\", \"train_loss\", \"val_loss\", \"learning_rate\"],\n    ):\n        \"\"\"Initialize attributes.\"\"\"\n        super().__init__()\n        self.filepath = filepath\n        self.keys = keys\n        self.initialized = False\n\n    def _init_file(self):\n        \"\"\"Create the .csv file.\"\"\"\n        if RANK in [0, -1]:  # Global rank 0 or -1 (non-distributed)\n            self.filepath.parent.mkdir(parents=True, exist_ok=True)\n            with open(self.filepath, \"w\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=self.keys)\n                writer.writeheader()\n        self.initialized = True\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Log metrics to csv at the end of validation epoch.\"\"\"\n        if trainer.is_global_zero:\n            if not self.initialized:\n                self._init_file()\n\n            metrics = trainer.callback_metrics\n            log_data = {}\n            for key in self.keys:\n                if key == \"epoch\":\n                    log_data[\"epoch\"] = trainer.current_epoch\n                else:\n                    value = metrics.get(key, None)\n                    log_data[key] = value.item() if value is not None else None\n\n            with open(self.filepath, \"a\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=self.keys)\n                writer.writerow(log_data)\n\n        # Sync all processes after file I/O\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CSVLoggerCallback.__init__","title":"<code>__init__(filepath, keys=['epoch', 'train_loss', 'val_loss', 'learning_rate'])</code>","text":"<p>Initialize attributes.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    filepath: Path,\n    keys: list = [\"epoch\", \"train_loss\", \"val_loss\", \"learning_rate\"],\n):\n    \"\"\"Initialize attributes.\"\"\"\n    super().__init__()\n    self.filepath = filepath\n    self.keys = keys\n    self.initialized = False\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CSVLoggerCallback.on_validation_epoch_end","title":"<code>on_validation_epoch_end(trainer, pl_module)</code>","text":"<p>Log metrics to csv at the end of validation epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n    \"\"\"Log metrics to csv at the end of validation epoch.\"\"\"\n    if trainer.is_global_zero:\n        if not self.initialized:\n            self._init_file()\n\n        metrics = trainer.callback_metrics\n        log_data = {}\n        for key in self.keys:\n            if key == \"epoch\":\n                log_data[\"epoch\"] = trainer.current_epoch\n            else:\n                value = metrics.get(key, None)\n                log_data[key] = value.item() if value is not None else None\n\n        with open(self.filepath, \"a\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=self.keys)\n            writer.writerow(log_data)\n\n    # Sync all processes after file I/O\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.MatplotlibSaver","title":"<code>MatplotlibSaver</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for saving images rendered with matplotlib during training.</p> <p>This is useful for saving visualizations of the training to disk. It will be called at the end of each epoch.</p> <p>Attributes:</p> Name Type Description <code>plot_fn</code> <p>Function with no arguments that returns a matplotlib figure handle.</p> <code>save_folder</code> <p>Path to a directory to save images to.</p> <code>prefix</code> <p>String that will be prepended to the filenames. This is useful for indicating which dataset the visualization was sampled from.</p> Notes <p>This will save images with the naming pattern:     \"{save_folder}/{prefix}.{epoch}.png\" or:     \"{save_folder}/{epoch}.png\" if a prefix is not specified.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize callback.</p> <code>on_train_epoch_end</code> <p>Save figure at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class MatplotlibSaver(Callback):\n    \"\"\"Callback for saving images rendered with matplotlib during training.\n\n    This is useful for saving visualizations of the training to disk. It will be called\n    at the end of each epoch.\n\n    Attributes:\n        plot_fn: Function with no arguments that returns a matplotlib figure handle.\n        save_folder: Path to a directory to save images to.\n        prefix: String that will be prepended to the filenames. This is useful for\n            indicating which dataset the visualization was sampled from.\n\n    Notes:\n        This will save images with the naming pattern:\n            \"{save_folder}/{prefix}.{epoch}.png\"\n        or:\n            \"{save_folder}/{epoch}.png\"\n        if a prefix is not specified.\n    \"\"\"\n\n    def __init__(\n        self,\n        save_folder: str,\n        plot_fn: Callable[[], matplotlib.figure.Figure],\n        prefix: Optional[str] = None,\n    ):\n        \"\"\"Initialize callback.\"\"\"\n        self.save_folder = save_folder\n        self.plot_fn = plot_fn\n        self.prefix = prefix\n        super().__init__()\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Save figure at the end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            # Call plotting function.\n            figure = self.plot_fn()\n\n            # Build filename.\n            prefix = \"\"\n            if self.prefix is not None:\n                prefix = self.prefix + \".\"\n            figure_path = (\n                Path(self.save_folder) / f\"{prefix}{trainer.current_epoch:04d}.png\"\n            ).as_posix()\n\n            # Save rendered figure.\n            figure.savefig(figure_path, format=\"png\", pad_inches=0)\n            plt.close(figure)\n\n        # Sync all processes after file I/O\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.MatplotlibSaver.__init__","title":"<code>__init__(save_folder, plot_fn, prefix=None)</code>","text":"<p>Initialize callback.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    save_folder: str,\n    plot_fn: Callable[[], matplotlib.figure.Figure],\n    prefix: Optional[str] = None,\n):\n    \"\"\"Initialize callback.\"\"\"\n    self.save_folder = save_folder\n    self.plot_fn = plot_fn\n    self.prefix = prefix\n    super().__init__()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.MatplotlibSaver.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Save figure at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Save figure at the end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        # Call plotting function.\n        figure = self.plot_fn()\n\n        # Build filename.\n        prefix = \"\"\n        if self.prefix is not None:\n            prefix = self.prefix + \".\"\n        figure_path = (\n            Path(self.save_folder) / f\"{prefix}{trainer.current_epoch:04d}.png\"\n        ).as_posix()\n\n        # Save rendered figure.\n        figure.savefig(figure_path, format=\"png\", pad_inches=0)\n        plt.close(figure)\n\n    # Sync all processes after file I/O\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ","title":"<code>ProgressReporterZMQ</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to publish training progress events to a ZMQ PUB socket.</p> <p>This is used to publish training metrics to the given socket.</p> <p>Attributes:</p> Name Type Description <code>address</code> <p>The ZMQ address to publish to, e.g., \"tcp://127.0.0.1:9001\".</p> <code>what</code> <p>Identifier tag for the type of training job (e.g., model name or job type).</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Close zmq socket and context when callback is destroyed.</p> <code>__init__</code> <p>Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.</p> <code>on_train_batch_end</code> <p>Called at the end of each training batch.</p> <code>on_train_batch_start</code> <p>Called at the beginning of each training batch.</p> <code>on_train_end</code> <p>Called at the end of training process.</p> <code>on_train_epoch_end</code> <p>Called at the end of each epoch.</p> <code>on_train_epoch_start</code> <p>Called at the beginning of each epoch.</p> <code>on_train_start</code> <p>Called at the beginning of training process.</p> <code>send</code> <p>Send a message over ZMQ.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class ProgressReporterZMQ(Callback):\n    \"\"\"Callback to publish training progress events to a ZMQ PUB socket.\n\n    This is used to publish training metrics to the given socket.\n\n    Attributes:\n        address: The ZMQ address to publish to, e.g., \"tcp://127.0.0.1:9001\".\n        what: Identifier tag for the type of training job (e.g., model name or job type).\n    \"\"\"\n\n    def __init__(self, address=\"tcp://127.0.0.1:9001\", what=\"\"):\n        \"\"\"Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.\"\"\"\n        super().__init__()\n        self.address = address\n        self.what = what\n\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.PUB)\n        self.socket.connect(self.address)\n\n        logger.info(\n            f\"ProgressReporterZMQ publishing to {self.address} for '{self.what}'\"\n        )\n\n    def __del__(self):\n        \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n        logger.info(f\"Closing ZMQ reporter.\")\n        self.socket.setsockopt(zmq.LINGER, 0)\n        self.socket.close()\n        self.context.term()\n\n    def send(self, event: str, logs=None, **kwargs):\n        \"\"\"Send a message over ZMQ.\"\"\"\n        msg = dict(what=self.what, event=event, logs=logs, **kwargs)\n        self.socket.send_string(jsonpickle.encode(msg))\n\n    def on_train_start(self, trainer, pl_module):\n        \"\"\"Called at the beginning of training process.\"\"\"\n        if trainer.is_global_zero:\n            self.send(\"train_begin\")\n        trainer.strategy.barrier()\n\n    def on_train_end(self, trainer, pl_module):\n        \"\"\"Called at the end of training process.\"\"\"\n        if trainer.is_global_zero:\n            self.send(\"train_end\")\n        trainer.strategy.barrier()\n\n    def on_train_epoch_start(self, trainer, pl_module):\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            self.send(\"epoch_begin\", epoch=trainer.current_epoch)\n        trainer.strategy.barrier()\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Called at the end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            logs = trainer.callback_metrics\n            self.send(\n                \"epoch_end\", epoch=trainer.current_epoch, logs=self._sanitize_logs(logs)\n            )\n        trainer.strategy.barrier()\n\n    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n        \"\"\"Called at the beginning of each training batch.\"\"\"\n        if trainer.is_global_zero:\n            self.send(\"batch_start\", batch=batch_idx)\n        trainer.strategy.barrier()\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Called at the end of each training batch.\"\"\"\n        if trainer.is_global_zero:\n            logs = trainer.callback_metrics\n            self.send(\n                \"batch_end\",\n                epoch=trainer.current_epoch,\n                batch=batch_idx,\n                logs=self._sanitize_logs(logs),\n            )\n        trainer.strategy.barrier()\n\n    def _sanitize_logs(self, logs):\n        \"\"\"Convert any torch tensors to Python floats for serialization.\"\"\"\n        return {\n            k: float(v.item()) if hasattr(v, \"item\") else v for k, v in logs.items()\n        }\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.__del__","title":"<code>__del__()</code>","text":"<p>Close zmq socket and context when callback is destroyed.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __del__(self):\n    \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n    logger.info(f\"Closing ZMQ reporter.\")\n    self.socket.setsockopt(zmq.LINGER, 0)\n    self.socket.close()\n    self.context.term()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.__init__","title":"<code>__init__(address='tcp://127.0.0.1:9001', what='')</code>","text":"<p>Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(self, address=\"tcp://127.0.0.1:9001\", what=\"\"):\n    \"\"\"Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.\"\"\"\n    super().__init__()\n    self.address = address\n    self.what = what\n\n    self.context = zmq.Context()\n    self.socket = self.context.socket(zmq.PUB)\n    self.socket.connect(self.address)\n\n    logger.info(\n        f\"ProgressReporterZMQ publishing to {self.address} for '{self.what}'\"\n    )\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_batch_end","title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Called at the end of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n    \"\"\"Called at the end of each training batch.\"\"\"\n    if trainer.is_global_zero:\n        logs = trainer.callback_metrics\n        self.send(\n            \"batch_end\",\n            epoch=trainer.current_epoch,\n            batch=batch_idx,\n            logs=self._sanitize_logs(logs),\n        )\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_batch_start","title":"<code>on_train_batch_start(trainer, pl_module, batch, batch_idx)</code>","text":"<p>Called at the beginning of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n    \"\"\"Called at the beginning of each training batch.\"\"\"\n    if trainer.is_global_zero:\n        self.send(\"batch_start\", batch=batch_idx)\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_end","title":"<code>on_train_end(trainer, pl_module)</code>","text":"<p>Called at the end of training process.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_end(self, trainer, pl_module):\n    \"\"\"Called at the end of training process.\"\"\"\n    if trainer.is_global_zero:\n        self.send(\"train_end\")\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Called at the end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        logs = trainer.callback_metrics\n        self.send(\n            \"epoch_end\", epoch=trainer.current_epoch, logs=self._sanitize_logs(logs)\n        )\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_epoch_start","title":"<code>on_train_epoch_start(trainer, pl_module)</code>","text":"<p>Called at the beginning of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_start(self, trainer, pl_module):\n    \"\"\"Called at the beginning of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        self.send(\"epoch_begin\", epoch=trainer.current_epoch)\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_start","title":"<code>on_train_start(trainer, pl_module)</code>","text":"<p>Called at the beginning of training process.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_start(self, trainer, pl_module):\n    \"\"\"Called at the beginning of training process.\"\"\"\n    if trainer.is_global_zero:\n        self.send(\"train_begin\")\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.send","title":"<code>send(event, logs=None, **kwargs)</code>","text":"<p>Send a message over ZMQ.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def send(self, event: str, logs=None, **kwargs):\n    \"\"\"Send a message over ZMQ.\"\"\"\n    msg = dict(what=self.what, event=event, logs=logs, **kwargs)\n    self.socket.send_string(jsonpickle.encode(msg))\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ","title":"<code>TrainingControllerZMQ</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Lightning callback to receive control commands during training via ZMQ.</p> <p>This is typically used to allow SLEAP GUI interface (SLEAP LossViewer) to dynamically control the training process (stopping early) by publishing commands to a ZMQ socket.</p> <p>Attributes:</p> Name Type Description <code>address</code> <p>ZMQ socket address to subscribe to.</p> <code>topic</code> <p>Topic filter for messages.</p> <code>timeout</code> <p>Poll timeout in milliseconds when checking for new messages.</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Close zmq socket and context when callback is destroyed.</p> <code>__init__</code> <p>Initialize the controller callback by connecting to the specified ZMQ PUB socket.</p> <code>on_train_batch_end</code> <p>Called at the end of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class TrainingControllerZMQ(Callback):\n    \"\"\"Lightning callback to receive control commands during training via ZMQ.\n\n    This is typically used to allow SLEAP GUI interface (SLEAP LossViewer)\n    to dynamically control the training process (stopping early) by publishing commands to a ZMQ socket.\n\n    Attributes:\n        address: ZMQ socket address to subscribe to.\n        topic: Topic filter for messages.\n        timeout: Poll timeout in milliseconds when checking for new messages.\n    \"\"\"\n\n    def __init__(self, address=\"tcp://127.0.0.1:9000\", topic=\"\", poll_timeout=10):\n        \"\"\"Initialize the controller callback by connecting to the specified ZMQ PUB socket.\"\"\"\n        super().__init__()\n        self.address = address\n        self.topic = topic\n        self.timeout = poll_timeout\n\n        # Initialize ZMQ\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.SUB)\n        self.socket.subscribe(self.topic)\n        self.socket.connect(self.address)\n        logger.info(\n            f\"Training controller subscribed to: {self.address} (topic: {self.topic})\"\n        )\n\n    def __del__(self):\n        \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n        logger.info(\"Closing the training controller socket/context.\")\n        self.socket.close()\n        self.context.term()\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Called at the end of each training batch.\"\"\"\n        if trainer.is_global_zero:\n            if self.socket.poll(self.timeout, zmq.POLLIN):\n                msg = jsonpickle.decode(self.socket.recv_string())\n                logger.info(f\"Received control message: {msg}\")\n\n                # Stop training\n                if msg.get(\"command\") == \"stop\":\n                    trainer.should_stop = True\n\n        # Sync all processes after ZMQ operations\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ.__del__","title":"<code>__del__()</code>","text":"<p>Close zmq socket and context when callback is destroyed.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __del__(self):\n    \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n    logger.info(\"Closing the training controller socket/context.\")\n    self.socket.close()\n    self.context.term()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ.__init__","title":"<code>__init__(address='tcp://127.0.0.1:9000', topic='', poll_timeout=10)</code>","text":"<p>Initialize the controller callback by connecting to the specified ZMQ PUB socket.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(self, address=\"tcp://127.0.0.1:9000\", topic=\"\", poll_timeout=10):\n    \"\"\"Initialize the controller callback by connecting to the specified ZMQ PUB socket.\"\"\"\n    super().__init__()\n    self.address = address\n    self.topic = topic\n    self.timeout = poll_timeout\n\n    # Initialize ZMQ\n    self.context = zmq.Context()\n    self.socket = self.context.socket(zmq.SUB)\n    self.socket.subscribe(self.topic)\n    self.socket.connect(self.address)\n    logger.info(\n        f\"Training controller subscribed to: {self.address} (topic: {self.topic})\"\n    )\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ.on_train_batch_end","title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Called at the end of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n    \"\"\"Called at the end of each training batch.\"\"\"\n    if trainer.is_global_zero:\n        if self.socket.poll(self.timeout, zmq.POLLIN):\n            msg = jsonpickle.decode(self.socket.recv_string())\n            logger.info(f\"Received control message: {msg}\")\n\n            # Stop training\n            if msg.get(\"command\") == \"stop\":\n                trainer.should_stop = True\n\n    # Sync all processes after ZMQ operations\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBPredImageLogger","title":"<code>WandBPredImageLogger</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for writing image predictions to wandb.</p> <p>Attributes:</p> Name Type Description <code>viz_folder</code> <p>Path to viz directory.</p> <code>wandb_run_name</code> <p>WandB run name.</p> <code>is_bottomup</code> <p>If the model type is bottomup or not.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attributes.</p> <code>on_train_epoch_end</code> <p>Called at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class WandBPredImageLogger(Callback):\n    \"\"\"Callback for writing image predictions to wandb.\n\n    Attributes:\n        viz_folder: Path to viz directory.\n        wandb_run_name: WandB run name.\n        is_bottomup: If the model type is bottomup or not.\n    \"\"\"\n\n    def __init__(\n        self,\n        viz_folder: str,\n        wandb_run_name: str,\n        is_bottomup: bool = False,\n    ):\n        \"\"\"Initialize attributes.\"\"\"\n        self.viz_folder = viz_folder\n        self.wandb_run_name = wandb_run_name\n        self.is_bottomup = is_bottomup\n        # Callback initialization\n        super().__init__()\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Called at the end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            epoch_num = trainer.current_epoch\n            train_img_path = (\n                Path(self.viz_folder) / f\"train.{epoch_num:04d}.png\"\n            ).as_posix()\n            val_img_path = (\n                Path(self.viz_folder) / f\"validation.{epoch_num:04d}.png\"\n            ).as_posix()\n            train_img = Image.open(train_img_path)\n            val_img = Image.open(val_img_path)\n\n            column_names = [\n                \"Run name\",\n                \"Epoch\",\n                \"Preds on train\",\n                \"Preds on validation\",\n            ]\n            data = [\n                [\n                    f\"{self.wandb_run_name}\",\n                    f\"{epoch_num}\",\n                    wandb.Image(train_img),\n                    wandb.Image(val_img),\n                ]\n            ]\n            if self.is_bottomup:\n                column_names.extend([\"Pafs Preds on train\", \"Pafs Preds on validation\"])\n                data = [\n                    [\n                        f\"{self.wandb_run_name}\",\n                        f\"{epoch_num}\",\n                        wandb.Image(train_img),\n                        wandb.Image(val_img),\n                        wandb.Image(\n                            Image.open(\n                                (\n                                    Path(self.viz_folder)\n                                    / f\"train.pafs_magnitude.{epoch_num:04d}.png\"\n                                ).as_posix()\n                            )\n                        ),\n                        wandb.Image(\n                            Image.open(\n                                (\n                                    Path(self.viz_folder)\n                                    / f\"validation.pafs_magnitude.{epoch_num:04d}.png\"\n                                ).as_posix()\n                            )\n                        ),\n                    ]\n                ]\n            table = wandb.Table(columns=column_names, data=data)\n            wandb.log({f\"{self.wandb_run_name}\": table})\n\n        # Sync all processes after wandb logging\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBPredImageLogger.__init__","title":"<code>__init__(viz_folder, wandb_run_name, is_bottomup=False)</code>","text":"<p>Initialize attributes.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    viz_folder: str,\n    wandb_run_name: str,\n    is_bottomup: bool = False,\n):\n    \"\"\"Initialize attributes.\"\"\"\n    self.viz_folder = viz_folder\n    self.wandb_run_name = wandb_run_name\n    self.is_bottomup = is_bottomup\n    # Callback initialization\n    super().__init__()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBPredImageLogger.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Called at the end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        epoch_num = trainer.current_epoch\n        train_img_path = (\n            Path(self.viz_folder) / f\"train.{epoch_num:04d}.png\"\n        ).as_posix()\n        val_img_path = (\n            Path(self.viz_folder) / f\"validation.{epoch_num:04d}.png\"\n        ).as_posix()\n        train_img = Image.open(train_img_path)\n        val_img = Image.open(val_img_path)\n\n        column_names = [\n            \"Run name\",\n            \"Epoch\",\n            \"Preds on train\",\n            \"Preds on validation\",\n        ]\n        data = [\n            [\n                f\"{self.wandb_run_name}\",\n                f\"{epoch_num}\",\n                wandb.Image(train_img),\n                wandb.Image(val_img),\n            ]\n        ]\n        if self.is_bottomup:\n            column_names.extend([\"Pafs Preds on train\", \"Pafs Preds on validation\"])\n            data = [\n                [\n                    f\"{self.wandb_run_name}\",\n                    f\"{epoch_num}\",\n                    wandb.Image(train_img),\n                    wandb.Image(val_img),\n                    wandb.Image(\n                        Image.open(\n                            (\n                                Path(self.viz_folder)\n                                / f\"train.pafs_magnitude.{epoch_num:04d}.png\"\n                            ).as_posix()\n                        )\n                    ),\n                    wandb.Image(\n                        Image.open(\n                            (\n                                Path(self.viz_folder)\n                                / f\"validation.pafs_magnitude.{epoch_num:04d}.png\"\n                            ).as_posix()\n                        )\n                    ),\n                ]\n            ]\n        table = wandb.Table(columns=column_names, data=data)\n        wandb.log({f\"{self.wandb_run_name}\": table})\n\n    # Sync all processes after wandb logging\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/lightning_modules/","title":"lightning_modules","text":""},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules","title":"<code>sleap_nn.training.lightning_modules</code>","text":"<p>This module has the LightningModule classes for all model types.</p> <p>Classes:</p> Name Description <code>BottomUpLightningModule</code> <p>Lightning Module for BottomUp Model.</p> <code>BottomUpMultiClassLightningModule</code> <p>Lightning Module for BottomUp ID Model.</p> <code>CentroidLightningModule</code> <p>Lightning Module for Centroid Model.</p> <code>LightningModel</code> <p>Base PyTorch Lightning Module for all sleap-nn models.</p> <code>SingleInstanceLightningModule</code> <p>Lightning Module for SingleInstance Model.</p> <code>TopDownCenteredInstanceLightningModule</code> <p>Lightning Module for TopDownCenteredInstance Model.</p> <code>TopDownCenteredInstanceMultiClassLightningModule</code> <p>Lightning Module for TopDownCenteredInstance ID Model.</p>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule","title":"<code>BottomUpLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for BottomUp Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to BottomUp model. Bottom-Up models predict all keypoints simultaneously and use Part Affinity Fields (PAFs) to group keypoints into individual animals.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>trainer_accelerator</code> <code>Optional[str]</code> <p>Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").</p> <code>'cpu'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> <code>visualize_pafs_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class BottomUpLightningModule(LightningModel):\n    \"\"\"Lightning Module for BottomUp Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to BottomUp model. Bottom-Up models predict all keypoints\n    simultaneously and use Part Affinity Fields (PAFs) to group keypoints into individual animals.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        trainer_accelerator: Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        trainer_accelerator: Optional[str] = \"cpu\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            trainer_accelerator=trainer_accelerator,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        paf_scorer = PAFScorer(\n            part_names=self.head_configs.bottomup.confmaps.part_names,\n            edges=self.head_configs.bottomup.pafs.edges,\n            pafs_stride=self.head_configs.bottomup.pafs.output_stride,\n        )\n        self.bottomup_inf_layer = BottomUpInferenceModel(\n            torch_model=self.forward,\n            paf_scorer=paf_scorer,\n            peak_threshold=0.2,\n            input_scale=1.0,\n            return_confmaps=True,\n            return_pafs=True,\n            cms_output_stride=self.head_configs.bottomup.confmaps.output_stride,\n            pafs_output_stride=self.head_configs.bottomup.pafs.output_stride,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.bottomup_inf_layer(ex)[0]\n        peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n        img = (\n            output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        gt_instances = ex[\"instances\"][0].cpu().numpy()\n        confmaps = (\n            output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n        plt.xlim(plt.xlim())\n        plt.ylim(plt.ylim())\n        plot_peaks(gt_instances, peaks, paired=False)\n        return fig\n\n    def visualize_pafs_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.bottomup_inf_layer(ex)[0]\n        img = (\n            output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        pafs = output[\"pred_part_affinity_fields\"].cpu().numpy()[0]  # (h, w, 2*edges)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n\n        pafs = pafs.reshape((pafs.shape[0], pafs.shape[1], -1, 2))\n        pafs_mag = np.sqrt(pafs[..., 0] ** 2 + pafs[..., 1] ** 2)\n        plot_confmaps(pafs_mag, output_scale=pafs_mag.shape[0] / img.shape[0])\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        output = self.model(img)\n        return {\n            \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n            \"PartAffinityFieldsHead\": output[\"PartAffinityFieldsHead\"],\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_paf = batch[\"part_affinity_fields\"]\n        preds = self.model(X)\n        pafs = preds[\"PartAffinityFieldsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            pafs_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_paf,\n                y_pr=pafs,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n            pafs_loss += pafs_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"PartAffinityFieldsHead\": pafs_loss,\n        }\n        loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        self.log(\n            \"train_loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_paf = batch[\"part_affinity_fields\"]\n\n        preds = self.model(X)\n        pafs = preds[\"PartAffinityFieldsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            pafs_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_paf,\n                y_pr=pafs,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n            pafs_loss += pafs_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"PartAffinityFieldsHead\": pafs_loss,\n        }\n\n        val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\",\n            lr,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val_loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', trainer_accelerator='cpu', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    trainer_accelerator: Optional[str] = \"cpu\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        trainer_accelerator=trainer_accelerator,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    paf_scorer = PAFScorer(\n        part_names=self.head_configs.bottomup.confmaps.part_names,\n        edges=self.head_configs.bottomup.pafs.edges,\n        pafs_stride=self.head_configs.bottomup.pafs.output_stride,\n    )\n    self.bottomup_inf_layer = BottomUpInferenceModel(\n        torch_model=self.forward,\n        paf_scorer=paf_scorer,\n        peak_threshold=0.2,\n        input_scale=1.0,\n        return_confmaps=True,\n        return_pafs=True,\n        cms_output_stride=self.head_configs.bottomup.confmaps.output_stride,\n        pafs_output_stride=self.head_configs.bottomup.pafs.output_stride,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    output = self.model(img)\n    return {\n        \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n        \"PartAffinityFieldsHead\": output[\"PartAffinityFieldsHead\"],\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_paf = batch[\"part_affinity_fields\"]\n    preds = self.model(X)\n    pafs = preds[\"PartAffinityFieldsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        pafs_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_paf,\n            y_pr=pafs,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n        pafs_loss += pafs_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"PartAffinityFieldsHead\": pafs_loss,\n    }\n    loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    self.log(\n        \"train_loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_paf = batch[\"part_affinity_fields\"]\n\n    preds = self.model(X)\n    pafs = preds[\"PartAffinityFieldsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        pafs_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_paf,\n            y_pr=pafs,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n        pafs_loss += pafs_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"PartAffinityFieldsHead\": pafs_loss,\n    }\n\n    val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n    self.log(\n        \"learning_rate\",\n        lr,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val_loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.bottomup_inf_layer(ex)[0]\n    peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n    img = (\n        output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    gt_instances = ex[\"instances\"][0].cpu().numpy()\n    confmaps = (\n        output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n    plt.xlim(plt.xlim())\n    plt.ylim(plt.ylim())\n    plot_peaks(gt_instances, peaks, paired=False)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.visualize_pafs_example","title":"<code>visualize_pafs_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_pafs_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.bottomup_inf_layer(ex)[0]\n    img = (\n        output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    pafs = output[\"pred_part_affinity_fields\"].cpu().numpy()[0]  # (h, w, 2*edges)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n\n    pafs = pafs.reshape((pafs.shape[0], pafs.shape[1], -1, 2))\n    pafs_mag = np.sqrt(pafs[..., 0] ** 2 + pafs[..., 1] ** 2)\n    plot_confmaps(pafs_mag, output_scale=pafs_mag.shape[0] / img.shape[0])\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule","title":"<code>BottomUpMultiClassLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for BottomUp ID Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to BottomUp ID model. Multi-Class Bottom-Up models predict all keypoints simultaneously and classify instances using class maps to identify individual animals across frames.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>trainer_accelerator</code> <code>Optional[str]</code> <p>Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").</p> <code>'cpu'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_class_maps_example</code> <p>Visualize predictions during training (used with callbacks).</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class BottomUpMultiClassLightningModule(LightningModel):\n    \"\"\"Lightning Module for BottomUp ID Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to BottomUp ID model. Multi-Class Bottom-Up models predict\n    all keypoints simultaneously and classify instances using class maps to identify\n    individual animals across frames.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        trainer_accelerator: Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        trainer_accelerator: Optional[str] = \"cpu\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            trainer_accelerator=trainer_accelerator,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n        self.bottomup_inf_layer = BottomUpMultiClassInferenceModel(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            input_scale=1.0,\n            return_confmaps=True,\n            return_class_maps=True,\n            cms_output_stride=self.head_configs.multi_class_bottomup.confmaps.output_stride,\n            class_maps_output_stride=self.head_configs.multi_class_bottomup.class_maps.output_stride,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.bottomup_inf_layer(ex)[0]\n        peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n        img = (\n            output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        gt_instances = ex[\"instances\"][0].cpu().numpy()\n        confmaps = (\n            output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n        plt.xlim(plt.xlim())\n        plt.ylim(plt.ylim())\n        plot_peaks(gt_instances, peaks, paired=False)\n        return fig\n\n    def visualize_class_maps_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.bottomup_inf_layer(ex)[0]\n        img = (\n            output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        classmaps = (\n            output[\"pred_class_maps\"].cpu().numpy()[0].transpose(1, 2, 0)\n        )  # (n_classes, h, w)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n\n        plot_confmaps(classmaps, output_scale=classmaps.shape[0] / img.shape[0])\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        output = self.model(img)\n        return {\n            \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n            \"ClassMapsHead\": output[\"ClassMapsHead\"],\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n        preds = self.model(X)\n        classmaps = preds[\"ClassMapsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"ClassMapsHead\": classmaps_loss,\n        }\n        loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        self.log(\n            \"train_loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n\n        preds = self.model(X)\n        classmaps = preds[\"ClassMapsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"ClassMapsHead\": classmaps_loss,\n        }\n\n        val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\",\n            lr,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val_loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', trainer_accelerator='cpu', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    trainer_accelerator: Optional[str] = \"cpu\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        trainer_accelerator=trainer_accelerator,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n    self.bottomup_inf_layer = BottomUpMultiClassInferenceModel(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        input_scale=1.0,\n        return_confmaps=True,\n        return_class_maps=True,\n        cms_output_stride=self.head_configs.multi_class_bottomup.confmaps.output_stride,\n        class_maps_output_stride=self.head_configs.multi_class_bottomup.class_maps.output_stride,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    output = self.model(img)\n    return {\n        \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n        \"ClassMapsHead\": output[\"ClassMapsHead\"],\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n    preds = self.model(X)\n    classmaps = preds[\"ClassMapsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"ClassMapsHead\": classmaps_loss,\n    }\n    loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    self.log(\n        \"train_loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n\n    preds = self.model(X)\n    classmaps = preds[\"ClassMapsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"ClassMapsHead\": classmaps_loss,\n    }\n\n    val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n    self.log(\n        \"learning_rate\",\n        lr,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val_loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.visualize_class_maps_example","title":"<code>visualize_class_maps_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_class_maps_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.bottomup_inf_layer(ex)[0]\n    img = (\n        output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    classmaps = (\n        output[\"pred_class_maps\"].cpu().numpy()[0].transpose(1, 2, 0)\n    )  # (n_classes, h, w)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n\n    plot_confmaps(classmaps, output_scale=classmaps.shape[0] / img.shape[0])\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.bottomup_inf_layer(ex)[0]\n    peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n    img = (\n        output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    gt_instances = ex[\"instances\"][0].cpu().numpy()\n    confmaps = (\n        output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n    plt.xlim(plt.xlim())\n    plt.ylim(plt.ylim())\n    plot_peaks(gt_instances, peaks, paired=False)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule","title":"<code>CentroidLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for Centroid Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to centroid model. Centroid models detect the center points of animals in the image, which are then used by Top-Down models for keypoint prediction.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>trainer_accelerator</code> <code>Optional[str]</code> <p>Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").</p> <code>'cpu'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class CentroidLightningModule(LightningModel):\n    \"\"\"Lightning Module for Centroid Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to centroid model. Centroid models detect the center points\n    of animals in the image, which are then used by Top-Down models for keypoint prediction.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        trainer_accelerator: Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        trainer_accelerator: Optional[str] = \"cpu\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            trainer_accelerator=trainer_accelerator,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        self.centroid_inf_layer = CentroidCrop(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            return_confmaps=True,\n            output_stride=self.head_configs.centroid.confmaps.output_stride,\n            input_scale=1.0,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        gt_centroids = ex[\"centroids\"].cpu().numpy()\n        output = self.centroid_inf_layer(ex)\n        peaks = output[\"centroids\"][0].cpu().numpy()\n        img = (\n            output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        confmaps = (\n            output[\"pred_centroid_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n        plot_peaks(gt_centroids, peaks, paired=False)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        return self.model(img)[\"CentroidConfmapsHead\"]\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n            batch[\"centroids_confidence_maps\"], dim=1\n        )\n\n        y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n        loss = nn.MSELoss()(y_preds, y)\n        self.log(\n            \"train_loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n            batch[\"centroids_confidence_maps\"], dim=1\n        )\n\n        y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n        val_loss = nn.MSELoss()(y_preds, y)\n        lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\",\n            lr,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val_loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', trainer_accelerator='cpu', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    trainer_accelerator: Optional[str] = \"cpu\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        trainer_accelerator=trainer_accelerator,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    self.centroid_inf_layer = CentroidCrop(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        return_confmaps=True,\n        output_stride=self.head_configs.centroid.confmaps.output_stride,\n        input_scale=1.0,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    return self.model(img)[\"CentroidConfmapsHead\"]\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n        batch[\"centroids_confidence_maps\"], dim=1\n    )\n\n    y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n    loss = nn.MSELoss()(y_preds, y)\n    self.log(\n        \"train_loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n        batch[\"centroids_confidence_maps\"], dim=1\n    )\n\n    y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n    val_loss = nn.MSELoss()(y_preds, y)\n    lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n    self.log(\n        \"learning_rate\",\n        lr,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val_loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    gt_centroids = ex[\"centroids\"].cpu().numpy()\n    output = self.centroid_inf_layer(ex)\n    peaks = output[\"centroids\"][0].cpu().numpy()\n    img = (\n        output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    confmaps = (\n        output[\"pred_centroid_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n    plot_peaks(gt_centroids, peaks, paired=False)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel","title":"<code>LightningModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Base PyTorch Lightning Module for all sleap-nn models.</p> <p>This class is a sub-class of Torch Lightning Module to configure the training and validation steps.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>trainer_accelerator</code> <code>Optional[str]</code> <p>Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").</p> <code>'cpu'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>configure_optimizers</code> <p>Configure optimiser and learning rate scheduler.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_lightning_model_from_config</code> <p>Get lightning model from config.</p> <code>on_train_epoch_end</code> <p>Configure the train timer at the end of every epoch.</p> <code>on_train_epoch_start</code> <p>Configure the train timer at the beginning of each epoch.</p> <code>on_validation_epoch_end</code> <p>Configure the val timer at the end of every epoch.</p> <code>on_validation_epoch_start</code> <p>Configure the val timer at the beginning of each epoch.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class LightningModel(L.LightningModule):\n    \"\"\"Base PyTorch Lightning Module for all sleap-nn models.\n\n    This class is a sub-class of Torch Lightning Module to configure the training and validation steps.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        trainer_accelerator: Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        trainer_accelerator: Optional[str] = \"cpu\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__()\n        self.model_type = model_type\n        self.backbone_type = backbone_type\n        if not isinstance(backbone_config, DictConfig):\n            backbone_cfg = get_backbone_config(backbone_config)\n            config = OmegaConf.structured(backbone_cfg)\n            OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n            config = DictConfig(config)\n        else:\n            config = backbone_config\n        self.backbone_config = config\n        self.head_configs = head_configs\n        self.pretrained_backbone_weights = pretrained_backbone_weights\n        self.pretrained_head_weights = pretrained_head_weights\n        self.in_channels = self.backbone_config[f\"{self.backbone_type}\"][\"in_channels\"]\n        self.input_expand_channels = self.in_channels\n        self.init_weights = init_weights\n        self.trainer_accelerator = trainer_accelerator\n        if self.trainer_accelerator == \"auto\":\n            if torch.cuda.is_available():\n                self.trainer_accelerator = \"cuda\"\n            elif torch.mps.is_available():\n                self.trainer_accelerator = \"mps\"\n            else:\n                self.trainer_accelerator = \"cpu\"\n        self.lr_scheduler = lr_scheduler\n        self.online_mining = online_mining\n        self.hard_to_easy_ratio = hard_to_easy_ratio\n        self.min_hard_keypoints = min_hard_keypoints\n        self.max_hard_keypoints = max_hard_keypoints\n        self.loss_scale = loss_scale\n        self.optimizer = optimizer\n        self.lr = learning_rate\n        self.amsgrad = amsgrad\n\n        self.model = Model(\n            backbone_type=self.backbone_type,\n            backbone_config=self.backbone_config[f\"{self.backbone_type}\"],\n            head_configs=self.head_configs[self.model_type],\n            model_type=self.model_type,\n        )\n\n        if len(self.head_configs[self.model_type]) &gt; 1:\n            self.loss_weights = [\n                (\n                    self.head_configs[self.model_type][x].loss_weight\n                    if self.head_configs[self.model_type][x].loss_weight is not None\n                    else 1.0\n                )\n                for x in self.head_configs[self.model_type]\n            ]\n\n        self.training_loss = {}\n        self.val_loss = {}\n        self.learning_rate = {}\n\n        # Initialization for encoder and decoder stacks.\n        if self.init_weights == \"xavier\":\n            self.model.apply(xavier_init_weights)\n\n        # Pre-trained weights for the encoder stack - only for swint and convnext\n        if self.backbone_type == \"convnext\" or self.backbone_type == \"swint\":\n            if (\n                self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n                is not None\n            ):\n                ckpt = MODEL_WEIGHTS[\n                    self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n                ].DEFAULT.get_state_dict(progress=True, check_hash=True)\n                self.model.backbone.enc.load_state_dict(ckpt, strict=False)\n\n        # Initializing backbone (encoder + decoder) with trained ckpts\n        if self.pretrained_backbone_weights is not None:\n            logger.info(\n                f\"Loading backbone weights from `{self.pretrained_backbone_weights}` ...\"\n            )\n            if self.pretrained_backbone_weights.endswith(\".ckpt\"):\n                ckpt = torch.load(\n                    self.pretrained_backbone_weights,\n                    map_location=self.trainer_accelerator,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif self.pretrained_backbone_weights.endswith(\".h5\"):\n                # load from sleap model weights\n                load_legacy_model_weights(\n                    self.model.backbone, self.pretrained_backbone_weights\n                )\n\n            else:\n                message = f\"Unsupported file extension for pretrained backbone weights. Please provide a .ckpt or .h5 file.\"\n                logger.error(message)\n                raise ValueError(message)\n\n        # Initializing head layers with trained ckpts.\n        if self.pretrained_head_weights is not None:\n            logger.info(\n                f\"Loading head weights from `{self.pretrained_head_weights}` ...\"\n            )\n            if self.pretrained_head_weights.endswith(\".ckpt\"):\n                ckpt = torch.load(\n                    self.pretrained_head_weights,\n                    map_location=self.trainer_accelerator,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif self.pretrained_head_weights.endswith(\".h5\"):\n                # load from sleap model weights\n                load_legacy_model_weights(\n                    self.model.head_layers, self.pretrained_head_weights\n                )\n\n            else:\n                message = f\"Unsupported file extension for pretrained head weights. Please provide a .ckpt or .h5 file.\"\n                logger.error(message)\n                raise ValueError(message)\n\n    @classmethod\n    def get_lightning_model_from_config(cls, config: DictConfig):\n        \"\"\"Get lightning model from config.\"\"\"\n        model_type = get_model_type_from_cfg(config)\n        backbone_type = get_backbone_type_from_cfg(config)\n\n        lightning_models = {\n            \"single_instance\": SingleInstanceLightningModule,\n            \"centroid\": CentroidLightningModule,\n            \"centered_instance\": TopDownCenteredInstanceLightningModule,\n            \"bottomup\": BottomUpLightningModule,\n            \"multi_class_bottomup\": BottomUpMultiClassLightningModule,\n            \"multi_class_topdown\": TopDownCenteredInstanceMultiClassLightningModule,\n        }\n\n        if model_type not in lightning_models:\n            message = f\"Incorrect model type. Please check if one of the following keys in the head configs is not None: [`single_instance`, `centroid`, `centered_instance`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`]\"\n            logger.error(message)\n            raise ValueError(message)\n\n        lightning_model = lightning_models[model_type](\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=config.model_config.backbone_config,\n            head_configs=config.model_config.head_configs,\n            pretrained_backbone_weights=config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=config.model_config.pretrained_head_weights,\n            init_weights=config.model_config.init_weights,\n            trainer_accelerator=config.trainer_config.trainer_accelerator,\n            lr_scheduler=config.trainer_config.lr_scheduler,\n            online_mining=config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=config.trainer_config.optimizer_name,\n            learning_rate=config.trainer_config.optimizer.lr,\n            amsgrad=config.trainer_config.optimizer.amsgrad,\n        )\n\n        return lightning_model\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        pass\n\n    def on_train_epoch_start(self):\n        \"\"\"Configure the train timer at the beginning of each epoch.\"\"\"\n        self.train_start_time = time.time()\n\n    def on_train_epoch_end(self):\n        \"\"\"Configure the train timer at the end of every epoch.\"\"\"\n        train_time = time.time() - self.train_start_time\n        self.log(\n            \"train_time\",\n            train_time,\n            prog_bar=False,\n            on_step=False,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n\n    def on_validation_epoch_start(self):\n        \"\"\"Configure the val timer at the beginning of each epoch.\"\"\"\n        self.val_start_time = time.time()\n\n    def on_validation_epoch_end(self):\n        \"\"\"Configure the val timer at the end of every epoch.\"\"\"\n        val_time = time.time() - self.val_start_time\n        self.log(\n            \"val_time\",\n            val_time,\n            prog_bar=False,\n            on_step=False,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        pass\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        pass\n\n    def configure_optimizers(self):\n        \"\"\"Configure optimiser and learning rate scheduler.\"\"\"\n        if self.optimizer == \"Adam\":\n            optim = torch.optim.Adam\n        elif self.optimizer == \"AdamW\":\n            optim = torch.optim.AdamW\n\n        optimizer = optim(\n            self.parameters(),\n            lr=self.lr,\n            amsgrad=self.amsgrad,\n        )\n\n        lr_scheduler_cfg = LRSchedulerConfig()\n        if self.lr_scheduler is None:\n            return {\n                \"optimizer\": optimizer,\n            }\n\n        scheduler = None\n        if isinstance(self.lr_scheduler, str):\n            if self.lr_scheduler == \"step_lr\":\n                lr_scheduler_cfg.step_lr = StepLRConfig()\n            elif self.lr_scheduler == \"reduce_lr_on_plateau\":\n                lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig()\n\n        elif isinstance(self.lr_scheduler, dict):\n            lr_scheduler_cfg = self.lr_scheduler\n\n        for k, v in self.lr_scheduler.items():\n            if v is not None:\n                if k == \"step_lr\":\n                    scheduler = torch.optim.lr_scheduler.StepLR(\n                        optimizer=optimizer,\n                        step_size=self.lr_scheduler.step_lr.step_size,\n                        gamma=self.lr_scheduler.step_lr.gamma,\n                    )\n                    break\n                elif k == \"reduce_lr_on_plateau\":\n                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                        optimizer,\n                        mode=\"min\",\n                        threshold=self.lr_scheduler.reduce_lr_on_plateau.threshold,\n                        threshold_mode=self.lr_scheduler.reduce_lr_on_plateau.threshold_mode,\n                        cooldown=self.lr_scheduler.reduce_lr_on_plateau.cooldown,\n                        patience=self.lr_scheduler.reduce_lr_on_plateau.patience,\n                        factor=self.lr_scheduler.reduce_lr_on_plateau.factor,\n                        min_lr=self.lr_scheduler.reduce_lr_on_plateau.min_lr,\n                    )\n                    break\n        if scheduler is None:\n            return {\n                \"optimizer\": optimizer,\n            }\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n            },\n        }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', trainer_accelerator='cpu', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    trainer_accelerator: Optional[str] = \"cpu\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__()\n    self.model_type = model_type\n    self.backbone_type = backbone_type\n    if not isinstance(backbone_config, DictConfig):\n        backbone_cfg = get_backbone_config(backbone_config)\n        config = OmegaConf.structured(backbone_cfg)\n        OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n        config = DictConfig(config)\n    else:\n        config = backbone_config\n    self.backbone_config = config\n    self.head_configs = head_configs\n    self.pretrained_backbone_weights = pretrained_backbone_weights\n    self.pretrained_head_weights = pretrained_head_weights\n    self.in_channels = self.backbone_config[f\"{self.backbone_type}\"][\"in_channels\"]\n    self.input_expand_channels = self.in_channels\n    self.init_weights = init_weights\n    self.trainer_accelerator = trainer_accelerator\n    if self.trainer_accelerator == \"auto\":\n        if torch.cuda.is_available():\n            self.trainer_accelerator = \"cuda\"\n        elif torch.mps.is_available():\n            self.trainer_accelerator = \"mps\"\n        else:\n            self.trainer_accelerator = \"cpu\"\n    self.lr_scheduler = lr_scheduler\n    self.online_mining = online_mining\n    self.hard_to_easy_ratio = hard_to_easy_ratio\n    self.min_hard_keypoints = min_hard_keypoints\n    self.max_hard_keypoints = max_hard_keypoints\n    self.loss_scale = loss_scale\n    self.optimizer = optimizer\n    self.lr = learning_rate\n    self.amsgrad = amsgrad\n\n    self.model = Model(\n        backbone_type=self.backbone_type,\n        backbone_config=self.backbone_config[f\"{self.backbone_type}\"],\n        head_configs=self.head_configs[self.model_type],\n        model_type=self.model_type,\n    )\n\n    if len(self.head_configs[self.model_type]) &gt; 1:\n        self.loss_weights = [\n            (\n                self.head_configs[self.model_type][x].loss_weight\n                if self.head_configs[self.model_type][x].loss_weight is not None\n                else 1.0\n            )\n            for x in self.head_configs[self.model_type]\n        ]\n\n    self.training_loss = {}\n    self.val_loss = {}\n    self.learning_rate = {}\n\n    # Initialization for encoder and decoder stacks.\n    if self.init_weights == \"xavier\":\n        self.model.apply(xavier_init_weights)\n\n    # Pre-trained weights for the encoder stack - only for swint and convnext\n    if self.backbone_type == \"convnext\" or self.backbone_type == \"swint\":\n        if (\n            self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n            is not None\n        ):\n            ckpt = MODEL_WEIGHTS[\n                self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n            ].DEFAULT.get_state_dict(progress=True, check_hash=True)\n            self.model.backbone.enc.load_state_dict(ckpt, strict=False)\n\n    # Initializing backbone (encoder + decoder) with trained ckpts\n    if self.pretrained_backbone_weights is not None:\n        logger.info(\n            f\"Loading backbone weights from `{self.pretrained_backbone_weights}` ...\"\n        )\n        if self.pretrained_backbone_weights.endswith(\".ckpt\"):\n            ckpt = torch.load(\n                self.pretrained_backbone_weights,\n                map_location=self.trainer_accelerator,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif self.pretrained_backbone_weights.endswith(\".h5\"):\n            # load from sleap model weights\n            load_legacy_model_weights(\n                self.model.backbone, self.pretrained_backbone_weights\n            )\n\n        else:\n            message = f\"Unsupported file extension for pretrained backbone weights. Please provide a .ckpt or .h5 file.\"\n            logger.error(message)\n            raise ValueError(message)\n\n    # Initializing head layers with trained ckpts.\n    if self.pretrained_head_weights is not None:\n        logger.info(\n            f\"Loading head weights from `{self.pretrained_head_weights}` ...\"\n        )\n        if self.pretrained_head_weights.endswith(\".ckpt\"):\n            ckpt = torch.load(\n                self.pretrained_head_weights,\n                map_location=self.trainer_accelerator,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif self.pretrained_head_weights.endswith(\".h5\"):\n            # load from sleap model weights\n            load_legacy_model_weights(\n                self.model.head_layers, self.pretrained_head_weights\n            )\n\n        else:\n            message = f\"Unsupported file extension for pretrained head weights. Please provide a .ckpt or .h5 file.\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimiser and learning rate scheduler.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimiser and learning rate scheduler.\"\"\"\n    if self.optimizer == \"Adam\":\n        optim = torch.optim.Adam\n    elif self.optimizer == \"AdamW\":\n        optim = torch.optim.AdamW\n\n    optimizer = optim(\n        self.parameters(),\n        lr=self.lr,\n        amsgrad=self.amsgrad,\n    )\n\n    lr_scheduler_cfg = LRSchedulerConfig()\n    if self.lr_scheduler is None:\n        return {\n            \"optimizer\": optimizer,\n        }\n\n    scheduler = None\n    if isinstance(self.lr_scheduler, str):\n        if self.lr_scheduler == \"step_lr\":\n            lr_scheduler_cfg.step_lr = StepLRConfig()\n        elif self.lr_scheduler == \"reduce_lr_on_plateau\":\n            lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig()\n\n    elif isinstance(self.lr_scheduler, dict):\n        lr_scheduler_cfg = self.lr_scheduler\n\n    for k, v in self.lr_scheduler.items():\n        if v is not None:\n            if k == \"step_lr\":\n                scheduler = torch.optim.lr_scheduler.StepLR(\n                    optimizer=optimizer,\n                    step_size=self.lr_scheduler.step_lr.step_size,\n                    gamma=self.lr_scheduler.step_lr.gamma,\n                )\n                break\n            elif k == \"reduce_lr_on_plateau\":\n                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer,\n                    mode=\"min\",\n                    threshold=self.lr_scheduler.reduce_lr_on_plateau.threshold,\n                    threshold_mode=self.lr_scheduler.reduce_lr_on_plateau.threshold_mode,\n                    cooldown=self.lr_scheduler.reduce_lr_on_plateau.cooldown,\n                    patience=self.lr_scheduler.reduce_lr_on_plateau.patience,\n                    factor=self.lr_scheduler.reduce_lr_on_plateau.factor,\n                    min_lr=self.lr_scheduler.reduce_lr_on_plateau.min_lr,\n                )\n                break\n    if scheduler is None:\n        return {\n            \"optimizer\": optimizer,\n        }\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n        },\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.get_lightning_model_from_config","title":"<code>get_lightning_model_from_config(config)</code>  <code>classmethod</code>","text":"<p>Get lightning model from config.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>@classmethod\ndef get_lightning_model_from_config(cls, config: DictConfig):\n    \"\"\"Get lightning model from config.\"\"\"\n    model_type = get_model_type_from_cfg(config)\n    backbone_type = get_backbone_type_from_cfg(config)\n\n    lightning_models = {\n        \"single_instance\": SingleInstanceLightningModule,\n        \"centroid\": CentroidLightningModule,\n        \"centered_instance\": TopDownCenteredInstanceLightningModule,\n        \"bottomup\": BottomUpLightningModule,\n        \"multi_class_bottomup\": BottomUpMultiClassLightningModule,\n        \"multi_class_topdown\": TopDownCenteredInstanceMultiClassLightningModule,\n    }\n\n    if model_type not in lightning_models:\n        message = f\"Incorrect model type. Please check if one of the following keys in the head configs is not None: [`single_instance`, `centroid`, `centered_instance`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`]\"\n        logger.error(message)\n        raise ValueError(message)\n\n    lightning_model = lightning_models[model_type](\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=config.model_config.backbone_config,\n        head_configs=config.model_config.head_configs,\n        pretrained_backbone_weights=config.model_config.pretrained_backbone_weights,\n        pretrained_head_weights=config.model_config.pretrained_head_weights,\n        init_weights=config.model_config.init_weights,\n        trainer_accelerator=config.trainer_config.trainer_accelerator,\n        lr_scheduler=config.trainer_config.lr_scheduler,\n        online_mining=config.trainer_config.online_hard_keypoint_mining.online_mining,\n        hard_to_easy_ratio=config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n        min_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n        max_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n        loss_scale=config.trainer_config.online_hard_keypoint_mining.loss_scale,\n        optimizer=config.trainer_config.optimizer_name,\n        learning_rate=config.trainer_config.optimizer.lr,\n        amsgrad=config.trainer_config.optimizer.amsgrad,\n    )\n\n    return lightning_model\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_train_epoch_end","title":"<code>on_train_epoch_end()</code>","text":"<p>Configure the train timer at the end of every epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_train_epoch_end(self):\n    \"\"\"Configure the train timer at the end of every epoch.\"\"\"\n    train_time = time.time() - self.train_start_time\n    self.log(\n        \"train_time\",\n        train_time,\n        prog_bar=False,\n        on_step=False,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>Configure the train timer at the beginning of each epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_train_epoch_start(self):\n    \"\"\"Configure the train timer at the beginning of each epoch.\"\"\"\n    self.train_start_time = time.time()\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Configure the val timer at the end of every epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Configure the val timer at the end of every epoch.\"\"\"\n    val_time = time.time() - self.val_start_time\n    self.log(\n        \"val_time\",\n        val_time,\n        prog_bar=False,\n        on_step=False,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_validation_epoch_start","title":"<code>on_validation_epoch_start()</code>","text":"<p>Configure the val timer at the beginning of each epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_validation_epoch_start(self):\n    \"\"\"Configure the val timer at the beginning of each epoch.\"\"\"\n    self.val_start_time = time.time()\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule","title":"<code>SingleInstanceLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for SingleInstance Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to Single Instance model. Single Instance models predict keypoint locations directly from the input image without requiring a separate detection step.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>trainer_accelerator</code> <code>Optional[str]</code> <p>Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").</p> <code>'cpu'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class SingleInstanceLightningModule(LightningModel):\n    \"\"\"Lightning Module for SingleInstance Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps and\n    forward pass specific to Single Instance model. Single Instance models predict keypoint locations\n    directly from the input image without requiring a separate detection step.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        trainer_accelerator: Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        trainer_accelerator: Optional[str] = \"cpu\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            trainer_accelerator=trainer_accelerator,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        self.single_instance_inf_layer = SingleInstanceInferenceModel(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            input_scale=1.0,\n            return_confmaps=True,\n            output_stride=self.head_configs.single_instance.confmaps.output_stride,\n        )\n        self.node_names = self.head_configs.single_instance.confmaps.part_names\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.single_instance_inf_layer(ex)[0]\n        peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n        img = (\n            output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        gt_instances = ex[\"instances\"][0].cpu().numpy()\n        confmaps = (\n            output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n        plot_peaks(gt_instances, peaks, paired=True)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        return self.model(img)[\"SingleInstanceConfmapsHead\"]\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n            batch[\"confidence_maps\"], dim=1\n        )\n\n        y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n\n        loss = nn.MSELoss()(y_preds, y)\n\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            loss = loss + ohkm_loss\n\n        # for part-wise loss\n        if self.node_names is not None:\n            batch_size, _, h, w = y.shape\n            mse = (y - y_preds) ** 2\n            channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n            for node_idx, name in enumerate(self.node_names):\n                self.log(\n                    f\"{name}\",\n                    channel_wise_loss[node_idx],\n                    prog_bar=True,\n                    on_step=True,\n                    on_epoch=True,\n                    logger=True,\n                    sync_dist=True,\n                )\n        self.log(\n            \"train_loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n            batch[\"confidence_maps\"], dim=1\n        )\n\n        y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n        val_loss = nn.MSELoss()(y_preds, y)\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            val_loss = val_loss + ohkm_loss\n        lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\",\n            lr,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val_loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', trainer_accelerator='cpu', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    trainer_accelerator: Optional[str] = \"cpu\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        trainer_accelerator=trainer_accelerator,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    self.single_instance_inf_layer = SingleInstanceInferenceModel(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        input_scale=1.0,\n        return_confmaps=True,\n        output_stride=self.head_configs.single_instance.confmaps.output_stride,\n    )\n    self.node_names = self.head_configs.single_instance.confmaps.part_names\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    return self.model(img)[\"SingleInstanceConfmapsHead\"]\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n        batch[\"confidence_maps\"], dim=1\n    )\n\n    y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n\n    loss = nn.MSELoss()(y_preds, y)\n\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        loss = loss + ohkm_loss\n\n    # for part-wise loss\n    if self.node_names is not None:\n        batch_size, _, h, w = y.shape\n        mse = (y - y_preds) ** 2\n        channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n        for node_idx, name in enumerate(self.node_names):\n            self.log(\n                f\"{name}\",\n                channel_wise_loss[node_idx],\n                prog_bar=True,\n                on_step=True,\n                on_epoch=True,\n                logger=True,\n                sync_dist=True,\n            )\n    self.log(\n        \"train_loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X, y = torch.squeeze(batch[\"image\"], dim=1), torch.squeeze(\n        batch[\"confidence_maps\"], dim=1\n    )\n\n    y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n    val_loss = nn.MSELoss()(y_preds, y)\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        val_loss = val_loss + ohkm_loss\n    lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n    self.log(\n        \"learning_rate\",\n        lr,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val_loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.single_instance_inf_layer(ex)[0]\n    peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n    img = (\n        output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    gt_instances = ex[\"instances\"][0].cpu().numpy()\n    confmaps = (\n        output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n    plot_peaks(gt_instances, peaks, paired=True)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule","title":"<code>TopDownCenteredInstanceLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for TopDownCenteredInstance Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to TopDown Centered instance model. Top-Down models use a two-stage approach: first detecting centroids, then predicting keypoints for each detected centroid.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>trainer_accelerator</code> <code>Optional[str]</code> <p>Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").</p> <code>'cpu'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Perform validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class TopDownCenteredInstanceLightningModule(LightningModel):\n    \"\"\"Lightning Module for TopDownCenteredInstance Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to TopDown Centered instance model. Top-Down models use a two-stage\n    approach: first detecting centroids, then predicting keypoints for each detected centroid.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        trainer_accelerator: Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        trainer_accelerator: Optional[str] = \"cpu\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            trainer_accelerator=trainer_accelerator,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        self.instance_peaks_inf_layer = FindInstancePeaks(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            return_confmaps=True,\n            output_stride=self.head_configs.centered_instance.confmaps.output_stride,\n        )\n\n        self.node_names = self.head_configs.centered_instance.confmaps.part_names\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n        output = self.instance_peaks_inf_layer(ex)\n        peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n        img = (\n            output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        gt_instances = ex[\"instance\"].cpu().numpy()\n        confmaps = (\n            output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n        plot_peaks(gt_instances, peaks, paired=True)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        return self.model(img)[\"CenteredInstanceConfmapsHead\"]\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X, y = torch.squeeze(batch[\"instance_image\"], dim=1), torch.squeeze(\n            batch[\"confidence_maps\"], dim=1\n        )\n\n        y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n\n        loss = nn.MSELoss()(y_preds, y)\n\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            loss = loss + ohkm_loss\n\n        # for part-wise loss\n        if self.node_names is not None:\n            batch_size, _, h, w = y.shape\n            mse = (y - y_preds) ** 2\n            channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n            for node_idx, name in enumerate(self.node_names):\n                self.log(\n                    f\"{name}\",\n                    channel_wise_loss[node_idx],\n                    prog_bar=True,\n                    on_step=True,\n                    on_epoch=True,\n                    logger=True,\n                    sync_dist=True,\n                )\n\n        self.log(\n            \"train_loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Perform validation step.\"\"\"\n        X, y = torch.squeeze(batch[\"instance_image\"], dim=1), torch.squeeze(\n            batch[\"confidence_maps\"], dim=1\n        )\n\n        y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n        val_loss = nn.MSELoss()(y_preds, y)\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            val_loss = val_loss + ohkm_loss\n        lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\",\n            lr,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val_loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', trainer_accelerator='cpu', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    trainer_accelerator: Optional[str] = \"cpu\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        trainer_accelerator=trainer_accelerator,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    self.instance_peaks_inf_layer = FindInstancePeaks(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        return_confmaps=True,\n        output_stride=self.head_configs.centered_instance.confmaps.output_stride,\n    )\n\n    self.node_names = self.head_configs.centered_instance.confmaps.part_names\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    return self.model(img)[\"CenteredInstanceConfmapsHead\"]\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X, y = torch.squeeze(batch[\"instance_image\"], dim=1), torch.squeeze(\n        batch[\"confidence_maps\"], dim=1\n    )\n\n    y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n\n    loss = nn.MSELoss()(y_preds, y)\n\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        loss = loss + ohkm_loss\n\n    # for part-wise loss\n    if self.node_names is not None:\n        batch_size, _, h, w = y.shape\n        mse = (y - y_preds) ** 2\n        channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n        for node_idx, name in enumerate(self.node_names):\n            self.log(\n                f\"{name}\",\n                channel_wise_loss[node_idx],\n                prog_bar=True,\n                on_step=True,\n                on_epoch=True,\n                logger=True,\n                sync_dist=True,\n            )\n\n    self.log(\n        \"train_loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Perform validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Perform validation step.\"\"\"\n    X, y = torch.squeeze(batch[\"instance_image\"], dim=1), torch.squeeze(\n        batch[\"confidence_maps\"], dim=1\n    )\n\n    y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n    val_loss = nn.MSELoss()(y_preds, y)\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        val_loss = val_loss + ohkm_loss\n    lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n    self.log(\n        \"learning_rate\",\n        lr,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val_loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n    output = self.instance_peaks_inf_layer(ex)\n    peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n    img = (\n        output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    gt_instances = ex[\"instance\"].cpu().numpy()\n    confmaps = (\n        output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n    plot_peaks(gt_instances, peaks, paired=True)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule","title":"<code>TopDownCenteredInstanceMultiClassLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for TopDownCenteredInstance ID Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to TopDown Centered instance model. Multi-Class Top-Down models use a two-stage approach: first detecting centroids, then predicting keypoints and classifying instances using supervised learning with ground truth track IDs.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>trainer_accelerator</code> <code>Optional[str]</code> <p>Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").</p> <code>'cpu'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Perform validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class TopDownCenteredInstanceMultiClassLightningModule(LightningModel):\n    \"\"\"Lightning Module for TopDownCenteredInstance ID Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to TopDown Centered instance model. Multi-Class Top-Down models\n    use a two-stage approach: first detecting centroids, then predicting keypoints and\n    classifying instances using supervised learning with ground truth track IDs.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        trainer_accelerator: Training accelerator. One of (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\").\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        trainer_accelerator: Optional[str] = \"cpu\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            trainer_accelerator=trainer_accelerator,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n        self.instance_peaks_inf_layer = TopDownMultiClassFindInstancePeaks(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            return_confmaps=True,\n            output_stride=self.head_configs.multi_class_topdown.confmaps.output_stride,\n        )\n\n        self.node_names = self.head_configs.multi_class_topdown.confmaps.part_names\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n        output = self.instance_peaks_inf_layer(ex)\n        peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n        img = (\n            output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        gt_instances = ex[\"instance\"].cpu().numpy()\n        confmaps = (\n            output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n        )  # convert from (C, H, W) to (H, W, C)\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n        plot_peaks(gt_instances, peaks, paired=True)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        output = self.model(img)\n        return {\n            \"CenteredInstanceConfmapsHead\": output[\"CenteredInstanceConfmapsHead\"],\n            \"ClassVectorsHead\": output[\"ClassVectorsHead\"],\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X = torch.squeeze(batch[\"instance_image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classvector = batch[\"class_vectors\"]\n        preds = self.model(X)\n        classvector = preds[\"ClassVectorsHead\"]\n        confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"CenteredInstanceConfmapsHead\": confmap_loss,\n            \"ClassVectorsHead\": classvector_loss,\n        }\n        loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n\n        # for part-wise loss\n        if self.node_names is not None:\n            batch_size, _, h, w = y_confmap.shape\n            mse = (y_confmap - confmaps) ** 2\n            channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n            for node_idx, name in enumerate(self.node_names):\n                self.log(\n                    f\"{name}\",\n                    channel_wise_loss[node_idx],\n                    prog_bar=True,\n                    on_step=True,\n                    on_epoch=True,\n                    logger=True,\n                    sync_dist=True,\n                )\n\n        self.log(\n            \"train_loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Perform validation step.\"\"\"\n        X = torch.squeeze(batch[\"instance_image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classvector = batch[\"class_vectors\"]\n        preds = self.model(X)\n        classvector = preds[\"ClassVectorsHead\"]\n        confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"CenteredInstanceConfmapsHead\": confmap_loss,\n            \"ClassVectorsHead\": classvector_loss,\n        }\n        val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n\n        lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\",\n            lr,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val_loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=True,\n            logger=True,\n            sync_dist=True,\n        )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', trainer_accelerator='cpu', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    trainer_accelerator: Optional[str] = \"cpu\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        trainer_accelerator=trainer_accelerator,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n    self.instance_peaks_inf_layer = TopDownMultiClassFindInstancePeaks(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        return_confmaps=True,\n        output_stride=self.head_configs.multi_class_topdown.confmaps.output_stride,\n    )\n\n    self.node_names = self.head_configs.multi_class_topdown.confmaps.part_names\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    output = self.model(img)\n    return {\n        \"CenteredInstanceConfmapsHead\": output[\"CenteredInstanceConfmapsHead\"],\n        \"ClassVectorsHead\": output[\"ClassVectorsHead\"],\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X = torch.squeeze(batch[\"instance_image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classvector = batch[\"class_vectors\"]\n    preds = self.model(X)\n    classvector = preds[\"ClassVectorsHead\"]\n    confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"CenteredInstanceConfmapsHead\": confmap_loss,\n        \"ClassVectorsHead\": classvector_loss,\n    }\n    loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n\n    # for part-wise loss\n    if self.node_names is not None:\n        batch_size, _, h, w = y_confmap.shape\n        mse = (y_confmap - confmaps) ** 2\n        channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n        for node_idx, name in enumerate(self.node_names):\n            self.log(\n                f\"{name}\",\n                channel_wise_loss[node_idx],\n                prog_bar=True,\n                on_step=True,\n                on_epoch=True,\n                logger=True,\n                sync_dist=True,\n            )\n\n    self.log(\n        \"train_loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Perform validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Perform validation step.\"\"\"\n    X = torch.squeeze(batch[\"instance_image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classvector = batch[\"class_vectors\"]\n    preds = self.model(X)\n    classvector = preds[\"ClassVectorsHead\"]\n    confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"CenteredInstanceConfmapsHead\": confmap_loss,\n        \"ClassVectorsHead\": classvector_loss,\n    }\n    val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n\n    lr = self.optimizers().optimizer.param_groups[0][\"lr\"]\n    self.log(\n        \"learning_rate\",\n        lr,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val_loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=True,\n        logger=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n    output = self.instance_peaks_inf_layer(ex)\n    peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n    img = (\n        output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    gt_instances = ex[\"instance\"].cpu().numpy()\n    confmaps = (\n        output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n    )  # convert from (C, H, W) to (H, W, C)\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    plot_confmaps(confmaps, output_scale=confmaps.shape[0] / img.shape[0])\n    plot_peaks(gt_instances, peaks, paired=True)\n    return fig\n</code></pre>"},{"location":"api/training/losses/","title":"losses","text":""},{"location":"api/training/losses/#sleap_nn.training.losses","title":"<code>sleap_nn.training.losses</code>","text":"<p>Custom loss functions.</p> <p>Functions:</p> Name Description <code>compute_ohkm_loss</code> <p>Compute the online hard keypoint mining loss.</p>"},{"location":"api/training/losses/#sleap_nn.training.losses.compute_ohkm_loss","title":"<code>compute_ohkm_loss(y_gt, y_pr, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0)</code>","text":"<p>Compute the online hard keypoint mining loss.</p> Source code in <code>sleap_nn/training/losses.py</code> <pre><code>def compute_ohkm_loss(\n    y_gt: torch.Tensor,\n    y_pr: torch.Tensor,\n    hard_to_easy_ratio: float = 2.0,\n    min_hard_keypoints: int = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: float = 5.0,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the online hard keypoint mining loss.\"\"\"\n    if max_hard_keypoints is None:\n        max_hard_keypoints = -1\n    # Compute elementwise squared difference.\n    loss = (y_pr - y_gt) ** 2\n\n    # Store initial shape for normalization.\n    batch_shape = loss.shape\n\n    # Reduce over everything but channels axis.\n    l = torch.sum(loss, dim=(0, 2, 3))\n\n    # Compute the loss for the \"easy\" keypoint.\n    best_loss = torch.min(l)\n\n    # Find the number of hard keypoints.\n    is_hard_keypoint = (l / best_loss) &gt;= hard_to_easy_ratio\n    n_hard_keypoints = torch.sum(is_hard_keypoint.to(torch.int32))\n\n    # Work out the actual final number of keypoints to consider as hard.\n    if max_hard_keypoints &lt; 0:\n        max_hard_keypoints = l.shape[0]\n    else:\n        max_hard_keypoints = min(\n            max_hard_keypoints,\n            l.shape[0],\n        )\n    k = min(\n        max(\n            n_hard_keypoints,\n            min_hard_keypoints,\n        ),\n        max_hard_keypoints,\n    )\n\n    # Pull out the top hard values.\n    k_vals, k_inds = torch.topk(l, k=k, largest=True, sorted=False)\n\n    # Apply weights.\n    k_loss = k_vals * loss_scale\n\n    # Reduce over all channels.\n    n_elements = batch_shape[0] * batch_shape[2] * batch_shape[3] * k\n    k_loss = torch.sum(k_loss) / n_elements\n\n    return k_loss\n</code></pre>"},{"location":"api/training/model_trainer/","title":"model_trainer","text":""},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer","title":"<code>sleap_nn.training.model_trainer</code>","text":"<p>This module is to train a sleap-nn model using Lightning.</p> <p>Classes:</p> Name Description <code>ModelTrainer</code> <p>Train sleap-nn model using PyTorch Lightning.</p>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer","title":"<code>ModelTrainer</code>","text":"<p>Train sleap-nn model using PyTorch Lightning.</p> <p>This class is used to create dataloaders, train a sleap-nn model and save the model checkpoints/ logs with options to logging with wandb and csvlogger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>OmegaConf dictionary which has the following:     (i) data_config: data loading pre-processing configs.     (ii) model_config: backbone and head configs to be passed to <code>Model</code> class.     (iii) trainer_config: trainer configs like accelerator, optimiser params, etc.</p> required <code>train_labels</code> <p>List of <code>sio.Labels</code> objects for training dataset.</p> required <code>val_labels</code> <p>List of <code>sio.Labels</code> objects for validation dataset.</p> required <code>skeletons</code> <p>List of <code>sio.Skeleton</code> objects in a single slp file.</p> required <code>lightning_model</code> <p>One of the child classes of <code>sleap_nn.training.lightning_modules.LightningModel</code>.</p> required <code>model_type</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>trainer</code> <p>Instance of the <code>lightning.Trainer</code> initialized with loggers and callbacks.</p> required <p>Methods:</p> Name Description <code>get_model_trainer_from_config</code> <p>Create a model trainer instance from config.</p> <code>setup_config</code> <p>Compute config parameters.</p> <code>train</code> <p>Train the lightning model.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>@attrs.define\nclass ModelTrainer:\n    \"\"\"Train sleap-nn model using PyTorch Lightning.\n\n    This class is used to create dataloaders, train a sleap-nn model and save the model checkpoints/ logs with options to logging\n    with wandb and csvlogger.\n\n    Args:\n        config: OmegaConf dictionary which has the following:\n                (i) data_config: data loading pre-processing configs.\n                (ii) model_config: backbone and head configs to be passed to `Model` class.\n                (iii) trainer_config: trainer configs like accelerator, optimiser params, etc.\n        train_labels: List of `sio.Labels` objects for training dataset.\n        val_labels: List of `sio.Labels` objects for validation dataset.\n        skeletons: List of `sio.Skeleton` objects in a single slp file.\n        lightning_model: One of the child classes of `sleap_nn.training.lightning_modules.LightningModel`.\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        trainer: Instance of the `lightning.Trainer` initialized with loggers and callbacks.\n    \"\"\"\n\n    config: DictConfig\n    _initial_config: Optional[DictConfig] = None\n    train_labels: List[sio.Labels] = []\n    val_labels: List[sio.Labels] = []\n    skeletons: Optional[List[sio.Skeleton]] = None\n\n    lightning_model: Optional[LightningModel] = None\n    model_type: Optional[str] = None\n    backbone_type: Optional[str] = None\n\n    _profilers: dict = {\n        \"advanced\": AdvancedProfiler(),\n        \"passthrough\": PassThroughProfiler(),\n        \"pytorch\": PyTorchProfiler(),\n        \"simple\": SimpleProfiler(),\n    }\n\n    trainer: Optional[L.Trainer] = None\n\n    @classmethod\n    def get_model_trainer_from_config(\n        cls,\n        config: DictConfig,\n        train_labels: Optional[List[sio.Labels]] = None,\n        val_labels: Optional[List[sio.Labels]] = None,\n    ):\n        \"\"\"Create a model trainer instance from config.\"\"\"\n        model_trainer = cls(config=config)\n\n        model_trainer.model_type = get_model_type_from_cfg(model_trainer.config)\n        model_trainer.backbone_type = get_backbone_type_from_cfg(model_trainer.config)\n\n        if train_labels is None and val_labels is None:\n            # read labels from paths provided in the config\n            train_labels = [\n                sio.load_slp(path)\n                for path in model_trainer.config.data_config.train_labels_path\n            ]\n            val_labels = (\n                [\n                    sio.load_slp(path)\n                    for path in model_trainer.config.data_config.val_labels_path\n                ]\n                if model_trainer.config.data_config.val_labels_path is not None\n                else None\n            )\n            model_trainer._setup_train_val_labels(\n                labels=train_labels, val_labels=val_labels\n            )\n        else:\n            model_trainer._setup_train_val_labels(\n                labels=train_labels, val_labels=val_labels\n            )\n\n        model_trainer._initial_config = model_trainer.config.copy()\n        # update config parameters\n        model_trainer.setup_config()\n\n        # Check if all videos exist across all labels\n        all_videos_exist = all(\n            video.exists(check_all=True)\n            for labels in [*model_trainer.train_labels, *model_trainer.val_labels]\n            for video in labels.videos\n        )\n\n        if not all_videos_exist:\n            raise FileNotFoundError(\n                \"One or more video files do not exist or are not accessible.\"\n            )\n\n        return model_trainer\n\n    def _setup_train_val_labels(\n        self,\n        labels: Optional[List[sio.Labels]] = None,\n        val_labels: Optional[List[sio.Labels]] = None,\n    ):\n        \"\"\"Create train and val labels objects. (Initialize `self.train_labels` and `self.val_labels`).\"\"\"\n        logger.info(f\"Creating train-val split...\")\n        total_train_lfs = 0\n        total_val_lfs = 0\n        self.skeletons = labels[0].skeletons\n\n        # check if all `.slp` file shave same skeleton structure (if multiple slp file paths are provided)\n        skeleton = self.skeletons[0]\n        for index, train_label in enumerate(labels):\n            skel_temp = train_label.skeletons[0]\n            nodes_equal = [node.name for node in skeleton.nodes] == [\n                node.name for node in skel_temp.nodes\n            ]\n            edge_inds_equal = [tuple(edge) for edge in skeleton.edge_inds] == [\n                tuple(edge) for edge in skel_temp.edge_inds\n            ]\n            skeletons_equal = nodes_equal and edge_inds_equal\n            if skeletons_equal:\n                total_train_lfs += len(train_label)\n            else:\n                message = f\"The skeletons in the training labels: {index+1} do not match the skeleton in the first training label file.\"\n                logger.error(message)\n                raise ValueError(message)\n\n        if val_labels is None or not len(val_labels):\n            # if val labels are not provided, split from train\n            total_train_lfs = 0\n            val_fraction = OmegaConf.select(\n                self.config, \"data_config.validation_fraction\", default=0.1\n            )\n            for label in labels:\n                train_split, val_split = label.make_training_splits(\n                    n_train=1 - val_fraction, n_val=val_fraction, seed=42\n                )\n                self.train_labels.append(train_split)\n                self.val_labels.append(val_split)\n                total_train_lfs += len(train_split)\n                total_val_lfs += len(val_split)\n        else:\n            self.train_labels = labels\n            self.val_labels = val_labels\n            for val_l in self.val_labels:\n                total_val_lfs += len(val_l)\n\n        logger.info(f\"# Train Labeled frames: {total_train_lfs}\")\n        logger.info(f\"# Val Labeled frames: {total_val_lfs}\")\n\n    def _setup_preprocessing_config(self):\n        \"\"\"Setup preprocessing config.\"\"\"\n        # compute max_heigt, max_width, and crop_hw (if not provided in the config)\n        max_height = self.config.data_config.preprocessing.max_height\n        max_width = self.config.data_config.preprocessing.max_width\n        if self.model_type == \"centered_instance\":\n            crop_hw = self.config.data_config.preprocessing.crop_hw\n\n        max_h, max_w = 0, 0\n        max_crop_size = 0\n\n        for train_label in self.train_labels:\n            # compute max h and w from slp file if not provided\n            if max_height is None or max_width is None:\n                current_max_h, current_max_w = get_max_height_width(train_label)\n\n                if current_max_h &gt; max_h:\n                    max_h = current_max_h\n                if current_max_w &gt; max_w:\n                    max_w = current_max_w\n\n            if self.model_type == \"centered_instance\":\n                # compute crop size if not provided in config\n                if crop_hw is None:\n\n                    crop_size = find_instance_crop_size(\n                        labels=train_label,\n                        maximum_stride=self.config.model_config.backbone_config[\n                            f\"{self.backbone_type}\"\n                        ][\"max_stride\"],\n                        min_crop_size=self.config.data_config.preprocessing.min_crop_size,\n                        input_scaling=self.config.data_config.preprocessing.scale,\n                    )\n\n                    if crop_size &gt; max_crop_size:\n                        max_crop_size = crop_size\n\n        # if preprocessing params were None, replace with computed params\n        if max_height is None or max_width is None:\n            self.config.data_config.preprocessing.max_height = max_h\n            self.config.data_config.preprocessing.max_width = max_w\n\n        if self.model_type == \"centered_instance\" and crop_hw is None:\n            self.config.data_config.preprocessing.crop_hw = [\n                max_crop_size,\n                max_crop_size,\n            ]\n\n    def _setup_head_config(self):\n        \"\"\"Setup node, edge and class names in head config.\"\"\"\n        # if edges and part names aren't set in head configs, get it from labels object.\n        head_config = self.config.model_config.head_configs[self.model_type]\n        for key in head_config:\n            if \"part_names\" in head_config[key].keys():\n                if head_config[key][\"part_names\"] is None:\n                    self.config.model_config.head_configs[self.model_type][key][\n                        \"part_names\"\n                    ] = self.skeletons[0].node_names\n\n            if \"edges\" in head_config[key].keys():\n                if head_config[key][\"edges\"] is None:\n                    edges = [\n                        (x.source.name, x.destination.name)\n                        for x in self.skeletons[0].edges\n                    ]\n                    self.config.model_config.head_configs[self.model_type][key][\n                        \"edges\"\n                    ] = edges\n\n            if \"classes\" in head_config[key].keys():\n                if head_config[key][\"classes\"] is None:\n                    tracks = []\n                    for train_label in self.train_labels:\n                        tracks.extend(\n                            [x.name for x in train_label.tracks if x is not None]\n                        )\n                    classes = list(set(tracks))\n                    if not len(classes):\n                        message = (\n                            f\"No tracks found. ID models need tracks to be defined.\"\n                        )\n                        logger.error(message)\n                        raise Exception(message)\n                    self.config.model_config.head_configs[self.model_type][key][\n                        \"classes\"\n                    ] = classes\n\n    def _setup_ckpt_path(self):\n        \"\"\"Setup checkpoint path.\"\"\"\n        # if save_ckpt_path is None, assign a new dir name\n        ckpt_path = self.config.trainer_config.save_ckpt_path\n        if ckpt_path is None:\n            trainer_devices = (\n                self.config.trainer_config.trainer_devices\n                if self.config.trainer_config.trainer_devices is not None\n                else \"auto\"\n            )\n            if trainer_devices == \"auto\":\n                if torch.cuda.is_available():\n                    trainer_devices = torch.cuda.device_count()\n                elif torch.backends.mps.is_available():\n                    trainer_devices = 1\n                elif torch.xpu.is_available():\n                    trainer_devices = torch.xpu.device_count()\n                else:\n                    trainer_devices = 1\n            if trainer_devices &gt; 1:\n                ckpt_path = (\n                    f\"{self.model_type}.n={len(self.train_labels)+len(self.val_labels)}\"\n                )\n            else:\n                ckpt_path = (\n                    datetime.now().strftime(\"%y%m%d_%H%M%S\")\n                    + f\".{self.model_type}.n={len(self.train_labels)+len(self.val_labels)}\"\n                )\n\n        # If checkpoint path already exists, add suffix to prevent overwriting\n        if Path(ckpt_path).exists():\n            for i in count(1):\n                new_ckpt_path = f\"{ckpt_path}-{i}\"\n                if not Path(new_ckpt_path).exists():\n                    ckpt_path = new_ckpt_path\n                    break\n\n        self.config.trainer_config.save_ckpt_path = ckpt_path\n\n        # set output dir for cache img\n        if self.config.data_config.data_pipeline_fw == \"torch_dataset_cache_img_disk\":\n            if self.config.data_config.cache_img_path is None:\n                self.config.data_config.cache_img_path = Path(\n                    self.config.trainer_config.save_ckpt_path\n                )\n\n    def _verify_model_input_channels(self):\n        \"\"\"Verify input channels in model_config based on input image and pretrained model weights.\"\"\"\n        # check in channels, verify with img channels / ensure_rgb/ ensure_grayscale\n        if self.train_labels[0] is not None:\n            img_channels = self.train_labels[0][0].image.shape[-1]\n            if self.config.data_config.preprocessing.ensure_rgb:\n                img_channels = 3\n            if self.config.data_config.preprocessing.ensure_grayscale:\n                img_channels = 1\n            if (\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels\n                != img_channels\n            ):\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels = img_channels\n                logger.info(\n                    f\"Updating backbone in_channels to {img_channels} based on the input image channels.\"\n                )\n\n        # verify input img channels with pretrained model ckpts (if any)\n        if (\n            self.backbone_type == \"convnext\" or self.backbone_type == \"swint\"\n        ) and self.config.model_config.backbone_config[\n            f\"{self.backbone_type}\"\n        ].pre_trained_weights is not None:\n            if (\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels\n                != 3\n            ):\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels = 3\n                self.config.data_config.preprocessing.ensure_rgb = True\n                self.config.data_config.preprocessing.ensure_grayscale = False\n                logger.info(\n                    f\"Updating backbone in_channels to 3 based on the pretrained model weights.\"\n                )\n\n        elif (\n            self.backbone_type == \"unet\"\n            and self.config.model_config.pretrained_backbone_weights is not None\n        ):\n\n            if self.config.model_config.pretrained_backbone_weights.endswith(\".ckpt\"):\n                pretrained_backbone_ckpt = torch.load(\n                    self.config.model_config.pretrained_backbone_weights,\n                    map_location=(\n                        self.config.trainer_config.trainer_accelerator\n                        if self.config.trainer_config.trainer_accelerator is not None\n                        or self.config.trainer_config.trainer_accelerator != \"auto\"\n                        else \"cpu\"\n                    ),\n                    weights_only=False,\n                )\n                input_channels = list(pretrained_backbone_ckpt[\"state_dict\"].values())[\n                    0\n                ].shape[\n                    -3\n                ]  # get input channels from first layer\n                if (\n                    self.config.model_config.backbone_config.unet.in_channels\n                    != input_channels\n                ):\n                    self.config.model_config.backbone_config.unet.in_channels = (\n                        input_channels\n                    )\n                    logger.info(\n                        f\"Updating backbone in_channels to {input_channels} based on the pretrained model weights.\"\n                    )\n\n                    if input_channels == 1:\n                        self.config.data_config.preprocessing.ensure_grayscale = True\n                        self.config.data_config.preprocessing.ensure_rgb = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_grayscale to True based on the pretrained model weights.\"\n                        )\n                    elif input_channels == 3:\n                        self.config.data_config.preprocessing.ensure_rgb = True\n                        self.config.data_config.preprocessing.ensure_grayscale = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_rgb to True based on the pretrained model weights.\"\n                        )\n\n            elif self.config.model_config.pretrained_backbone_weights.endswith(\".h5\"):\n                input_channels = get_keras_first_layer_channels(\n                    self.config.model_config.pretrained_backbone_weights\n                )\n                if (\n                    self.config.model_config.backbone_config.unet.in_channels\n                    != input_channels\n                ):\n                    self.config.model_config.backbone_config.unet.in_channels = (\n                        input_channels\n                    )\n                    logger.info(\n                        f\"Updating backbone in_channels to {input_channels} based on the pretrained model weights.\"\n                    )\n\n                    if input_channels == 1:\n                        self.config.data_config.preprocessing.ensure_grayscale = True\n                        self.config.data_config.preprocessing.ensure_rgb = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_grayscale to True based on the pretrained model weights.\"\n                        )\n                    elif input_channels == 3:\n                        self.config.data_config.preprocessing.ensure_rgb = True\n                        self.config.data_config.preprocessing.ensure_grayscale = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_rgb to True based on the pretrained model weights.\"\n                        )\n\n    def setup_config(self):\n        \"\"\"Compute config parameters.\"\"\"\n        # Verify config structure.\n        logger.info(\"Setting up config...\")\n        self.config = verify_training_cfg(self.config)\n\n        # compute preprocessing parameters from the labels objects and fill in the config\n        self._setup_preprocessing_config()\n\n        # save skeleton to config\n        skeleton_yaml = yaml.safe_load(SkeletonYAMLEncoder().encode(self.skeletons))\n        skeleton_names = skeleton_yaml.keys()\n        self.config[\"data_config\"][\"skeletons\"] = []\n        for skeleton_name in skeleton_names:\n            skl = skeleton_yaml[skeleton_name]\n            skl[\"name\"] = skeleton_name\n            self.config[\"data_config\"][\"skeletons\"].append(skl)\n\n        # setup head config - partnames, edges and class names\n        self._setup_head_config()\n\n        # set max stride for the backbone: convnext and swint\n        if self.backbone_type == \"convnext\":\n            self.config.model_config.backbone_config.convnext.max_stride = (\n                self.config.model_config.backbone_config.convnext.stem_patch_stride\n                * (2**3)\n                * 2\n            )\n        elif self.backbone_type == \"swint\":\n            self.config.model_config.backbone_config.swint.max_stride = (\n                self.config.model_config.backbone_config.swint.stem_patch_stride\n                * (2**3)\n                * 2\n            )\n\n        # set output stride for backbone from head config and verify max stride\n        self.config = check_output_strides(self.config)\n\n        # setup checkpoint path\n        self._setup_ckpt_path()\n\n        # verify input_channels in model_config based on input image and pretrained model weights\n        self._verify_model_input_channels()\n\n    def _setup_model_ckpt_dir(self):\n        \"\"\"Create the model ckpt folder.\"\"\"\n        ckpt_path = self.config.trainer_config.save_ckpt_path\n        logger.info(f\"Setting up model ckpt dir: `{ckpt_path}`...\")\n\n        if not Path(ckpt_path).exists():\n            try:\n                Path(ckpt_path).mkdir(parents=True, exist_ok=True)\n            except OSError as e:\n                message = f\"Cannot create a new folder in {ckpt_path}.\\n {e}\"\n                logger.error(message)\n                raise OSError(message)\n\n        if RANK in [0, -1]:\n            for idx, (train, val) in enumerate(zip(self.train_labels, self.val_labels)):\n                train.save(\n                    Path(ckpt_path) / f\"labels_train_gt_{idx}.slp\",\n                    restore_original_videos=False,\n                )\n                val.save(\n                    Path(ckpt_path) / f\"labels_val_gt_{idx}.slp\",\n                    restore_original_videos=False,\n                )\n\n    def _setup_viz_datasets(self):\n        \"\"\"Setup dataloaders.\"\"\"\n        data_viz_config = self.config.copy()\n        data_viz_config.data_config.data_pipeline_fw = \"torch_dataset\"\n\n        return get_train_val_datasets(\n            train_labels=self.train_labels,\n            val_labels=self.val_labels,\n            config=data_viz_config,\n            rank=-1,\n        )\n\n    def _setup_datasets(self):\n        \"\"\"Setup dataloaders.\"\"\"\n        base_cache_img_path = None\n        if self.config.data_config.data_pipeline_fw == \"torch_dataset_cache_img_memory\":\n            # check available memory. If insufficient memory, default to disk caching.\n            mem_available = check_cache_memory(\n                self.train_labels, self.val_labels, memory_buffer=MEMORY_BUFFER\n            )\n            if not mem_available:\n                self.config.data_config.data_pipeline_fw = (\n                    \"torch_dataset_cache_img_disk\"\n                )\n                base_cache_img_path = Path(\"./\")\n                logger.info(\n                    f\"Insufficient memory for in-memory caching. `jpg` files will be created for disk-caching.\"\n                )\n            self.config.data_config.cache_img_path = base_cache_img_path\n\n        elif self.config.data_config.data_pipeline_fw == \"torch_dataset_cache_img_disk\":\n            # Get cache img path\n            base_cache_img_path = (\n                Path(self.config.data_config.cache_img_path)\n                if self.config.data_config.cache_img_path is not None\n                else Path(self.config.trainer_config.save_ckpt_path)\n            )\n\n            if self.config.data_config.cache_img_path is None:\n                self.config.data_config.cache_img_path = base_cache_img_path\n\n        return get_train_val_datasets(\n            train_labels=self.train_labels,\n            val_labels=self.val_labels,\n            config=self.config,\n            rank=self.trainer.global_rank,\n        )\n\n    def _setup_loggers_callbacks(self, viz_train_dataset, viz_val_dataset):\n        \"\"\"Create loggers and callbacks.\"\"\"\n        logger.info(\"Setting up callbacks and loggers...\")\n        loggers = []\n        callbacks = []\n        if self.config.trainer_config.save_ckpt:\n\n            # checkpoint callback\n            checkpoint_callback = ModelCheckpoint(\n                save_top_k=self.config.trainer_config.model_ckpt.save_top_k,\n                save_last=self.config.trainer_config.model_ckpt.save_last,\n                dirpath=self.config.trainer_config.save_ckpt_path,\n                filename=\"best\",\n                monitor=\"val_loss\",\n                mode=\"min\",\n            )\n            callbacks.append(checkpoint_callback)\n\n            # csv log callback\n            csv_log_keys = [\n                \"epoch\",\n                \"train_loss\",\n                \"val_loss\",\n                \"learning_rate\",\n                \"train_time\",\n                \"val_time\",\n            ]\n            if self.model_type in [\n                \"single_instance\",\n                \"centered_instance\",\n                \"multi_class_topdown\",\n            ]:\n                csv_log_keys.extend(self.skeletons[0].node_names)\n            csv_logger = CSVLoggerCallback(\n                filepath=Path(self.config.trainer_config.save_ckpt_path)\n                / \"training_log.csv\",\n                keys=csv_log_keys,\n            )\n            callbacks.append(csv_logger)\n\n        if self.config.trainer_config.early_stopping.stop_training_on_plateau:\n            # early stopping callback\n            callbacks.append(\n                EarlyStopping(\n                    monitor=\"val_loss\",\n                    mode=\"min\",\n                    verbose=False,\n                    min_delta=self.config.trainer_config.early_stopping.min_delta,\n                    patience=self.config.trainer_config.early_stopping.patience,\n                )\n            )\n\n        if self.config.trainer_config.use_wandb:\n            # wandb logger\n            wandb_config = self.config.trainer_config.wandb\n            if wandb_config.wandb_mode == \"offline\":\n                os.environ[\"WANDB_MODE\"] = \"offline\"\n            else:\n                if RANK in [0, -1]:\n                    wandb.login(key=self.config.trainer_config.wandb.api_key)\n            wandb_logger = WandbLogger(\n                entity=wandb_config.entity,\n                project=wandb_config.project,\n                name=wandb_config.name,\n                save_dir=self.config.trainer_config.save_ckpt_path,\n                id=self.config.trainer_config.wandb.prv_runid,\n                group=self.config.trainer_config.wandb.group,\n            )\n            loggers.append(wandb_logger)\n\n            # save the configs as yaml in the checkpoint dir\n            self.config.trainer_config.wandb.api_key = \"\"\n\n        # zmq callbacks\n        controller_address = OmegaConf.select(\n            self.config, \"trainer_config.zmq.controller_address\", default=None\n        )\n        publish_address = OmegaConf.select(\n            self.config, \"trainer_config.zmq.publish_address\", default=None\n        )\n        if controller_address is not None:\n            callbacks.append(TrainingControllerZMQ(address=controller_address))\n        if publish_address is not None:\n            callbacks.append(ProgressReporterZMQ(address=publish_address))\n\n        # viz callbacks\n        if self.config.trainer_config.visualize_preds_during_training:\n            train_viz_pipeline = cycle(viz_train_dataset)\n            val_viz_pipeline = cycle(viz_val_dataset)\n\n            viz_dir = Path(self.config.trainer_config.save_ckpt_path) / \"viz\"\n            if not Path(viz_dir).exists():\n                if RANK in [0, -1]:\n                    Path(viz_dir).mkdir(parents=True, exist_ok=True)\n\n            callbacks.append(\n                MatplotlibSaver(\n                    save_folder=viz_dir,\n                    plot_fn=lambda: self.lightning_model.visualize_example(\n                        next(train_viz_pipeline)\n                    ),\n                    prefix=\"train\",\n                )\n            )\n            callbacks.append(\n                MatplotlibSaver(\n                    save_folder=viz_dir,\n                    plot_fn=lambda: self.lightning_model.visualize_example(\n                        next(val_viz_pipeline)\n                    ),\n                    prefix=\"validation\",\n                )\n            )\n\n            if self.model_type == \"bottomup\":\n                train_viz_pipeline1 = cycle(copy.deepcopy(viz_train_dataset))\n                val_viz_pipeline1 = cycle(copy.deepcopy(viz_val_dataset))\n                callbacks.append(\n                    MatplotlibSaver(\n                        save_folder=viz_dir,\n                        plot_fn=lambda: self.lightning_model.visualize_pafs_example(\n                            next(train_viz_pipeline1)\n                        ),\n                        prefix=\"train.pafs_magnitude\",\n                    )\n                )\n                callbacks.append(\n                    MatplotlibSaver(\n                        save_folder=viz_dir,\n                        plot_fn=lambda: self.lightning_model.visualize_pafs_example(\n                            next(val_viz_pipeline1)\n                        ),\n                        prefix=\"validation.pafs_magnitude\",\n                    )\n                )\n\n            if self.model_type == \"multi_class_bottomup\":\n                train_viz_pipeline1 = cycle(copy.deepcopy(viz_train_dataset))\n                val_viz_pipeline1 = cycle(copy.deepcopy(viz_val_dataset))\n                callbacks.append(\n                    MatplotlibSaver(\n                        save_folder=viz_dir,\n                        plot_fn=lambda: self.lightning_model.visualize_class_maps_example(\n                            next(train_viz_pipeline1)\n                        ),\n                        prefix=\"train.class_maps\",\n                    )\n                )\n                callbacks.append(\n                    MatplotlibSaver(\n                        save_folder=viz_dir,\n                        plot_fn=lambda: self.lightning_model.visualize_class_maps_example(\n                            next(val_viz_pipeline1)\n                        ),\n                        prefix=\"validation.class_maps\",\n                    )\n                )\n\n            if self.config.trainer_config.use_wandb:\n                callbacks.append(\n                    WandBPredImageLogger(\n                        viz_folder=viz_dir,\n                        wandb_run_name=self.config.trainer_config.wandb.name,\n                        is_bottomup=(self.model_type == \"bottomup\"),\n                    )\n                )\n\n        return loggers, callbacks\n\n    def _delete_cache_imgs(self):\n        \"\"\"Delete cache images in disk.\"\"\"\n        base_cache_img_path = Path(self.config.data_config.cache_img_path)\n        train_cache_img_path = Path(base_cache_img_path) / \"train_imgs\"\n        val_cache_img_path = Path(base_cache_img_path) / \"val_imgs\"\n\n        if (train_cache_img_path).exists():\n            logger.info(f\"Deleting cache imgs from `{train_cache_img_path}`...\")\n            shutil.rmtree(\n                (train_cache_img_path).as_posix(),\n                ignore_errors=True,\n            )\n\n        if (val_cache_img_path).exists():\n            logger.info(f\"Deleting cache imgs from `{val_cache_img_path}`...\")\n            shutil.rmtree(\n                (val_cache_img_path).as_posix(),\n                ignore_errors=True,\n            )\n\n    def train(self):\n        \"\"\"Train the lightning model.\"\"\"\n        logger.info(f\"Setting up for training...\")\n        start_setup_time = time.time()\n\n        # initialize the labels object and update config.\n        if not len(self.train_labels) or not len(self.val_labels):\n            self._setup_train_val_labels(self.config)\n            self.setup_config()\n\n        # create the ckpt dir.\n        self._setup_model_ckpt_dir()\n\n        # create the train and val datasets for visualization.\n        viz_train_dataset = None\n        viz_val_dataset = None\n        if self.config.trainer_config.visualize_preds_during_training:\n            logger.info(f\"Setting up visualization train and val datasets...\")\n            viz_train_dataset, viz_val_dataset = self._setup_viz_datasets()\n\n        # setup loggers and callbacks for Trainer.\n        logger.info(f\"Setting up Trainer...\")\n        loggers, callbacks = self._setup_loggers_callbacks(\n            viz_train_dataset=viz_train_dataset, viz_val_dataset=viz_val_dataset\n        )\n        # set up the strategy (for multi-gpu training)\n        strategy = OmegaConf.select(\n            self.config, \"trainer_config.trainer_strategy\", default=\"auto\"\n        )\n        # set up profilers\n        cfg_profiler = self.config.trainer_config.profiler\n        profiler = None\n        if cfg_profiler is not None:\n            if cfg_profiler in self._profilers:\n                profiler = self._profilers[cfg_profiler]\n            else:\n                message = f\"{cfg_profiler} is not a valid option. Please choose one of {list(self._profilers.keys())}\"\n                logger.error(message)\n                raise ValueError(message)\n\n        # create lightning.Trainer instance.\n        self.trainer = L.Trainer(\n            callbacks=callbacks,\n            logger=loggers,\n            enable_checkpointing=self.config.trainer_config.save_ckpt,\n            devices=self.config.trainer_config.trainer_devices,\n            max_epochs=self.config.trainer_config.max_epochs,\n            accelerator=self.config.trainer_config.trainer_accelerator,\n            enable_progress_bar=self.config.trainer_config.enable_progress_bar,\n            strategy=strategy,\n            profiler=profiler,\n            log_every_n_steps=1,\n        )\n\n        self.trainer.strategy.barrier()\n\n        # setup datasets\n        train_dataset, val_dataset = self._setup_datasets()\n\n        # set-up steps per epoch\n        train_steps_per_epoch = self.config.trainer_config.train_steps_per_epoch\n        if train_steps_per_epoch is None:\n            train_steps_per_epoch = get_steps_per_epoch(\n                dataset=train_dataset,\n                batch_size=self.config.trainer_config.train_data_loader.batch_size,\n            )\n        if self.config.trainer_config.min_train_steps_per_epoch &gt; train_steps_per_epoch:\n            train_steps_per_epoch = self.config.trainer_config.min_train_steps_per_epoch\n        self.config.trainer_config.train_steps_per_epoch = train_steps_per_epoch\n\n        val_steps_per_epoch = get_steps_per_epoch(\n            dataset=val_dataset,\n            batch_size=self.config.trainer_config.val_data_loader.batch_size,\n        )\n\n        # set devices and accelrator\n        if (\n            self.config.trainer_config.trainer_devices is None\n            or self.config.trainer_config.trainer_devices == \"auto\"\n        ):\n            self.config.trainer_config.trainer_devices = self.trainer.num_devices\n        if (\n            self.config.trainer_config.trainer_accelerator is None\n            or self.config.trainer_config.trainer_accelerator == \"auto\"\n        ):\n            self.config.trainer_config.trainer_accelerator = (\n                self.trainer.strategy.root_device\n            )\n\n        # initialize the lightning model.\n        # need to initialize after Trainer is initialized (for trainer accelerator)\n        logger.info(f\"Setting up lightning module for {self.model_type} model...\")\n        self.lightning_model = LightningModel.get_lightning_model_from_config(\n            config=self.config\n        )\n        total_params = sum(p.numel() for p in self.lightning_model.parameters())\n        self.config.model_config.total_params = total_params\n\n        # setup dataloaders\n        # need to set up dataloaders after Trainer is initialized (for ddp). DistributedSampler depends on the rank\n        train_dataloader, val_dataloader = get_train_val_dataloaders(\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            config=self.config,\n            rank=self.trainer.global_rank,\n            train_steps_per_epoch=self.config.trainer_config.train_steps_per_epoch,\n            val_steps_per_epoch=val_steps_per_epoch,\n            trainer_devices=self.config.trainer_config.trainer_devices,\n        )\n\n        if self.trainer.global_rank == 0:  # save config only in rank 0 process\n            ckpt_path = self.config.trainer_config.save_ckpt_path\n            OmegaConf.save(\n                self._initial_config,\n                (Path(ckpt_path) / \"initial_config.yaml\").as_posix(),\n            )\n\n            if self.config.trainer_config.use_wandb:\n                if wandb.run is None:\n                    wandb.init(\n                        dir=self.config.trainer_config.save_ckpt_path,\n                        project=self.config.trainer_config.wandb.project,\n                        entity=self.config.trainer_config.wandb.entity,\n                        name=self.config.trainer_config.wandb.name,\n                        id=self.config.trainer_config.wandb.prv_runid,\n                        group=self.config.trainer_config.wandb.group,\n                    )\n                self.config.trainer_config.wandb.current_run_id = wandb.run.id\n                wandb.config[\"run_name\"] = self.config.trainer_config.wandb.name\n                wandb.config[\"run_config\"] = OmegaConf.to_container(\n                    self.config, resolve=True\n                )\n\n            OmegaConf.save(\n                self.config,\n                (\n                    Path(self.config.trainer_config.save_ckpt_path)\n                    / \"training_config.yaml\"\n                ).as_posix(),\n            )\n\n        self.trainer.strategy.barrier()\n\n        try:\n            logger.info(\n                f\"Finished trainer set up. [{time.time() - start_setup_time:.1f}s]\"\n            )\n            logger.info(f\"Starting training loop...\")\n            start_train_time = time.time()\n            self.trainer.fit(\n                self.lightning_model,\n                train_dataloader,\n                val_dataloader,\n                ckpt_path=self.config.trainer_config.resume_ckpt_path,\n            )\n\n        except KeyboardInterrupt:\n            logger.info(\"Stopping training...\")\n\n        finally:\n            logger.info(\n                f\"Finished training loop. [{(time.time() - start_train_time) / 60:.1f} min]\"\n            )\n            if self.trainer.global_rank == 0 and self.config.trainer_config.use_wandb:\n                wandb.finish()\n\n            # delete image disk caching\n            if (\n                self.config.data_config.data_pipeline_fw\n                == \"torch_dataset_cache_img_disk\"\n                and self.config.data_config.delete_cache_imgs_after_training\n            ):\n                if self.trainer.global_rank == 0:\n                    self._delete_cache_imgs()\n\n            # delete viz folder if requested\n            if (\n                self.config.trainer_config.visualize_preds_during_training\n                and not self.config.trainer_config.keep_viz\n            ):\n                if self.trainer.global_rank == 0:\n                    viz_dir = Path(self.config.trainer_config.save_ckpt_path) / \"viz\"\n                    if viz_dir.exists():\n                        logger.info(f\"Deleting viz folder at {viz_dir}...\")\n                        shutil.rmtree(viz_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer.get_model_trainer_from_config","title":"<code>get_model_trainer_from_config(config, train_labels=None, val_labels=None)</code>  <code>classmethod</code>","text":"<p>Create a model trainer instance from config.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>@classmethod\ndef get_model_trainer_from_config(\n    cls,\n    config: DictConfig,\n    train_labels: Optional[List[sio.Labels]] = None,\n    val_labels: Optional[List[sio.Labels]] = None,\n):\n    \"\"\"Create a model trainer instance from config.\"\"\"\n    model_trainer = cls(config=config)\n\n    model_trainer.model_type = get_model_type_from_cfg(model_trainer.config)\n    model_trainer.backbone_type = get_backbone_type_from_cfg(model_trainer.config)\n\n    if train_labels is None and val_labels is None:\n        # read labels from paths provided in the config\n        train_labels = [\n            sio.load_slp(path)\n            for path in model_trainer.config.data_config.train_labels_path\n        ]\n        val_labels = (\n            [\n                sio.load_slp(path)\n                for path in model_trainer.config.data_config.val_labels_path\n            ]\n            if model_trainer.config.data_config.val_labels_path is not None\n            else None\n        )\n        model_trainer._setup_train_val_labels(\n            labels=train_labels, val_labels=val_labels\n        )\n    else:\n        model_trainer._setup_train_val_labels(\n            labels=train_labels, val_labels=val_labels\n        )\n\n    model_trainer._initial_config = model_trainer.config.copy()\n    # update config parameters\n    model_trainer.setup_config()\n\n    # Check if all videos exist across all labels\n    all_videos_exist = all(\n        video.exists(check_all=True)\n        for labels in [*model_trainer.train_labels, *model_trainer.val_labels]\n        for video in labels.videos\n    )\n\n    if not all_videos_exist:\n        raise FileNotFoundError(\n            \"One or more video files do not exist or are not accessible.\"\n        )\n\n    return model_trainer\n</code></pre>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer.setup_config","title":"<code>setup_config()</code>","text":"<p>Compute config parameters.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>def setup_config(self):\n    \"\"\"Compute config parameters.\"\"\"\n    # Verify config structure.\n    logger.info(\"Setting up config...\")\n    self.config = verify_training_cfg(self.config)\n\n    # compute preprocessing parameters from the labels objects and fill in the config\n    self._setup_preprocessing_config()\n\n    # save skeleton to config\n    skeleton_yaml = yaml.safe_load(SkeletonYAMLEncoder().encode(self.skeletons))\n    skeleton_names = skeleton_yaml.keys()\n    self.config[\"data_config\"][\"skeletons\"] = []\n    for skeleton_name in skeleton_names:\n        skl = skeleton_yaml[skeleton_name]\n        skl[\"name\"] = skeleton_name\n        self.config[\"data_config\"][\"skeletons\"].append(skl)\n\n    # setup head config - partnames, edges and class names\n    self._setup_head_config()\n\n    # set max stride for the backbone: convnext and swint\n    if self.backbone_type == \"convnext\":\n        self.config.model_config.backbone_config.convnext.max_stride = (\n            self.config.model_config.backbone_config.convnext.stem_patch_stride\n            * (2**3)\n            * 2\n        )\n    elif self.backbone_type == \"swint\":\n        self.config.model_config.backbone_config.swint.max_stride = (\n            self.config.model_config.backbone_config.swint.stem_patch_stride\n            * (2**3)\n            * 2\n        )\n\n    # set output stride for backbone from head config and verify max stride\n    self.config = check_output_strides(self.config)\n\n    # setup checkpoint path\n    self._setup_ckpt_path()\n\n    # verify input_channels in model_config based on input image and pretrained model weights\n    self._verify_model_input_channels()\n</code></pre>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer.train","title":"<code>train()</code>","text":"<p>Train the lightning model.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>def train(self):\n    \"\"\"Train the lightning model.\"\"\"\n    logger.info(f\"Setting up for training...\")\n    start_setup_time = time.time()\n\n    # initialize the labels object and update config.\n    if not len(self.train_labels) or not len(self.val_labels):\n        self._setup_train_val_labels(self.config)\n        self.setup_config()\n\n    # create the ckpt dir.\n    self._setup_model_ckpt_dir()\n\n    # create the train and val datasets for visualization.\n    viz_train_dataset = None\n    viz_val_dataset = None\n    if self.config.trainer_config.visualize_preds_during_training:\n        logger.info(f\"Setting up visualization train and val datasets...\")\n        viz_train_dataset, viz_val_dataset = self._setup_viz_datasets()\n\n    # setup loggers and callbacks for Trainer.\n    logger.info(f\"Setting up Trainer...\")\n    loggers, callbacks = self._setup_loggers_callbacks(\n        viz_train_dataset=viz_train_dataset, viz_val_dataset=viz_val_dataset\n    )\n    # set up the strategy (for multi-gpu training)\n    strategy = OmegaConf.select(\n        self.config, \"trainer_config.trainer_strategy\", default=\"auto\"\n    )\n    # set up profilers\n    cfg_profiler = self.config.trainer_config.profiler\n    profiler = None\n    if cfg_profiler is not None:\n        if cfg_profiler in self._profilers:\n            profiler = self._profilers[cfg_profiler]\n        else:\n            message = f\"{cfg_profiler} is not a valid option. Please choose one of {list(self._profilers.keys())}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    # create lightning.Trainer instance.\n    self.trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=loggers,\n        enable_checkpointing=self.config.trainer_config.save_ckpt,\n        devices=self.config.trainer_config.trainer_devices,\n        max_epochs=self.config.trainer_config.max_epochs,\n        accelerator=self.config.trainer_config.trainer_accelerator,\n        enable_progress_bar=self.config.trainer_config.enable_progress_bar,\n        strategy=strategy,\n        profiler=profiler,\n        log_every_n_steps=1,\n    )\n\n    self.trainer.strategy.barrier()\n\n    # setup datasets\n    train_dataset, val_dataset = self._setup_datasets()\n\n    # set-up steps per epoch\n    train_steps_per_epoch = self.config.trainer_config.train_steps_per_epoch\n    if train_steps_per_epoch is None:\n        train_steps_per_epoch = get_steps_per_epoch(\n            dataset=train_dataset,\n            batch_size=self.config.trainer_config.train_data_loader.batch_size,\n        )\n    if self.config.trainer_config.min_train_steps_per_epoch &gt; train_steps_per_epoch:\n        train_steps_per_epoch = self.config.trainer_config.min_train_steps_per_epoch\n    self.config.trainer_config.train_steps_per_epoch = train_steps_per_epoch\n\n    val_steps_per_epoch = get_steps_per_epoch(\n        dataset=val_dataset,\n        batch_size=self.config.trainer_config.val_data_loader.batch_size,\n    )\n\n    # set devices and accelrator\n    if (\n        self.config.trainer_config.trainer_devices is None\n        or self.config.trainer_config.trainer_devices == \"auto\"\n    ):\n        self.config.trainer_config.trainer_devices = self.trainer.num_devices\n    if (\n        self.config.trainer_config.trainer_accelerator is None\n        or self.config.trainer_config.trainer_accelerator == \"auto\"\n    ):\n        self.config.trainer_config.trainer_accelerator = (\n            self.trainer.strategy.root_device\n        )\n\n    # initialize the lightning model.\n    # need to initialize after Trainer is initialized (for trainer accelerator)\n    logger.info(f\"Setting up lightning module for {self.model_type} model...\")\n    self.lightning_model = LightningModel.get_lightning_model_from_config(\n        config=self.config\n    )\n    total_params = sum(p.numel() for p in self.lightning_model.parameters())\n    self.config.model_config.total_params = total_params\n\n    # setup dataloaders\n    # need to set up dataloaders after Trainer is initialized (for ddp). DistributedSampler depends on the rank\n    train_dataloader, val_dataloader = get_train_val_dataloaders(\n        train_dataset=train_dataset,\n        val_dataset=val_dataset,\n        config=self.config,\n        rank=self.trainer.global_rank,\n        train_steps_per_epoch=self.config.trainer_config.train_steps_per_epoch,\n        val_steps_per_epoch=val_steps_per_epoch,\n        trainer_devices=self.config.trainer_config.trainer_devices,\n    )\n\n    if self.trainer.global_rank == 0:  # save config only in rank 0 process\n        ckpt_path = self.config.trainer_config.save_ckpt_path\n        OmegaConf.save(\n            self._initial_config,\n            (Path(ckpt_path) / \"initial_config.yaml\").as_posix(),\n        )\n\n        if self.config.trainer_config.use_wandb:\n            if wandb.run is None:\n                wandb.init(\n                    dir=self.config.trainer_config.save_ckpt_path,\n                    project=self.config.trainer_config.wandb.project,\n                    entity=self.config.trainer_config.wandb.entity,\n                    name=self.config.trainer_config.wandb.name,\n                    id=self.config.trainer_config.wandb.prv_runid,\n                    group=self.config.trainer_config.wandb.group,\n                )\n            self.config.trainer_config.wandb.current_run_id = wandb.run.id\n            wandb.config[\"run_name\"] = self.config.trainer_config.wandb.name\n            wandb.config[\"run_config\"] = OmegaConf.to_container(\n                self.config, resolve=True\n            )\n\n        OmegaConf.save(\n            self.config,\n            (\n                Path(self.config.trainer_config.save_ckpt_path)\n                / \"training_config.yaml\"\n            ).as_posix(),\n        )\n\n    self.trainer.strategy.barrier()\n\n    try:\n        logger.info(\n            f\"Finished trainer set up. [{time.time() - start_setup_time:.1f}s]\"\n        )\n        logger.info(f\"Starting training loop...\")\n        start_train_time = time.time()\n        self.trainer.fit(\n            self.lightning_model,\n            train_dataloader,\n            val_dataloader,\n            ckpt_path=self.config.trainer_config.resume_ckpt_path,\n        )\n\n    except KeyboardInterrupt:\n        logger.info(\"Stopping training...\")\n\n    finally:\n        logger.info(\n            f\"Finished training loop. [{(time.time() - start_train_time) / 60:.1f} min]\"\n        )\n        if self.trainer.global_rank == 0 and self.config.trainer_config.use_wandb:\n            wandb.finish()\n\n        # delete image disk caching\n        if (\n            self.config.data_config.data_pipeline_fw\n            == \"torch_dataset_cache_img_disk\"\n            and self.config.data_config.delete_cache_imgs_after_training\n        ):\n            if self.trainer.global_rank == 0:\n                self._delete_cache_imgs()\n\n        # delete viz folder if requested\n        if (\n            self.config.trainer_config.visualize_preds_during_training\n            and not self.config.trainer_config.keep_viz\n        ):\n            if self.trainer.global_rank == 0:\n                viz_dir = Path(self.config.trainer_config.save_ckpt_path) / \"viz\"\n                if viz_dir.exists():\n                    logger.info(f\"Deleting viz folder at {viz_dir}...\")\n                    shutil.rmtree(viz_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/training/utils/","title":"utils","text":""},{"location":"api/training/utils/#sleap_nn.training.utils","title":"<code>sleap_nn.training.utils</code>","text":"<p>Miscellaneous utility functions for training.</p> <p>Functions:</p> Name Description <code>get_dist_rank</code> <p>Return the rank of the current process if torch.distributed is initialized.</p> <code>imgfig</code> <p>Create a tight figure for image plotting.</p> <code>is_distributed_initialized</code> <p>Check if distributed processes are initialized.</p> <code>plot_confmaps</code> <p>Plot confidence maps reduced over channels.</p> <code>plot_img</code> <p>Plot an image in a tight figure.</p> <code>plot_peaks</code> <p>Plot ground truth and detected peaks.</p> <code>xavier_init_weights</code> <p>Function to initilaise the model weights with Xavier initialization method.</p>"},{"location":"api/training/utils/#sleap_nn.training.utils.get_dist_rank","title":"<code>get_dist_rank()</code>","text":"<p>Return the rank of the current process if torch.distributed is initialized.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def get_dist_rank():\n    \"\"\"Return the rank of the current process if torch.distributed is initialized.\"\"\"\n    return dist.get_rank() if is_distributed_initialized() else None\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.imgfig","title":"<code>imgfig(size=6, dpi=72, scale=1.0)</code>","text":"<p>Create a tight figure for image plotting.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>float | tuple</code> <p>Scalar or 2-tuple specifying the (width, height) of the figure in inches. If scalar, will assume equal width and height.</p> <code>6</code> <code>dpi</code> <code>int</code> <p>Dots per inch, controlling the resolution of the image.</p> <code>72</code> <code>scale</code> <code>float</code> <p>Factor to scale the size of the figure by. This is a convenience for increasing the size of the plot at the same DPI.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def imgfig(\n    size: float | tuple = 6, dpi: int = 72, scale: float = 1.0\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Create a tight figure for image plotting.\n\n    Args:\n        size: Scalar or 2-tuple specifying the (width, height) of the figure in inches.\n            If scalar, will assume equal width and height.\n        dpi: Dots per inch, controlling the resolution of the image.\n        scale: Factor to scale the size of the figure by. This is a convenience for\n            increasing the size of the plot at the same DPI.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    if not isinstance(size, (tuple, list)):\n        size = (size, size)\n    fig = plt.figure(figsize=(scale * size[0], scale * size[1]), dpi=dpi)\n    ax = fig.add_axes([0, 0, 1, 1], frameon=False)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.autoscale(tight=True)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.grid(False)\n    return fig\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.is_distributed_initialized","title":"<code>is_distributed_initialized()</code>","text":"<p>Check if distributed processes are initialized.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def is_distributed_initialized():\n    \"\"\"Check if distributed processes are initialized.\"\"\"\n    return dist.is_available() and dist.is_initialized()\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.plot_confmaps","title":"<code>plot_confmaps(confmaps, output_scale=1.0)</code>","text":"<p>Plot confidence maps reduced over channels.</p> <p>Parameters:</p> Name Type Description Default <code>confmaps</code> <code>ndarray</code> <p>Confidence maps to plot with shape (height, width, channel).</p> required <code>output_scale</code> <code>float</code> <p>Factor to scale the size of the figure by.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def plot_confmaps(confmaps: np.ndarray, output_scale: float = 1.0):\n    \"\"\"Plot confidence maps reduced over channels.\n\n    Args:\n        confmaps: Confidence maps to plot with shape (height, width, channel).\n        output_scale: Factor to scale the size of the figure by.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    ax = plt.gca()\n    return ax.imshow(\n        np.squeeze(confmaps.max(axis=-1)),\n        alpha=0.5,\n        origin=\"upper\",\n        vmin=0,\n        vmax=1,\n        extent=[\n            -0.5,\n            confmaps.shape[1] / output_scale - 0.5,\n            confmaps.shape[0] / output_scale - 0.5,\n            -0.5,\n        ],\n    )\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.plot_img","title":"<code>plot_img(img, dpi=72, scale=1.0)</code>","text":"<p>Plot an image in a tight figure.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Image to plot of shape (height, width, channel).</p> required <code>dpi</code> <code>int</code> <p>Dots per inch, controlling the resolution of the image.</p> <code>72</code> <code>scale</code> <code>float</code> <p>Factor to scale the size of the figure by. This is a convenience for increasing the size of the plot at the same DPI.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def plot_img(\n    img: np.ndarray, dpi: int = 72, scale: float = 1.0\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot an image in a tight figure.\n\n    Args:\n        img: Image to plot of shape (height, width, channel).\n        dpi: Dots per inch, controlling the resolution of the image.\n        scale: Factor to scale the size of the figure by. This is a convenience for\n            increasing the size of the plot at the same DPI.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    if hasattr(img, \"numpy\"):\n        img = img.numpy()\n\n    if img.shape[0] == 1:\n        # Squeeze out batch singleton dimension.\n        img = img.squeeze(axis=0)\n\n    # Check if image is grayscale (single channel).\n    grayscale = img.shape[-1] == 1\n    if grayscale:\n        # Squeeze out singleton channel.\n        img = img.squeeze(axis=-1)\n\n    # Normalize the range of pixel values.\n    img_min = img.min()\n    img_max = img.max()\n    if img_min &lt; 0.0 or img_max &gt; 1.0:\n        img = (img - img_min) / (img_max - img_min)\n\n    fig = imgfig(\n        size=(float(img.shape[1]) / dpi, float(img.shape[0]) / dpi),\n        dpi=dpi,\n        scale=scale,\n    )\n\n    ax = fig.gca()\n    ax.imshow(\n        img,\n        cmap=\"gray\" if grayscale else None,\n        origin=\"upper\",\n        extent=[-0.5, img.shape[1] - 0.5, img.shape[0] - 0.5, -0.5],\n    )\n    return fig\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.plot_peaks","title":"<code>plot_peaks(pts_gt, pts_pr=None, paired=False)</code>","text":"<p>Plot ground truth and detected peaks.</p> <p>Parameters:</p> Name Type Description Default <code>pts_gt</code> <code>ndarray</code> <p>Ground-truth keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2).</p> required <code>pts_pr</code> <code>ndarray | None</code> <p>Predicted keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2)</p> <code>None</code> <code>paired</code> <code>bool</code> <p>True if error lines should be plotted else False.</p> <code>False</code> <p>Returns:</p> Type Description <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def plot_peaks(\n    pts_gt: np.ndarray, pts_pr: np.ndarray | None = None, paired: bool = False\n):\n    \"\"\"Plot ground truth and detected peaks.\n\n    Args:\n        pts_gt: Ground-truth keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2).\n        pts_pr: Predicted keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2)\n        paired: True if error lines should be plotted else False.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    handles = []\n    ax = plt.gca()\n    if paired and pts_pr is not None:\n        for pt_gt, pt_pr in zip(pts_gt, pts_pr):\n            for p_gt, p_pr in zip(pt_gt, pt_pr):\n                handles.append(\n                    ax.plot(\n                        [p_gt[0], p_pr[0]], [p_gt[1], p_pr[1]], \"r-\", alpha=0.5, lw=2\n                    )\n                )\n    if pts_pr is not None:\n        handles.append(\n            ax.plot(\n                pts_gt[..., 0].ravel(),\n                pts_gt[..., 1].ravel(),\n                \"g.\",\n                alpha=0.7,\n                ms=10,\n                mew=1,\n                mec=\"w\",\n            )\n        )\n        handles.append(\n            ax.plot(\n                pts_pr[..., 0].ravel(),\n                pts_pr[..., 1].ravel(),\n                \"r.\",\n                alpha=0.7,\n                ms=10,\n                mew=1,\n                mec=\"w\",\n            )\n        )\n    else:\n        cmap = sns.color_palette(\"tab20\")\n        for i, pts in enumerate(pts_gt):\n            handles.append(\n                ax.plot(\n                    pts[:, 0],\n                    pts[:, 1],\n                    \".\",\n                    alpha=0.7,\n                    ms=15,\n                    mew=1,\n                    mfc=cmap[i % len(cmap)],\n                    mec=\"w\",\n                )\n            )\n    return handles\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.xavier_init_weights","title":"<code>xavier_init_weights(x)</code>","text":"<p>Function to initilaise the model weights with Xavier initialization method.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def xavier_init_weights(x):\n    \"\"\"Function to initilaise the model weights with Xavier initialization method.\"\"\"\n    if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear):\n        nn.init.xavier_uniform_(x.weight)\n        nn.init.constant_(x.bias, 0)\n</code></pre>"}]}