{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SLEAP-NN","text":"<p>SLEAP-NN is the PyTorch backend for SLEAP, providing neural network training and inference for multi-animal pose estimation. It offers an end-to-end workflow from labeled data to tracked predictions, with seamless integration into SLEAP's GUI and command-line tools.</p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Multiple model types \u2013 Single instance, top-down, bottom-up, and centroid models</li> <li>Modern backbones \u2013 UNet, ConvNeXt, and Swin Transformer architectures</li> <li>Multi-GPU training \u2013 PyTorch Lightning with DDP support</li> <li>Production export \u2013 ONNX and TensorRT for fast deployment</li> <li>Flexible config \u2013 Hydra/OmegaConf for reproducible experiments</li> </ul> <p>Let's start SLEAP-NNing! \ud83d\udc2d\ud83d\udc2d</p>"},{"location":"#explore-the-docs","title":"\ud83d\udcda Explore the Docs","text":"<ul> <li> <p>\ud83d\ude80 Quick Start</p> <p>Install and train your first model in 5 minutes.</p> <p> Get Started</p> </li> <li> <p>\ud83d\udcd6 Tutorials</p> <p>Quick start, first model tutorial, and example notebooks.</p> <p> Learn</p> </li> <li> <p>\ud83d\udcda Guides</p> <p>Training, inference, and tracking workflows.</p> <p> Explore</p> </li> <li> <p>\u2699\ufe0f Configuration</p> <p>Customize your training config.</p> <p> Configure</p> </li> <li> <p>\ud83d\udcbb CLI Reference</p> <p>Command-line interface documentation.</p> <p> Commands</p> </li> <li> <p>\ud83d\udd27 API Reference</p> <p>Full Python API documentation.</p> <p> API Docs</p> </li> </ul>"},{"location":"#coming-from-sleap-v15","title":"\ud83d\udd04 Coming from SLEAP &lt; v1.5?","text":"<p> Load legacy models for inference</p> <p> Convert legacy config</p> SLEAP &lt; v1.5 SLEAP-NN TensorFlow/Keras PyTorch/Lightning Single GPU Multi-GPU (DDP) Limited export ONNX + TensorRT"},{"location":"#get-help","title":"Get Help","text":"<ul> <li> <p> FAQ</p> <p>Common questions answered. View FAQ</p> </li> <li> <p> Report Issues</p> <p>Found a bug? Create an issue</p> </li> <li> <p> Discussions</p> <p>Questions? Start a discussion</p> </li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v010","title":"v0.1.0SummaryInstallationBreaking ChangesNew FeaturesPerformance ImprovementsBug FixesDocumentationChangelog","text":"SLEAP-NN v0.1.0 Release Notes <p>We are excited to announce SLEAP-NN v0.1.0, the first stable release of the v0.1.x series! This major release brings significant improvements across the entire stack: simplified installation, faster data pipelines, multi-GPU training support, ONNX/TensorRT export, and comprehensive documentation.</p> <p>Key highlights:</p> <ul> <li>Simplified Installation: One-command install with automatic GPU detection via <code>--torch-backend auto</code></li> <li>2x Faster Data Pipeline: New Skia-based augmentation backend replaces Kornia</li> <li>Multi-GPU Training: Full DDP support with synchronized caching and callbacks</li> <li>ONNX/TensorRT Export: 3-6x faster inference with optimized model formats</li> <li>51x Faster Peak Refinement: Optimized tensor indexing for centroid/instance finding</li> <li>Parallel Image Caching: Multi-threaded caching for faster training startup</li> <li>Real-time Evaluation: Epoch-end metrics logged directly to WandB</li> <li>Revamped Documentation: Comprehensive guides, tutorials, and API reference</li> </ul> Install with uv (Recommended) <pre># Automatic GPU detection (CUDA, MPS or CPU)\nuv tool install sleap-nn[torch] --torch-backend auto</pre> Verify Installation <pre>sleap-nn --version\n# Expected output: 0.1.0\n\nsleap-nn system\n# Shows full system diagnostics including GPU info</pre> Optional Dependencies <pre># ONNX export (CPU inference)\nuv tool install \"sleap-nn[torch,export]\" --torch-backend auto\n\n# ONNX export (GPU inference)\nuv tool install \"sleap-nn[torch,export-gpu]\" --torch-backend auto\n\n# TensorRT support (Linux/Windows only)\nuv tool install \"sleap-nn[torch,tensorrt]\" --torch-backend auto</pre> 1. Crop Size Semantics for Top-Down Models <p>The scaling behavior for top-down (centered-instance) models has changed:</p> Aspect Old Behavior New Behavior Order Resize full image first, then crop Crop first, then resize <code>crop_size</code> meaning Region size in scaled coordinates Region size in original image coordinates <p>Migration: Review your <code>crop_size</code> configuration values. Previously trained models may produce different results.</p> 2. Model Run Folder File Naming <p>File naming conventions have been standardized:</p> Old Pattern New Pattern <code>labels_train_gt_0.slp</code> <code>labels_gt.train.0.slp</code> <code>labels_val_gt_0.slp</code> <code>labels_gt.val.0.slp</code> <code>pred_train_0.slp</code> <code>labels_pr.train.0.slp</code> <code>pred_val_0.slp</code> <code>labels_pr.val.0.slp</code> <code>train_0_pred_metrics.npz</code> <code>metrics.train.0.npz</code> <code>val_0_pred_metrics.npz</code> <code>metrics.val.0.npz</code> 3. <code>load_metrics()</code> API Changes Change Old New Parameter name <code>model_path</code> <code>path</code> Default split <code>\"val\"</code> <code>\"test\"</code> 4. Video Path Mapping CLI Syntax <pre># Old syntax (no longer works)\nsleap-nn train -c config --video-path-map \"/old/path-&gt;/new/path\"\n\n# New syntax\nsleap-nn train -c config --video-path-map /old/path /new/path</pre> Simplified Installation (#405) <p>Install with automatic GPU detection:</p> <pre>uv tool install sleap-nn[torch] --torch-backend auto</pre> <p>Supports CUDA 11.8, 12.8, 13.0, Apple Silicon (MPS), and CPU-only installations.</p> Skia-Based Augmentation Backend (#431, #434) <p>Replaced Kornia with Skia-python for 2x faster augmentation:</p> Backend Throughput Relative Kornia 142 samples/sec 1.0x Skia 285 samples/sec 2.0x <p>The new pipeline also maintains uint8 images until GPU transfer, achieving 4x bandwidth savings.</p> Multi-GPU Training Support (#435, #436, #437, #453) <p>Full DDP (Distributed Data Parallel) support:</p> <pre># Train on multiple GPUs\nsleap-nn train config.yaml trainer_config.devices=4 trainer_config.strategy=ddp</pre> <p>Features:</p> <ul> <li>Synchronized run_name generation across workers</li> <li>Proper GPU device ordering to prevent NCCL errors</li> <li>DDP-compatible callbacks with barrier synchronization</li> <li>Subprocess-based launcher for reliable multi-GPU caching</li> </ul> ONNX/TensorRT Export (#418, #456) <p>Export trained models for optimized inference:</p> <pre># Export to ONNX\nsleap-nn export /path/to/model -o exports/my_model --format onnx\n\n# Export to TensorRT FP16\nsleap-nn export /path/to/model -o exports/my_model --format both\n\n# Run inference on exported model\nsleap-nn predict exports/my_model video.mp4 -o predictions.slp</pre> <p>Performance (NVIDIA RTX A6000, batch size 8):</p> Model PyTorch TensorRT FP16 Speedup single_instance 3,111 FPS 11,039 FPS 3.5x topdown 94 FPS 525 FPS 5.6x bottomup 113 FPS 524 FPS 4.6x Parallel Image Caching (#432) <p>Multi-threaded caching for faster training startup:</p> <pre>data_config:\n  preprocessing:\n    parallel_caching: true\n    cache_workers: 4  # Number of parallel caching threads</pre> Epoch-End Evaluation Metrics (#414, #449) <p>Real-time evaluation metrics logged to WandB during training:</p> <pre>trainer_config:\n  eval:\n    enabled: true\n    frequency: 1  # Evaluate every epoch</pre> <p>Metrics include: mOKS, mAP, mAR, PCK@5, PCK@10, distance percentiles, and visibility precision/recall.</p> Post-Inference Filtering (#420) <p>Remove overlapping/duplicate predictions:</p> <pre>sleap-nn track -i video.mp4 -m model/ \\\n    --filter_overlapping \\\n    --filter_overlapping_method oks \\\n    --filter_overlapping_threshold 0.5</pre> Simplified Train CLI (#429) <p>Training can now be started with a single config file path:</p> <pre># Simple usage\nsleap-nn train path/to/config.yaml\n\n# With overrides\nsleap-nn train config.yaml trainer_config.max_epochs=100</pre> GUI Integration (#424) <p>New <code>--gui</code> flag for JSON progress output:</p> <pre>sleap-nn track --data_path video.mp4 --model_paths model/ --gui</pre> Warmup Learning Rate Schedulers (#442) <p>New scheduler options:</p> <ul> <li><code>linear_warmup_cosine_annealing</code>: Linear warmup followed by cosine decay</li> <li><code>linear_warmup_linear_decay</code>: Linear warmup followed by linear decay</li> </ul> System Diagnostics (#391) <p>New diagnostic commands:</p> <pre>sleap-nn --version  # Show version\nsleap-nn system     # Full system diagnostics</pre> Provenance Metadata (#407) <p>Inference outputs now include full reproducibility metadata in SLP files.</p> 51x Faster Peak Refinement (#426) <p>Replaced kornia's <code>crop_and_resize</code> with fast tensor indexing:</p> Platform Before After Speedup MPS (Apple Silicon) 21.45 ms 0.42 ms 51x CUDA (RTX A6000) 2.64 ms 0.15 ms 17x GPU-Accelerated Normalization (#406) <p>Image normalization now runs on GPU, reducing PCIe bandwidth by 4x:</p> Image Size Before After Speedup 1024x1280 grayscale 55.2 FPS 64.7 FPS 17% 3307x3304 RGB 6.7 FPS 10.1 FPS 50% Default Augmentations (#445, #447) <p>Augmentations are now enabled by default with sensible presets:</p> <ul> <li>Rotation: -15 to +15 degrees</li> <li>Scale: 0.9 to 1.1</li> </ul> <ul> <li>#428: Fixed skip connection channel mismatch in ConvNext/SwinT decoders</li> <li>#429: Fixed crop device mismatch during top-down inference</li> <li>#423: Fixed CSV logger not capturing learning_rate</li> <li>#436: Fixed multi-GPU DDP duplicate GPU detection error</li> <li>#437: Fixed DDP synchronization in training callbacks</li> <li>#439: Fixed confusing weight loading logging for legacy models</li> <li>#440: Fixed cache memory estimation to account for DataLoader workers</li> <li>#441: Fixed self.log warnings when no logger is configured</li> <li>#451: Fixed critical bugs in weight verification</li> <li>#454: Fixed caching progress bar not showing in subprocess</li> <li>#382: Fixed max_instances handling in centroid-only inference</li> <li>#385: Fixed crash on frames with empty instances</li> <li>#392: Clean up run folder when training canceled via GUI</li> <li>#395, #401, #402: Various WandB visualization fixes</li> </ul> Revamped Documentation (#444, #455) <p>Completely restructured documentation at nn.sleap.ai:</p> <ul> <li>Tutorials: Quick Start guide, Your First Model walkthrough</li> <li>Guides: Training, Inference, Evaluation, Tracking, Export, Multi-GPU</li> <li>Configuration Reference: Detailed docs for data, model, and trainer configs</li> <li>API Reference: Auto-generated from docstrings</li> <li>Evaluation Metrics Reference (#448): Comprehensive guide to all metrics</li> </ul> Features PR Title #405 Add CUDA 13 support and simplify installation with --torch-backend #418 Add ONNX/TensorRT export module #420 Add post-inference filtering for overlapping instances #424 Add --gui flag for JSON progress output in inference #429 Add --config flag for simpler train CLI #431 Add Skia-based augmentation backend for faster data pipeline #432 Add parallel image caching for faster dataset preparation #433 Add UnifiedVizCallback for consolidated visualization outputs #442 Add warmup learning rate schedulers #445 Enable default augmentations with rotation and scale #449 Add centroid-specific evaluation callback with distance-based metrics #453 Add multi-GPU training support with subprocess-based run_name sync #456 Add TensorRT as uv-managed optional dependency Performance PR Title #406 Optimize inference by deferring normalization to GPU #426 Replace kornia crop_and_resize with fast tensor indexing (17-51x speedup) #434 Fix uint8 pipeline to achieve 4x GPU bandwidth savings Bug Fixes PR Title #423 Fix CSV logger not capturing learning_rate #428 Fix skip connection channel mismatch in ConvNext/SwinT decoders #435 Fix multi-GPU disk caching run_name synchronization issue #436 Fix multi-GPU DDP duplicate GPU detection error #437 Fix DDP synchronization in training callbacks #439 Fix confusing weight loading logging for legacy models #440 Fix cache memory estimation to account for DataLoader workers #441 Fix self.log warnings when no logger is configured #451 Fix critical bugs in weight verification, docs, and tests #454 Fix caching progress bar not showing in subprocess Documentation PR Title #390 Add CLI reference page and Colab notebooks #419 Add Exporting guide to How-to guides section #425 Add prerelease alias to docs deployment #444 Revamp documentation structure and content #448 Add evaluation metrics reference page to docs #455 Fix docs issues and add installation instructions Breaking Changes PR Title #381 Fix crop size behavior for top-down models #389 Fix train CLI path replacement syntax #408 Standardize model run folder file naming conventions #409 Improve load_metrics with format compatibility and flexible paths <p>Full Changelog: v0.0.5...v0.1.0</p>"},{"location":"changelog/#v010a4","title":"v0.1.0a4SummaryWhat's New in v0.1.0a4InstallationUpgrading from v0.1.0a3Changelog","text":"v0.1.0a4 Release Notes <p>This pre-release focuses on bug fixes, performance improvements, and CLI usability enhancements:</p> <ul> <li>Simpler Train CLI: New <code>--config</code> flag and positional config support for <code>sleap-nn train</code></li> <li>17-51x Faster Peak Refinement: Replaced kornia-based cropping with fast tensor indexing</li> <li>ConvNext/SwinT Bug Fix: Fixed skip connection channel mismatch that broke training with these backbones</li> <li>GUI Integration: New <code>--gui</code> flag for SLEAP frontend progress reporting</li> </ul> <p>For the full list of major features, breaking changes, and improvements introduced in the v0.1.0 series, see the v0.1.0a0 release notes.</p> Features Simplified Train CLI (#429) <p>Training can now be started with a single config file path:</p> <pre># NEW: Positional config path\nsleap-nn train path/to/config.yaml\n\n# NEW: --config flag\nsleap-nn train --config path/to/config.yaml\n\n# With Hydra overrides\nsleap-nn train config.yaml trainer_config.max_epochs=100\n\n# Legacy flags still work\nsleap-nn train --config-dir /path/to/dir --config-name myconfig</pre> <p>The CLI now uses <code>rich-click</code> for styled help output with better formatting and readability.</p> GUI Progress Mode (#424) <p>New <code>--gui</code> flag enables JSON progress output for SLEAP GUI integration:</p> <pre>sleap-nn track --data_path video.mp4 --model_paths model/ --gui</pre> <p>Output format:</p> <pre>{\"n_processed\": 100, \"n_total\": 1410, \"rate\": 38.4, \"eta\": 34.1}\n{\"n_processed\": 200, \"n_total\": 1410, \"rate\": 39.2, \"eta\": 30.8}</pre> <p>This enables real-time progress updates when running inference from the SLEAP GUI.</p> Performance 17-51x Faster Peak Refinement (#426) <p>Replaced kornia's <code>crop_and_resize</code> with fast tensor indexing for peak refinement:</p> Platform Before After Speedup MPS (M-series Mac) 21.45 ms 0.42 ms 51x CUDA (RTX A6000) 2.64 ms 0.15 ms 17x <p>This also enables integral refinement on Mac - the MPS workaround that disabled it has been removed.</p> Bug Fixes ConvNext/SwinT Skip Connection Fix (#428) <p>Fixed <code>RuntimeError: Given groups=1, weight of size [X, Y, 3, 3], expected input to have Y channels</code> when training with ConvNext or SwinT backbones.</p> <p>What was broken: Training with ConvNext/SwinT backbones crashed during validation due to channel mismatch in skip connections. The decoder assumed skip channels matched computed decoder filters, but ConvNext/SwinT encoder stages have different channel counts.</p> <p>Impact: Users can now successfully train models with ConvNext and SwinT backbones. All 24 architecture tests pass.</p> Crop Device Mismatch Fix (#429) <p>Fixed <code>RuntimeError: indices should be either on cpu or on the same device as the indexed tensor</code> during top-down inference when bboxes tensor was on GPU but images were on CPU.</p> CSV Learning Rate Logging Fix (#423) <p>Fixed regression from v0.1.0a2 where <code>learning_rate</code> column in <code>training_log.csv</code> was always empty.</p> <p>What was broken: PR #417 changed learning rate logging from <code>lr-Adam</code> to <code>train/lr</code>, but the CSV logger only checked for the old format.</p> <p>Now: The CSV logger checks for <code>train/lr</code> (new format), <code>lr-*</code> (legacy), and <code>learning_rate</code> (direct) in that order. Also adds model-specific loss columns for better parity with wandb logging.</p> GUI Progress 99% Fix (#429) <p>Fixed inference progress ending at 99% instead of 100% in GUI mode. The throttled progress reporting was skipping the final update.</p> Documentation Prerelease Docs Alias (#425) <p>Pre-release documentation is now accessible at both:</p> <ul> <li>Version-specific: <code>https://sleap.ai/sleap-nn/v0.1.0a4/</code></li> <li>Alias: <code>https://sleap.ai/sleap-nn/prerelease/</code></li> </ul> Internal Test Suite Optimization (#427) <p>Optimized the 10 slowest tests for faster CI runs:</p> Test Before After Improvement test_main_cli 54.44s 21.76s 60% test_bottomup_predictor 6.71s 1.76s 74% test_predict_main 15.97s 5.35s 67% <p>Total estimated savings: ~55% reduction for slowest tests.</p> <p>This is an alpha pre-release. Pre-releases are excluded by default per PEP 440 - you must explicitly opt in.</p> Install with uv (Recommended) <pre># With --prerelease flag (requires uv 0.9.20+)\nuv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow\n\n# Or pin to exact version\nuv tool install \"sleap-nn[torch]==0.1.0a4\" --torch-backend auto</pre> Run with uvx (One-off execution) <pre>uvx --from \"sleap-nn[torch]\" --prerelease=allow --torch-backend auto sleap-nn system</pre> Verify Installation <pre>sleap-nn --version\n# Expected output: 0.1.0a4\n\nsleap-nn system\n# Shows full system diagnostics including GPU info</pre> <p>If you already have v0.1.0a3 installed with <code>--prerelease=allow</code>:</p> <pre># Simple upgrade (retains original settings like --prerelease=allow)\nuv tool upgrade sleap-nn</pre> <p>To force a complete reinstall:</p> <pre>uv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow --force</pre> PR Category Title #423 Bug Fix Fix CSV logger not capturing learning_rate #424 Feature Add --gui flag for JSON progress output in inference #425 Documentation Add prerelease alias to docs deployment #426 Performance Replace kornia crop_and_resize with fast tensor indexing #427 Internal Optimize slow tests for faster CI runs #428 Bug Fix Fix skip connection channel mismatch in ConvNext/SwinT decoders #429 Feature Add --config flag for simpler train CLI + fix crop device mismatch <p>Full Changelog: v0.1.0a3...v0.1.0a4</p>"},{"location":"changelog/#v010a3","title":"v0.1.0a3SummaryWhat's New in v0.1.0a3InstallationUpgrading from v0.1.0a2Changelog","text":"<p>This pre-release adds powerful new capabilities for high-performance inference and post-processing:</p> <ul> <li>ONNX/TensorRT Export: Export trained models to optimized formats for 3-6x faster inference</li> <li>Post-Inference Filtering: Remove overlapping/duplicate predictions using IOU or OKS similarity</li> <li>Improved WandB Logging: Better metrics organization and run naming</li> </ul> <p>For the full list of major features, breaking changes, and improvements introduced in the v0.1.0 series, see the v0.1.0a0 release notes.</p> Features ONNX/TensorRT Export Module (#418) <p>A complete model export system for high-performance inference:</p> <pre># Export to ONNX\nsleap-nn export /path/to/model -o exports/my_model --format onnx\n\n# Export to both ONNX and TensorRT FP16\nsleap-nn export /path/to/model -o exports/my_model --format both\n\n# Run inference on exported model\nsleap-nn predict exports/my_model video.mp4 -o predictions.slp</pre> <p>Performance Benchmarks (NVIDIA RTX A6000):</p> <p>Batch size 1 (latency-optimized):</p> Model Resolution PyTorch ONNX-GPU TensorRT FP16 Speedup single_instance 192\u00d7192 1.8 ms 1.3 ms 0.31 ms 5.9x centroid 1024\u00d71024 2.5 ms 2.7 ms 0.77 ms 3.2x topdown 1024\u00d71024 11.4 ms 9.7 ms 2.31 ms 4.9x bottomup 1024\u00d71280 12.3 ms 9.6 ms 2.52 ms 4.9x multiclass_topdown 1024\u00d71024 8.3 ms 9.1 ms 1.84 ms 4.5x multiclass_bottomup 1024\u00d71024 9.4 ms 9.4 ms 2.64 ms 3.6x <p>Batch size 8 (throughput-optimized):</p> Model Resolution PyTorch ONNX-GPU TensorRT FP16 Speedup single_instance 192\u00d7192 3,111 FPS 3,165 FPS 11,039 FPS 3.5x centroid 1024\u00d71024 453 FPS 474 FPS 1,829 FPS 4.0x topdown 1024\u00d71024 94 FPS 122 FPS 525 FPS 5.6x bottomup 1024\u00d71280 113 FPS 121 FPS 524 FPS 4.6x multiclass_topdown 1024\u00d71024 127 FPS 145 FPS 735 FPS 5.8x multiclass_bottomup 1024\u00d71024 116 FPS 120 FPS 470 FPS 4.1x <p>Speedup is relative to PyTorch baseline.</p> <p>Supported model types:</p> <ul> <li>Single Instance, Centroid, Centered Instance</li> <li>Top-Down (combined centroid + instance)</li> <li>Bottom-Up (multi-instance with PAF grouping)</li> <li>Multi-class Top-Down and Bottom-Up (with identity classification)</li> </ul> <p>New CLI commands:</p> <ul> <li><code>sleap-nn export</code> - Export models to ONNX/TensorRT</li> <li><code>sleap-nn predict</code> - Run inference on exported models</li> </ul> <p>New optional dependencies:</p> <pre>uv pip install \"sleap-nn[export]\"      # ONNX CPU inference\nuv pip install \"sleap-nn[export-gpu]\"  # ONNX GPU inference\nuv pip install \"sleap-nn[tensorrt]\"    # TensorRT support</pre> <p>See the Export Guide for full documentation.</p> Post-Inference Filtering for Overlapping Instances (#420) <p>New capability to remove duplicate/overlapping pose predictions after model inference:</p> <pre># Filter with IOU method (default)\nsleap-nn track -i video.mp4 -m model/ --filter_overlapping\n\n# Use OKS method with custom threshold\nsleap-nn track -i video.mp4 -m model/ \\\n    --filter_overlapping \\\n    --filter_overlapping_method oks \\\n    --filter_overlapping_threshold 0.5</pre> <p>New CLI options for <code>sleap-nn track</code>:</p> Option Default Description <code>--filter_overlapping</code> <code>False</code> Enable filtering using greedy NMS <code>--filter_overlapping_method</code> <code>iou</code> Similarity method: <code>iou</code> (bbox) or <code>oks</code> (keypoints) <code>--filter_overlapping_threshold</code> <code>0.8</code> Similarity threshold (lower = more aggressive) <p>Programmatic API:</p> <pre>from sleap_nn.inference.postprocessing import filter_overlapping_instances\n\nlabels = filter_overlapping_instances(labels, threshold=0.5, method=\"oks\")</pre> <p>Why use this? Previously, IOU-based filtering only existed in the tracking pipeline. This feature allows filtering overlapping predictions without requiring <code>--tracking</code>.</p> Improvements WandB Run Naming and Metrics Logging (#417) <ul> <li>Fixed run naming: WandB runs now correctly use auto-generated run names</li> <li>Improved metrics organization: All metrics use <code>/</code> separator for automatic panel grouping in WandB UI: <ul> <li><code>train/loss</code>, <code>train/lr</code> - Training metrics (epoch x-axis)</li> <li><code>val/loss</code> - Validation metrics (epoch x-axis)</li> <li><code>eval/val/</code> - Epoch-end evaluation metrics</li> <li><code>eval/test.X/</code> - Post-training test set metrics</li> </ul> </li> <li>New metrics logged: <ul> <li><code>train/lr</code> - Learning rate (useful for monitoring LR schedulers)</li> <li><code>PCK@5</code>, <code>PCK@10</code> - PCK at 5px and 10px thresholds</li> <li><code>distance/p95</code>, <code>distance/p99</code> - Additional distance percentiles</li> </ul> </li> </ul> Documentation <ul> <li>Exporting Guide (#419): Added comprehensive export documentation to How-to guides navigation</li> </ul> <p>This is an alpha pre-release. Pre-releases are excluded by default per PEP 440 - you must explicitly opt in.</p> Install with uv (Recommended) <pre># With --prerelease flag (requires uv 0.9.20+)\nuv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow\n\n# Or pin to exact version\nuv tool install \"sleap-nn[torch]==0.1.0a3\" --torch-backend auto</pre> Run with uvx (One-off execution) <pre>uvx --from \"sleap-nn[torch]\" --prerelease=allow --torch-backend auto sleap-nn system</pre> Verify Installation <pre>sleap-nn --version\n# Expected output: 0.1.0a3\n\nsleap-nn system\n# Shows full system diagnostics including GPU info</pre> <p>If you already have v0.1.0a2 installed with <code>--prerelease=allow</code>:</p> <pre># Simple upgrade (retains original settings like --prerelease=allow)\nuv tool upgrade sleap-nn</pre> <p>To force a complete reinstall:</p> <pre>uv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow --force</pre> PR Category Title #417 Improvement Fix wandb run naming and improve metrics logging #418 Feature Add ONNX/TensorRT export module #419 Documentation Add Exporting guide to How-to guides section #420 Feature Add post-inference filtering for overlapping instances <p>Full Changelog: v0.1.0a2...v0.1.0a3</p>"},{"location":"changelog/#v010a2","title":"v0.1.0a2SummaryWhat's New in v0.1.0a2InstallationUpgrading from v0.1.0a1Changelog","text":"<p>This pre-release adds real-time evaluation metrics during training and improves video matching robustness in the evaluation pipeline:</p> <ul> <li>Epoch-End Evaluation: New metrics logged to WandB at the end of each validation epoch (mOKS, mAP, mAR, PCK, distance metrics)</li> <li>Robust Video Matching: Improved evaluation video matching using sleap-io's <code>Labels.match()</code> API</li> </ul> <p>For the full list of major features, breaking changes, and improvements introduced in the v0.1.0 series, see the v0.1.0a0 release notes.</p> Features <ul> <li> <p>Epoch-End Evaluation Metrics (#414): Real-time evaluation metrics are now computed at the end of each validation epoch and logged to WandB. This enables monitoring training quality without waiting for post-training evaluation.</p> <p>New metrics logged:</p> Metric Description <code>val_mOKS</code> Mean Object Keypoint Similarity [0-1] <code>val_oks_voc_mAP</code> VOC-style mean Average Precision [0-1] <code>val_oks_voc_mAR</code> VOC-style mean Average Recall [0-1] <code>val_avg_distance</code> Mean Euclidean distance error (pixels) <code>val_p50_distance</code> Median Euclidean distance error (pixels) <code>val_mPCK</code> Mean Percentage of Correct Keypoints [0-1] <code>val_visibility_precision</code> Precision for visible keypoint detection <code>val_visibility_recall</code> Recall for visible keypoint detection <p>Enable in your training config:</p> <pre>trainer_config:\n  eval:\n    enabled: true      # Enable epoch-end evaluation\n    frequency: 1       # Evaluate every epoch (or higher for less frequent)\n    oks_stddev: 0.025  # OKS standard deviation parameter</pre> </li> </ul> Improvements <ul> <li>Robust Video Matching in Evaluation (#415): The evaluation module now uses sleap-io's <code>Labels.match()</code> API for more robust video matching between ground truth and prediction labels. This fixes several common failure scenarios: <ul> <li>Embedded videos (<code>.pkg.slp</code>) with different internal paths</li> <li>Cross-platform path differences (Windows vs Linux)</li> <li>Renamed or moved video files</li> </ul> </li> </ul> Bug Fixes <ul> <li>Embedded video handling (#414): <code>get_instances()</code> now correctly handles embedded videos that lack <code>backend.filename</code> attributes, preventing errors during evaluation.</li> <li>Centroid model ground truth matching (#414): Centroid models now properly match centroids to ground truth instances for epoch-end evaluation.</li> <li>Bottom-up training stability (#414): Added <code>max_peaks_per_node=100</code> guardrail to prevent combinatorial explosion when noisy early-training confidence maps produce spurious peaks.</li> </ul> Dependencies <ul> <li>sleap-io: Minimum version bumped from <code>&gt;=0.6.0</code> to <code>&gt;=0.6.2</code> for <code>Labels.match()</code> API support</li> </ul> <p>This is an alpha pre-release. Pre-releases are excluded by default per PEP 440 - you must explicitly opt in.</p> Install with uv (Recommended) <pre># With --prerelease flag (requires uv 0.9.20+)\nuv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow\n\n# Or pin to exact version\nuv tool install \"sleap-nn[torch]==0.1.0a2\" --torch-backend auto</pre> Run with uvx (One-off execution) <pre>uvx --from \"sleap-nn[torch]\" --prerelease=allow --torch-backend auto sleap-nn system</pre> Verify Installation <pre>sleap-nn --version\n# Expected output: 0.1.0a2\n\nsleap-nn system\n# Shows full system diagnostics including GPU info</pre> <p>If you already have v0.1.0a1 installed with <code>--prerelease=allow</code>:</p> <pre># Simple upgrade (retains original settings like --prerelease=allow)\nuv tool upgrade sleap-nn</pre> <p>To force a complete reinstall:</p> <pre>uv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow --force</pre> PR Category Title #414 Feature Add epoch-end evaluation metrics to WandB logging #415 Improvement Use sleap-io Labels.match() API for robust video matching in evaluation <p>Full Changelog: v0.1.0a1...v0.1.0a2</p>"},{"location":"changelog/#v010a1","title":"v0.1.0a1SummaryWhat's New in v0.1.0a1InstallationUpgrading from v0.1.0a0Changelog","text":"<p>This pre-release is a minor update to v0.1.0a0 with quality-of-life improvements for training workflows:</p> <ul> <li>Progress Feedback: Rich progress bar during dataset caching eliminates the \"freeze\" after startup</li> <li>Disk Space Management: Automatic cleanup of WandB local logs (saves GB of disk space per run)</li> </ul> <p>For the full list of major features, breaking changes, and improvements introduced in the v0.1.0 series, see the v0.1.0a0 release notes.</p> Features <ul> <li>WandB Local Log Cleanup (#412): Added <code>delete_local_logs</code> option to <code>WandBConfig</code> that automatically deletes the local <code>wandb/</code> folder after training completes. By default, logs are automatically deleted when syncing online and kept when logging offline. This can save several GB of disk space per training run. Set <code>trainer_config.wandb.delete_local_logs=false</code> to keep local logs.</li> </ul> Improvements <ul> <li>Training Startup Progress Bar (#411): Added a rich progress bar during dataset caching to provide visual feedback during training startup. Previously, there was no indication while images were being cached to disk or memory after the \"Input image shape\" log message.</li> <li>Simplified Log Format (#411): Cleaned up log output by removing module names and log level fields for more user-friendly output.</li> </ul> <p>This is an alpha pre-release. Pre-releases are excluded by default per PEP 440 - you must explicitly opt in.</p> Install with uv (Recommended) <pre># With --prerelease flag (requires uv 0.9.20+)\nuv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow\n\n# Or pin to exact version\nuv tool install \"sleap-nn[torch]==0.1.0a1\" --torch-backend auto</pre> Run with uvx (One-off execution) <pre>uvx --from \"sleap-nn[torch]\" --prerelease=allow --torch-backend auto sleap-nn system</pre> Verify Installation <pre>sleap-nn --version\n# Expected output: 0.1.0a1\n\nsleap-nn system\n# Shows full system diagnostics including GPU info</pre> <p>If you already have v0.1.0a0 installed with <code>--prerelease=allow</code>:</p> <pre># Simple upgrade (retains original settings like --prerelease=allow)\nuv tool upgrade sleap-nn</pre> <p>To force a complete reinstall:</p> <pre>uv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow --force</pre> PR Category Title #411 Improvement Improve logging during training startup #412 Feature Add option to clean up wandb local logs after training <p>Full Changelog: v0.1.0a0...v0.1.0a1</p>"},{"location":"changelog/#v010a0","title":"v0.1.0a0SummaryInstallationBreaking ChangesPerformance ImprovementsNew FeaturesBug FixesImprovementsChangelog","text":"<p>This pre-release introduces major improvements to sleap-nn including simplified installation, enhanced training controls, comprehensive inference provenance, and significant performance optimizations. It also includes several breaking changes that warrant testing before the stable v0.1.0 release.</p> <p>Key highlights:</p> <ul> <li>Simplified Installation: New <code>--torch-backend auto</code> flag for automatic GPU detection</li> <li>CUDA 13.0 Support: Full support for latest CUDA version</li> <li>GPU-accelerated Inference: Up to 50% faster inference via GPU normalization</li> <li>Provenance Tracking: Full reproducibility metadata in output SLP files</li> <li>Enhanced Training Controls: Independent augmentation probabilities, auto crop padding</li> <li>System Diagnostics: New <code>sleap-nn system</code> command for troubleshooting</li> </ul> <p>This is an alpha pre-release. Pre-releases are excluded by default per PEP 440 - you must explicitly opt in.</p> Install with uv (Recommended) <pre># With --prerelease flag (requires uv 0.9.20+)\nuv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow\n\n# Or pin to exact version\nuv tool install \"sleap-nn[torch]==0.1.0a0\" --torch-backend auto</pre> Install with uvx (One-off execution) <pre>uvx --from \"sleap-nn[torch]\" --prerelease=allow --torch-backend auto sleap-nn system</pre> Install with pip <pre>pip install --pre sleap-nn[torch] --index-url https://pypi.org/simple --extra-index-url https://download.pytorch.org/whl/cu128</pre> Verify Installation <pre>sleap-nn --version\n# Expected output: 0.1.0a0\n\nsleap-nn system\n# Shows full system diagnostics including GPU info</pre> 1. Crop Size Semantics for Top-Down Models (PR #381) <p>Impact: High - affects model training and inference</p> <p>The scaling behavior for top-down (centered-instance) models has changed:</p> Aspect Old Behavior New Behavior Order Resize full image first, then crop Crop first, then resize <code>crop_size</code> meaning Region size in scaled coordinates Region size in original image coordinates <p>Migration: Review your <code>crop_size</code> configuration values. Previously trained models may produce different results.</p> 2. Model Run Folder File Naming (PR #408) <p>Impact: Medium - affects scripts that read model outputs</p> <p>File naming conventions standardized:</p> Old Pattern New Pattern <code>labels_train_gt_0.slp</code> <code>labels_gt.train.0.slp</code> <code>labels_val_gt_0.slp</code> <code>labels_gt.val.0.slp</code> <code>pred_train_0.slp</code> <code>labels_pr.train.0.slp</code> <code>pred_val_0.slp</code> <code>labels_pr.val.0.slp</code> <code>train_0_pred_metrics.npz</code> <code>metrics.train.0.npz</code> <code>val_0_pred_metrics.npz</code> <code>metrics.val.0.npz</code> 3. <code>load_metrics()</code> API Changes (PR #409) <p>Impact: Low - affects programmatic metrics loading</p> Change Old New Parameter name <code>model_path</code> <code>path</code> Default split <code>\"val\"</code> <code>\"test\"</code> 4. Video Path Mapping CLI Syntax (PR #389) <p>Impact: Low - affects CLI users with path remapping</p> <pre># Old syntax (no longer works)\nsleap-nn train -c config --video-path-map \"/old/path-&gt;/new/path\"\n\n# New syntax\nsleap-nn train -c config --video-path-map /old/path /new/path</pre> GPU-Accelerated Normalization (PR #406) <p>Image normalization now runs on GPU, reducing PCIe bandwidth by 4x.</p> Image Size Before After Speedup 1024x1280 grayscale 55.2 FPS 64.7 FPS 17% 3307x3304 RGB 6.7 FPS 10.1 FPS 50% <ul> <li>Simplified Installation (PR #405): <code>uv tool install sleap-nn[torch] --torch-backend auto</code></li> <li>CUDA 13.0 Support (PR #405): New <code>--torch-backend cu130</code> option</li> <li>System Diagnostics (PR #391): <code>sleap-nn system</code> command and <code>--version</code> flag</li> <li>Provenance Metadata (PR #407): Full reproducibility tracking in output SLP files</li> <li>Video Path Remapping (PR #387, #389): Remap paths at training time</li> <li>Frame Filtering (PR #396, #397): <code>--exclude_user_labeled</code> and <code>--only_predicted_frames</code></li> <li>Enhanced Data Pipeline (PR #394): Auto crop padding, independent augmentation probabilities</li> <li>Multiple Test Files (PR #383): Evaluate against multiple test datasets</li> <li>Enhanced WandB (PR #393, #395, #401): Interactive visualizations, per-head loss logging</li> <li>Centroid Confmaps (PR #386): Return centroid confidence maps in top-down inference</li> </ul> <ul> <li>#382: Fixed max_instances handling in centroid-only inference</li> <li>#385: Fixed crash on frames with empty instances</li> <li>#395: Fixed WandB visualization issues</li> <li>#397: Fixed <code>--exclude_user_labeled</code> being ignored with <code>--video_index</code></li> <li>#401: Fixed PAF visualization scaling</li> <li>#402: Fixed WandB deprecation warning</li> <li>#394: Fixed user_instances_only handling bugs</li> </ul> <ul> <li>#380: Use sleap-io built-in video matching methods</li> <li>#390: Added CLI reference page and Colab notebooks to docs</li> <li>#392: Run folders cleaned up when training canceled via GUI</li> <li>#398: Comprehensive test coverage improvements</li> <li>#400: WandB URL reported via ZMQ on train start</li> <li>#403: Migrated dev deps to PEP 735 dependency-groups</li> </ul> PR Category Title #380 Improvement Use sleap-io built-in video matching methods #381 Breaking Fix crop size behavior for top-down models #382 Fix Fix max_instances handling in centroid-only inference #383 Feature Support list of paths for test_file_path #384 Feature Add source image to FindInstancePeaksGroundTruth output #385 Fix Fix running inference on frames with empty instances #386 Feature Return centroid confmaps when running topdown inference #387 Feature Add video path remapping options to train CLI #389 Breaking Fix train CLI path replacement syntax #390 Docs Add CLI reference page and Colab notebooks #391 Feature Add system diagnostics command and --version flag #392 Fix Clean up run folder when training is canceled via GUI #393 Feature Improve wandb visualization with slider support #394 Feature Enhance data pipeline with auto crop padding #395 Fix Fix wandb visualization issues #396 Feature Add --exclude_user_labeled and --only_predicted_frames flags #397 Fix Fix --exclude_user_labeled flag being ignored with --video_index #398 Tests Add comprehensive test coverage #400 Feature Report WandB URL via ZMQ on train start #401 Feature Add per-head loss logging and fix PAF visualization #402 Fix Fix wandb deprecation warning #403 Improvement Move dev dependencies to PEP 735 dependency-groups #405 Feature Add CUDA 13 support and simplify installation #406 Performance Optimize inference by deferring normalization to GPU #407 Feature Add provenance metadata to inference output SLP files #408 Breaking Standardize model run folder file naming #409 Breaking Improve load_metrics with format compatibility <p>Full Changelog: v0.0.5...v0.1.0a0</p>"},{"location":"changelog/#v005","title":"v0.0.5SummaryMajor changesChangelog","text":"<p>This release includes important bug fixes, usability improvements, and configuration enhancements. Key highlights include automatic video-specific output naming for multi-video predictions, improved progress tracking, better handling of edge cases in configuration files, and enhanced security for API key storage.</p> New Features Progress Bar for Tracking (#366) <p>Added visual progress tracking during tracking operations, providing real-time feedback on tracking progress for better user experience.</p> Video-Specific Output Paths (#378) <p>When running inference with the <code>video_index</code> parameter on multi-video .slp files, output files now automatically include the video name to prevent overwrites. Previously, all predictions would save to the same path (e.g., <code>labels.predictions.slp</code>), requiring users to manually specify unique output paths. Now, predictions are saved with the format <code>&lt;labels_file&gt;.&lt;video_name&gt;.predictions.slp</code>, enabling seamless batch processing of multiple videos from the same project file.</p> Bug Fixes Resume Checkpoint Mapping (#370) <p>Fixed checkpoint mapping when resuming training from PyTorch model checkpoints, ensuring proper state restoration for torch models.</p> Metrics Format Compatibility (#371) <p>Updated metrics saving format to match SLEAP 1.4 specifications and eliminated code duplication in metrics handling, ensuring cross-compatibility between SLEAP-NN and SLEAP 1.4.</p> Configuration Parameter Handling (#377) <p>Improved handling of <code>run_name</code> and <code>ckpt_dir</code> configuration parameters when set to empty strings or the string literal \"None\" in YAML files. This prevents unexpected behavior and ensures consistent defaults are applied.</p> Security Improvements API Key Protection (#372) <p>WandB API keys are now automatically masked when saving <code>initial_config.yaml</code> files, preventing accidental exposure of sensitive credentials in saved configurations.</p> Configuration &amp; Training Improvements Optimized Default Parameters (#374, #375) <p>Updated default trainer configuration parameters based on extensive training experiments, improving training stability and convergence behavior out of the box.</p> Documentation Dependency Update Instructions (#376) <p>Added comprehensive instructions for updating dependencies across all installation methods (GPU, CPU, and Apple Silicon), making it easier for users to maintain up-to-date environments.</p> <ul> <li>Add progress bar to tracker by @gitttt-1234 in #366</li> <li>Fix resume checkpoint mapping for torch models only by @gitttt-1234 in #370</li> <li>Fix metrics saving format to match SLEAP 1.4 and eliminate code duplication by @gitttt-1234 in #371</li> <li>Mask wandb API key in initial_config.yaml by @gitttt-1234 in #372</li> <li>Update default trainer configuration parameters for improved training stability by @gitttt-1234 in #374</li> <li>Update default configuration values for improved training by @gitttt-1234 in #375</li> <li>Add dependency update instructions for all installation methods by @gitttt-1234 in #376</li> <li>Handle empty and \"None\" string values for run_name and ckpt_dir config parameters by @gitttt-1234 in #377</li> <li>Append video name to output path when video_index is specified by @gitttt-1234 in #378</li> <li>Bump version to 0.0.5 by @gitttt-1234 in #379</li> </ul> <p>Full Changelog: v0.0.4...v0.0.5</p>"},{"location":"changelog/#v004","title":"v0.0.4SummaryMajor changesChangelog","text":"<p>This release includes a dependency version bump and a critical bug fix for empty instance handling. The minimum torchvision version has been updated to 0.20.0, and sleap-io minimum version has been set to 0.5.7 to ensure compatibility with the latest features and improvements.</p> Dependency Version Updates (#365) <ul> <li>Minimum torchvision version: Set to 0.20.0 across all torch extras (torch, torch-cpu, torch-cuda118, torch-cuda128)</li> <li>Minimum sleap-io version: Updated to 0.5.7 for improved compatibility</li> </ul> Bug Fixes <ul> <li>Fixed empty instance handling (#364): Improved handling of instances with only NaN keypoints in the instance cropping method and <code>CenteredInstanceDataset</code> class. Previously, these instances would trigger \"NaN values encountered\" warnings when computing bounding boxes. The fix ensures only non-empty instances are processed for crop size computation and removes redundant filtering logic.</li> </ul> <ul> <li>Fix empty instance handling (#364)</li> <li>Bump minimum torchvision version to 0.20.0 (#365)</li> </ul> <p>Full Changelog: v0.0.3...v0.0.4</p>"},{"location":"changelog/#v003","title":"v0.0.3SummaryMajor changesChangelog","text":"<p>This release delivers critical bug fixes for multiprocessing support, enhanced tracking capabilities, and significant improvements to the inference workflow. The v0.0.3 release resolves HDF5 pickling issues that prevented proper multiprocessing on macOS/Windows, fixes ID models, and introduces new track cleaning parameters for better tracking performance.</p> Fixed Multiprocessing Bug with num_workers &gt; 0 (#359) <p>Resolved HDF5 pickling issues that prevented proper multiprocessing on macOS/Windows systems. This fix enables users to utilize multiple workers for faster data loading during training and inference when caching is enabled.</p> Fixed ID Models (#345) <p>Fixed  minor issues with TopDown and BottomUp ID models.</p> <ul> <li>The ID models dataset classes were re-computing the tracks from the labels file. However, they should just grab it from the head config classes parameter.</li> <li>Fix shape mismatch issue with BottomUp ID models</li> </ul> Added Track Cleaning Arguments (#349) <p>Added new parameters for better track management and cleanup:</p> <ul> <li>tracking_clean_instance_count: Target number of instances to clean after tracking</li> <li>tracking_clean_iou_threshold: IOU threshold for cleaning overlapping instances</li> <li>tracking_pre_cull_to_target: Pre-culling instances before tracking</li> <li>tracking_pre_cull_iou_threshold: IOU threshold for pre-culling</li> </ul> Updated Installation Documentation (#348, #351) <p>Added comprehensive <code>uv add</code> installation instructions for modern Python package management instead of <code>uv pip install</code> method. Added warning for 3.14 python version to prevent installation issues.</p> Inference workflow enhancements (#360, #361) <p>Enhanced bottom-up model inference capabilities with improved performance and stability. Fix logger encoding issues on windows and better handle integral refinement error on mps accelerator.</p> <ul> <li>Fix ID models by @gitttt-1234 in #345</li> <li>Fix changelog.md by @gitttt-1234 in #346</li> <li>Add warning for Python v3.14 by @gitttt-1234 in #348</li> <li>Add track cleaning args by @gitttt-1234 in #349</li> <li>Update uv add installation docs by @gitttt-1234 in #351</li> <li>Fix marimo usage docs by @gitttt-1234 in #352</li> <li>Fix target instance count parameter by @gitttt-1234 in #358</li> <li>Fix multiprocessing bug with num_workers&gt;0 by @gitttt-1234 in #359</li> <li>Minor fixes to inference workflow by @gitttt-1234 in #360</li> <li>Update bottomup inference and add note on num_workers by @gitttt-1234 in #361</li> <li>Bump version to v0.0.3 by @gitttt-1234 in #362</li> </ul>"},{"location":"changelog/#v002","title":"v0.0.2SummaryMajor changesWhat's Changed","text":"<p>This release focuses on several bug fixes and improvements across the training, inference, and CLI components of sleap-nn. It includes bug fixes for model backbones and loaders, enhancements to the configuration and CLI experience, improved robustness in multi-GPU training, and new options for device selection and tracking. Documentation and installation guides have also been updated, along with internal refactors to streamline the code consistency.</p> <ul> <li> <p>Backbones &amp; Models:</p> <ul> <li>Fixed bugs in Swin Transformer and UNet backbone filter computations.</li> <li>Corrected weight mapping for legacy TopDown ID models.</li> </ul> </li> <li> <p>Inference &amp; Tracking:</p> <ul> <li>Removed unintended loading of pretrained weights during inference.</li> <li>Fixed inference with suggestion frames and improved stalling handling.</li> <li>Added option to run tracking on selected frames and video indices.</li> <li>Added thread-safe video access to prevent backend crashes.</li> <li>Added function to load metrics for better evaluation reporting.</li> </ul> </li> <li> <p>Training Pipeline:</p> <ul> <li>Fixed bugs in the training workflow with the infinite dataloader handling.</li> <li>Improved seeding behavior for reproducible label splits in multi-GPU setups.</li> <li>Fixed experiment run name generation across multi-GPU workers.</li> </ul> </li> <li> <p>CLI &amp; Config:</p> <ul> <li>Introduced unified sleap-nn CLI with subcommands (train, track, eval) and more robust help injection.</li> <li>Removed deprecated CLI commands and cleaned up legacy imports.</li> <li>Added option to specify which devices to use, with auto-selection of GPUs based on available memory.</li> <li>Updated sample configs and sleap-io skeleton function usage.</li> <li>Minor parameter name and default updates for consistency with SLEAP.</li> </ul> </li> <li> <p>Documentation &amp; Installation:</p> <ul> <li>Fixed broken documentation pages and improved menu structure.</li> <li>Updated installation instructions with CUDA support for uv-based workflows.</li> </ul> </li> </ul> <ul> <li>Fix Bug for SwinT Backbone Model by @7174Andy in #304</li> <li>More robust help injection in CLI by @tom21100227 in #303</li> <li>Remove loading pretrained weights during inference pipeline by @gitttt-1234 in #305</li> <li>Remove <code>back</code> import in lightning module by @gitttt-1234 in #312</li> <li>Fix compute filters in unet by @gitttt-1234 in #313</li> <li>Update CLI commands by @gitttt-1234 in #314</li> <li>Update sleap-io skeleton functions usage by @gitttt-1234 in #315</li> <li>Minor updates to config parameters by @gitttt-1234 in #316</li> <li>Minor bug fixes by @gitttt-1234 in #317</li> <li>Fix Inference on SuggestionFrames by @7174Andy in #318</li> <li>Add pck to voc metrics by @gitttt-1234 in #320</li> <li>Fix bugs in training pipeline by @gitttt-1234 in #322</li> <li>Add option to specify which devices to use by @gitttt-1234 in #327</li> <li>Fix bug in infinite data loader by @gitttt-1234 in #325</li> <li>Add thread-safe video access by @gitttt-1234 in #326</li> <li>Fix bugs in docs by @gitttt-1234 in #319</li> <li>Change zmq address to port arguments by @gitttt-1234 in #328</li> <li>Add option to run tracking on select frames by @gitttt-1234 in #329</li> <li>Fix seeding in training workflow by @gitttt-1234 in #330</li> <li>Fix inference stalling by @gitttt-1234 in #331</li> <li>Make wandb artifact logging optional by @gitttt-1234 in #332</li> <li>Auto-select GPUs by @gitttt-1234 in #333</li> <li>Add function to load metrics by @gitttt-1234 in #334</li> <li>Fix experiment run name in multi-gpu training by @gitttt-1234 in #336</li> <li>Add option to pass labels and video objects by @gitttt-1234 in #337</li> <li>Fix mapping for legacy topdown id models by @gitttt-1234 in #339</li> <li>Modify uv installation docs for cuda support by @gitttt-1234 in #340</li> <li>Update sample configs by @gitttt-1234 in #338</li> <li>Bump up sleap-nn version for v0.0.2 by @gitttt-1234 in #341</li> </ul> <p>Full Changelog: v0.0.1...v0.0.2</p>"},{"location":"changelog/#v001","title":"v0.0.1SLEAP-NN v0.0.1 - Initial ReleaseQuick startWhat's ChangedNew Contributors","text":"<p>SLEAP-NN is a PyTorch-based deep learning framework for pose estimation, built on top of the SLEAP (Social LEAP Estimates Animal Poses) platform. This framework provides efficient training, inference, and evaluation tools for multi-animal pose estimation tasks.</p> <p>Documentation: https://nn.sleap.ai/</p> <pre># Install with PyTorch CPU support\npip install sleap-nn[torch-cpu]\n\n# Train a model\nsleap-nn train --config-name config.yaml --config-dir configs/\n\n# Run inference\nsleap-nn track --model_paths model.ckpt --data_path video.mp4\n\n# Evaluate predictions\nsleap-nn eval --ground_truth_path gt.slp --predicted_path pred.slp\n</pre> <ul> <li>Core Data Loader Implementation by @davidasamy in #4</li> <li>Add centroid finder block by @davidasamy in #7</li> <li>Add DataBlocks for rotation and scaling by @gitttt-1234 in #8</li> <li>Refactor datapipes by @talmo in #9</li> <li>Instance Cropping by @davidasamy in #13</li> <li>Add more Kornia augmentations by @alckasoc in #12</li> <li>Confidence Map Generation by @davidasamy in #11</li> <li>Peak finding by @alckasoc in #14</li> <li>UNet Implementation by @alckasoc in #15</li> <li>Top-down Centered-instance Pipeline by @alckasoc in #16</li> <li>Adding ruff to ci.yml by @alckasoc in #21</li> <li>Implement base Model and Head classes by @alckasoc in #17</li> <li>Add option to Filter to user instances by @gitttt-1234 in #20</li> <li>Add Evaluation Module by @gitttt-1234 in #22</li> <li>Add metadata to dictionary by @gitttt-1234 in #24</li> <li>Added SingleInstanceConfmapsPipeline by @alckasoc in #23</li> <li>modify keys by @gitttt-1234 in #31</li> <li>Small fix to find_global_peaks_rough by @alckasoc in #28</li> <li>Add trainer by @gitttt-1234 in #29</li> <li>PAF Grouping by @alckasoc in #33</li> <li>Add predictor class by @gitttt-1234 in #36</li> <li>Edge Maps by @alckasoc in #38</li> <li>Add ConvNext Backbone by @gitttt-1234 in #40</li> <li>Add VideoReader by @gitttt-1234 in #45</li> <li>Refactor model pipeline by @gitttt-1234 in #51</li> <li>Add BottomUp model pipeline by @gitttt-1234 in #52</li> <li>Remove Part-names and Edge dependency in config by @gitttt-1234 in #54</li> <li>Refactor model config by @gitttt-1234 in #61</li> <li>Refactor Augmentation config by @gitttt-1234 in #67</li> <li>Add minimal pretrained checkpoints for tests and fix PAF grouping interpolation by @gqcpm in #73</li> <li>Fix augmentation in TopdownConfmaps pipeline by @gitttt-1234 in #86</li> <li>Implement tracker module by @gitttt-1234 in #87</li> <li>Resume training and automatically compute crop size for TopDownConfmaps pipeline by @gitttt-1234 in #88</li> <li>LitData Refactor PR1: Get individual functions for data pipelines by @gitttt-1234 in #90</li> <li>Add function to load trained weights for backbone model by @gitttt-1234 in #95</li> <li>Remove IterDataPipe from Inference pipeline by @gitttt-1234 in #96</li> <li>Move ld.optimize to a subprocess by @gitttt-1234 in #100</li> <li>Auto compute max height and width from labels by @gitttt-1234 in #101</li> <li>Fix sizematcher in Inference data pipline by @gitttt-1234 in #102</li> <li>Convert Tensor images to PIL by @gitttt-1234 in #105</li> <li>Add threshold mode in config for learning rate scheduler by @gitttt-1234 in #106</li> <li>Add option to specify <code>.bin</code> file directory in config by @gitttt-1234 in #107</li> <li>Add StepLR scheduler by @gitttt-1234 in #109</li> <li>Add config to WandB by @gitttt-1234 in #113</li> <li>Add option to load trained weights for Head layers by @gitttt-1234 in #114</li> <li>Add option to load ckpts for backbone and head for running inference by @gitttt-1234 in #115</li> <li>Add option to reuse <code>.bin</code> files by @gitttt-1234 in #116</li> <li>Fix Normalization order in data pipelines by @gitttt-1234 in #118</li> <li>Add torch Dataset classes by @gitttt-1234 in #120</li> <li>Fix Pafs shape by @gitttt-1234 in #121</li> <li>Add caching to Torch Datasets pipeline by @gitttt-1234 in #123</li> <li>Remove <code>random_crop</code> augmentation by @gitttt-1234 in #124</li> <li>Generate np chunks for caching by @gitttt-1234 in #125</li> <li>Add <code>group</code> to wandb config by @gitttt-1234 in #126</li> <li>Fix crop size by @gitttt-1234 in #127</li> <li>Resize images before cropping in Centered-instance model by @gitttt-1234 in #129</li> <li>Check memory before caching by @gitttt-1234 in #130</li> <li>Replace <code>eval</code> with an explicit mapping dictionary by @gitttt-1234 in #131</li> <li>Add <code>CyclerDataLoader</code> to ensure minimum steps per epoch by @gitttt-1234 in #132</li> <li>Fix running inference on Bottom-up models with CUDA by @gitttt-1234 in #133</li> <li>Fix caching in datasets by @gitttt-1234 in #134</li> <li>Save <code>.slp</code> file after inference by @gitttt-1234 in #135</li> <li>Add option to reuse np chunks by @gitttt-1234 in #136</li> <li>Filter instances while generating indices by @gitttt-1234 in #138</li> <li>Fix config format while logging to wandb by @gitttt-1234 in #144</li> <li>Add multi-gpu support by @gitttt-1234 in #145</li> <li>Implement Omegaconfig PR1: basic functionality by @gqcpm in #97</li> <li>Move all params to config by @gitttt-1234 in #146</li> <li>Add output stride to backbone config by @gitttt-1234 in #147</li> <li>Change backbone config structure by @gitttt-1234 in #149</li> <li>Add an entry point train function by @gitttt-1234 in #150</li> <li>Add logger by @gqcpm in #148</li> <li>Fix preprocessing during inference by @gitttt-1234 in #156</li> <li>Add CLI for training by @gitttt-1234 in #155</li> <li>Specify custom anchor index in Inference pipeline by @gitttt-1234 in #157</li> <li>Fix lr scheduler config by @gitttt-1234 in #158</li> <li>Add max stride to Convnext and Swint backbones by @gitttt-1234 in #159</li> <li>Fix length in custom datasets by @gitttt-1234 in #160</li> <li>Add <code>scale</code> argument to custom datasets by @gitttt-1234 in #166</li> <li>Fix size matcher by @gitttt-1234 in #167</li> <li>Fix max instances in TopDown Inference by @gitttt-1234 in #168</li> <li>Move lightning modules by @gitttt-1234 in #169</li> <li>Save config with chunks by @gitttt-1234 in #174</li> <li>Add profiler and strategy parameters by @gitttt-1234 in #175</li> <li>Add docker img for remote dev by @gitttt-1234 in #176</li> <li>Save files only in rank: 0 by @gitttt-1234 in #177</li> <li>Minor changes to validate configs by @gitttt-1234 in #179</li> <li>Fix multi-gpu training by @gitttt-1234 in #184</li> <li>Cache only images by @gitttt-1234 in #186</li> <li>Add a new data pipeline strategy without caching by @gitttt-1234 in #187</li> <li>Minor fixes to lightning modules by @gitttt-1234 in #189</li> <li>Fix caching when imgs path already exist by @gitttt-1234 in #191</li> <li>Ensure caching of images to disk in rank:0 by @gitttt-1234 in #193</li> <li>Fix bug in caching images to disk by @gitttt-1234 in #194</li> <li>Close videos before creating data loaders by @gitttt-1234 in #195</li> <li>Update instance creation for sleap-io v0.3.0 compatibility by @gitttt-1234 in #196</li> <li>Fix up block computation for swint and convnext by @gitttt-1234 in #197</li> <li>Bump up to python 3.11 by @gitttt-1234 in #200</li> <li>Map legacy SLEAP <code>json</code> configs to SLEAP-NN <code>OmegaConf</code> objects by @gqcpm in #162</li> <li>Add option to get validation data from train labels by @gitttt-1234 in #201</li> <li>Fix anchor part in config by @gitttt-1234 in #203</li> <li>Minor fixes to config mapper by @gitttt-1234 in #204</li> <li>Save labels with centroid inference by @gitttt-1234 in #205</li> <li>Add custom callbacks to publish metrics during training by @gitttt-1234 in #207</li> <li>Add visualizer by @gitttt-1234 in #208</li> <li>Add CLI for inference by @gitttt-1234 in #209</li> <li>Add option to parse frame ranges for videos by @gitttt-1234 in #211</li> <li>Add control flags to run inference on select LabeledFrames by @gitttt-1234 in #212</li> <li>Add support to run inference on specific video in a .slp file by @gitttt-1234 in #213</li> <li>Minor fixes to tracking by @gitttt-1234 in #214</li> <li>Fix bug in evaluation by @gitttt-1234 in #215</li> <li>Save train, val, test predictions after training by @gitttt-1234 in #216</li> <li>Add option to auto-select device for inference by @gitttt-1234 in #217</li> <li>Add option to pass multiple .slp files for training by @papamanu in #218</li> <li>Add logs by @gitttt-1234 in #219</li> <li>Bug fixes to model architecture and trainer by @gitttt-1234 in #220</li> <li>Remove <code>nested</code> tensors to support <code>mps</code> for BottomUp models by @gitttt-1234 in #221</li> <li>Modify viz functions by @gitttt-1234 in #223</li> <li>Add <code>ensure_grayscale</code> parameter by @gitttt-1234 in #224</li> <li>Fix bugs with zmq config by @gitttt-1234 in #225</li> <li>Log part-wise losses by @gitttt-1234 in #226</li> <li>Fix trainer config mappings and add option to load config from json str by @gitttt-1234 in #227</li> <li>Refactor ModelTrainer class by @gitttt-1234 in #228</li> <li>Fix infinite data loader and update steps per epoch by @gitttt-1234 in #229</li> <li>Add online hard keypoint mining by @gitttt-1234 in #222</li> <li>Add more features to Tracker by @gitttt-1234 in #231</li> <li>Remove litdata and iterdatapipe pipelines by @gitttt-1234 in #232</li> <li>Add CLAUDE.md and update .gitignore for Claude Code integration by @talmo in #233</li> <li>Add length parameter to InfiniteDataLoader by @gitttt-1234 in #237</li> <li>Get sleap-nn pip package ready to publish by @eberrigan in #236</li> <li>Map sleap (json) skeleton to sleap-nn (yaml) format by @gitttt-1234 in #238</li> <li>Enable tracking on user-labeled instances by @gitttt-1234 in #239</li> <li>Add ID models by @gitttt-1234 in #234</li> <li>Add codespell workflow for spell checking by @talmo in #241</li> <li>Make torch dependencies optional by @eberrigan in #243</li> <li>Reorganize assets and revise checkpoints by @gitttt-1234 in #242</li> <li>Refactor architectures per SLEAP by @gitttt-1234 in #245</li> <li>Add <code>keep-viz</code> parameter by @gitttt-1234 in #246</li> <li>Format <code>config.md</code> by @gitttt-1234 in #249</li> <li>Fix minor bugs by @gitttt-1234 in #250</li> <li>Setup docs by @talmo in #251</li> <li>Remove broken Docker image by @gitttt-1234 in #254</li> <li>Import legacy SLEAP model weights by @talmo in #235</li> <li>Fix wandb logging by @gitttt-1234 in #255</li> <li>Fix preprocess config in inference by @gitttt-1234 in #257</li> <li>Update ckpts and cfgs by @gitttt-1234 in #259</li> <li>Minor bug fixes in training pipeline by @gitttt-1234 in #260</li> <li>Revert \"Minor bug fixes in training pipeline\" by @gitttt-1234 in #261</li> <li>Fix minor bugs by @gitttt-1234 in #262</li> <li>Update in channels for torch with keras weights by @gitttt-1234 in #263</li> <li>Move convnext/ swint pretrained weights by @gitttt-1234 in #264</li> <li>Refactor lightning module parameters by @gitttt-1234 in #265</li> <li>Fix skeletons structure in config by @gitttt-1234 in #266</li> <li>Ensure consistent types for augmentation parameters by @gitttt-1234 in #267</li> <li>Add CLI entry-point functions and shortcuts by @gitttt-1234 in #270</li> <li>Ensure only rank-0 handles writing files in ddp training by @gitttt-1234 in #271</li> <li>Fix bug in centered-instance dataset by @gitttt-1234 in #272</li> <li>Add eff_scale to dataset by @gitttt-1234 in #273</li> <li>Update docs by @gitttt-1234 in #256</li> <li>Add self-hosted runner to CI by @talmo in #277</li> <li>Minor changes to data pipeline and training guide notebook by @gitttt-1234 in #280</li> <li>Add support to load keras weights for model init by @gitttt-1234 in #285</li> <li>Self-hosted Runners Tests and Trainer Accelerator by @alicup29 in #281</li> <li>Check memory with source images by @eberrigan in #283</li> <li>Fix <code>@oneof</code> Validation and Add Support for None for train_labels_path by @7174Andy in #282</li> <li>Parallelizing dataset caching by @emdavis02 in #284</li> <li>Remove Permanent File Creations After Testing Locally by @7174Andy in #292</li> <li>Add file existence tracking to trainer and related tests by @tom21100227 in #291</li> <li>Fix transitive torch/torchvision installation &amp; platform compatilibility by @alicup29 in #268</li> <li>Add more documentation by @gitttt-1234 in #287</li> <li>Update build CI by @talmo in #295</li> <li>Add build ci option to release to testpypi by @gitttt-1234 in #296</li> <li>Add testpypi index to toml by @gitttt-1234 in #297</li> <li>Minor fixes to pyproject.toml by @gitttt-1234 in #298</li> <li>Add Hyphen to Checkpoint Paths when Duplication Found by @7174Andy in #299</li> <li>Add helpful CLI message to <code>sleap-nn-train</code> by @tom21100227 in #294</li> </ul> <ul> <li>@davidasamy made their first contribution in #4</li> <li>@gqcpm made their first contribution in #73</li> <li>@papamanu made their first contribution in #218</li> <li>@eberrigan made their first contribution in #236</li> <li>@alicup29 made their first contribution in #281</li> <li>@7174Andy made their first contribution in #282</li> <li>@emdavis02 made their first contribution in #284</li> <li>@tom21100227 made their first contribution in #291</li> </ul> <p>Full Changelog: https://github.com/talmolab/sleap-nn/commits/v0.0.1</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#before-you-start","title":"Before You Start","text":"<p>SLEAP-NN uses uv for installation and environment management. If you're coming from conda/pip, this section explains why.</p> Why uv? <p>GPU dependencies are complex. PyTorch requires matching CUDA versions, platform-specific wheels, and careful index configuration. Traditional pip/conda installs often result in CPU-only PyTorch or version conflicts.</p> <p>uv solves this with the <code>--torch-backend</code> flag:</p> <ul> <li><code>auto</code> \u2013 Detects your GPU and installs the right PyTorch</li> <li><code>cu130</code> / <code>cu128</code> \u2013 Explicit CUDA versions</li> <li><code>cpu</code> \u2013 CPU-only (smaller install)</li> </ul> <p>One command, correct GPU support. No manual index URLs or environment debugging.</p> <p>uv version requirement</p> <p>The <code>--torch-backend</code> flag for <code>uv tool install</code> requires uv 0.9.20+. Run <code>uv self update</code> if you encounter errors.</p> Can I still use pip/conda? <p>Yes. See the pip installation section below. You'll need to manually configure PyTorch index URLs.</p>"},{"location":"installation/#install-sleap-nn","title":"Install SLEAP-NN","text":"Linux / WindowsmacOSCPU Only <p>Step 1: Install uv</p> Linux / macOS / WSLWindows (PowerShell) <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Step 2: Install sleap-nn</p> <p>Python 3.14 not supported</p> <p>If you don't have Python installed, uv will automatically download the latest version (Python 3.14), which is not yet supported. Add <code>--python 3.13</code> to specify a compatible version: <pre><code>uv tool install --python 3.13 sleap-nn[torch] --torch-backend auto\n</code></pre></p> <pre><code>uv tool install sleap-nn[torch] --torch-backend auto\n</code></pre> <p>Step 3: Verify</p> <pre><code>sleap-nn system\n</code></pre> <p>Step 1: Install uv</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Step 2: Install sleap-nn</p> <p>Python 3.14 not supported</p> <p>If you don't have Python installed, uv will automatically download the latest version (Python 3.14), which is not yet supported. Add <code>--python 3.13</code> to specify a compatible version: <pre><code>uv tool install --python 3.13 \"sleap-nn[torch]\"\n</code></pre></p> <pre><code>uv tool install \"sleap-nn[torch]\"\n</code></pre> <p>Apple Silicon</p> <p>PyTorch uses Metal Performance Shaders (MPS) for GPU acceleration on M1/M2/M3 Macs. No additional configuration needed.</p> <p>Step 3: Verify</p> <pre><code>sleap-nn system\n</code></pre> <p>Step 1: Install uv</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Step 2: Install sleap-nn</p> <p>Python 3.14 not supported</p> <p>If you don't have Python installed, uv will automatically download the latest version (Python 3.14), which is not yet supported. Add <code>--python 3.13</code> to specify a compatible version: <pre><code>uv tool install --python 3.13 sleap-nn[torch] --torch-backend cpu\n</code></pre></p> <pre><code>uv tool install sleap-nn[torch] --torch-backend cpu\n</code></pre> <p>Step 3: Verify</p> <pre><code>sleap-nn system\n</code></pre>"},{"location":"installation/#updating","title":"Updating","text":"Update to latestUpdate to specific versionReinstall (fix issues)DowngradeUninstall <pre><code>uv tool upgrade sleap-nn\n</code></pre> <p>Note</p> <p>This preserves the extras (<code>[torch]</code>) and torch backend from your original installation. If you need to change the torch backend, use the reinstall option.</p> <pre><code>uv tool install \"sleap-nn[torch]==0.1.0\" --torch-backend auto --force\n</code></pre> <pre><code>uv tool install sleap-nn[torch] --torch-backend auto --reinstall\n</code></pre> <p>When to use <code>--reinstall</code></p> <p>Use this when you've updated CUDA drivers, changed GPUs, or have import errors.</p> <pre><code>uv tool install \"sleap-nn[torch]==0.0.5\" --torch-backend auto --force\n</code></pre> <pre><code>uv tool uninstall sleap-nn\n</code></pre>"},{"location":"installation/#pre-release-versions","title":"Pre-release Versions","text":"<p>Install alpha/beta releases to test new features:</p> <pre><code>uv tool install sleap-nn[torch] --torch-backend auto --prerelease=allow\n</code></pre> <p>Install a specific pre-release:</p> <pre><code>uv tool install \"sleap-nn[torch]==0.1.0a4\" --torch-backend auto\n</code></pre>"},{"location":"installation/#alternative-methods","title":"Alternative Methods","text":""},{"location":"installation/#uvx-no-install","title":"uvx (No Install)","text":"<p>Run sleap-nn without permanent installation. Each command creates a temporary environment.</p> <pre><code># Train\nuvx --from \"sleap-nn[torch]\" --torch-backend auto sleap-nn train --config config.yaml\n\n# Inference\nuvx --from \"sleap-nn[torch]\" --torch-backend auto sleap-nn track -i video.mp4 -m models/\n</code></pre> <p>Always latest</p> <p>uvx uses the latest version each run. Great for testing or one-off tasks.</p>"},{"location":"installation/#pip","title":"pip","text":"<p>Use pip when working within conda/mamba environments.</p> <p>Create environment:</p> <pre><code>conda create -n sleap-nn python=3.13\nconda activate sleap-nn\n</code></pre> <p>Install with GPU support:</p> CUDA 12.8CUDA 11.8CPU OnlymacOS <pre><code>pip install sleap-nn[torch] \\\n    --index-url https://pypi.org/simple \\\n    --extra-index-url https://download.pytorch.org/whl/cu128\n</code></pre> <pre><code>pip install sleap-nn[torch] \\\n    --index-url https://pypi.org/simple \\\n    --extra-index-url https://download.pytorch.org/whl/cu118\n</code></pre> <pre><code>pip install sleap-nn[torch] \\\n    --index-url https://pypi.org/simple \\\n    --extra-index-url https://download.pytorch.org/whl/cpu\n</code></pre> <pre><code>pip install \"sleap-nn[torch]\"\n</code></pre> <p>Other CUDA versions</p> <p>You can install any CUDA version supported by PyTorch by changing the index URL. Replace <code>cu128</code> with your desired version (e.g., <code>cu124</code>, <code>cu121</code>, <code>cu118</code>). See PyTorch Get Started for available versions.</p> <p>Verify:</p> <pre><code>sleap-nn system\n</code></pre>"},{"location":"installation/#from-source","title":"From Source","text":"<p>For development and contributing.</p> <p>Step 1: Clone repository</p> <pre><code>git clone https://github.com/talmolab/sleap-nn.git\ncd sleap-nn\n</code></pre> <p>Step 2: Install uv (if needed)</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Step 3: Install in development mode</p> <pre><code>uv sync --extra torch-cuda130\n</code></pre> <p>Other backends</p> <p>Replace <code>torch-cuda130</code> with <code>torch-cuda128</code>, <code>torch-cpu</code>, or <code>torch</code> (macOS) as needed.</p> <p>Step 4: Run commands</p> <pre><code>uv run sleap-nn --help\n</code></pre> <pre><code>uv run pytest tests/\n</code></pre> <p>See Contributing for development guidelines.</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"Requirement Minimum Recommended Python 3.11 3.13 RAM 8 GB 16+ GB <p>Apple Silicon</p> <p>M1/M2/M3 Macs are fully supported via Metal Performance Shaders (MPS).</p> <p>Python 3.14</p> <p>Not yet supported. Use <code>--python 3.13</code> with uv commands.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"Command not found after install <p>Restart your terminal or source your shell config:</p> <pre><code>source ~/.bashrc  # or ~/.zshrc on macOS\n</code></pre> <p>Verify the tool is installed:</p> <pre><code>uv tool list\n</code></pre> CUDA not detected <ol> <li> <p>Check NVIDIA drivers: <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Check sleap-nn detects CUDA: <pre><code>sleap-nn system\n</code></pre></p> </li> <li> <p>Reinstall with explicit CUDA version: <pre><code>uv tool install sleap-nn[torch] --torch-backend cu128 --reinstall\n</code></pre></p> </li> </ol> Import errors or missing modules <p>Reinstall with the torch extras:</p> <pre><code>uv tool install sleap-nn[torch] --torch-backend auto --reinstall\n</code></pre> Wrong Python version <p>Specify the Python version explicitly:</p> <pre><code>uv tool install --python 3.13 sleap-nn[torch] --torch-backend auto\n</code></pre> uv version too old <p>The <code>--torch-backend</code> flag requires uv 0.9.20+.</p> <pre><code># Check version\nuv --version\n\n# Update uv\nuv self update\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start</p> <p>Train your first model in 5 minutes.</p> <p> Get started</p> </li> <li> <p> Your First Model</p> <p>Complete walkthrough from data to predictions.</p> <p> Tutorial</p> </li> </ul>"},{"location":"api/","title":"sleap_nn","text":""},{"location":"api/#sleap_nn","title":"<code>sleap_nn</code>","text":"<p>Main module for sleap_nn package.</p> <p>Modules:</p> Name Description <code>architectures</code> <p>Modules related to model architectures.</p> <code>cli</code> <p>Unified CLI for SLEAP-NN using rich-click for styled output.</p> <code>config</code> <p>Configuration modules for sleap-nn.</p> <code>data</code> <p>Modules related to data loading and processing.</p> <code>evaluation</code> <p>This module is to compute evaluation metrics for trained models.</p> <code>export</code> <p>Export utilities for sleap-nn.</p> <code>inference</code> <p>Inference-related modules.</p> <code>legacy_models</code> <p>Utilities for loading legacy SLEAP models.</p> <code>predict</code> <p>Entry point for running inference.</p> <code>system_info</code> <p>System diagnostics and compatibility checking for sleap-nn.</p> <code>tracking</code> <p>Tracker related modules.</p> <code>train</code> <p>Entry point for sleap_nn training.</p> <code>training</code> <p>Training-related modules.</p> <p>Functions:</p> Name Description <code>load_metrics</code> <p>Load metrics from a model folder or metrics file.</p>"},{"location":"api/#sleap_nn.load_metrics","title":"<code>load_metrics(path, split='test', dataset_idx=0)</code>","text":"<p>Load metrics from a model folder or metrics file.</p> <p>This function supports both the new format (single \"metrics\" key) and the old format (individual metric keys at top level). It also handles both old and new file naming conventions in model folders.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a model folder or metrics file (.npz).</p> required <code>split</code> <code>str</code> <p>Name of the split to load. Must be \"train\", \"val\", or \"test\". Default: \"test\". If \"test\" is not found, falls back to \"val\". Ignored if path points directly to a .npz file.</p> <code>'test'</code> <code>dataset_idx</code> <code>int</code> <p>Index of the dataset (for multi-dataset training). Default: 0. Ignored if path points directly to a .npz file.</p> <code>0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing metrics with keys: voc_metrics, mOKS, distance_metrics, pck_metrics, visibility_metrics.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no metrics file is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load from model folder (tries test, falls back to val)\n&gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\")\n&gt;&gt;&gt; print(metrics[\"mOKS\"][\"mOKS\"])\n</code></pre> <pre><code>&gt;&gt;&gt; # Load specific split and dataset\n&gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\", split=\"val\", dataset_idx=1)\n</code></pre> <pre><code>&gt;&gt;&gt; # Load directly from npz file\n&gt;&gt;&gt; metrics = load_metrics(\"/path/to/metrics.val.0.npz\")\n</code></pre> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def load_metrics(\n    path: str,\n    split: str = \"test\",\n    dataset_idx: int = 0,\n) -&gt; dict:\n    \"\"\"Load metrics from a model folder or metrics file.\n\n    This function supports both the new format (single \"metrics\" key) and the old\n    format (individual metric keys at top level). It also handles both old and new\n    file naming conventions in model folders.\n\n    Args:\n        path: Path to a model folder or metrics file (.npz).\n        split: Name of the split to load. Must be \"train\", \"val\", or \"test\".\n            Default: \"test\". If \"test\" is not found, falls back to \"val\".\n            Ignored if path points directly to a .npz file.\n        dataset_idx: Index of the dataset (for multi-dataset training).\n            Default: 0. Ignored if path points directly to a .npz file.\n\n    Returns:\n        Dictionary containing metrics with keys: voc_metrics, mOKS,\n        distance_metrics, pck_metrics, visibility_metrics.\n\n    Raises:\n        FileNotFoundError: If no metrics file is found.\n\n    Examples:\n        &gt;&gt;&gt; # Load from model folder (tries test, falls back to val)\n        &gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\")\n        &gt;&gt;&gt; print(metrics[\"mOKS\"][\"mOKS\"])\n\n        &gt;&gt;&gt; # Load specific split and dataset\n        &gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\", split=\"val\", dataset_idx=1)\n\n        &gt;&gt;&gt; # Load directly from npz file\n        &gt;&gt;&gt; metrics = load_metrics(\"/path/to/metrics.val.0.npz\")\n    \"\"\"\n    path = Path(path)\n\n    if path.suffix == \".npz\":\n        metrics_path = path\n    else:\n        metrics_path = _find_metrics_file(path, split, dataset_idx)\n\n    if not metrics_path.exists():\n        raise FileNotFoundError(f\"Metrics file not found at {metrics_path}\")\n\n    return _load_npz_metrics(metrics_path)\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>sleap_nn<ul> <li>architectures<ul> <li>common</li> <li>convnext</li> <li>encoder_decoder</li> <li>heads</li> <li>model</li> <li>swint</li> <li>unet</li> <li>utils</li> </ul> </li> <li>cli</li> <li>config<ul> <li>data_config</li> <li>get_config</li> <li>model_config</li> <li>trainer_config</li> <li>training_job_config</li> <li>utils</li> </ul> </li> <li>data<ul> <li>augmentation</li> <li>confidence_maps</li> <li>custom_datasets</li> <li>edge_maps</li> <li>identity</li> <li>instance_centroids</li> <li>instance_cropping</li> <li>normalization</li> <li>providers</li> <li>resizing</li> <li>skia_augmentation</li> <li>utils</li> </ul> </li> <li>evaluation</li> <li>export<ul> <li>cli</li> <li>exporters<ul> <li>onnx_exporter</li> <li>tensorrt_exporter</li> </ul> </li> <li>metadata</li> <li>predictors<ul> <li>base</li> <li>onnx</li> <li>tensorrt</li> </ul> </li> <li>utils</li> <li>wrappers<ul> <li>base</li> <li>bottomup</li> <li>bottomup_multiclass</li> <li>centered_instance</li> <li>centroid</li> <li>single_instance</li> <li>topdown</li> <li>topdown_multiclass</li> </ul> </li> </ul> </li> <li>inference<ul> <li>bottomup</li> <li>identity</li> <li>paf_grouping</li> <li>peak_finding</li> <li>postprocessing</li> <li>predictors</li> <li>provenance</li> <li>single_instance</li> <li>topdown</li> <li>utils</li> </ul> </li> <li>legacy_models</li> <li>predict</li> <li>system_info</li> <li>tracking<ul> <li>candidates<ul> <li>fixed_window</li> <li>local_queues</li> </ul> </li> <li>track_instance</li> <li>tracker</li> <li>utils</li> </ul> </li> <li>train</li> <li>training<ul> <li>callbacks</li> <li>lightning_modules</li> <li>losses</li> <li>model_trainer</li> <li>schedulers</li> <li>utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/cli/","title":"cli","text":""},{"location":"api/cli/#sleap_nn.cli","title":"<code>sleap_nn.cli</code>","text":"<p>Unified CLI for SLEAP-NN using rich-click for styled output.</p> <p>Classes:</p> Name Description <code>TrainCommand</code> <p>Custom command class that overrides help behavior for train command.</p> <p>Functions:</p> Name Description <code>cli</code> <p>SLEAP-NN: Neural network backend for training and inference for animal pose estimation.</p> <code>eval</code> <p>Run evaluation workflow.</p> <code>is_config_path</code> <p>Check if an argument looks like a config file path.</p> <code>parse_path_map</code> <p>Parse (old, new) path pairs into a dictionary for path mapping options.</p> <code>print_version</code> <p>Print version and exit.</p> <code>show_training_help</code> <p>Display training help information with rich formatting.</p> <code>split_config_path</code> <p>Split a full config path into (config_dir, config_name).</p> <code>system</code> <p>Display system information and GPU status.</p> <code>track</code> <p>Run Inference and Tracking workflow.</p> <code>train</code> <p>Run training workflow with Hydra config overrides.</p>"},{"location":"api/cli/#sleap_nn.cli.TrainCommand","title":"<code>TrainCommand</code>","text":"<p>               Bases: <code>Command</code></p> <p>Custom command class that overrides help behavior for train command.</p> <p>Methods:</p> Name Description <code>format_help</code> <p>Override the help formatting to show custom training help.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>class TrainCommand(Command):\n    \"\"\"Custom command class that overrides help behavior for train command.\"\"\"\n\n    def format_help(self, ctx, formatter):\n        \"\"\"Override the help formatting to show custom training help.\"\"\"\n        show_training_help()\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.TrainCommand.format_help","title":"<code>format_help(ctx, formatter)</code>","text":"<p>Override the help formatting to show custom training help.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>def format_help(self, ctx, formatter):\n    \"\"\"Override the help formatting to show custom training help.\"\"\"\n    show_training_help()\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.cli","title":"<code>cli()</code>","text":"<p>SLEAP-NN: Neural network backend for training and inference for animal pose estimation.</p> <p>Use subcommands to run different workflows:</p> <p>train    - Run training workflow (auto-handles multi-GPU) track    - Run inference/tracking workflow eval     - Run evaluation workflow system   - Display system information and GPU status</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@click.group()\n@click.option(\n    \"--version\",\n    \"-v\",\n    is_flag=True,\n    callback=print_version,\n    expose_value=False,\n    is_eager=True,\n    help=\"Show version and exit.\",\n)\ndef cli():\n    \"\"\"SLEAP-NN: Neural network backend for training and inference for animal pose estimation.\n\n    Use subcommands to run different workflows:\n\n    train    - Run training workflow (auto-handles multi-GPU)\n    track    - Run inference/tracking workflow\n    eval     - Run evaluation workflow\n    system   - Display system information and GPU status\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.eval","title":"<code>eval(**kwargs)</code>","text":"<p>Run evaluation workflow.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--ground_truth_path\",\n    \"-g\",\n    type=str,\n    required=True,\n    help=\"Path to ground truth labels file (.slp)\",\n)\n@click.option(\n    \"--predicted_path\",\n    \"-p\",\n    type=str,\n    required=True,\n    help=\"Path to predicted labels file (.slp)\",\n)\n@click.option(\"--save_metrics\", \"-s\", type=str, help=\"Path to save metrics (.npz file)\")\n@click.option(\n    \"--oks_stddev\",\n    type=float,\n    default=0.05,\n    help=\"Standard deviation for OKS calculation\",\n)\n@click.option(\"--oks_scale\", type=float, help=\"Scale factor for OKS calculation\")\n@click.option(\n    \"--match_threshold\", type=float, default=0.0, help=\"Threshold for instance matching\"\n)\n@click.option(\n    \"--user_labels_only\", is_flag=True, help=\"Only evaluate user-labeled frames\"\n)\ndef eval(**kwargs):\n    \"\"\"Run evaluation workflow.\"\"\"\n    run_evaluation(**kwargs)\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.is_config_path","title":"<code>is_config_path(arg)</code>","text":"<p>Check if an argument looks like a config file path.</p> <p>Returns True if the arg ends with .yaml or .yml.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>def is_config_path(arg: str) -&gt; bool:\n    \"\"\"Check if an argument looks like a config file path.\n\n    Returns True if the arg ends with .yaml or .yml.\n    \"\"\"\n    return arg.endswith(\".yaml\") or arg.endswith(\".yml\")\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.parse_path_map","title":"<code>parse_path_map(ctx, param, value)</code>","text":"<p>Parse (old, new) path pairs into a dictionary for path mapping options.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>def parse_path_map(ctx, param, value):\n    \"\"\"Parse (old, new) path pairs into a dictionary for path mapping options.\"\"\"\n    if not value:\n        return None\n    result = {}\n    for old_path, new_path in value:\n        result[old_path] = Path(new_path).as_posix()\n    return result\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.print_version","title":"<code>print_version(ctx, param, value)</code>","text":"<p>Print version and exit.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>def print_version(ctx, param, value):\n    \"\"\"Print version and exit.\"\"\"\n    if not value or ctx.resilient_parsing:\n        return\n    click.echo(f\"sleap-nn {__version__}\")\n    ctx.exit()\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.show_training_help","title":"<code>show_training_help()</code>","text":"<p>Display training help information with rich formatting.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>def show_training_help():\n    \"\"\"Display training help information with rich formatting.\"\"\"\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.markdown import Markdown\n\n    console = Console()\n\n    help_md = \"\"\"\n## Usage\n\n```\nsleap-nn train &lt;config.yaml&gt; [overrides]\nsleap-nn train --config &lt;path/to/config.yaml&gt; [overrides]\n```\n\n## Common Overrides\n\n| Override | Description |\n|----------|-------------|\n| `trainer_config.max_epochs=100` | Set maximum training epochs |\n| `trainer_config.batch_size=32` | Set batch size |\n| `trainer_config.save_ckpt=true` | Enable checkpoint saving |\n\n## Examples\n\n**Start a new training run:**\n```bash\nsleap-nn train path/to/config.yaml\nsleap-nn train --config path/to/config.yaml\n```\n\n**With overrides:**\n```bash\nsleap-nn train config.yaml trainer_config.max_epochs=100\n```\n\n**Resume training:**\n```bash\nsleap-nn train config.yaml trainer_config.resume_ckpt_path=/path/to/ckpt\n```\n\n**Legacy usage (still supported):**\n```bash\nsleap-nn train --config-dir /path/to/dir --config-name myrun\n```\n\n## Tips\n\n- Use `-m/--multirun` for sweeps; outputs go under `hydra.sweep.dir`\n- For Hydra flags and completion, use `--hydra-help`\n- Config documentation: https://nn.sleap.ai/config/\n\"\"\"\n    console.print(\n        Panel(\n            Markdown(help_md),\n            title=\"[bold cyan]sleap-nn train[/bold cyan]\",\n            subtitle=\"Train SLEAP models from a config YAML file\",\n            border_style=\"cyan\",\n        )\n    )\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.split_config_path","title":"<code>split_config_path(config_path)</code>","text":"<p>Split a full config path into (config_dir, config_name).</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Full path to a config file.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (config_dir, config_name) where config_dir is an absolute path.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>def split_config_path(config_path: str) -&gt; tuple:\n    \"\"\"Split a full config path into (config_dir, config_name).\n\n    Args:\n        config_path: Full path to a config file.\n\n    Returns:\n        Tuple of (config_dir, config_name) where config_dir is an absolute path.\n    \"\"\"\n    path = Path(config_path).resolve()\n    return path.parent.as_posix(), path.name\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.system","title":"<code>system()</code>","text":"<p>Display system information and GPU status.</p> <p>Shows Python version, platform, PyTorch version, CUDA availability, driver version with compatibility check, GPU details, and package versions.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@cli.command()\ndef system():\n    \"\"\"Display system information and GPU status.\n\n    Shows Python version, platform, PyTorch version, CUDA availability,\n    driver version with compatibility check, GPU details, and package versions.\n    \"\"\"\n    from sleap_nn.system_info import print_system_info\n\n    print_system_info()\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.track","title":"<code>track(**kwargs)</code>","text":"<p>Run Inference and Tracking workflow.</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--data_path\",\n    \"-i\",\n    type=str,\n    required=True,\n    help=\"Path to data to predict on. This can be a labels (.slp) file or any supported video format.\",\n)\n@click.option(\n    \"--model_paths\",\n    \"-m\",\n    multiple=True,\n    help=\"Path to trained model directory (with training_config.json). Multiple models can be specified, each preceded by --model_paths.\",\n)\n@click.option(\n    \"--output_path\",\n    \"-o\",\n    type=str,\n    default=None,\n    help=\"The output filename to use for the predicted data. If not provided, defaults to '[data_path].slp'.\",\n)\n@click.option(\n    \"--device\",\n    \"-d\",\n    type=str,\n    default=\"auto\",\n    help=\"Device on which torch.Tensor will be allocated. One of the ('cpu', 'cuda', 'mps', 'auto'). Default: 'auto' (based on available backend either cuda, mps or cpu is chosen). If `cuda` is available, you could also use `cuda:0` to specify the device.\",\n)\n@click.option(\n    \"--batch_size\",\n    \"-b\",\n    type=int,\n    default=4,\n    help=\"Number of frames to predict at a time. Larger values result in faster inference speeds, but require more memory.\",\n)\n@click.option(\n    \"--tracking\",\n    \"-t\",\n    is_flag=True,\n    default=False,\n    help=\"If True, runs tracking on the predicted instances.\",\n)\n@click.option(\n    \"-n\",\n    \"--max_instances\",\n    type=int,\n    default=None,\n    help=\"Limit maximum number of instances in multi-instance models. Not available for ID models. Defaults to None.\",\n)\n@click.option(\n    \"--backbone_ckpt_path\",\n    type=str,\n    default=None,\n    help=\"To run inference on any `.ckpt` other than `best.ckpt` from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\",\n)\n@click.option(\n    \"--head_ckpt_path\",\n    type=str,\n    default=None,\n    help=\"Path to `.ckpt` file if a different set of head layer weights are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt from `backbone_ckpt_path` if provided.)\",\n)\n@click.option(\n    \"--max_height\",\n    type=int,\n    default=None,\n    help=\"Maximum height the image should be padded to. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--max_width\",\n    type=int,\n    default=None,\n    help=\"Maximum width the image should be padded to. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--input_scale\",\n    type=float,\n    default=None,\n    help=\"Scale factor to apply to the input image. If not provided, the values from the training config are used. Default: None.\",\n)\n@click.option(\n    \"--ensure_rgb\",\n    is_flag=True,\n    default=False,\n    help=\"True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to `True`, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the values from the training config are used. Default: `None`.\",\n)\n@click.option(\n    \"--ensure_grayscale\",\n    is_flag=True,\n    default=False,\n    help=\"True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the values from the training config are used. Default: `None`.\",\n)\n@click.option(\n    \"--anchor_part\",\n    type=str,\n    default=None,\n    help=\"The node name to use as the anchor for the centroid. If not provided, the anchor part in the `training_config.yaml` is used. Default: `None`.\",\n)\n@click.option(\n    \"--only_labeled_frames\",\n    is_flag=True,\n    default=False,\n    help=\"Only run inference on user labeled frames when running on labels dataset. This is useful for generating predictions to compare against ground truth.\",\n)\n@click.option(\n    \"--only_suggested_frames\",\n    is_flag=True,\n    default=False,\n    help=\"Only run inference on unlabeled suggested frames when running on labels dataset. This is useful for generating predictions for initialization during labeling.\",\n)\n@click.option(\n    \"--exclude_user_labeled\",\n    is_flag=True,\n    default=False,\n    help=\"Skip frames that have user-labeled instances. Useful when predicting on entire video but skipping already-labeled frames.\",\n)\n@click.option(\n    \"--only_predicted_frames\",\n    is_flag=True,\n    default=False,\n    help=\"Only run inference on frames that already have predictions. Requires .slp input file. Useful for re-predicting with a different model.\",\n)\n@click.option(\n    \"--no_empty_frames\",\n    is_flag=True,\n    default=False,\n    help=(\"Clear empty frames that did not have predictions before saving to output.\"),\n)\n@click.option(\n    \"--video_index\",\n    type=int,\n    default=None,\n    help=\"Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.\",\n)\n@click.option(\n    \"--video_dataset\", type=str, default=None, help=\"The dataset for HDF5 videos.\"\n)\n@click.option(\n    \"--video_input_format\",\n    type=str,\n    default=\"channels_last\",\n    help=\"The input_format for HDF5 videos.\",\n)\n@click.option(\n    \"--frames\",\n    type=str,\n    default=\"\",\n    help=\"List of frames to predict when running on a video. Can be specified as a comma separated list (e.g. 1,2,3) or a range separated by hyphen (e.g., 1-3, for 1,2,3). If not provided, defaults to predicting on the entire video.\",\n)\n@click.option(\n    \"--integral_patch_size\",\n    type=int,\n    default=5,\n    help=\"Size of patches to crop around each rough peak as an integer scalar. Default: 5.\",\n)\n@click.option(\n    \"--max_edge_length_ratio\",\n    type=float,\n    default=0.25,\n    help=\"The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching. Default: 0.25.\",\n)\n@click.option(\n    \"--dist_penalty_weight\",\n    type=float,\n    default=1.0,\n    help=\"A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly. Default: 1.0.\",\n)\n@click.option(\n    \"--n_points\",\n    type=int,\n    default=10,\n    help=\"Number of points to sample along the line integral. Default: 10.\",\n)\n@click.option(\n    \"--min_instance_peaks\",\n    type=float,\n    default=0,\n    help=\"Minimum number of peaks the instance should have to be considered a real instance. Instances with fewer peaks than this will be discarded (useful for filtering spurious detections). Default: 0.\",\n)\n@click.option(\n    \"--min_line_scores\",\n    type=float,\n    default=0.25,\n    help=\"Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones. Default: 0.25.\",\n)\n@click.option(\n    \"--queue_maxsize\",\n    type=int,\n    default=32,\n    help=\"Maximum size of the frame buffer queue.\",\n)\n@click.option(\n    \"--crop_size\",\n    type=int,\n    default=None,\n    help=\"Crop size. If not provided, the crop size from training_config.yaml is used. If `input_scale` is provided, then the cropped image will be resized according to `input_scale`.\",\n)\n@click.option(\n    \"--peak_threshold\",\n    type=float,\n    default=0.2,\n    help=\"Minimum confidence map value to consider a peak as valid.\",\n)\n@click.option(\n    \"--filter_overlapping\",\n    is_flag=True,\n    default=False,\n    help=(\n        \"Enable filtering of overlapping instances after inference using greedy NMS. \"\n        \"Applied independently of tracking. (default: False)\"\n    ),\n)\n@click.option(\n    \"--filter_overlapping_method\",\n    type=click.Choice([\"iou\", \"oks\"]),\n    default=\"iou\",\n    help=(\n        \"Similarity metric for filtering overlapping instances. \"\n        \"'iou': bounding box intersection-over-union. \"\n        \"'oks': Object Keypoint Similarity (pose-based). (default: iou)\"\n    ),\n)\n@click.option(\n    \"--filter_overlapping_threshold\",\n    type=float,\n    default=0.8,\n    help=(\n        \"Similarity threshold for filtering overlapping instances. \"\n        \"Instances with similarity above this threshold are removed, \"\n        \"keeping the higher-scoring instance. \"\n        \"Typical values: 0.3 (aggressive) to 0.8 (permissive). (default: 0.8)\"\n    ),\n)\n@click.option(\n    \"--integral_refinement\",\n    type=str,\n    default=\"integral\",\n    help=\"If `None`, returns the grid-aligned peaks with no refinement. If `'integral'`, peaks will be refined with integral regression. Default: 'integral'.\",\n)\n@click.option(\n    \"--tracking_window_size\",\n    type=int,\n    default=5,\n    help=\"Number of frames to look for in the candidate instances to match with the current detections.\",\n)\n@click.option(\n    \"--min_new_track_points\",\n    type=int,\n    default=0,\n    help=\"We won't spawn a new track for an instance with fewer than this many points.\",\n)\n@click.option(\n    \"--candidates_method\",\n    type=str,\n    default=\"fixed_window\",\n    help=\"Either of `fixed_window` or `local_queues`. In fixed window method, candidates from the last `window_size` frames. In local queues, last `window_size` instances for each track ID is considered for matching against the current detection.\",\n)\n@click.option(\n    \"--min_match_points\",\n    type=int,\n    default=0,\n    help=\"Minimum non-NaN points for match candidates.\",\n)\n@click.option(\n    \"--features\",\n    type=str,\n    default=\"keypoints\",\n    help=\"Feature representation for the candidates to update current detections. One of [`keypoints`, `centroids`, `bboxes`, `image`].\",\n)\n@click.option(\n    \"--scoring_method\",\n    type=str,\n    default=\"oks\",\n    help=\"Method to compute association score between features from the current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`, `euclidean_dist`].\",\n)\n@click.option(\n    \"--scoring_reduction\",\n    type=str,\n    default=\"mean\",\n    help=\"Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [`mean`, `max`, `robust_quantile`].\",\n)\n@click.option(\n    \"--robust_best_instance\",\n    type=float,\n    default=1.0,\n    help=\"If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.\",\n)\n@click.option(\n    \"--track_matching_method\",\n    type=str,\n    default=\"hungarian\",\n    help=\"Track matching algorithm. One of `hungarian`, `greedy`.\",\n)\n@click.option(\n    \"--max_tracks\",\n    type=int,\n    default=None,\n    help=\"Maximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate)\",\n)\n@click.option(\n    \"--use_flow\",\n    is_flag=True,\n    default=False,\n    help=\"If True, `FlowShiftTracker` is used, where the poses are matched using optical flow shifts.\",\n)\n@click.option(\n    \"--of_img_scale\",\n    type=float,\n    default=1.0,\n    help=\"Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements.\",\n)\n@click.option(\n    \"--of_window_size\",\n    type=int,\n    default=21,\n    help=\"Optical flow window size to consider at each pyramid scale level.\",\n)\n@click.option(\n    \"--of_max_levels\",\n    type=int,\n    default=3,\n    help=\"Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling.\",\n)\n@click.option(\n    \"--post_connect_single_breaks\",\n    is_flag=True,\n    default=False,\n    help=\"If True and `max_tracks` is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.\",\n)\n@click.option(\n    \"--tracking_target_instance_count\",\n    type=int,\n    default=None,\n    help=\"Target number of instances to track per frame. (default: 0)\",\n)\n@click.option(\n    \"--tracking_pre_cull_to_target\",\n    type=int,\n    default=0,\n    help=(\n        \"If non-zero and target_instance_count is also non-zero, then cull instances \"\n        \"over target count per frame *before* tracking. (default: 0)\"\n    ),\n)\n@click.option(\n    \"--tracking_pre_cull_iou_threshold\",\n    type=float,\n    default=0,\n    help=(\n        \"If non-zero and pre_cull_to_target also set, then use IOU threshold to remove \"\n        \"overlapping instances over count *before* tracking. (default: 0)\"\n    ),\n)\n@click.option(\n    \"--tracking_clean_instance_count\",\n    type=int,\n    default=0,\n    help=\"Target number of instances to clean *after* tracking. (default: 0)\",\n)\n@click.option(\n    \"--tracking_clean_iou_threshold\",\n    type=float,\n    default=0,\n    help=\"IOU to use when culling instances *after* tracking. (default: 0)\",\n)\n@click.option(\n    \"--gui\",\n    is_flag=True,\n    default=False,\n    help=\"Output JSON progress for GUI integration instead of Rich progress bar.\",\n)\ndef track(**kwargs):\n    \"\"\"Run Inference and Tracking workflow.\"\"\"\n    # Convert model_paths from tuple to list\n    if \"model_paths\" in kwargs and kwargs[\"model_paths\"]:\n        kwargs[\"model_paths\"] = list(kwargs[\"model_paths\"])\n    else:\n        kwargs[\"model_paths\"] = None\n\n    # Convert frames string to list\n    if \"frames\" in kwargs and kwargs[\"frames\"]:\n        kwargs[\"frames\"] = frame_list(kwargs[\"frames\"])\n    else:\n        kwargs[\"frames\"] = None\n\n    # Call the original function\n    return run_inference(**kwargs)\n</code></pre>"},{"location":"api/cli/#sleap_nn.cli.train","title":"<code>train(config, config_name, config_dir, video_paths, video_path_map, prefix_map, video_config, overrides)</code>","text":"<p>Run training workflow with Hydra config overrides.</p> <p>Automatically detects multi-GPU setups and handles run_name synchronization by spawning training in a subprocess with a pre-generated config.</p> <p>Examples:</p> <p>sleap-nn train path/to/config.yaml sleap-nn train --config path/to/config.yaml trainer_config.max_epochs=100 sleap-nn train config.yaml trainer_config.trainer_devices=4</p> Source code in <code>sleap_nn/cli.py</code> <pre><code>@cli.command(cls=TrainCommand)\n@click.option(\n    \"--config\",\n    type=str,\n    help=\"Path to configuration file (e.g., path/to/config.yaml)\",\n)\n@click.option(\"--config-name\", \"-c\", type=str, help=\"Configuration file name (legacy)\")\n@click.option(\n    \"--config-dir\", \"-d\", type=str, default=\".\", help=\"Configuration directory (legacy)\"\n)\n@click.option(\n    \"--video-paths\",\n    \"-v\",\n    multiple=True,\n    help=\"Video paths to replace existing paths in the labels file. \"\n    \"Order must match the order of videos in the labels file. \"\n    \"Can be specified multiple times. \"\n    \"Example: --video-paths /path/to/vid1.mp4 --video-paths /path/to/vid2.mp4\",\n)\n@click.option(\n    \"--video-path-map\",\n    nargs=2,\n    multiple=True,\n    callback=parse_path_map,\n    metavar=\"OLD NEW\",\n    help=\"Map old video path to new path. Takes two arguments: old path and new path. \"\n    \"Can be specified multiple times. \"\n    'Example: --video-path-map \"/old/vid.mp`4\" \"/new/vid.mp4\"',\n)\n@click.option(\n    \"--prefix-map\",\n    nargs=2,\n    multiple=True,\n    callback=parse_path_map,\n    metavar=\"OLD NEW\",\n    help=\"Map old path prefix to new prefix. Takes two arguments: old prefix and new prefix. \"\n    \"Updates ALL videos that share the same prefix. Useful when moving data between machines. \"\n    \"Can be specified multiple times. \"\n    'Example: --prefix-map \"/old/server/path\" \"/new/local/path\"',\n)\n@click.option(\n    \"--video-config\",\n    type=str,\n    hidden=True,\n    help=\"Path to video replacement config YAML (internal use for multi-GPU).\",\n)\n@click.argument(\"overrides\", nargs=-1, type=click.UNPROCESSED)\ndef train(\n    config,\n    config_name,\n    config_dir,\n    video_paths,\n    video_path_map,\n    prefix_map,\n    video_config,\n    overrides,\n):\n    \"\"\"Run training workflow with Hydra config overrides.\n\n    Automatically detects multi-GPU setups and handles run_name synchronization\n    by spawning training in a subprocess with a pre-generated config.\n\n    Examples:\n        sleap-nn train path/to/config.yaml\n        sleap-nn train --config path/to/config.yaml trainer_config.max_epochs=100\n        sleap-nn train config.yaml trainer_config.trainer_devices=4\n    \"\"\"\n    # Convert overrides to a mutable list\n    overrides = list(overrides)\n\n    # Check if the first positional arg is a config path (not a Hydra override)\n    config_from_positional = None\n    if overrides and is_config_path(overrides[0]):\n        config_from_positional = overrides.pop(0)\n\n    # Resolve config path with priority:\n    # 1. Positional config path (e.g., sleap-nn train config.yaml)\n    # 2. --config flag (e.g., sleap-nn train --config config.yaml)\n    # 3. Legacy --config-dir/--config-name flags\n    if config_from_positional:\n        config_dir, config_name = split_config_path(config_from_positional)\n    elif config:\n        config_dir, config_name = split_config_path(config)\n    elif config_name:\n        config_dir = Path(config_dir).resolve().as_posix()\n    else:\n        # No config provided - show help\n        show_training_help()\n        return\n\n    # Check video path options early\n    # If --video-config is provided (from subprocess), load from file\n    if video_config:\n        video_cfg = OmegaConf.load(video_config)\n        video_paths = tuple(video_cfg.video_paths) if video_cfg.video_paths else ()\n        video_path_map = (\n            dict(video_cfg.video_path_map) if video_cfg.video_path_map else None\n        )\n        prefix_map = dict(video_cfg.prefix_map) if video_cfg.prefix_map else None\n\n    has_video_paths = len(video_paths) &gt; 0\n    has_video_path_map = video_path_map is not None\n    has_prefix_map = prefix_map is not None\n    options_used = sum([has_video_paths, has_video_path_map, has_prefix_map])\n\n    if options_used &gt; 1:\n        raise click.UsageError(\n            \"Cannot use multiple path replacement options. \"\n            \"Choose one of: --video-paths, --video-path-map, or --prefix-map.\"\n        )\n\n    # Load config to detect device count\n    with hydra.initialize_config_dir(config_dir=config_dir, version_base=None):\n        cfg = hydra.compose(config_name=config_name, overrides=overrides)\n\n        # Validate config\n        if not hasattr(cfg, \"model_config\") or not cfg.model_config:\n            click.echo(\n                \"No model config found! Use `sleap-nn train --help` for more information.\"\n            )\n            raise click.Abort()\n\n        num_devices = _get_num_devices_from_config(cfg)\n\n        # Check if run_name is already set (means we're a subprocess or user provided it)\n        run_name = OmegaConf.select(cfg, \"trainer_config.run_name\", default=None)\n        run_name_is_set = run_name is not None and run_name != \"\" and run_name != \"None\"\n\n    # Multi-GPU path: spawn subprocess with finalized config\n    # Only do this if run_name is NOT set (otherwise we'd loop infinitely or user set it)\n    if num_devices &gt; 1 and not run_name_is_set:\n        logger.info(\n            f\"Detected {num_devices} devices, using subprocess for run_name sync...\"\n        )\n\n        # Load and finalize config (generate run_name, apply overrides)\n        with hydra.initialize_config_dir(config_dir=config_dir, version_base=None):\n            cfg = hydra.compose(config_name=config_name, overrides=overrides)\n            cfg = _finalize_config(cfg)\n\n        # Save finalized config to temp file\n        temp_dir = tempfile.mkdtemp(prefix=\"sleap_nn_train_\")\n        temp_config_path = Path(temp_dir) / \"training_config.yaml\"\n        OmegaConf.save(cfg, temp_config_path)\n        logger.info(f\"Saved finalized config to: {temp_config_path}\")\n\n        # Save video replacement config if needed (so subprocess doesn't need CLI args)\n        temp_video_config_path = None\n        if options_used == 1:\n            video_replacement_config = {\n                \"video_paths\": list(video_paths) if has_video_paths else None,\n                \"video_path_map\": dict(video_path_map) if has_video_path_map else None,\n                \"prefix_map\": dict(prefix_map) if has_prefix_map else None,\n            }\n            temp_video_config_path = Path(temp_dir) / \"video_replacement.yaml\"\n            OmegaConf.save(\n                OmegaConf.create(video_replacement_config), temp_video_config_path\n            )\n            logger.info(f\"Saved video replacement config to: {temp_video_config_path}\")\n\n        # Build subprocess command (no video args - they're in the temp file)\n        cmd = [sys.executable, \"-m\", \"sleap_nn.cli\", \"train\", str(temp_config_path)]\n        if temp_video_config_path:\n            cmd.extend([\"--video-config\", str(temp_video_config_path)])\n\n        logger.info(f\"Launching subprocess: {' '.join(cmd)}\")\n\n        try:\n            process = subprocess.Popen(cmd)\n            result = process.wait()\n            if result != 0:\n                logger.error(f\"Training failed with exit code {result}\")\n                sys.exit(result)\n        except KeyboardInterrupt:\n            logger.info(\"Training interrupted, terminating subprocess...\")\n            process.terminate()\n            try:\n                process.wait(timeout=5)\n            except subprocess.TimeoutExpired:\n                process.kill()\n                process.wait()\n            sys.exit(1)\n        finally:\n            shutil.rmtree(temp_dir, ignore_errors=True)\n            logger.info(\"Cleaned up temporary files\")\n\n        return\n\n    # Single GPU (or subprocess worker): run directly\n    with hydra.initialize_config_dir(config_dir=config_dir, version_base=None):\n        cfg = hydra.compose(config_name=config_name, overrides=overrides)\n\n        logger.info(\"Input config:\")\n        logger.info(\"\\n\" + OmegaConf.to_yaml(cfg))\n\n        # Handle video path replacement options\n        train_labels = None\n        val_labels = None\n\n        if options_used == 1:\n            # Load train labels\n            train_labels = [\n                sio.load_slp(path) for path in cfg.data_config.train_labels_path\n            ]\n\n            # Load val labels if they exist\n            if (\n                cfg.data_config.val_labels_path is not None\n                and len(cfg.data_config.val_labels_path) &gt; 0\n            ):\n                val_labels = [\n                    sio.load_slp(path) for path in cfg.data_config.val_labels_path\n                ]\n\n            # Build replacement arguments based on option used\n            if has_video_paths:\n                replace_kwargs = {\n                    \"new_filenames\": [Path(p).as_posix() for p in video_paths]\n                }\n            elif has_video_path_map:\n                replace_kwargs = {\"filename_map\": video_path_map}\n            else:  # has_prefix_map\n                replace_kwargs = {\"prefix_map\": prefix_map}\n\n            # Apply replacement to train labels\n            for labels in train_labels:\n                labels.replace_filenames(**replace_kwargs)\n\n            # Apply replacement to val labels if they exist\n            if val_labels:\n                for labels in val_labels:\n                    labels.replace_filenames(**replace_kwargs)\n\n        run_training(config=cfg, train_labels=train_labels, val_labels=val_labels)\n</code></pre>"},{"location":"api/evaluation/","title":"evaluation","text":""},{"location":"api/evaluation/#sleap_nn.evaluation","title":"<code>sleap_nn.evaluation</code>","text":"<p>This module is to compute evaluation metrics for trained models.</p> <p>Classes:</p> Name Description <code>Evaluator</code> <p>Compute the standard evaluation metrics with the predicted and the ground-truth Labels.</p> <code>MatchInstance</code> <p>Class to have a new structure for sio.Instance object.</p> <p>Functions:</p> Name Description <code>compute_dists</code> <p>Compute Euclidean distances between matched pairs of instances.</p> <code>compute_instance_area</code> <p>Compute the area of the bounding box of a set of keypoints.</p> <code>compute_oks</code> <p>Compute the object keypoints similarity between sets of points.</p> <code>find_frame_pairs</code> <p>Find corresponding frames across two sets of labels.</p> <code>get_instances</code> <p>Get a list of instances of type MatchInstance from the Labeled Frame.</p> <code>load_metrics</code> <p>Load metrics from a model folder or metrics file.</p> <code>match_frame_pairs</code> <p>Match all ground truth and predicted instances within each pair of frames.</p> <code>match_instances</code> <p>Match pairs of instances between ground truth and predictions in a frame.</p> <code>run_evaluation</code> <p>Evaluate SLEAP-NN model predictions against ground truth labels.</p>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator","title":"<code>Evaluator</code>","text":"<p>Compute the standard evaluation metrics with the predicted and the ground-truth Labels.</p> <p>This class is used to calculate the common metrics for pose estimation models which includes voc metrics (with oks and pck), mOKS, distance metrics, pck metrics and visibility metrics.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_instances</code> <code>Labels</code> <p>The <code>sio.Labels</code> dataset object with ground truth labels.</p> required <code>predicted_instances</code> <code>Labels</code> <p>The <code>sio.Labels</code> dataset object with predicted labels.</p> required <code>oks_stddev</code> <code>float</code> <p>The standard deviation to use for calculating object keypoint similarity; see <code>compute_oks</code> function for details.</p> <code>0.025</code> <code>oks_scale</code> <code>Optional[float]</code> <p>The scale to use for calculating object keypoint similarity; see <code>compute_oks</code> function for details.</p> <code>None</code> <code>match_threshold</code> <code>float</code> <p>The threshold to use on oks scores when determining which instances match between ground truth and predicted frames.</p> <code>0</code> <code>user_labels_only</code> <code>bool</code> <p>If False, predicted instances in the ground truth frame may be considered for matching.</p> <code>True</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the Evaluator class with ground-truth and predicted labels.</p> <code>distance_metrics</code> <p>Compute the Euclidean distance error at different percentiles using the pairwise distances.</p> <code>evaluate</code> <p>Return the evaluation metrics.</p> <code>mOKS</code> <p>Return the meanOKS value.</p> <code>pck_metrics</code> <p>Compute PCK across a range of thresholds using the pair-wise distances.</p> <code>visibility_metrics</code> <p>Compute node visibility metrics for the matched pair of instances.</p> <code>voc_metrics</code> <p>Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>class Evaluator:\n    \"\"\"Compute the standard evaluation metrics with the predicted and the ground-truth Labels.\n\n    This class is used to calculate the common metrics for pose estimation models which\n    includes voc metrics (with oks and pck), mOKS, distance metrics, pck metrics and\n    visibility metrics.\n\n    Args:\n        ground_truth_instances: The `sio.Labels` dataset object with ground truth labels.\n        predicted_instances: The `sio.Labels` dataset object with predicted labels.\n        oks_stddev: The standard deviation to use for calculating object\n            keypoint similarity; see `compute_oks` function for details.\n        oks_scale: The scale to use for calculating object\n            keypoint similarity; see `compute_oks` function for details.\n        match_threshold: The threshold to use on oks scores when determining\n            which instances match between ground truth and predicted frames.\n        user_labels_only: If False, predicted instances in the ground truth frame may be\n            considered for matching.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ground_truth_instances: sio.Labels,\n        predicted_instances: sio.Labels,\n        oks_stddev: float = 0.025,\n        oks_scale: Optional[float] = None,\n        match_threshold: float = 0,\n        user_labels_only: bool = True,\n    ):\n        \"\"\"Initialize the Evaluator class with ground-truth and predicted labels.\"\"\"\n        self.ground_truth_instances = ground_truth_instances\n        self.predicted_instances = predicted_instances\n        self.match_threshold = match_threshold\n        self.oks_stddev = oks_stddev\n        self.oks_scale = oks_scale\n        self.user_labels_only = user_labels_only\n\n        self._process_frames()\n\n    def _process_frames(self):\n        self.frame_pairs = find_frame_pairs(\n            self.ground_truth_instances, self.predicted_instances, self.user_labels_only\n        )\n        if not self.frame_pairs:\n            message = \"Empty Frame Pairs. No match found for the video frames\"\n            logger.error(message)\n            raise Exception(message)\n\n        self.positive_pairs, self.false_negatives = match_frame_pairs(\n            self.frame_pairs,\n            stddev=self.oks_stddev,\n            scale=self.oks_scale,\n            threshold=self.match_threshold,\n        )\n\n        self.dists_dict = compute_dists(self.positive_pairs)\n\n    def voc_metrics(\n        self,\n        match_score_by=\"oks\",\n        match_score_thresholds: np.ndarray = np.linspace(\n            0.5, 0.95, 10\n        ),  # 0.5:0.05:0.95\n        recall_thresholds: np.ndarray = np.linspace(0, 1, 101),  # 0.0:0.01:1.00\n    ):\n        \"\"\"Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.\n\n        Args:\n            match_score_by: The score to be used for computing the metrics. \"ock\" or \"pck\"\n            match_score_thresholds: Score thresholds at which to consider matches as a true\n                positive match.\n            recall_thresholds: Recall thresholds at which to evaluate Average Precision.\n\n        Returns:\n            A dictionary of VOC metrics.\n        \"\"\"\n        if match_score_by == \"oks\":\n            match_scores = np.array([oks for _, _, oks in self.positive_pairs])\n            name = \"oks_voc\"\n        elif match_score_by == \"pck\":\n            pck_metrics = self.pck_metrics()\n            match_scores = pck_metrics[\"pcks\"].mean(axis=-1).mean(axis=-1)\n            name = \"pck_voc\"\n        else:\n            message = \"Invalid Option for match_score_by. Choose either `oks` or `pck`\"\n            logger.error(message)\n            raise Exception(message)\n\n        detection_scores = np.array(\n            [pp[1].instance.score for pp in self.positive_pairs]\n        )\n\n        inds = np.argsort(-detection_scores, kind=\"mergesort\")\n        detection_scores = detection_scores[inds]\n        match_scores = match_scores[inds]\n\n        precisions = []\n        recalls = []\n\n        npig = len(self.positive_pairs) + len(\n            self.false_negatives\n        )  # total number of GT instances\n\n        for match_score_threshold in match_score_thresholds:\n            tp = np.cumsum(match_scores &gt;= match_score_threshold)\n            fp = np.cumsum(match_scores &lt; match_score_threshold)\n\n            if tp.size == 0:\n                return {\n                    name + \".match_score_thresholds\": 0,\n                    name + \".recall_thresholds\": 0,\n                    name + \".match_scores\": 0,\n                    name + \".precisions\": 0,\n                    name + \".recalls\": 0,\n                    name + \".AP\": 0,\n                    name + \".AR\": 0,\n                    name + \".mAP\": 0,\n                    name + \".mAR\": 0,\n                }\n\n            rc = tp / npig\n            pr = tp / (fp + tp + np.spacing(1))\n\n            recall = rc[-1]  # best recall at this OKS threshold\n\n            # Ensure strictly decreasing precisions.\n            for i in range(len(pr) - 1, 0, -1):\n                if pr[i] &gt; pr[i - 1]:\n                    pr[i - 1] = pr[i]\n\n            # Find best precision at each recall threshold.\n            rc_inds = np.searchsorted(rc, recall_thresholds, side=\"left\")\n            precision = np.zeros(rc_inds.shape)\n            is_valid_rc_ind = rc_inds &lt; len(pr)\n            precision[is_valid_rc_ind] = pr[rc_inds[is_valid_rc_ind]]\n\n            precisions.append(precision)\n            recalls.append(recall)\n\n        precisions = np.array(precisions)\n        recalls = np.array(recalls)\n\n        AP = precisions.mean(\n            axis=1\n        )  # AP = average precision over fixed set of recall thresholds\n        AR = recalls  # AR = max recall given a fixed number of detections per image\n\n        mAP = precisions.mean()  # mAP = mean over all OKS thresholds\n        mAR = recalls.mean()  # mAR = mean over all OKS thresholds\n\n        return {\n            name + \".match_score_thresholds\": match_score_thresholds,\n            name + \".recall_thresholds\": recall_thresholds,\n            name + \".match_scores\": match_scores,\n            name + \".precisions\": precisions,\n            name + \".recalls\": recalls,\n            name + \".AP\": AP,\n            name + \".AR\": AR,\n            name + \".mAP\": mAP,\n            name + \".mAR\": mAR,\n        }\n\n    def mOKS(self):\n        \"\"\"Return the meanOKS value.\"\"\"\n        pair_oks = np.array([oks for _, _, oks in self.positive_pairs])\n        return {\"mOKS\": pair_oks.mean()}\n\n    def distance_metrics(self):\n        \"\"\"Compute the Euclidean distance error at different percentiles using the pairwise distances.\n\n        Returns:\n            A dictionary of distance metrics.\n        \"\"\"\n        dists = self.dists_dict[\"dists\"]\n        results = {\n            \"frame_idxs\": self.dists_dict[\"frame_idxs\"],\n            \"video_paths\": self.dists_dict[\"video_paths\"],\n            \"dists\": dists,\n            \"avg\": np.nanmean(dists),\n            \"p50\": np.nan,\n            \"p75\": np.nan,\n            \"p90\": np.nan,\n            \"p95\": np.nan,\n            \"p99\": np.nan,\n        }\n\n        is_non_nan = ~np.isnan(dists)\n        if np.any(is_non_nan):\n            non_nans = dists[is_non_nan]\n            for ptile in (50, 75, 90, 95, 99):\n                results[f\"p{ptile}\"] = np.percentile(non_nans, ptile)\n\n        return results\n\n    def pck_metrics(self, thresholds: np.ndarray = np.linspace(1, 10, 10)):\n        \"\"\"Compute PCK across a range of thresholds using the pair-wise distances.\n\n        Args:\n            thresholds: A list of distance thresholds in pixels.\n\n        Returns:\n            A dictionary of PCK metrics evaluated at each threshold.\n        \"\"\"\n        dists = self.dists_dict[\"dists\"]\n        dists = np.copy(dists)\n        dists[np.isnan(dists)] = np.inf\n        pcks = np.expand_dims(dists, -1) &lt; np.reshape(thresholds, (1, 1, -1))\n        mPCK_parts = pcks.mean(axis=0).mean(axis=-1)\n        mPCK = mPCK_parts.mean()\n\n        # Precompute PCK at common thresholds\n        idx_5 = np.argmin(np.abs(thresholds - 5))\n        idx_10 = np.argmin(np.abs(thresholds - 10))\n        pck5 = pcks[:, :, idx_5].mean()\n        pck10 = pcks[:, :, idx_10].mean()\n\n        return {\n            \"thresholds\": thresholds,\n            \"pcks\": pcks,\n            \"mPCK_parts\": mPCK_parts,\n            \"mPCK\": mPCK,\n            \"PCK@5\": pck5,\n            \"PCK@10\": pck10,\n        }\n\n    def visibility_metrics(self):\n        \"\"\"Compute node visibility metrics for the matched pair of instances.\n\n        Returns:\n            A dictionary of visibility metrics, including the confusion matrix.\n        \"\"\"\n        vis_tp = 0\n        vis_fn = 0\n        vis_fp = 0\n        vis_tn = 0\n\n        for instance_gt, instance_pr, _ in self.positive_pairs:\n            missing_nodes_gt = np.isnan(instance_gt.instance.numpy()).any(axis=-1)\n            missing_nodes_pr = np.isnan(instance_pr.instance.numpy()).any(axis=-1)\n\n            vis_tn += ((missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n            vis_fn += ((~missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n            vis_fp += ((missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n            vis_tp += ((~missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n\n        return {\n            \"tp\": vis_tp,\n            \"fp\": vis_fp,\n            \"tn\": vis_tn,\n            \"fn\": vis_fn,\n            \"precision\": vis_tp / (vis_tp + vis_fp) if (vis_tp + vis_fp) else np.nan,\n            \"recall\": vis_tp / (vis_tp + vis_fn) if (vis_tp + vis_fn) else np.nan,\n        }\n\n    def evaluate(self):\n        \"\"\"Return the evaluation metrics.\"\"\"\n        metrics = {}\n        metrics[\"voc_metrics\"] = self.voc_metrics(match_score_by=\"oks\")\n        metrics[\"voc_metrics\"].update(self.voc_metrics(match_score_by=\"pck\"))\n        metrics[\"mOKS\"] = self.mOKS()\n        metrics[\"distance_metrics\"] = self.distance_metrics()\n        metrics[\"pck_metrics\"] = self.pck_metrics()\n        metrics[\"visibility_metrics\"] = self.visibility_metrics()\n\n        return metrics\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.__init__","title":"<code>__init__(ground_truth_instances, predicted_instances, oks_stddev=0.025, oks_scale=None, match_threshold=0, user_labels_only=True)</code>","text":"<p>Initialize the Evaluator class with ground-truth and predicted labels.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def __init__(\n    self,\n    ground_truth_instances: sio.Labels,\n    predicted_instances: sio.Labels,\n    oks_stddev: float = 0.025,\n    oks_scale: Optional[float] = None,\n    match_threshold: float = 0,\n    user_labels_only: bool = True,\n):\n    \"\"\"Initialize the Evaluator class with ground-truth and predicted labels.\"\"\"\n    self.ground_truth_instances = ground_truth_instances\n    self.predicted_instances = predicted_instances\n    self.match_threshold = match_threshold\n    self.oks_stddev = oks_stddev\n    self.oks_scale = oks_scale\n    self.user_labels_only = user_labels_only\n\n    self._process_frames()\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.distance_metrics","title":"<code>distance_metrics()</code>","text":"<p>Compute the Euclidean distance error at different percentiles using the pairwise distances.</p> <p>Returns:</p> Type Description <p>A dictionary of distance metrics.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def distance_metrics(self):\n    \"\"\"Compute the Euclidean distance error at different percentiles using the pairwise distances.\n\n    Returns:\n        A dictionary of distance metrics.\n    \"\"\"\n    dists = self.dists_dict[\"dists\"]\n    results = {\n        \"frame_idxs\": self.dists_dict[\"frame_idxs\"],\n        \"video_paths\": self.dists_dict[\"video_paths\"],\n        \"dists\": dists,\n        \"avg\": np.nanmean(dists),\n        \"p50\": np.nan,\n        \"p75\": np.nan,\n        \"p90\": np.nan,\n        \"p95\": np.nan,\n        \"p99\": np.nan,\n    }\n\n    is_non_nan = ~np.isnan(dists)\n    if np.any(is_non_nan):\n        non_nans = dists[is_non_nan]\n        for ptile in (50, 75, 90, 95, 99):\n            results[f\"p{ptile}\"] = np.percentile(non_nans, ptile)\n\n    return results\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.evaluate","title":"<code>evaluate()</code>","text":"<p>Return the evaluation metrics.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def evaluate(self):\n    \"\"\"Return the evaluation metrics.\"\"\"\n    metrics = {}\n    metrics[\"voc_metrics\"] = self.voc_metrics(match_score_by=\"oks\")\n    metrics[\"voc_metrics\"].update(self.voc_metrics(match_score_by=\"pck\"))\n    metrics[\"mOKS\"] = self.mOKS()\n    metrics[\"distance_metrics\"] = self.distance_metrics()\n    metrics[\"pck_metrics\"] = self.pck_metrics()\n    metrics[\"visibility_metrics\"] = self.visibility_metrics()\n\n    return metrics\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.mOKS","title":"<code>mOKS()</code>","text":"<p>Return the meanOKS value.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def mOKS(self):\n    \"\"\"Return the meanOKS value.\"\"\"\n    pair_oks = np.array([oks for _, _, oks in self.positive_pairs])\n    return {\"mOKS\": pair_oks.mean()}\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.pck_metrics","title":"<code>pck_metrics(thresholds=np.linspace(1, 10, 10))</code>","text":"<p>Compute PCK across a range of thresholds using the pair-wise distances.</p> <p>Parameters:</p> Name Type Description Default <code>thresholds</code> <code>ndarray</code> <p>A list of distance thresholds in pixels.</p> <code>linspace(1, 10, 10)</code> <p>Returns:</p> Type Description <p>A dictionary of PCK metrics evaluated at each threshold.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def pck_metrics(self, thresholds: np.ndarray = np.linspace(1, 10, 10)):\n    \"\"\"Compute PCK across a range of thresholds using the pair-wise distances.\n\n    Args:\n        thresholds: A list of distance thresholds in pixels.\n\n    Returns:\n        A dictionary of PCK metrics evaluated at each threshold.\n    \"\"\"\n    dists = self.dists_dict[\"dists\"]\n    dists = np.copy(dists)\n    dists[np.isnan(dists)] = np.inf\n    pcks = np.expand_dims(dists, -1) &lt; np.reshape(thresholds, (1, 1, -1))\n    mPCK_parts = pcks.mean(axis=0).mean(axis=-1)\n    mPCK = mPCK_parts.mean()\n\n    # Precompute PCK at common thresholds\n    idx_5 = np.argmin(np.abs(thresholds - 5))\n    idx_10 = np.argmin(np.abs(thresholds - 10))\n    pck5 = pcks[:, :, idx_5].mean()\n    pck10 = pcks[:, :, idx_10].mean()\n\n    return {\n        \"thresholds\": thresholds,\n        \"pcks\": pcks,\n        \"mPCK_parts\": mPCK_parts,\n        \"mPCK\": mPCK,\n        \"PCK@5\": pck5,\n        \"PCK@10\": pck10,\n    }\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.visibility_metrics","title":"<code>visibility_metrics()</code>","text":"<p>Compute node visibility metrics for the matched pair of instances.</p> <p>Returns:</p> Type Description <p>A dictionary of visibility metrics, including the confusion matrix.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def visibility_metrics(self):\n    \"\"\"Compute node visibility metrics for the matched pair of instances.\n\n    Returns:\n        A dictionary of visibility metrics, including the confusion matrix.\n    \"\"\"\n    vis_tp = 0\n    vis_fn = 0\n    vis_fp = 0\n    vis_tn = 0\n\n    for instance_gt, instance_pr, _ in self.positive_pairs:\n        missing_nodes_gt = np.isnan(instance_gt.instance.numpy()).any(axis=-1)\n        missing_nodes_pr = np.isnan(instance_pr.instance.numpy()).any(axis=-1)\n\n        vis_tn += ((missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n        vis_fn += ((~missing_nodes_gt) &amp; (missing_nodes_pr)).sum()\n        vis_fp += ((missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n        vis_tp += ((~missing_nodes_gt) &amp; (~missing_nodes_pr)).sum()\n\n    return {\n        \"tp\": vis_tp,\n        \"fp\": vis_fp,\n        \"tn\": vis_tn,\n        \"fn\": vis_fn,\n        \"precision\": vis_tp / (vis_tp + vis_fp) if (vis_tp + vis_fp) else np.nan,\n        \"recall\": vis_tp / (vis_tp + vis_fn) if (vis_tp + vis_fn) else np.nan,\n    }\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.Evaluator.voc_metrics","title":"<code>voc_metrics(match_score_by='oks', match_score_thresholds=np.linspace(0.5, 0.95, 10), recall_thresholds=np.linspace(0, 1, 101))</code>","text":"<p>Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.</p> <p>Parameters:</p> Name Type Description Default <code>match_score_by</code> <p>The score to be used for computing the metrics. \"ock\" or \"pck\"</p> <code>'oks'</code> <code>match_score_thresholds</code> <code>ndarray</code> <p>Score thresholds at which to consider matches as a true positive match.</p> <code>linspace(0.5, 0.95, 10)</code> <code>recall_thresholds</code> <code>ndarray</code> <p>Recall thresholds at which to evaluate Average Precision.</p> <code>linspace(0, 1, 101)</code> <p>Returns:</p> Type Description <p>A dictionary of VOC metrics.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def voc_metrics(\n    self,\n    match_score_by=\"oks\",\n    match_score_thresholds: np.ndarray = np.linspace(\n        0.5, 0.95, 10\n    ),  # 0.5:0.05:0.95\n    recall_thresholds: np.ndarray = np.linspace(0, 1, 101),  # 0.0:0.01:1.00\n):\n    \"\"\"Compute VOC metrics for a matched pairs of instances positive pairs and false negatives.\n\n    Args:\n        match_score_by: The score to be used for computing the metrics. \"ock\" or \"pck\"\n        match_score_thresholds: Score thresholds at which to consider matches as a true\n            positive match.\n        recall_thresholds: Recall thresholds at which to evaluate Average Precision.\n\n    Returns:\n        A dictionary of VOC metrics.\n    \"\"\"\n    if match_score_by == \"oks\":\n        match_scores = np.array([oks for _, _, oks in self.positive_pairs])\n        name = \"oks_voc\"\n    elif match_score_by == \"pck\":\n        pck_metrics = self.pck_metrics()\n        match_scores = pck_metrics[\"pcks\"].mean(axis=-1).mean(axis=-1)\n        name = \"pck_voc\"\n    else:\n        message = \"Invalid Option for match_score_by. Choose either `oks` or `pck`\"\n        logger.error(message)\n        raise Exception(message)\n\n    detection_scores = np.array(\n        [pp[1].instance.score for pp in self.positive_pairs]\n    )\n\n    inds = np.argsort(-detection_scores, kind=\"mergesort\")\n    detection_scores = detection_scores[inds]\n    match_scores = match_scores[inds]\n\n    precisions = []\n    recalls = []\n\n    npig = len(self.positive_pairs) + len(\n        self.false_negatives\n    )  # total number of GT instances\n\n    for match_score_threshold in match_score_thresholds:\n        tp = np.cumsum(match_scores &gt;= match_score_threshold)\n        fp = np.cumsum(match_scores &lt; match_score_threshold)\n\n        if tp.size == 0:\n            return {\n                name + \".match_score_thresholds\": 0,\n                name + \".recall_thresholds\": 0,\n                name + \".match_scores\": 0,\n                name + \".precisions\": 0,\n                name + \".recalls\": 0,\n                name + \".AP\": 0,\n                name + \".AR\": 0,\n                name + \".mAP\": 0,\n                name + \".mAR\": 0,\n            }\n\n        rc = tp / npig\n        pr = tp / (fp + tp + np.spacing(1))\n\n        recall = rc[-1]  # best recall at this OKS threshold\n\n        # Ensure strictly decreasing precisions.\n        for i in range(len(pr) - 1, 0, -1):\n            if pr[i] &gt; pr[i - 1]:\n                pr[i - 1] = pr[i]\n\n        # Find best precision at each recall threshold.\n        rc_inds = np.searchsorted(rc, recall_thresholds, side=\"left\")\n        precision = np.zeros(rc_inds.shape)\n        is_valid_rc_ind = rc_inds &lt; len(pr)\n        precision[is_valid_rc_ind] = pr[rc_inds[is_valid_rc_ind]]\n\n        precisions.append(precision)\n        recalls.append(recall)\n\n    precisions = np.array(precisions)\n    recalls = np.array(recalls)\n\n    AP = precisions.mean(\n        axis=1\n    )  # AP = average precision over fixed set of recall thresholds\n    AR = recalls  # AR = max recall given a fixed number of detections per image\n\n    mAP = precisions.mean()  # mAP = mean over all OKS thresholds\n    mAR = recalls.mean()  # mAR = mean over all OKS thresholds\n\n    return {\n        name + \".match_score_thresholds\": match_score_thresholds,\n        name + \".recall_thresholds\": recall_thresholds,\n        name + \".match_scores\": match_scores,\n        name + \".precisions\": precisions,\n        name + \".recalls\": recalls,\n        name + \".AP\": AP,\n        name + \".AR\": AR,\n        name + \".mAP\": mAP,\n        name + \".mAR\": mAR,\n    }\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.MatchInstance","title":"<code>MatchInstance</code>","text":"<p>Class to have a new structure for sio.Instance object.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>@attrs.define(auto_attribs=True, slots=True)\nclass MatchInstance:\n    \"\"\"Class to have a new structure for sio.Instance object.\"\"\"\n\n    instance: sio.Instance\n    frame_idx: int\n    video_path: str\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.compute_dists","title":"<code>compute_dists(positive_pairs)</code>","text":"<p>Compute Euclidean distances between matched pairs of instances.</p> <p>Parameters:</p> Name Type Description Default <code>positive_pairs</code> <code>List[Tuple[Instance, PredictedInstance, Any]]</code> <p>A list of tuples of the form <code>(instance_gt, instance_pr, _)</code> containing the matched pair of instances.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, List[int], List[str]]]</code> <p>A dictionary with the following keys:     dists: An array of pairwise distances of shape <code>(n_positive_pairs, n_nodes)</code>     frame_idxs: A list of frame indices corresponding to the <code>dists</code>     video_paths: A list of video paths corresponding to the <code>dists</code></p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def compute_dists(\n    positive_pairs: List[Tuple[sio.Instance, sio.PredictedInstance, Any]],\n) -&gt; Dict[str, Union[np.ndarray, List[int], List[str]]]:\n    \"\"\"Compute Euclidean distances between matched pairs of instances.\n\n    Args:\n        positive_pairs: A list of tuples of the form `(instance_gt, instance_pr, _)`\n            containing the matched pair of instances.\n\n    Returns:\n        A dictionary with the following keys:\n            dists: An array of pairwise distances of shape `(n_positive_pairs, n_nodes)`\n            frame_idxs: A list of frame indices corresponding to the `dists`\n            video_paths: A list of video paths corresponding to the `dists`\n    \"\"\"\n    dists = []\n    frame_idxs = []\n    video_paths = []\n    for instance_gt, instance_pr, _ in positive_pairs:\n        points_gt = instance_gt.instance.numpy()\n        points_pr = instance_pr.instance.numpy()\n\n        dists.append(np.linalg.norm(points_pr - points_gt, axis=-1))\n        frame_idxs.append(instance_gt.frame_idx)\n        video_paths.append(instance_gt.video_path)\n\n    dists = np.array(dists)\n\n    # Bundle everything into a dictionary\n    dists_dict = {\n        \"dists\": dists,\n        \"frame_idxs\": frame_idxs,\n        \"video_paths\": video_paths,\n    }\n\n    return dists_dict\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.compute_instance_area","title":"<code>compute_instance_area(points)</code>","text":"<p>Compute the area of the bounding box of a set of keypoints.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>A numpy array of coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The area of the bounding box of the points.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def compute_instance_area(points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the area of the bounding box of a set of keypoints.\n\n    Args:\n        points: A numpy array of coordinates.\n\n    Returns:\n        The area of the bounding box of the points.\n    \"\"\"\n    if points.ndim == 2:\n        points = np.expand_dims(points, axis=0)\n\n    min_pt = np.nanmin(points, axis=-2)\n    max_pt = np.nanmax(points, axis=-2)\n\n    return np.prod(max_pt - min_pt, axis=-1)\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.compute_oks","title":"<code>compute_oks(points_gt, points_pr, scale=None, stddev=0.025, use_cocoeval=True)</code>","text":"<p>Compute the object keypoints similarity between sets of points.</p> <p>Parameters:</p> Name Type Description Default <code>points_gt</code> <code>ndarray</code> <p>Ground truth instances of shape (n_gt, n_nodes, n_ed), where n_nodes is the number of body parts/keypoint types, and n_ed is the number of Euclidean dimensions (typically 2 or 3). Keypoints that are missing/not visible should be represented as NaNs.</p> required <code>points_pr</code> <code>ndarray</code> <p>Predicted instance of shape (n_pr, n_nodes, n_ed).</p> required <code>use_cocoeval</code> <code>bool</code> <p>Indicates whether the OKS score is calculated like cocoeval method or not. True indicating the score is calculated using the cocoeval method (widely used and the code can be found here at https://github.com/cocodataset/cocoapi/blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/PythonAPI/pycocotools/cocoeval.py#L192C5-L233C20) and False indicating the score is calculated using the method exactly as given in the paper referenced in the Notes below.</p> <code>True</code> <code>scale</code> <code>Optional[float]</code> <p>Size scaling factor to use when weighing the scores, typically the area of the bounding box of the instance (in pixels). This should be of the length n_gt. If a scalar is provided, the same number is used for all ground truth instances. If set to None, the bounding box area of the ground truth instances will be calculated.</p> <code>None</code> <code>stddev</code> <code>float</code> <p>The standard deviation associated with the spread in the localization accuracy of each node/keypoint type. This should be of the length n_nodes. \"Easier\" keypoint types will have lower values to reflect the smaller spread expected in localizing it.</p> <code>0.025</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The object keypoints similarity between every pair of ground truth and predicted instance, a numpy array of of shape (n_gt, n_pr) in the range of [0, 1.0], with 1.0 denoting a perfect match.</p> Notes <p>It's important to set the stddev appropriately when accounting for the difficulty of each keypoint type. For reference, the median value for all keypoint types in COCO is 0.072. The \"easiest\" keypoint is the left eye, with stddev of 0.025, since it is easy to precisely locate the eyes when labeling. The \"hardest\" keypoint is the left hip, with stddev of 0.107, since it's hard to locate the left hip bone without external anatomical features and since it is often occluded by clothing.</p> <p>The implementation here is based off of the descriptions in: Ronch &amp; Perona. \"Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation.\" ICCV (2017).</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def compute_oks(\n    points_gt: np.ndarray,\n    points_pr: np.ndarray,\n    scale: Optional[float] = None,\n    stddev: float = 0.025,\n    use_cocoeval: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Compute the object keypoints similarity between sets of points.\n\n    Args:\n        points_gt: Ground truth instances of shape (n_gt, n_nodes, n_ed),\n            where n_nodes is the number of body parts/keypoint types, and n_ed\n            is the number of Euclidean dimensions (typically 2 or 3). Keypoints\n            that are missing/not visible should be represented as NaNs.\n        points_pr: Predicted instance of shape (n_pr, n_nodes, n_ed).\n        use_cocoeval: Indicates whether the OKS score is calculated like cocoeval\n            method or not. True indicating the score is calculated using the\n            cocoeval method (widely used and the code can be found here at\n            https://github.com/cocodataset/cocoapi/blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/PythonAPI/pycocotools/cocoeval.py#L192C5-L233C20)\n            and False indicating the score is calculated using the method exactly\n            as given in the paper referenced in the Notes below.\n        scale: Size scaling factor to use when weighing the scores, typically\n            the area of the bounding box of the instance (in pixels). This\n            should be of the length n_gt. If a scalar is provided, the same\n            number is used for all ground truth instances. If set to None, the\n            bounding box area of the ground truth instances will be calculated.\n        stddev: The standard deviation associated with the spread in the\n            localization accuracy of each node/keypoint type. This should be of\n            the length n_nodes. \"Easier\" keypoint types will have lower values\n            to reflect the smaller spread expected in localizing it.\n\n    Returns:\n        The object keypoints similarity between every pair of ground truth and\n        predicted instance, a numpy array of of shape (n_gt, n_pr) in the range\n        of [0, 1.0], with 1.0 denoting a perfect match.\n\n    Notes:\n        It's important to set the stddev appropriately when accounting for the\n        difficulty of each keypoint type. For reference, the median value for\n        all keypoint types in COCO is 0.072. The \"easiest\" keypoint is the left\n        eye, with stddev of 0.025, since it is easy to precisely locate the\n        eyes when labeling. The \"hardest\" keypoint is the left hip, with stddev\n        of 0.107, since it's hard to locate the left hip bone without external\n        anatomical features and since it is often occluded by clothing.\n\n        The implementation here is based off of the descriptions in:\n        Ronch &amp; Perona. \"Benchmarking and Error Diagnosis in Multi-Instance Pose\n        Estimation.\" ICCV (2017).\n    \"\"\"\n    if points_gt.ndim == 2:\n        points_gt = np.expand_dims(points_gt, axis=0)\n    if points_pr.ndim == 2:\n        points_pr = np.expand_dims(points_pr, axis=0)\n\n    if scale is None:\n        scale = compute_instance_area(points_gt)\n\n    n_gt, n_nodes, n_ed = points_gt.shape  # n_ed = 2 or 3 (euclidean dimensions)\n    n_pr = points_pr.shape[0]\n\n    # If scalar scale was provided, use the same for each ground truth instance.\n    if np.isscalar(scale):\n        scale = np.full(n_gt, scale)\n\n    # If scalar standard deviation was provided, use the same for each node.\n    if np.isscalar(stddev):\n        stddev = np.full(n_nodes, stddev)\n\n    # Compute displacement between each pair.\n    displacement = np.reshape(points_gt, (n_gt, 1, n_nodes, n_ed)) - np.reshape(\n        points_pr, (1, n_pr, n_nodes, n_ed)\n    )\n    assert displacement.shape == (n_gt, n_pr, n_nodes, n_ed)\n\n    # Convert to pairwise Euclidean distances.\n    distance = (displacement**2).sum(axis=-1)  # (n_gt, n_pr, n_nodes)\n    assert distance.shape == (n_gt, n_pr, n_nodes)\n\n    # Compute the normalization factor per keypoint.\n    if use_cocoeval:\n        # If use_cocoeval is True, then compute normalization factor according to cocoeval.\n        spread_factor = (2 * stddev) ** 2\n        scale_factor = 2 * (scale + np.spacing(1))\n    else:\n        # If use_cocoeval is False, then compute normalization factor according to the paper.\n        spread_factor = stddev**2\n        scale_factor = 2 * ((scale + np.spacing(1)) ** 2)\n    normalization_factor = np.reshape(spread_factor, (1, 1, n_nodes)) * np.reshape(\n        scale_factor, (n_gt, 1, 1)\n    )\n    assert normalization_factor.shape == (n_gt, 1, n_nodes)\n\n    # Since a \"miss\" is considered as KS &lt; 0.5, we'll set the\n    # distances for predicted points that are missing to inf.\n    missing_pr = np.any(np.isnan(points_pr), axis=-1)  # (n_pr, n_nodes)\n    assert missing_pr.shape == (n_pr, n_nodes)\n    distance[:, missing_pr] = np.inf\n\n    # Compute the keypoint similarity as per the top of Eq. 1.\n    ks = np.exp(-(distance / normalization_factor))  # (n_gt, n_pr, n_nodes)\n    assert ks.shape == (n_gt, n_pr, n_nodes)\n\n    # Set the KS for missing ground truth points to 0.\n    # This is equivalent to the visibility delta function of the bottom\n    # of Eq. 1.\n    missing_gt = np.any(np.isnan(points_gt), axis=-1)  # (n_gt, n_nodes)\n    assert missing_gt.shape == (n_gt, n_nodes)\n    ks[np.expand_dims(missing_gt, axis=1)] = 0\n\n    # Compute the OKS.\n    n_visible_gt = np.sum(\n        (~missing_gt).astype(\"float32\"), axis=-1, keepdims=True\n    )  # (n_gt, 1)\n    oks = np.sum(ks, axis=-1) / n_visible_gt\n    assert oks.shape == (n_gt, n_pr)\n\n    return oks\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.find_frame_pairs","title":"<code>find_frame_pairs(labels_gt, labels_pr, user_labels_only=True)</code>","text":"<p>Find corresponding frames across two sets of labels.</p> <p>This function uses sleap-io's robust video matching API to handle various scenarios including embedded videos, cross-platform paths, and videos with different metadata.</p> <p>Parameters:</p> Name Type Description Default <code>labels_gt</code> <code>Labels</code> <p>A <code>sio.Labels</code> instance with ground truth instances.</p> required <code>labels_pr</code> <code>Labels</code> <p>A <code>sio.Labels</code> instance with predicted instances.</p> required <code>user_labels_only</code> <code>bool</code> <p>If False, frames with predicted instances in <code>labels_gt</code> will also be considered for matching.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Tuple[LabeledFrame, LabeledFrame]]</code> <p>A list of pairs of <code>sio.LabeledFrame</code>s in the form <code>(frame_gt, frame_pr)</code>.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def find_frame_pairs(\n    labels_gt: sio.Labels, labels_pr: sio.Labels, user_labels_only: bool = True\n) -&gt; List[Tuple[sio.LabeledFrame, sio.LabeledFrame]]:\n    \"\"\"Find corresponding frames across two sets of labels.\n\n    This function uses sleap-io's robust video matching API to handle various\n    scenarios including embedded videos, cross-platform paths, and videos with\n    different metadata.\n\n    Args:\n        labels_gt: A `sio.Labels` instance with ground truth instances.\n        labels_pr: A `sio.Labels` instance with predicted instances.\n        user_labels_only: If False, frames with predicted instances in `labels_gt` will\n            also be considered for matching.\n\n    Returns:\n        A list of pairs of `sio.LabeledFrame`s in the form `(frame_gt, frame_pr)`.\n    \"\"\"\n    # Use sleap-io's robust video matching API (added in 0.6.2)\n    # The match() method returns a MatchResult with video_map: {pred_video: gt_video}\n    match_result = labels_gt.match(labels_pr)\n\n    frame_pairs = []\n    # Iterate over matched video pairs (pred_video -&gt; gt_video mapping)\n    for video_pr, video_gt in match_result.video_map.items():\n        if video_gt is None:\n            # No match found for this prediction video\n            continue\n\n        # Find labeled frames in this video.\n        labeled_frames_gt = labels_gt.find(video_gt)\n        if user_labels_only:\n            for lf in labeled_frames_gt:\n                lf.instances = lf.user_instances\n            labeled_frames_gt = [\n                lf for lf in labeled_frames_gt if len(lf.user_instances) &gt; 0\n            ]\n\n        # Attempt to match each labeled frame in the ground truth.\n        for labeled_frame_gt in labeled_frames_gt:\n            labeled_frames_pr = labels_pr.find(\n                video_pr, frame_idx=labeled_frame_gt.frame_idx\n            )\n\n            if not labeled_frames_pr:\n                # No match\n                continue\n            elif len(labeled_frames_pr) == 1:\n                # Match!\n                frame_pairs.append((labeled_frame_gt, labeled_frames_pr[0]))\n\n    return frame_pairs\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.get_instances","title":"<code>get_instances(labeled_frame)</code>","text":"<p>Get a list of instances of type MatchInstance from the Labeled Frame.</p> <p>Parameters:</p> Name Type Description Default <code>labeled_frame</code> <code>LabeledFrame</code> <p>Input Labeled frame of type sio.LabeledFrame.</p> required <p>Returns:</p> Type Description <code>List[MatchInstance]</code> <p>List of MatchInstance objects for the given labeled frame.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def get_instances(labeled_frame: sio.LabeledFrame) -&gt; List[MatchInstance]:\n    \"\"\"Get a list of instances of type MatchInstance from the Labeled Frame.\n\n    Args:\n        labeled_frame: Input Labeled frame of type sio.LabeledFrame.\n\n    Returns:\n        List of MatchInstance objects for the given labeled frame.\n    \"\"\"\n    instance_list = []\n    frame_idx = labeled_frame.frame_idx\n\n    # Extract video path with fallbacks for embedded videos\n    video = labeled_frame.video\n    video_path = None\n    if video is not None:\n        backend = getattr(video, \"backend\", None)\n        if backend is not None:\n            # Try source_filename first (for embedded videos with provenance)\n            video_path = getattr(backend, \"source_filename\", None)\n            if video_path is None:\n                video_path = getattr(backend, \"filename\", None)\n        # Fallback to video.filename if backend doesn't have it\n        if video_path is None:\n            video_path = getattr(video, \"filename\", None)\n            # Handle list filenames (image sequences)\n            if isinstance(video_path, list) and video_path:\n                video_path = video_path[0]\n    # Final fallback: use a unique identifier\n    if video_path is None:\n        video_path = f\"video_{id(video)}\" if video is not None else \"unknown\"\n\n    for instance in labeled_frame.instances:\n        match_instance = MatchInstance(\n            instance=instance, frame_idx=frame_idx, video_path=video_path\n        )\n        instance_list.append(match_instance)\n    return instance_list\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.load_metrics","title":"<code>load_metrics(path, split='test', dataset_idx=0)</code>","text":"<p>Load metrics from a model folder or metrics file.</p> <p>This function supports both the new format (single \"metrics\" key) and the old format (individual metric keys at top level). It also handles both old and new file naming conventions in model folders.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a model folder or metrics file (.npz).</p> required <code>split</code> <code>str</code> <p>Name of the split to load. Must be \"train\", \"val\", or \"test\". Default: \"test\". If \"test\" is not found, falls back to \"val\". Ignored if path points directly to a .npz file.</p> <code>'test'</code> <code>dataset_idx</code> <code>int</code> <p>Index of the dataset (for multi-dataset training). Default: 0. Ignored if path points directly to a .npz file.</p> <code>0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing metrics with keys: voc_metrics, mOKS, distance_metrics, pck_metrics, visibility_metrics.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no metrics file is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load from model folder (tries test, falls back to val)\n&gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\")\n&gt;&gt;&gt; print(metrics[\"mOKS\"][\"mOKS\"])\n</code></pre> <pre><code>&gt;&gt;&gt; # Load specific split and dataset\n&gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\", split=\"val\", dataset_idx=1)\n</code></pre> <pre><code>&gt;&gt;&gt; # Load directly from npz file\n&gt;&gt;&gt; metrics = load_metrics(\"/path/to/metrics.val.0.npz\")\n</code></pre> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def load_metrics(\n    path: str,\n    split: str = \"test\",\n    dataset_idx: int = 0,\n) -&gt; dict:\n    \"\"\"Load metrics from a model folder or metrics file.\n\n    This function supports both the new format (single \"metrics\" key) and the old\n    format (individual metric keys at top level). It also handles both old and new\n    file naming conventions in model folders.\n\n    Args:\n        path: Path to a model folder or metrics file (.npz).\n        split: Name of the split to load. Must be \"train\", \"val\", or \"test\".\n            Default: \"test\". If \"test\" is not found, falls back to \"val\".\n            Ignored if path points directly to a .npz file.\n        dataset_idx: Index of the dataset (for multi-dataset training).\n            Default: 0. Ignored if path points directly to a .npz file.\n\n    Returns:\n        Dictionary containing metrics with keys: voc_metrics, mOKS,\n        distance_metrics, pck_metrics, visibility_metrics.\n\n    Raises:\n        FileNotFoundError: If no metrics file is found.\n\n    Examples:\n        &gt;&gt;&gt; # Load from model folder (tries test, falls back to val)\n        &gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\")\n        &gt;&gt;&gt; print(metrics[\"mOKS\"][\"mOKS\"])\n\n        &gt;&gt;&gt; # Load specific split and dataset\n        &gt;&gt;&gt; metrics = load_metrics(\"/path/to/model\", split=\"val\", dataset_idx=1)\n\n        &gt;&gt;&gt; # Load directly from npz file\n        &gt;&gt;&gt; metrics = load_metrics(\"/path/to/metrics.val.0.npz\")\n    \"\"\"\n    path = Path(path)\n\n    if path.suffix == \".npz\":\n        metrics_path = path\n    else:\n        metrics_path = _find_metrics_file(path, split, dataset_idx)\n\n    if not metrics_path.exists():\n        raise FileNotFoundError(f\"Metrics file not found at {metrics_path}\")\n\n    return _load_npz_metrics(metrics_path)\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.match_frame_pairs","title":"<code>match_frame_pairs(frame_pairs, stddev=0.025, scale=None, threshold=0)</code>","text":"<p>Match all ground truth and predicted instances within each pair of frames.</p> <p>This is a wrapper for <code>match_instances()</code> but operates on lists of frames.</p> <p>Parameters:</p> Name Type Description Default <code>frame_pairs</code> <code>List[Tuple[LabeledFrame, LabeledFrame]]</code> <p>A list of pairs of <code>sleap.LabeledFrame</code>s in the form <code>(frame_gt, frame_pr)</code>. These can be obtained with <code>find_frame_pairs()</code>.</p> required <code>stddev</code> <code>float</code> <p>The expected spread of coordinates for OKS computation.</p> <code>0.025</code> <code>scale</code> <code>Optional[float]</code> <p>The scale for normalizing the OKS. If not set, the bounding box area will be used.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The minimum OKS between a candidate pair of instances to be considered a match.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[List[Tuple[Instance, PredictedInstance, float]], List[Instance]]</code> <p>A tuple of (<code>positive_pairs</code>, <code>false_negatives</code>).</p> <p><code>positive_pairs</code> is a list of 3-tuples of the form <code>(instance_gt, instance_pr, oks)</code> containing the matched pair of instances and their OKS.</p> <p><code>false_negatives</code> is a list of ground truth <code>sio.Instance</code>s that could not be matched.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def match_frame_pairs(\n    frame_pairs: List[Tuple[sio.LabeledFrame, sio.LabeledFrame]],\n    stddev: float = 0.025,\n    scale: Optional[float] = None,\n    threshold: float = 0,\n) -&gt; Tuple[List[Tuple[sio.Instance, sio.PredictedInstance, float]], List[sio.Instance]]:\n    \"\"\"Match all ground truth and predicted instances within each pair of frames.\n\n    This is a wrapper for `match_instances()` but operates on lists of frames.\n\n    Args:\n        frame_pairs: A list of pairs of `sleap.LabeledFrame`s in the form\n            `(frame_gt, frame_pr)`. These can be obtained with `find_frame_pairs()`.\n        stddev: The expected spread of coordinates for OKS computation.\n        scale: The scale for normalizing the OKS. If not set, the bounding box area will\n            be used.\n        threshold: The minimum OKS between a candidate pair of instances to be\n            considered a match.\n\n    Returns:\n        A tuple of (`positive_pairs`, `false_negatives`).\n\n        `positive_pairs` is a list of 3-tuples of the form\n        `(instance_gt, instance_pr, oks)` containing the matched pair of instances and\n        their OKS.\n\n        `false_negatives` is a list of ground truth `sio.Instance`s that could not be\n        matched.\n    \"\"\"\n    positive_pairs = []\n    false_negatives = []\n    for frame_gt, frame_pr in frame_pairs:\n        positive_pairs_frame, false_negatives_frame = match_instances(\n            frame_gt,\n            frame_pr,\n            stddev=stddev,\n            scale=scale,\n            threshold=threshold,\n        )\n        positive_pairs.extend(positive_pairs_frame)\n        false_negatives.extend(false_negatives_frame)\n\n    return positive_pairs, false_negatives\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.match_instances","title":"<code>match_instances(frame_gt, frame_pr, stddev=0.025, scale=None, threshold=0)</code>","text":"<p>Match pairs of instances between ground truth and predictions in a frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame_gt</code> <code>LabeledFrame</code> <p>A <code>sio.LabeledFrame</code> with ground truth instances.</p> required <code>frame_pr</code> <code>LabeledFrame</code> <p>A <code>sio.LabeledFrame</code> with predicted instances.</p> required <code>stddev</code> <code>float</code> <p>The expected spread of coordinates for OKS computation.</p> <code>0.025</code> <code>scale</code> <code>Optional[float]</code> <p>The scale for normalizing the OKS. If not set, the bounding box area will be used.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>The minimum OKS between a candidate pair of instances to be considered a match.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[List[Tuple[Instance, PredictedInstance, float]], List[Instance]]</code> <p>A tuple of (<code>positive_pairs</code>, <code>false_negatives</code>).</p> <p><code>positive_pairs</code> is a list of 3-tuples of the form <code>(instance_gt, instance_pr, oks)</code> containing the matched pair of instances and their OKS.</p> <p><code>false_negatives</code> is a list of ground truth <code>sleap.Instance</code>s that could not be matched.</p> Notes <p>This function uses the approach from the PASCAL VOC scoring procedure. Briefly, predictions are sorted descending by their instance-level prediction scores and greedily matched to ground truth instances which are then removed from the pool of available instances.</p> <p>Ground truth instances that remain unmatched are considered false negatives.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def match_instances(\n    frame_gt: sio.LabeledFrame,\n    frame_pr: sio.LabeledFrame,\n    stddev: float = 0.025,\n    scale: Optional[float] = None,\n    threshold: float = 0,\n) -&gt; Tuple[List[Tuple[sio.Instance, sio.PredictedInstance, float]], List[sio.Instance]]:\n    \"\"\"Match pairs of instances between ground truth and predictions in a frame.\n\n    Args:\n        frame_gt: A `sio.LabeledFrame` with ground truth instances.\n        frame_pr: A `sio.LabeledFrame` with predicted instances.\n        stddev: The expected spread of coordinates for OKS computation.\n        scale: The scale for normalizing the OKS. If not set, the bounding box area will\n            be used.\n        threshold: The minimum OKS between a candidate pair of instances to be\n            considered a match.\n\n    Returns:\n        A tuple of (`positive_pairs`, `false_negatives`).\n\n        `positive_pairs` is a list of 3-tuples of the form\n        `(instance_gt, instance_pr, oks)` containing the matched pair of instances and\n        their OKS.\n\n        `false_negatives` is a list of ground truth `sleap.Instance`s that could not be\n        matched.\n\n    Notes:\n        This function uses the approach from the PASCAL VOC scoring procedure. Briefly,\n        predictions are sorted descending by their instance-level prediction scores and\n        greedily matched to ground truth instances which are then removed from the pool\n        of available instances.\n\n        Ground truth instances that remain unmatched are considered false negatives.\n    \"\"\"\n    # Sort predicted instances by score.\n    frame_pr_match_instances = get_instances(frame_pr)\n\n    scores_pr = np.array(\n        [\n            m.instance.score\n            for m in frame_pr_match_instances\n            if hasattr(m.instance, \"score\")\n        ]\n    )\n    idxs_pr = np.argsort(-scores_pr, kind=\"mergesort\")  # descending\n    scores_pr = scores_pr[idxs_pr]\n\n    available_instances_gt = get_instances(frame_gt)\n    available_instances_gt_idxs = list(range(len(available_instances_gt)))\n\n    positive_pairs = []\n    for idx_pr in idxs_pr:\n        # Pull out predicted instance.\n        instance_pr = frame_pr_match_instances[idx_pr]\n\n        # Convert instances to point arrays.\n        points_pr = np.expand_dims(instance_pr.instance.numpy(), axis=0)\n        points_gt = np.stack(\n            [\n                available_instances_gt[idx].instance.numpy()\n                for idx in available_instances_gt_idxs\n            ],\n            axis=0,\n        )\n\n        # Find the best match by computing OKS.\n        oks = compute_oks(points_gt, points_pr, stddev=stddev, scale=scale)\n        oks = np.squeeze(oks, axis=1)\n        assert oks.shape == (len(points_gt),)\n\n        oks[oks &lt;= threshold] = np.nan\n        best_match_gt_idx = np.argsort(-oks, kind=\"mergesort\")[0]\n        best_match_oks = oks[best_match_gt_idx]\n        if np.isnan(best_match_oks):\n            continue\n\n        # Remove matched ground truth instance and add as a positive pair.\n        instance_gt_idx = available_instances_gt_idxs.pop(best_match_gt_idx)\n        instance_gt = available_instances_gt[instance_gt_idx]\n        positive_pairs.append((instance_gt, instance_pr, best_match_oks))\n\n        # Stop matching lower scoring instances if we run out of candidates in the\n        # ground truth.\n        if not available_instances_gt_idxs:\n            break\n\n    # Any remaining ground truth instances are considered false negatives.\n    false_negatives = [\n        available_instances_gt[idx] for idx in available_instances_gt_idxs\n    ]\n\n    return positive_pairs, false_negatives\n</code></pre>"},{"location":"api/evaluation/#sleap_nn.evaluation.run_evaluation","title":"<code>run_evaluation(ground_truth_path, predicted_path, oks_stddev=0.025, oks_scale=None, match_threshold=0, user_labels_only=True, save_metrics=None)</code>","text":"<p>Evaluate SLEAP-NN model predictions against ground truth labels.</p> Source code in <code>sleap_nn/evaluation.py</code> <pre><code>def run_evaluation(\n    ground_truth_path: str,\n    predicted_path: str,\n    oks_stddev: float = 0.025,\n    oks_scale: Optional[float] = None,\n    match_threshold: float = 0,\n    user_labels_only: bool = True,\n    save_metrics: Optional[str] = None,\n):\n    \"\"\"Evaluate SLEAP-NN model predictions against ground truth labels.\"\"\"\n    logger.info(\"Loading ground truth labels...\")\n    ground_truth_instances = sio.load_slp(ground_truth_path)\n    logger.info(\n        f\"  Ground truth: {len(ground_truth_instances.videos)} videos, \"\n        f\"{len(ground_truth_instances.labeled_frames)} frames\"\n    )\n\n    logger.info(\"Loading predicted labels...\")\n    predicted_instances = sio.load_slp(predicted_path)\n    logger.info(\n        f\"  Predictions: {len(predicted_instances.videos)} videos, \"\n        f\"{len(predicted_instances.labeled_frames)} frames\"\n    )\n\n    logger.info(\"Matching videos and frames...\")\n    # Get match stats before creating evaluator\n    match_result = ground_truth_instances.match(predicted_instances)\n    logger.info(\n        f\"  Videos matched: {match_result.n_videos_matched}/{len(match_result.video_map)}\"\n    )\n\n    logger.info(\"Matching instances...\")\n    evaluator = Evaluator(\n        ground_truth_instances=ground_truth_instances,\n        predicted_instances=predicted_instances,\n        oks_stddev=oks_stddev,\n        oks_scale=oks_scale,\n        match_threshold=match_threshold,\n        user_labels_only=user_labels_only,\n    )\n    logger.info(\n        f\"  Frame pairs: {len(evaluator.frame_pairs)}, \"\n        f\"Matched instances: {len(evaluator.positive_pairs)}, \"\n        f\"Unmatched GT: {len(evaluator.false_negatives)}\"\n    )\n\n    logger.info(\"Computing evaluation metrics...\")\n    metrics = evaluator.evaluate()\n\n    # Compute PCK at specific thresholds (5 and 10 pixels)\n    dists = metrics[\"distance_metrics\"][\"dists\"]\n    dists_clean = np.copy(dists)\n    dists_clean[np.isnan(dists_clean)] = np.inf\n    pck_5 = (dists_clean &lt; 5).mean()\n    pck_10 = (dists_clean &lt; 10).mean()\n\n    # Print key metrics\n    logger.info(\"Evaluation Results:\")\n    logger.info(f\"  mOKS: {metrics['mOKS']['mOKS']:.4f}\")\n    logger.info(f\"  mAP (OKS VOC): {metrics['voc_metrics']['oks_voc.mAP']:.4f}\")\n    logger.info(f\"  mAR (OKS VOC): {metrics['voc_metrics']['oks_voc.mAR']:.4f}\")\n    logger.info(f\"  Average Distance: {metrics['distance_metrics']['avg']:.2f} px\")\n    logger.info(f\"  dist.p50: {metrics['distance_metrics']['p50']:.2f} px\")\n    logger.info(f\"  dist.p95: {metrics['distance_metrics']['p95']:.2f} px\")\n    logger.info(f\"  dist.p99: {metrics['distance_metrics']['p99']:.2f} px\")\n    logger.info(f\"  mPCK: {metrics['pck_metrics']['mPCK']:.4f}\")\n    logger.info(f\"  PCK@5px: {pck_5:.4f}\")\n    logger.info(f\"  PCK@10px: {pck_10:.4f}\")\n    logger.info(\n        f\"  Visibility Precision: {metrics['visibility_metrics']['precision']:.4f}\"\n    )\n    logger.info(f\"  Visibility Recall: {metrics['visibility_metrics']['recall']:.4f}\")\n\n    # Save metrics if path provided\n    if save_metrics:\n        logger.info(f\"Saving metrics to {save_metrics}...\")\n        save_path = Path(save_metrics)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Save metrics in SLEAP 1.4 format (single \"metrics\" key)\n        np.savez_compressed(save_path, **{\"metrics\": metrics})\n        logger.info(f\"Metrics saved successfully to {save_path}\")\n\n    return metrics\n</code></pre>"},{"location":"api/legacy_models/","title":"legacy_models","text":""},{"location":"api/legacy_models/#sleap_nn.legacy_models","title":"<code>sleap_nn.legacy_models</code>","text":"<p>Utilities for loading legacy SLEAP models.</p> <p>This module provides functions to convert SLEAP models trained with the TensorFlow/Keras backend to PyTorch format compatible with sleap-nn.</p> <p>Functions:</p> Name Description <code>convert_keras_to_pytorch_conv2d</code> <p>Convert Keras Conv2D weights to PyTorch format.</p> <code>convert_keras_to_pytorch_conv2d_transpose</code> <p>Convert Keras Conv2DTranspose weights to PyTorch format.</p> <code>create_model_from_legacy_config</code> <p>Create a PyTorch model from a legacy training config.</p> <code>filter_legacy_weights_by_component</code> <p>Filter legacy weights based on component type.</p> <code>get_keras_first_layer_channels</code> <p>Extract the number of input channels from the first layer of a Keras model.</p> <code>load_keras_weights</code> <p>Load all weights from a Keras HDF5 model file.</p> <code>load_legacy_model</code> <p>Load a complete legacy SLEAP model including weights.</p> <code>load_legacy_model_weights</code> <p>Load legacy Keras weights into a PyTorch model.</p> <code>map_legacy_to_pytorch_layers</code> <p>Create mapping between legacy Keras layers and PyTorch model layers.</p> <code>parse_keras_layer_name</code> <p>Parse a Keras layer path to extract basic information.</p> <code>update_backbone_in_channels</code> <p>Update the backbone configuration's in_channels if it's different from the Keras model.</p>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.convert_keras_to_pytorch_conv2d","title":"<code>convert_keras_to_pytorch_conv2d(keras_weight)</code>","text":"<p>Convert Keras Conv2D weights to PyTorch format.</p> <p>Parameters:</p> Name Type Description Default <code>keras_weight</code> <code>ndarray</code> <p>Numpy array with shape (H, W, C_in, C_out) from Keras</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>PyTorch tensor with shape (C_out, C_in, H, W)</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def convert_keras_to_pytorch_conv2d(keras_weight: np.ndarray) -&gt; torch.Tensor:\n    \"\"\"Convert Keras Conv2D weights to PyTorch format.\n\n    Args:\n        keras_weight: Numpy array with shape (H, W, C_in, C_out) from Keras\n\n    Returns:\n        PyTorch tensor with shape (C_out, C_in, H, W)\n    \"\"\"\n    if keras_weight.ndim != 4:\n        raise ValueError(\n            f\"Expected 4D array for Conv2D weights, got shape {keras_weight.shape}\"\n        )\n\n    # Keras: (H, W, C_in, C_out) -&gt; PyTorch: (C_out, C_in, H, W)\n    pytorch_weight = keras_weight.transpose(3, 2, 0, 1)\n    return torch.from_numpy(pytorch_weight.copy()).float()\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.convert_keras_to_pytorch_conv2d_transpose","title":"<code>convert_keras_to_pytorch_conv2d_transpose(keras_weight)</code>","text":"<p>Convert Keras Conv2DTranspose weights to PyTorch format.</p> <p>Parameters:</p> Name Type Description Default <code>keras_weight</code> <code>ndarray</code> <p>Numpy array with shape (H, W, C_out, C_in) from Keras</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>PyTorch tensor with shape (C_in, C_out, H, W)</p> Note <p>Keras stores transposed conv weights differently than regular conv.</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def convert_keras_to_pytorch_conv2d_transpose(keras_weight: np.ndarray) -&gt; torch.Tensor:\n    \"\"\"Convert Keras Conv2DTranspose weights to PyTorch format.\n\n    Args:\n        keras_weight: Numpy array with shape (H, W, C_out, C_in) from Keras\n\n    Returns:\n        PyTorch tensor with shape (C_in, C_out, H, W)\n\n    Note:\n        Keras stores transposed conv weights differently than regular conv.\n    \"\"\"\n    if keras_weight.ndim != 4:\n        raise ValueError(\n            f\"Expected 4D array for Conv2DTranspose weights, got shape {keras_weight.shape}\"\n        )\n\n    # Keras: (H, W, C_out, C_in) -&gt; PyTorch: (C_in, C_out, H, W)\n    pytorch_weight = keras_weight.transpose(3, 2, 0, 1)\n    return torch.from_numpy(pytorch_weight.copy()).float()\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.create_model_from_legacy_config","title":"<code>create_model_from_legacy_config(config_path)</code>","text":"<p>Create a PyTorch model from a legacy training config.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the legacy training_config.json file</p> required <p>Returns:</p> Type Description <code>Model</code> <p>Model instance configured to match the legacy architecture</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def create_model_from_legacy_config(config_path: str) -&gt; Model:\n    \"\"\"Create a PyTorch model from a legacy training config.\n\n    Args:\n        config_path: Path to the legacy training_config.json file\n\n    Returns:\n        Model instance configured to match the legacy architecture\n    \"\"\"\n    # Load config using existing functionality\n    config_path = Path(config_path)\n    if config_path.is_dir():\n        config_path = config_path / \"training_config.json\"\n\n    # Use the existing config loader\n    config = TrainingJobConfig.load_sleap_config(str(config_path))\n\n    # Determine backbone type from config\n    backbone_type = \"unet\"  # Default for legacy models\n\n    # Get backbone config (should be under the unet key for legacy models)\n    backbone_config = config.model_config.backbone_config.unet\n\n    # Check if there's a corresponding .h5 file to extract input channels\n    model_dir = config_path.parent\n    h5_path = model_dir / \"best_model.h5\"\n\n    if h5_path.exists():\n        keras_in_channels = get_keras_first_layer_channels(str(h5_path))\n        if keras_in_channels is not None:\n            backbone_config = update_backbone_in_channels(\n                backbone_config, keras_in_channels\n            )\n\n    # Determine model type from head configs\n    head_configs = config.model_config.head_configs\n    model_type = None\n    active_head_config = None\n\n    if head_configs.centroid is not None:\n        model_type = \"centroid\"\n        active_head_config = head_configs.centroid\n    elif head_configs.centered_instance is not None:\n        model_type = \"centered_instance\"\n        active_head_config = head_configs.centered_instance\n    elif head_configs.single_instance is not None:\n        model_type = \"single_instance\"\n        active_head_config = head_configs.single_instance\n    elif head_configs.bottomup is not None:\n        model_type = \"bottomup\"\n        active_head_config = head_configs.bottomup\n    elif head_configs.multi_class_topdown is not None:\n        model_type = \"multi_class_topdown\"\n        active_head_config = head_configs.multi_class_topdown\n    elif head_configs.multi_class_bottomup is not None:\n        model_type = \"multi_class_bottomup\"\n        active_head_config = head_configs.multi_class_bottomup\n    else:\n        raise ValueError(\"Could not determine model type from head configs\")\n\n    # Create model using the from_config method\n    model = Model.from_config(\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=active_head_config,\n        model_type=model_type,\n    )\n\n    return model\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.filter_legacy_weights_by_component","title":"<code>filter_legacy_weights_by_component(legacy_weights, component)</code>","text":"<p>Filter legacy weights based on component type.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_weights</code> <code>Dict[str, ndarray]</code> <p>Dictionary of legacy weights from load_keras_weights()</p> required <code>component</code> <code>Optional[str]</code> <p>Component type to filter for. One of: - \"backbone\": Keep only encoder/decoder weights (exclude heads) - \"head\": Keep only head layer weights - None: No filtering (keep all weights)</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Filtered dictionary of legacy weights</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def filter_legacy_weights_by_component(\n    legacy_weights: Dict[str, np.ndarray], component: Optional[str]\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Filter legacy weights based on component type.\n\n    Args:\n        legacy_weights: Dictionary of legacy weights from load_keras_weights()\n        component: Component type to filter for. One of:\n            - \"backbone\": Keep only encoder/decoder weights (exclude heads)\n            - \"head\": Keep only head layer weights\n            - None: No filtering (keep all weights)\n\n    Returns:\n        Filtered dictionary of legacy weights\n    \"\"\"\n    if component is None:\n        return legacy_weights\n\n    filtered = {}\n    for path, weight in legacy_weights.items():\n        # Check if this is a head layer (contains \"Head\" in the path)\n        is_head_layer = \"Head\" in path\n\n        if component == \"backbone\" and not is_head_layer:\n            filtered[path] = weight\n        elif component == \"head\" and is_head_layer:\n            filtered[path] = weight\n\n    return filtered\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.get_keras_first_layer_channels","title":"<code>get_keras_first_layer_channels(h5_path)</code>","text":"<p>Extract the number of input channels from the first layer of a Keras model.</p> <p>Parameters:</p> Name Type Description Default <code>h5_path</code> <code>str</code> <p>Path to the .h5 model file</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Number of input channels in the first layer, or None if not found</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def get_keras_first_layer_channels(h5_path: str) -&gt; Optional[int]:\n    \"\"\"Extract the number of input channels from the first layer of a Keras model.\n\n    Args:\n        h5_path: Path to the .h5 model file\n\n    Returns:\n        Number of input channels in the first layer, or None if not found\n    \"\"\"\n    try:\n        with h5py.File(h5_path, \"r\") as f:\n            # Look for the first convolutional layer weights\n            kernel_weights = []\n\n            def collect_kernel_weights(name, obj):\n                if isinstance(obj, h5py.Dataset) and name.startswith(\"model_weights/\"):\n                    # Skip optimizer weights\n                    if \"optimizer_weights\" in name:\n                        return\n\n                    # Look for kernel weights (not bias)\n                    if \"kernel\" in name and obj.ndim == 4:\n                        kernel_weights.append((name, obj.shape))\n\n            # Visit all items to collect kernel weights\n            f.visititems(collect_kernel_weights)\n\n            if not kernel_weights:\n                return None\n\n            # Look for the known first layer patterns (stem0_conv0 or stack0_enc0_conv0)\n            for name, shape in kernel_weights:\n                input_channels = shape[2]\n                layer_name = name.split(\"/\")[1] if len(name.split(\"/\")) &gt; 1 else name\n\n                # Check for the known first layer patterns\n                if \"stem0_conv0\" in layer_name or \"stack0_enc0_conv0\" in layer_name:\n                    logger.info(\n                        f\"Found first layer '{name}' with {input_channels} input channels\"\n                    )\n                    return input_channels\n\n            # If no known first layer patterns are found, return None\n            logger.warning(\n                f\"No known first layer patterns (stem0_conv0 or stack0_enc0_conv0) found in {h5_path}\"\n            )\n            return None\n\n    except Exception as e:\n        logger.warning(f\"Could not extract first layer channels from {h5_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.load_keras_weights","title":"<code>load_keras_weights(h5_path)</code>","text":"<p>Load all weights from a Keras HDF5 model file.</p> <p>Parameters:</p> Name Type Description Default <code>h5_path</code> <code>str</code> <p>Path to the .h5 model file</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary mapping layer paths to weight arrays</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def load_keras_weights(h5_path: str) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Load all weights from a Keras HDF5 model file.\n\n    Args:\n        h5_path: Path to the .h5 model file\n\n    Returns:\n        Dictionary mapping layer paths to weight arrays\n    \"\"\"\n    weights = {}\n\n    with h5py.File(h5_path, \"r\") as f:\n\n        def extract_weights(name, obj):\n            if isinstance(obj, h5py.Dataset) and name.startswith(\"model_weights/\"):\n                # Skip optimizer weights\n                if \"optimizer_weights\" in name:\n                    return\n                weights[name] = obj[:]\n\n        f.visititems(extract_weights)\n\n    return weights\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.load_legacy_model","title":"<code>load_legacy_model(model_dir, load_weights=True)</code>","text":"<p>Load a complete legacy SLEAP model including weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Path to the legacy model directory containing        training_config.json and best_model.h5</p> required <code>load_weights</code> <code>bool</code> <p>Whether to load the weights. If False, only           creates the model architecture.</p> <code>True</code> <p>Returns:</p> Type Description <code>Model</code> <p>Model instance with loaded weights</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def load_legacy_model(model_dir: str, load_weights: bool = True) -&gt; Model:\n    \"\"\"Load a complete legacy SLEAP model including weights.\n\n    Args:\n        model_dir: Path to the legacy model directory containing\n                   training_config.json and best_model.h5\n        load_weights: Whether to load the weights. If False, only\n                      creates the model architecture.\n\n    Returns:\n        Model instance with loaded weights\n    \"\"\"\n    model_dir = Path(model_dir)\n\n    # Create model from config\n    model = create_model_from_legacy_config(str(model_dir))\n    model.eval()\n\n    # Load weights if requested\n    if load_weights:\n        h5_path = model_dir / \"best_model.h5\"\n        if h5_path.exists():\n            load_legacy_model_weights(model, str(h5_path))\n\n        else:\n            message = f\"Model weights not found at {h5_path}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    return model\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.load_legacy_model_weights","title":"<code>load_legacy_model_weights(pytorch_model, h5_path, mapping=None, component=None)</code>","text":"<p>Load legacy Keras weights into a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>pytorch_model</code> <code>Module</code> <p>PyTorch model to load weights into</p> required <code>h5_path</code> <code>str</code> <p>Path to the legacy .h5 model file</p> required <code>mapping</code> <code>Optional[Dict[str, str]]</code> <p>Optional manual mapping of layer names. If None,      will attempt automatic mapping.</p> <code>None</code> <code>component</code> <code>Optional[str]</code> <p>Optional component type for filtering weights. One of: - \"backbone\": Only load encoder/decoder weights (exclude heads) - \"head\": Only load head layer weights - None: Load all weights (default, for full model loading)</p> <code>None</code> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def load_legacy_model_weights(\n    pytorch_model: torch.nn.Module,\n    h5_path: str,\n    mapping: Optional[Dict[str, str]] = None,\n    component: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Load legacy Keras weights into a PyTorch model.\n\n    Args:\n        pytorch_model: PyTorch model to load weights into\n        h5_path: Path to the legacy .h5 model file\n        mapping: Optional manual mapping of layer names. If None,\n                 will attempt automatic mapping.\n        component: Optional component type for filtering weights. One of:\n            - \"backbone\": Only load encoder/decoder weights (exclude heads)\n            - \"head\": Only load head layer weights\n            - None: Load all weights (default, for full model loading)\n    \"\"\"\n    # Load legacy weights\n    legacy_weights = load_keras_weights(h5_path)\n\n    if mapping is None:\n        # Attempt automatic mapping\n        try:\n            mapping = map_legacy_to_pytorch_layers(\n                legacy_weights, pytorch_model, component=component\n            )\n        except Exception as e:\n            logger.error(f\"Failed to create weight mappings: {e}\")\n            return\n\n    # Apply weights\n    loaded_count = 0\n    errors = []\n\n    for legacy_path, pytorch_name in mapping.items():\n        if legacy_path not in legacy_weights:\n            logger.warning(f\"Legacy weight not found: {legacy_path}\")\n            continue\n\n        weight = legacy_weights[legacy_path]\n        info = parse_keras_layer_name(legacy_path)\n\n        # Convert weight format if needed\n        if info[\"weight_type\"] == \"kernel\" and weight.ndim == 4:\n            if \"trans_conv\" in legacy_path:\n                weight = convert_keras_to_pytorch_conv2d_transpose(weight)\n            else:\n                weight = convert_keras_to_pytorch_conv2d(weight)\n        elif info[\"weight_type\"] == \"kernel\" and weight.ndim != 4:\n            # for linear weights, we need to transpose the shape\n            weight = torch.from_numpy(weight.transpose(1, 0)).float()\n        else:\n            # Bias weights don't need conversion\n            weight = torch.from_numpy(weight).float()\n        # Set the parameter using state_dict\n        try:\n            state_dict = pytorch_model.state_dict()\n            if pytorch_name not in state_dict:\n                logger.warning(f\"PyTorch parameter not found: {pytorch_name}\")\n                continue\n\n            # Check shape compatibility\n            pytorch_shape = state_dict[pytorch_name].shape\n            if weight.shape != pytorch_shape:\n                logger.warning(\n                    f\"Shape mismatch for {pytorch_name}: \"\n                    f\"legacy {weight.shape} vs pytorch {pytorch_shape}\"\n                )\n                continue\n\n            # Update the parameter in the model\n            with torch.no_grad():\n                param = pytorch_model\n                for attr in pytorch_name.split(\".\")[:-1]:\n                    param = getattr(param, attr)\n                param_name = pytorch_name.split(\".\")[-1]\n                setattr(param, param_name, torch.nn.Parameter(weight))\n\n            loaded_count += 1\n\n        except Exception as e:\n            error_msg = f\"Error loading {pytorch_name}: {e}\"\n            logger.error(error_msg)\n            errors.append(error_msg)\n\n    # Log summary\n    if loaded_count == 0:\n        logger.info(\n            f\"No weights were successfully loaded. \"\n            f\"Attempted to load {len(mapping)} weights, but all failed.\"\n        )\n    else:\n        logger.info(\n            f\"Successfully loaded {loaded_count}/{len(mapping)} weights from legacy model\"\n        )\n\n    # Log any errors that occurred\n    if errors:\n        logger.info(\n            f\"Weight loading completed with {len(errors)} errors: {'; '.join(errors[:5])}\"\n        )\n\n    # Verify all loaded weights by comparing means\n    logger.info(\"Verifying weight assignments...\")\n    verification_errors = []\n\n    for legacy_path, pytorch_name in mapping.items():\n        if legacy_path not in legacy_weights:\n            continue\n\n        try:\n            original_weight = legacy_weights[legacy_path]\n            info = parse_keras_layer_name(legacy_path)\n\n            if info[\"weight_type\"] == \"kernel\" and original_weight.ndim == 4:\n                # Convert Keras to PyTorch format\n                torch_weight = convert_keras_to_pytorch_conv2d(original_weight)\n                # Keras: (H, W, C_in, C_out), PyTorch: (C_out, C_in, H, W)\n                keras_cout = original_weight.shape[-1]\n                torch_cout = torch_weight.shape[0]\n                assert (\n                    keras_cout == torch_cout\n                ), f\"Output channel mismatch: {keras_cout} vs {torch_cout}\"\n\n                # Check each output channel\n                channel_errors = []\n                for i in range(keras_cout):\n                    keras_ch_mean = np.mean(original_weight[..., i])\n                    torch_ch_mean = torch.mean(torch_weight[i]).item()\n                    diff = abs(keras_ch_mean - torch_ch_mean)\n                    if diff &gt; 1e-6:\n                        channel_errors.append(\n                            f\"channel {i}: keras={keras_ch_mean:.6f}, torch={torch_ch_mean:.6f}, diff={diff:.6e}\"\n                        )\n\n                if channel_errors:\n                    message = f\"Channel verification failed for {pytorch_name}: {'; '.join(channel_errors)}\"\n                    logger.error(message)\n                    verification_errors.append(message)\n            elif info[\"weight_type\"] == \"kernel\" and original_weight.ndim == 2:\n                # for linear weights, we need to transpose the shape\n                keras_mean = np.mean(original_weight.transpose(1, 0))\n                torch_mean = torch.mean(\n                    torch.from_numpy(original_weight.transpose(1, 0)).float()\n                ).item()\n                diff = abs(keras_mean - torch_mean)\n                if diff &gt; 1e-6:\n                    message = f\"Weight verification failed for {pytorch_name} (linear): keras={keras_mean:.6f}, torch={torch_mean:.6f}, diff={diff:.6e}\"\n                    logger.error(message)\n                    verification_errors.append(message)\n            else:\n                # Bias : just compare all values\n                keras_mean = np.mean(original_weight)\n                torch_mean = torch.mean(\n                    torch.from_numpy(original_weight).float()\n                ).item()\n                diff = abs(keras_mean - torch_mean)\n                if diff &gt; 1e-6:\n                    message = f\"Weight verification failed for {pytorch_name} (bias): keras={keras_mean:.6f}, torch={torch_mean:.6f}, diff={diff:.6e}\"\n                    logger.error(message)\n                    verification_errors.append(message)\n\n        except Exception as e:\n            error_msg = f\"Error verifying {pytorch_name}: {e}\"\n            logger.error(error_msg)\n            verification_errors.append(error_msg)\n\n    if not verification_errors:\n        logger.info(\"\u2713 All weight assignments verified successfully\")\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.map_legacy_to_pytorch_layers","title":"<code>map_legacy_to_pytorch_layers(legacy_weights, pytorch_model, component=None)</code>","text":"<p>Create mapping between legacy Keras layers and PyTorch model layers.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_weights</code> <code>Dict[str, ndarray]</code> <p>Dictionary of legacy weights from load_keras_weights()</p> required <code>pytorch_model</code> <code>Module</code> <p>PyTorch model instance to map to</p> required <code>component</code> <code>Optional[str]</code> <p>Optional component type for filtering weights before mapping. One of \"backbone\", \"head\", or None (no filtering).</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping legacy layer paths to PyTorch parameter names</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def map_legacy_to_pytorch_layers(\n    legacy_weights: Dict[str, np.ndarray],\n    pytorch_model: torch.nn.Module,\n    component: Optional[str] = None,\n) -&gt; Dict[str, str]:\n    \"\"\"Create mapping between legacy Keras layers and PyTorch model layers.\n\n    Args:\n        legacy_weights: Dictionary of legacy weights from load_keras_weights()\n        pytorch_model: PyTorch model instance to map to\n        component: Optional component type for filtering weights before mapping.\n            One of \"backbone\", \"head\", or None (no filtering).\n\n    Returns:\n        Dictionary mapping legacy layer paths to PyTorch parameter names\n    \"\"\"\n    # Filter weights based on component type\n    filtered_weights = filter_legacy_weights_by_component(legacy_weights, component)\n\n    if component is not None:\n        logger.info(\n            f\"Filtered legacy weights for {component}: \"\n            f\"{len(filtered_weights)}/{len(legacy_weights)} weights\"\n        )\n    mapping = {}\n\n    # Get all PyTorch parameters with their shapes\n    pytorch_params = {}\n    for name, param in pytorch_model.named_parameters():\n        pytorch_params[name] = param.shape\n\n    # For each legacy weight, find the corresponding PyTorch parameter\n    for legacy_path, weight in filtered_weights.items():\n        # Extract the layer name from the legacy path\n        # Legacy path format: \"model_weights/stack0_enc0_conv0/stack0_enc0_conv0/kernel:0\"\n        clean_path = legacy_path.replace(\"model_weights/\", \"\")\n        parts = clean_path.split(\"/\")\n\n        if len(parts) &lt; 2:\n            continue\n\n        layer_name = parts[0]  # e.g., \"stack0_enc0_conv0\" or \"CentroidConfmapsHead_0\"\n        weight_name = parts[-1]  # e.g., \"kernel:0\" or \"bias:0\"\n\n        # Convert Keras weight type to PyTorch weight type\n        weight_type = \"weight\" if \"kernel\" in weight_name else \"bias\"\n\n        # For head layers, strip numeric suffixes (e.g., \"CentroidConfmapsHead_0\" -&gt; \"CentroidConfmapsHead\")\n        # This handles cases where Keras uses suffixes like _0, _1, etc.\n        if \"Head\" in layer_name:\n            # Remove trailing _N where N is a number\n            layer_name_clean = re.sub(r\"_\\d+$\", \"\", layer_name)\n        else:\n            layer_name_clean = layer_name\n\n        # Find the PyTorch parameter that contains this layer name\n        # PyTorch names will be like: \"backbone.enc.encoder_stack.0.blocks.0.stack0_enc0_conv0.weight\"\n        matching_pytorch_name = None\n\n        for pytorch_name in pytorch_params.keys():\n            # Check if the PyTorch parameter name contains the layer name (or cleaned layer name for heads)\n            # and has the correct weight type\n            search_name = layer_name_clean if \"Head\" in layer_name else layer_name\n            if search_name in pytorch_name and pytorch_name.endswith(f\".{weight_type}\"):\n                # For kernel weights, we need to check shape after conversion\n                if weight_type == \"weight\" and weight.ndim == 4:\n                    # Convert Keras kernel to PyTorch format for shape comparison\n                    if \"trans_conv\" in legacy_path:\n                        converted_weight = convert_keras_to_pytorch_conv2d_transpose(\n                            weight\n                        )\n                    else:\n                        converted_weight = convert_keras_to_pytorch_conv2d(weight)\n                    shape_to_check = converted_weight.shape\n                elif weight_type == \"weight\" and weight.ndim == 2:\n                    # for linear weights, we need to transpose the shape\n                    shape_to_check = weight.shape[::-1]\n                else:\n                    # Bias weights don't need conversion\n                    shape_to_check = weight.shape\n\n                # Verify shape compatibility\n                if shape_to_check == pytorch_params[pytorch_name]:\n                    matching_pytorch_name = pytorch_name\n                    break\n\n        if matching_pytorch_name:\n            mapping[legacy_path] = matching_pytorch_name\n        else:\n            logger.warning(f\"No matching PyTorch parameter found for {legacy_path}\")\n\n    # Log mapping results\n    if not mapping:\n        logger.info(\n            f\"No mappings could be created between legacy weights and PyTorch model. \"\n            f\"Legacy weights: {len(filtered_weights)}, PyTorch parameters: {len(pytorch_params)}\"\n        )\n    else:\n        logger.info(\n            f\"Successfully mapped {len(mapping)}/{len(pytorch_params)} PyTorch parameters from legacy weights\"\n        )\n        unmatched_count = len(filtered_weights) - len(mapping)\n        if unmatched_count &gt; 0:\n            logger.warning(\n                f\"({unmatched_count} legacy weights did not match any parameters in this model component)\"\n            )\n\n    return mapping\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.parse_keras_layer_name","title":"<code>parse_keras_layer_name(layer_path)</code>","text":"<p>Parse a Keras layer path to extract basic information.</p> <p>Parameters:</p> Name Type Description Default <code>layer_path</code> <code>str</code> <p>Full path like \"model_weights/stack0_enc0_conv0/stack0_enc0_conv0/kernel:0\"</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with parsed information: - layer_name: Base layer name (e.g., \"stack0_enc0_conv0\") - weight_type: \"kernel\" or \"bias\"</p> Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def parse_keras_layer_name(layer_path: str) -&gt; Dict[str, Any]:\n    \"\"\"Parse a Keras layer path to extract basic information.\n\n    Args:\n        layer_path: Full path like \"model_weights/stack0_enc0_conv0/stack0_enc0_conv0/kernel:0\"\n\n    Returns:\n        Dictionary with parsed information:\n        - layer_name: Base layer name (e.g., \"stack0_enc0_conv0\")\n        - weight_type: \"kernel\" or \"bias\"\n    \"\"\"\n    # Remove model_weights prefix and split\n    clean_path = layer_path.replace(\"model_weights/\", \"\")\n    parts = clean_path.split(\"/\")\n\n    if len(parts) &lt; 2:\n        raise ValueError(f\"Invalid layer path: {layer_path}\")\n\n    layer_name = parts[0]\n    weight_name = parts[-1]  # e.g., \"kernel:0\" or \"bias:0\"\n\n    info = {\n        \"layer_name\": layer_name,\n        \"weight_type\": \"kernel\" if \"kernel\" in weight_name else \"bias\",\n    }\n\n    return info\n</code></pre>"},{"location":"api/legacy_models/#sleap_nn.legacy_models.update_backbone_in_channels","title":"<code>update_backbone_in_channels(backbone_config, keras_in_channels)</code>","text":"<p>Update the backbone configuration's in_channels if it's different from the Keras model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone_config</code> <p>The backbone configuration object</p> required <code>keras_in_channels</code> <code>int</code> <p>Number of input channels from the Keras model</p> required Source code in <code>sleap_nn/legacy_models.py</code> <pre><code>def update_backbone_in_channels(backbone_config, keras_in_channels: int):\n    \"\"\"Update the backbone configuration's in_channels if it's different from the Keras model.\n\n    Args:\n        backbone_config: The backbone configuration object\n        keras_in_channels: Number of input channels from the Keras model\n    \"\"\"\n    if backbone_config.in_channels != keras_in_channels:\n        logger.info(\n            f\"Updating backbone in_channels from {backbone_config.in_channels} to {keras_in_channels}\"\n        )\n        backbone_config.in_channels = keras_in_channels\n\n    return backbone_config\n</code></pre>"},{"location":"api/predict/","title":"predict","text":""},{"location":"api/predict/#sleap_nn.predict","title":"<code>sleap_nn.predict</code>","text":"<p>Entry point for running inference.</p> <p>Functions:</p> Name Description <code>frame_list</code> <p>Converts 'n-m' string to list of ints.</p> <code>run_inference</code> <p>Entry point to run inference on trained SLEAP-NN models.</p>"},{"location":"api/predict/#sleap_nn.predict.frame_list","title":"<code>frame_list(frame_str)</code>","text":"<p>Converts 'n-m' string to list of ints.</p> <p>Parameters:</p> Name Type Description Default <code>frame_str</code> <code>str</code> <p>string representing range</p> required <p>Returns:</p> Type Description <code>Optional[List[int]]</code> <p>List of ints, or None if string does not represent valid range.</p> Source code in <code>sleap_nn/predict.py</code> <pre><code>def frame_list(frame_str: str) -&gt; Optional[List[int]]:\n    \"\"\"Converts 'n-m' string to list of ints.\n\n    Args:\n        frame_str: string representing range\n\n    Returns:\n        List of ints, or None if string does not represent valid range.\n    \"\"\"\n    # Handle ranges of frames. Must be of the form \"1-200\" (or \"1,-200\")\n    if \"-\" in frame_str:\n        min_max = frame_str.split(\"-\")\n        min_frame = int(min_max[0].rstrip(\",\"))\n        max_frame = int(min_max[1])\n        return list(range(min_frame, max_frame + 1))\n\n    return [int(x) for x in frame_str.split(\",\")] if len(frame_str) else None\n</code></pre>"},{"location":"api/predict/#sleap_nn.predict.run_inference","title":"<code>run_inference(data_path=None, input_labels=None, input_video=None, model_paths=None, backbone_ckpt_path=None, head_ckpt_path=None, max_instances=None, max_width=None, max_height=None, ensure_rgb=None, input_scale=None, ensure_grayscale=None, anchor_part=None, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False, no_empty_frames=False, batch_size=4, queue_maxsize=32, video_index=None, video_dataset=None, video_input_format='channels_last', frames=None, crop_size=None, peak_threshold=0.2, filter_overlapping=False, filter_overlapping_method='iou', filter_overlapping_threshold=0.8, integral_refinement='integral', integral_patch_size=5, return_confmaps=False, return_pafs=False, return_paf_graph=False, max_edge_length_ratio=0.25, dist_penalty_weight=1.0, n_points=10, min_instance_peaks=0, min_line_scores=0.25, return_class_maps=False, return_class_vectors=False, make_labels=True, output_path=None, device='auto', tracking=False, tracking_window_size=5, min_new_track_points=0, candidates_method='fixed_window', min_match_points=0, features='keypoints', scoring_method='oks', scoring_reduction='mean', robust_best_instance=1.0, track_matching_method='hungarian', max_tracks=None, use_flow=False, of_img_scale=1.0, of_window_size=21, of_max_levels=3, post_connect_single_breaks=False, tracking_target_instance_count=None, tracking_pre_cull_to_target=0, tracking_pre_cull_iou_threshold=0, tracking_clean_instance_count=0, tracking_clean_iou_threshold=0, gui=False)</code>","text":"<p>Entry point to run inference on trained SLEAP-NN models.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> to run inference on.</p> <code>None</code> <code>input_labels</code> <code>Optional[Labels]</code> <p>(sio.Labels) Labels object to run inference on. This is an alternative to specifying the data_path.</p> <code>None</code> <code>input_video</code> <code>Optional[Video]</code> <p>(sio.Video) Video to run inference on. This is an alternative to specifying the data_path. If both input_labels and input_video are provided, input_labels are used.</p> <code>None</code> <code>model_paths</code> <code>Optional[List[str]]</code> <p>(List[str]) List of paths to the directory where the best.ckpt     and training_config.yaml are saved.</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code>     from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights     are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt     from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>(int) Maximum width the image should be padded to. If not provided, the     values from the training config are used. Default: None.</p> <code>None</code> <code>max_height</code> <code>Optional[int]</code> <p>(int) Maximum height the image should be padded to. If not provided, the     values from the training config are used. Default: None.</p> <code>None</code> <code>input_scale</code> <code>Optional[float]</code> <p>(float) Scale factor to apply to the input image. If not provided, the     values from the training config are used. Default: None.</p> <code>None</code> <code>ensure_rgb</code> <code>Optional[bool]</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one     channel when this is set to <code>True</code>, then the images from single-channel     is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the     values from the training config are used. Default: <code>None</code>.</p> <code>None</code> <code>ensure_grayscale</code> <code>Optional[bool]</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this     is set to True, then we convert the image to grayscale (single-channel)     image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the     values from the training config are used. Default: <code>None</code>.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The node name to use as the anchor for the centroid. If not     provided, the anchor part in the <code>training_config.yaml</code> is used. Default: <code>None</code>.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>exclude_user_labeled</code> <code>bool</code> <p>(bool) <code>True</code> to skip frames that have user-labeled instances. Default: <code>False</code>.</p> <code>False</code> <code>only_predicted_frames</code> <code>bool</code> <p>(bool) <code>True</code> to run inference only on frames that already have predictions. Default: <code>False</code>.</p> <code>False</code> <code>no_empty_frames</code> <code>bool</code> <p>(bool) <code>True</code> if empty frames that did not have predictions should be cleared before saving to output. Default: <code>False</code>.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 32.</p> <code>32</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with     an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <code>frames</code> <code>Optional[list]</code> <p>(list) List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>crop_size</code> <code>Optional[int]</code> <p>(int) Crop size. If not provided, the crop size from training_config.yaml is used.     If <code>input_scale</code> is provided, then the cropped image will be resized according to <code>input_scale</code>. Default: None.</p> <code>None</code> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below     this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown     centroid and centered-instance model, where the first element corresponds     to centroid model peak finding threshold and the second element is for     centered-instance model peak finding.</p> <code>0.2</code> <code>filter_overlapping</code> <code>bool</code> <p>(bool) If True, removes overlapping instances after     inference using greedy NMS. Applied independently of tracking.     Default: False.</p> <code>False</code> <code>filter_overlapping_method</code> <code>str</code> <p>(str) Similarity metric for filtering overlapping     instances. One of \"iou\" (bounding box) or \"oks\" (keypoint similarity).     Default: \"iou\".</p> <code>'iou'</code> <code>filter_overlapping_threshold</code> <code>float</code> <p>(float) Similarity threshold for filtering.     Instances with similarity &gt; threshold are removed (keeping higher-scoring).     Typical values: 0.3 (aggressive) to 0.8 (permissive). Default: 0.8.</p> <code>0.8</code> <code>integral_refinement</code> <code>Optional[str]</code> <p>(str) If <code>None</code>, returns the grid-aligned peaks with no refinement.     If <code>\"integral\"</code>, peaks will be refined with integral regression.     Default: <code>\"integral\"</code>.</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an     integer scalar. Default: 5.</p> <code>5</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned     along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>return_pafs</code> <code>bool</code> <p>(bool) If <code>True</code>, the part affinity fields will be returned together with     the predicted instances. This will result in slower inference times since     the data must be copied off of the GPU, but is useful for visualizing the     raw output of the model. Default: False.</p> <code>False</code> <code>return_class_vectors</code> <code>bool</code> <p>If <code>True</code>, the classification probabilities will be     returned together with the predicted peaks. This will not line up with the     grouped instances, for which the associtated class probabilities will always     be returned in <code>\"instance_scores\"</code>.</p> <code>False</code> <code>return_paf_graph</code> <code>bool</code> <p>(bool) If <code>True</code>, the part affinity field graph will be returned     together with the predicted instances. The graph is obtained by parsing the     part affinity fields with the <code>paf_scorer</code> instance and is an intermediate     representation used during instance grouping. Default: False.</p> <code>False</code> <code>max_edge_length_ratio</code> <code>float</code> <p>(float) The maximum expected length of a connected pair of points     as a fraction of the image size. Candidate connections longer than this     length will be penalized during matching. Default: 0.25.</p> <code>0.25</code> <code>dist_penalty_weight</code> <code>float</code> <p>(float) A coefficient to scale weight of the distance penalty as     a scalar float. Set to values greater than 1.0 to enforce the distance     penalty more strictly.Default: 1.0.</p> <code>1.0</code> <code>n_points</code> <code>int</code> <p>(int) Number of points to sample along the line integral. Default: 10.</p> <code>10</code> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Union[int, float] Minimum number of peaks the instance should     have to be considered a real instance. Instances with fewer peaks than     this will be discarded (useful for filtering spurious detections).     Default: 0.</p> <code>0</code> <code>min_line_scores</code> <code>float</code> <p>(float) Minimum line score (between -1 and 1) required to form a match     between candidate point pairs. Useful for rejecting spurious detections when     there are no better ones. Default: 0.25.</p> <code>0.25</code> <code>return_class_maps</code> <code>bool</code> <p>If <code>True</code>, the class maps will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>False</code> <code>make_labels</code> <code>bool</code> <p>(bool) If <code>True</code> (the default), returns a <code>sio.Labels</code> instance with     <code>sio.PredictedInstance</code>s. If <code>False</code>, just return a list of     dictionaries containing the raw arrays returned by the inference model.     Default: True.</p> <code>True</code> <code>output_path</code> <code>Optional[str]</code> <p>(str) Path to save the labels file if <code>make_labels</code> is True.     Default is current working directory.</p> <code>None</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the     ('cpu', 'cuda', 'mps', 'auto').     Default: \"auto\" (based on available backend either cuda, mps or cpu is chosen). If <code>cuda</code> is available, you could also use <code>cuda:0</code> to specify the device.</p> <code>'auto'</code> <code>tracking</code> <code>bool</code> <p>(bool) If True, runs tracking on the predicted instances.</p> <code>False</code> <code>tracking_window_size</code> <code>int</code> <p>Number of frames to look for in the candidate instances to match     with the current detections. Default: 5.</p> <code>5</code> <code>min_new_track_points</code> <code>int</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>0</code> <code>candidates_method</code> <code>str</code> <p>Either of <code>fixed_window</code> or <code>local_queues</code>. In fixed window method, candidates from the last <code>window_size</code> frames. In local queues, last <code>window_size</code> instances for each track ID is considered for matching against the current detection. Default: <code>fixed_window</code>.</p> <code>'fixed_window'</code> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>0</code> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>'keypoints'</code> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>'oks'</code> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>'mean'</code> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>1.0</code> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>'hungarian'</code> <code>max_tracks</code> <code>Optional[int]</code> <p>Meaximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate) Default: None.</p> <code>None</code> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using</p> <code>False</code> <code>optical flow shifts. Default</code> <p><code>False</code>.</p> required <code>of_img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0. (only if <code>use_flow</code> is True)</p> <code>1.0</code> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21. (only if <code>use_flow</code> is True)</p> <code>21</code> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling. Default: 3. (only if <code>use_flow</code> is True).</p> <code>3</code> <code>post_connect_single_breaks</code> <code>bool</code> <p>If True and <code>max_tracks</code> is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.</p> <code>False</code> <code>tracking_target_instance_count</code> <code>Optional[int]</code> <p>Target number of instances to track per frame. (default: None)</p> <code>None</code> <code>tracking_pre_cull_to_target</code> <code>int</code> <p>If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame before tracking. (default: 0)</p> <code>0</code> <code>tracking_pre_cull_iou_threshold</code> <code>float</code> <p>If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count before tracking. (default: 0)</p> <code>0</code> <code>tracking_clean_instance_count</code> <code>int</code> <p>Target number of instances to clean after tracking. (default: 0)</p> <code>0</code> <code>tracking_clean_iou_threshold</code> <code>float</code> <p>IOU to use when culling instances after tracking. (default: 0)</p> <code>0</code> <code>gui</code> <code>bool</code> <p>(bool) If True, outputs JSON progress lines for GUI integration instead     of Rich progress bars. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns <code>sio.Labels</code> object if <code>make_labels</code> is True. Else this function returns     a list of Dictionaries with the predictions.</p> Source code in <code>sleap_nn/predict.py</code> <pre><code>def run_inference(\n    data_path: Optional[str] = None,\n    input_labels: Optional[sio.Labels] = None,\n    input_video: Optional[sio.Video] = None,\n    model_paths: Optional[List[str]] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    max_instances: Optional[int] = None,\n    max_width: Optional[int] = None,\n    max_height: Optional[int] = None,\n    ensure_rgb: Optional[bool] = None,\n    input_scale: Optional[float] = None,\n    ensure_grayscale: Optional[bool] = None,\n    anchor_part: Optional[str] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n    no_empty_frames: bool = False,\n    batch_size: int = 4,\n    queue_maxsize: int = 32,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n    frames: Optional[list] = None,\n    crop_size: Optional[int] = None,\n    peak_threshold: Union[float, List[float]] = 0.2,\n    filter_overlapping: bool = False,\n    filter_overlapping_method: str = \"iou\",\n    filter_overlapping_threshold: float = 0.8,\n    integral_refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: bool = False,\n    return_pafs: bool = False,\n    return_paf_graph: bool = False,\n    max_edge_length_ratio: float = 0.25,\n    dist_penalty_weight: float = 1.0,\n    n_points: int = 10,\n    min_instance_peaks: Union[int, float] = 0,\n    min_line_scores: float = 0.25,\n    return_class_maps: bool = False,\n    return_class_vectors: bool = False,\n    make_labels: bool = True,\n    output_path: Optional[str] = None,\n    device: str = \"auto\",\n    tracking: bool = False,\n    tracking_window_size: int = 5,\n    min_new_track_points: int = 0,\n    candidates_method: str = \"fixed_window\",\n    min_match_points: int = 0,\n    features: str = \"keypoints\",\n    scoring_method: str = \"oks\",\n    scoring_reduction: str = \"mean\",\n    robust_best_instance: float = 1.0,\n    track_matching_method: str = \"hungarian\",\n    max_tracks: Optional[int] = None,\n    use_flow: bool = False,\n    of_img_scale: float = 1.0,\n    of_window_size: int = 21,\n    of_max_levels: int = 3,\n    post_connect_single_breaks: bool = False,\n    tracking_target_instance_count: Optional[int] = None,\n    tracking_pre_cull_to_target: int = 0,\n    tracking_pre_cull_iou_threshold: float = 0,\n    tracking_clean_instance_count: int = 0,\n    tracking_clean_iou_threshold: float = 0,\n    gui: bool = False,\n):\n    \"\"\"Entry point to run inference on trained SLEAP-NN models.\n\n    Args:\n        data_path: (str) Path to `.slp` file or `.mp4` to run inference on.\n        input_labels: (sio.Labels) Labels object to run inference on. This is an alternative to specifying the data_path.\n        input_video: (sio.Video) Video to run inference on. This is an alternative to specifying the data_path. If both input_labels and input_video are provided, input_labels are used.\n        model_paths: (List[str]) List of paths to the directory where the best.ckpt\n                and training_config.yaml are saved.\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n        max_instances: (int) Max number of instances to consider from the predictions.\n        max_width: (int) Maximum width the image should be padded to. If not provided, the\n                values from the training config are used. Default: None.\n        max_height: (int) Maximum height the image should be padded to. If not provided, the\n                values from the training config are used. Default: None.\n        input_scale: (float) Scale factor to apply to the input image. If not provided, the\n                values from the training config are used. Default: None.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n                channel when this is set to `True`, then the images from single-channel\n                is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. If not provided, the\n                values from the training config are used. Default: `None`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n                is set to True, then we convert the image to grayscale (single-channel)\n                image. If the source image has only one channel and this is set to False, then we retain the single channel input. If not provided, the\n                values from the training config are used. Default: `None`.\n        anchor_part: (str) The node name to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used. Default: `None`.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n        only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n        no_empty_frames: (bool) `True` if empty frames that did not have predictions should be cleared before saving to output. Default: `False`.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used with\n                an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n        frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n        crop_size: (int) Crop size. If not provided, the crop size from training_config.yaml is used.\n                If `input_scale` is provided, then the cropped image will be resized according to `input_scale`. Default: None.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n        filter_overlapping: (bool) If True, removes overlapping instances after\n                inference using greedy NMS. Applied independently of tracking.\n                Default: False.\n        filter_overlapping_method: (str) Similarity metric for filtering overlapping\n                instances. One of \"iou\" (bounding box) or \"oks\" (keypoint similarity).\n                Default: \"iou\".\n        filter_overlapping_threshold: (float) Similarity threshold for filtering.\n                Instances with similarity &gt; threshold are removed (keeping higher-scoring).\n                Typical values: 0.3 (aggressive) to 0.8 (permissive). Default: 0.8.\n        integral_refinement: (str) If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: `\"integral\"`.\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n        return_pafs: (bool) If `True`, the part affinity fields will be returned together with\n                the predicted instances. This will result in slower inference times since\n                the data must be copied off of the GPU, but is useful for visualizing the\n                raw output of the model. Default: False.\n        return_class_vectors: If `True`, the classification probabilities will be\n                returned together with the predicted peaks. This will not line up with the\n                grouped instances, for which the associtated class probabilities will always\n                be returned in `\"instance_scores\"`.\n        return_paf_graph: (bool) If `True`, the part affinity field graph will be returned\n                together with the predicted instances. The graph is obtained by parsing the\n                part affinity fields with the `paf_scorer` instance and is an intermediate\n                representation used during instance grouping. Default: False.\n        max_edge_length_ratio: (float) The maximum expected length of a connected pair of points\n                as a fraction of the image size. Candidate connections longer than this\n                length will be penalized during matching. Default: 0.25.\n        dist_penalty_weight: (float) A coefficient to scale weight of the distance penalty as\n                a scalar float. Set to values greater than 1.0 to enforce the distance\n                penalty more strictly.Default: 1.0.\n        n_points: (int) Number of points to sample along the line integral. Default: 10.\n        min_instance_peaks: Union[int, float] Minimum number of peaks the instance should\n                have to be considered a real instance. Instances with fewer peaks than\n                this will be discarded (useful for filtering spurious detections).\n                Default: 0.\n        min_line_scores: (float) Minimum line score (between -1 and 1) required to form a match\n                between candidate point pairs. Useful for rejecting spurious detections when\n                there are no better ones. Default: 0.25.\n        return_class_maps: If `True`, the class maps will be returned together with\n            the predicted instances. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        make_labels: (bool) If `True` (the default), returns a `sio.Labels` instance with\n                `sio.PredictedInstance`s. If `False`, just return a list of\n                dictionaries containing the raw arrays returned by the inference model.\n                Default: True.\n        output_path: (str) Path to save the labels file if `make_labels` is True.\n                Default is current working directory.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n                ('cpu', 'cuda', 'mps', 'auto').\n                Default: \"auto\" (based on available backend either cuda, mps or cpu is chosen). If `cuda` is available, you could also use `cuda:0` to specify the device.\n        tracking: (bool) If True, runs tracking on the predicted instances.\n        tracking_window_size: Number of frames to look for in the candidate instances to match\n                with the current detections. Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n            method, candidates from the last `window_size` frames. In local queues,\n            last `window_size` instances for each track ID is considered for matching\n            against the current detection. Default: `fixed_window`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n            (only for local queues candidate) Default: None.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n        optical flow shifts. Default: `False`.\n        of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0. (only if `use_flow` is True)\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21. (only if `use_flow` is True)\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n            Default: 3. (only if `use_flow` is True).\n        post_connect_single_breaks: If True and `max_tracks` is not None with local queues candidate method,\n            connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.\n        tracking_target_instance_count: Target number of instances to track per frame. (default: None)\n        tracking_pre_cull_to_target: If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame *before* tracking. (default: 0)\n        tracking_pre_cull_iou_threshold: If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count *before* tracking. (default: 0)\n        tracking_clean_instance_count: Target number of instances to clean *after* tracking. (default: 0)\n        tracking_clean_iou_threshold: IOU to use when culling instances *after* tracking. (default: 0)\n        gui: (bool) If True, outputs JSON progress lines for GUI integration instead\n                of Rich progress bars. Default: False.\n\n    Returns:\n        Returns `sio.Labels` object if `make_labels` is True. Else this function returns\n            a list of Dictionaries with the predictions.\n\n    \"\"\"\n    preprocess_config = {  # if not given, then use from training config\n        \"ensure_rgb\": ensure_rgb,\n        \"ensure_grayscale\": ensure_grayscale,\n        \"crop_size\": crop_size,\n        \"max_width\": max_width,\n        \"max_height\": max_height,\n        \"scale\": input_scale,\n    }\n\n    # Validate mutually exclusive frame filter flags\n    if only_labeled_frames and exclude_user_labeled:\n        message = (\n            \"--only_labeled_frames and --exclude_user_labeled are mutually exclusive \"\n            \"(would result in zero frames)\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n\n    if (\n        only_predicted_frames\n        and data_path is not None\n        and not data_path.endswith(\".slp\")\n    ):\n        message = (\n            \"--only_predicted_frames requires a .slp file input \"\n            \"(need Labels to know which frames have predictions)\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n\n    if model_paths is None or not len(\n        model_paths\n    ):  # if model paths is not provided, run tracking-only pipeline.\n        if not tracking:\n            message = \"\"\"Neither tracker nor path to trained models specified. Use `model_paths` to specify models to use. To retrack on predictions, set `tracking` to True.\"\"\"\n            logger.error(message)\n            raise ValueError(message)\n\n        else:\n            if (data_path is not None and not data_path.endswith(\".slp\")) or (\n                input_labels is not None and not isinstance(input_labels, sio.Labels)\n            ):\n                message = \"Data path is not a .slp file. To run track-only pipeline, data path must be an .slp file.\"\n                logger.error(message)\n                raise ValueError(message)\n\n            start_inf_time = time()\n            start_datetime = datetime.now()\n            start_timestamp = str(start_datetime)\n            logger.info(f\"Started tracking at: {start_timestamp}\")\n\n            labels = sio.load_slp(data_path) if input_labels is None else input_labels\n\n            lf_frames = labels.labeled_frames\n\n            # select video if video_index is provided\n            if video_index is not None:\n                lf_frames = labels.find(video=labels.videos[video_index])\n\n            # sort frames before tracking\n            lf_frames = sorted(lf_frames, key=lambda lf: lf.frame_idx)\n\n            if frames is not None:\n                filtered_frames = []\n                for lf in lf_frames:\n                    if lf.frame_idx in frames:\n                        filtered_frames.append(lf)\n                lf_frames = filtered_frames\n\n            if post_connect_single_breaks:\n                if max_tracks is None:\n                    max_tracks = max_instances\n\n            logger.info(f\"Running tracking on {len(lf_frames)} frames...\")\n\n            if post_connect_single_breaks or tracking_pre_cull_to_target:\n                if tracking_target_instance_count is None and max_instances is None:\n                    features_requested = []\n                    if post_connect_single_breaks:\n                        features_requested.append(\"--post_connect_single_breaks\")\n                    if tracking_pre_cull_to_target:\n                        features_requested.append(\"--tracking_pre_cull_to_target\")\n                    features_str = \" and \".join(features_requested)\n\n                    if max_tracks is not None:\n                        suggestion = f\"Add --tracking_target_instance_count {max_tracks} to your command (using your --max_tracks value).\"\n                    else:\n                        suggestion = \"Add --tracking_target_instance_count N where N is the expected number of instances per frame.\"\n\n                    message = (\n                        f\"{features_str} requires --tracking_target_instance_count to be set. \"\n                        f\"{suggestion}\"\n                    )\n                    logger.error(message)\n                    raise ValueError(message)\n                elif tracking_target_instance_count is None:\n                    tracking_target_instance_count = max_instances\n\n            tracked_frames = run_tracker(\n                untracked_frames=lf_frames,\n                window_size=tracking_window_size,\n                min_new_track_points=min_new_track_points,\n                candidates_method=candidates_method,\n                min_match_points=min_match_points,\n                features=features,\n                scoring_method=scoring_method,\n                scoring_reduction=scoring_reduction,\n                robust_best_instance=robust_best_instance,\n                track_matching_method=track_matching_method,\n                max_tracks=max_tracks,\n                use_flow=use_flow,\n                of_img_scale=of_img_scale,\n                of_window_size=of_window_size,\n                of_max_levels=of_max_levels,\n                post_connect_single_breaks=post_connect_single_breaks,\n                tracking_target_instance_count=tracking_target_instance_count,\n                tracking_pre_cull_to_target=tracking_pre_cull_to_target,\n                tracking_pre_cull_iou_threshold=tracking_pre_cull_iou_threshold,\n                tracking_clean_instance_count=tracking_clean_instance_count,\n                tracking_clean_iou_threshold=tracking_clean_iou_threshold,\n            )\n\n            end_datetime = datetime.now()\n            finish_timestamp = str(end_datetime)\n            total_elapsed = time() - start_inf_time\n            logger.info(f\"Finished tracking at: {finish_timestamp}\")\n            logger.info(f\"Total runtime: {total_elapsed} secs\")\n\n            # Build tracking-only provenance\n            tracking_params = {\n                \"window_size\": tracking_window_size,\n                \"min_new_track_points\": min_new_track_points,\n                \"candidates_method\": candidates_method,\n                \"min_match_points\": min_match_points,\n                \"features\": features,\n                \"scoring_method\": scoring_method,\n                \"scoring_reduction\": scoring_reduction,\n                \"robust_best_instance\": robust_best_instance,\n                \"track_matching_method\": track_matching_method,\n                \"max_tracks\": max_tracks,\n                \"use_flow\": use_flow,\n                \"post_connect_single_breaks\": post_connect_single_breaks,\n            }\n            provenance = build_tracking_only_provenance(\n                input_labels=labels,\n                input_path=data_path,\n                start_time=start_datetime,\n                end_time=end_datetime,\n                tracking_params=tracking_params,\n                frames_processed=len(tracked_frames),\n            )\n\n            output = sio.Labels(\n                labeled_frames=tracked_frames,\n                videos=labels.videos,\n                skeletons=labels.skeletons,\n                provenance=provenance,\n            )\n\n    else:\n        start_inf_time = time()\n        start_datetime = datetime.now()\n        start_timestamp = str(start_datetime)\n        logger.info(f\"Started inference at: {start_timestamp}\")\n        logger.info(get_startup_info_string())\n\n        # Convert device to string if it's a torch.device object\n        if hasattr(device, \"type\"):\n            device = str(device)\n\n        if device == \"auto\":\n            device = (\n                \"cuda\"\n                if torch.cuda.is_available()\n                else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n            )\n\n        logger.info(f\"Using device: {device}\")\n\n        # initializes the inference model\n        predictor = Predictor.from_model_paths(\n            model_paths,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=OmegaConf.create(preprocess_config),\n            anchor_part=anchor_part,\n        )\n\n        # Set GUI mode for progress output\n        predictor.gui = gui\n\n        if (\n            tracking\n            and not isinstance(predictor, BottomUpMultiClassPredictor)\n            and not isinstance(predictor, TopDownMultiClassPredictor)\n        ):\n            if post_connect_single_breaks or tracking_pre_cull_to_target:\n                if tracking_target_instance_count is None and max_instances is None:\n                    features_requested = []\n                    if post_connect_single_breaks:\n                        features_requested.append(\"--post_connect_single_breaks\")\n                    if tracking_pre_cull_to_target:\n                        features_requested.append(\"--tracking_pre_cull_to_target\")\n                    features_str = \" and \".join(features_requested)\n\n                    if max_tracks is not None:\n                        suggestion = f\"Add --tracking_target_instance_count {max_tracks} to your command (using your --max_tracks value).\"\n                    else:\n                        suggestion = \"Add --tracking_target_instance_count N or --max_instances N where N is the expected number of instances per frame.\"\n\n                    message = (\n                        f\"{features_str} requires --tracking_target_instance_count or --max_instances to be set. \"\n                        f\"{suggestion}\"\n                    )\n                    logger.error(message)\n                    raise ValueError(message)\n                elif tracking_target_instance_count is None:\n                    tracking_target_instance_count = max_instances\n            predictor.tracker = Tracker.from_config(\n                candidates_method=candidates_method,\n                min_match_points=min_match_points,\n                window_size=tracking_window_size,\n                min_new_track_points=min_new_track_points,\n                features=features,\n                scoring_method=scoring_method,\n                scoring_reduction=scoring_reduction,\n                robust_best_instance=robust_best_instance,\n                track_matching_method=track_matching_method,\n                max_tracks=max_tracks,\n                use_flow=use_flow,\n                of_img_scale=of_img_scale,\n                of_window_size=of_window_size,\n                of_max_levels=of_max_levels,\n                tracking_target_instance_count=tracking_target_instance_count,\n                tracking_pre_cull_to_target=tracking_pre_cull_to_target,\n                tracking_pre_cull_iou_threshold=tracking_pre_cull_iou_threshold,\n            )\n\n        if isinstance(predictor, BottomUpPredictor):\n            predictor.inference_model.paf_scorer.max_edge_length_ratio = (\n                max_edge_length_ratio\n            )\n            predictor.inference_model.paf_scorer.dist_penalty_weight = (\n                dist_penalty_weight\n            )\n            predictor.inference_model.return_pafs = return_pafs\n            predictor.inference_model.return_paf_graph = return_paf_graph\n            predictor.inference_model.paf_scorer.max_edge_length_ratio = (\n                max_edge_length_ratio\n            )\n            predictor.inference_model.paf_scorer.min_line_scores = min_line_scores\n            predictor.inference_model.paf_scorer.min_instance_peaks = min_instance_peaks\n            predictor.inference_model.paf_scorer.n_points = n_points\n\n        if isinstance(predictor, BottomUpMultiClassPredictor):\n            predictor.inference_model.return_class_maps = return_class_maps\n\n        if isinstance(predictor, TopDownMultiClassPredictor):\n            predictor.inference_model.instance_peaks.return_class_vectors = (\n                return_class_vectors\n            )\n\n        # initialize make_pipeline function\n\n        predictor.make_pipeline(\n            inference_object=(\n                input_labels\n                if input_labels is not None\n                else input_video if input_video is not None else data_path\n            ),\n            queue_maxsize=queue_maxsize,\n            frames=frames,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n            exclude_user_labeled=exclude_user_labeled,\n            only_predicted_frames=only_predicted_frames,\n            video_index=video_index,\n            video_dataset=video_dataset,\n            video_input_format=video_input_format,\n        )\n\n        # run predict\n        output = predictor.predict(\n            make_labels=make_labels,\n        )\n\n        # Filter overlapping instances (independent of tracking)\n        if filter_overlapping and make_labels:\n            from sleap_nn.inference.postprocessing import filter_overlapping_instances\n\n            output = filter_overlapping_instances(\n                output,\n                threshold=filter_overlapping_threshold,\n                method=filter_overlapping_method,\n            )\n            logger.info(\n                f\"Filtered overlapping instances with {filter_overlapping_method.upper()} \"\n                f\"threshold: {filter_overlapping_threshold}\"\n            )\n\n        if tracking:\n            lfs = [x for x in output]\n            if tracking_clean_instance_count &gt; 0:\n                lfs = cull_instances(\n                    lfs, tracking_clean_instance_count, tracking_clean_iou_threshold\n                )\n                if not post_connect_single_breaks:\n                    corrected_lfs = connect_single_breaks(\n                        lfs, tracking_clean_instance_count\n                    )\n            elif post_connect_single_breaks:\n                start_final_pass_time = time()\n                start_fp_timestamp = str(datetime.now())\n                logger.info(\n                    f\"Started final-pass (connecting single breaks) at: {start_fp_timestamp}\"\n                )\n                corrected_lfs = connect_single_breaks(\n                    lfs, max_instances=tracking_target_instance_count\n                )\n                finish_fp_timestamp = str(datetime.now())\n                total_fp_elapsed = time() - start_final_pass_time\n                logger.info(\n                    f\"Finished final-pass (connecting single breaks) at: {finish_fp_timestamp}\"\n                )\n                logger.info(f\"Total runtime: {total_fp_elapsed} secs\")\n            else:\n                corrected_lfs = lfs\n\n            output = sio.Labels(\n                labeled_frames=corrected_lfs,\n                videos=output.videos,\n                skeletons=output.skeletons,\n            )\n\n        end_datetime = datetime.now()\n        finish_timestamp = str(end_datetime)\n        total_elapsed = time() - start_inf_time\n        logger.info(f\"Finished inference at: {finish_timestamp}\")\n        logger.info(f\"Total runtime: {total_elapsed} secs\")\n\n        # Determine input labels for provenance preservation\n        input_labels_for_prov = None\n        if input_labels is not None:\n            input_labels_for_prov = input_labels\n        elif data_path is not None and data_path.endswith(\".slp\"):\n            # Load input labels to preserve provenance (if not already loaded)\n            try:\n                input_labels_for_prov = sio.load_slp(data_path)\n            except Exception:\n                pass\n\n        # Build inference parameters for provenance\n        inference_params = {\n            \"peak_threshold\": peak_threshold,\n            \"filter_overlapping\": filter_overlapping,\n            \"filter_overlapping_method\": filter_overlapping_method,\n            \"filter_overlapping_threshold\": filter_overlapping_threshold,\n            \"integral_refinement\": integral_refinement,\n            \"integral_patch_size\": integral_patch_size,\n            \"batch_size\": batch_size,\n            \"max_instances\": max_instances,\n            \"crop_size\": crop_size,\n            \"input_scale\": input_scale,\n            \"anchor_part\": anchor_part,\n        }\n\n        # Build tracking parameters if tracking was enabled\n        tracking_params_prov = None\n        if tracking:\n            tracking_params_prov = {\n                \"window_size\": tracking_window_size,\n                \"min_new_track_points\": min_new_track_points,\n                \"candidates_method\": candidates_method,\n                \"min_match_points\": min_match_points,\n                \"features\": features,\n                \"scoring_method\": scoring_method,\n                \"scoring_reduction\": scoring_reduction,\n                \"robust_best_instance\": robust_best_instance,\n                \"track_matching_method\": track_matching_method,\n                \"max_tracks\": max_tracks,\n                \"use_flow\": use_flow,\n                \"post_connect_single_breaks\": post_connect_single_breaks,\n            }\n\n        # Determine frame selection method\n        frame_selection_method = \"all\"\n        if only_labeled_frames:\n            frame_selection_method = \"labeled\"\n        elif only_suggested_frames:\n            frame_selection_method = \"suggested\"\n        elif only_predicted_frames:\n            frame_selection_method = \"predicted\"\n        elif frames is not None:\n            frame_selection_method = \"specified\"\n\n        # Determine model type from predictor class\n        predictor_type_map = {\n            \"TopDownPredictor\": \"top_down\",\n            \"SingleInstancePredictor\": \"single_instance\",\n            \"BottomUpPredictor\": \"bottom_up\",\n            \"BottomUpMultiClassPredictor\": \"bottom_up_multi_class\",\n            \"TopDownMultiClassPredictor\": \"top_down_multi_class\",\n        }\n        model_type = predictor_type_map.get(type(predictor).__name__)\n\n        # Build and set provenance (only for Labels objects)\n        if make_labels and isinstance(output, sio.Labels):\n            provenance = build_inference_provenance(\n                model_paths=model_paths,\n                model_type=model_type,\n                start_time=start_datetime,\n                end_time=end_datetime,\n                input_labels=input_labels_for_prov,\n                input_path=data_path,\n                frames_processed=(\n                    len(output.labeled_frames)\n                    if hasattr(output, \"labeled_frames\")\n                    else None\n                ),\n                frame_selection_method=frame_selection_method,\n                inference_params=inference_params,\n                tracking_params=tracking_params_prov,\n                device=device,\n            )\n            output.provenance = provenance\n\n    if no_empty_frames:\n        output.clean(frames=True, skeletons=False)\n\n    if make_labels:\n        if output_path is None:\n            base_path = Path(data_path if data_path is not None else \"results\")\n\n            # If video_index is specified, append video name to output path\n            if video_index is not None and len(output.videos) &gt; video_index:\n                video = output.videos[video_index]\n                # Get video filename and sanitize it for use in path\n                video_name = (\n                    Path(video.filename).stem\n                    if isinstance(video.filename, str)\n                    else f\"video_{video_index}\"\n                )\n                # Insert video name before .predictions.slp extension\n                output_path = (\n                    base_path.parent / f\"{base_path.stem}.{video_name}.predictions.slp\"\n                )\n            else:\n                output_path = base_path.with_suffix(\".predictions.slp\")\n        output.save(Path(output_path).as_posix(), restore_original_videos=False)\n    finish_timestamp = str(datetime.now())\n    logger.info(f\"Predictions output path: {output_path}\")\n    logger.info(f\"Saved file at: {finish_timestamp}\")\n\n    return output\n</code></pre>"},{"location":"api/system_info/","title":"system_info","text":""},{"location":"api/system_info/#sleap_nn.system_info","title":"<code>sleap_nn.system_info</code>","text":"<p>System diagnostics and compatibility checking for sleap-nn.</p> <p>Functions:</p> Name Description <code>check_driver_compatibility</code> <p>Check if driver version is compatible with CUDA version.</p> <code>get_min_driver_for_cuda</code> <p>Get minimum driver versions for a CUDA version.</p> <code>get_nvidia_driver_version</code> <p>Get NVIDIA driver version from nvidia-smi.</p> <code>get_package_info</code> <p>Get package version, location, and install source.</p> <code>get_startup_info_string</code> <p>Get a concise system info string for startup logging.</p> <code>get_system_info_dict</code> <p>Get system information as a dictionary.</p> <code>parse_driver_version</code> <p>Parse driver version string into comparable tuple.</p> <code>print_system_info</code> <p>Print comprehensive system diagnostics to console.</p> <code>test_gpu_operations</code> <p>Test that GPU tensor operations work.</p>"},{"location":"api/system_info/#sleap_nn.system_info.check_driver_compatibility","title":"<code>check_driver_compatibility(driver_version, cuda_version)</code>","text":"<p>Check if driver version is compatible with CUDA version.</p> <p>Parameters:</p> Name Type Description Default <code>driver_version</code> <code>str</code> <p>Installed driver version string</p> required <code>cuda_version</code> <code>str</code> <p>CUDA version from PyTorch</p> required <p>Returns:</p> Type Description <code>tuple[bool, Optional[str]]</code> <p>Tuple of (is_compatible, min_required_version). If CUDA version is unknown, returns (True, None).</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def check_driver_compatibility(\n    driver_version: str, cuda_version: str\n) -&gt; tuple[bool, Optional[str]]:\n    \"\"\"Check if driver version is compatible with CUDA version.\n\n    Args:\n        driver_version: Installed driver version string\n        cuda_version: CUDA version from PyTorch\n\n    Returns:\n        Tuple of (is_compatible, min_required_version).\n        If CUDA version is unknown, returns (True, None).\n    \"\"\"\n    min_versions = get_min_driver_for_cuda(cuda_version)\n    if not min_versions:\n        return True, None  # Unknown CUDA version, skip check\n\n    if sys.platform == \"win32\":\n        min_version = min_versions[1]\n    else:\n        min_version = min_versions[0]\n\n    current = parse_driver_version(driver_version)\n    required = parse_driver_version(min_version)\n\n    # Pad tuples to same length for comparison\n    max_len = max(len(current), len(required))\n    current = current + (0,) * (max_len - len(current))\n    required = required + (0,) * (max_len - len(required))\n\n    return current &gt;= required, min_version\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.get_min_driver_for_cuda","title":"<code>get_min_driver_for_cuda(cuda_version)</code>","text":"<p>Get minimum driver versions for a CUDA version.</p> <p>Parameters:</p> Name Type Description Default <code>cuda_version</code> <code>str</code> <p>CUDA version string (e.g., \"12.6\" or \"12.6.1\")</p> required <p>Returns:</p> Type Description <code>Optional[tuple[str, str]]</code> <p>Tuple of (min_linux, min_windows) or None if unknown version.</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def get_min_driver_for_cuda(cuda_version: str) -&gt; Optional[tuple[str, str]]:\n    \"\"\"Get minimum driver versions for a CUDA version.\n\n    Args:\n        cuda_version: CUDA version string (e.g., \"12.6\" or \"12.6.1\")\n\n    Returns:\n        Tuple of (min_linux, min_windows) or None if unknown version.\n    \"\"\"\n    if not cuda_version:\n        return None\n    # Match major.minor (e.g., \"12.6\" from \"12.6.1\")\n    parts = cuda_version.split(\".\")\n    if len(parts) &gt;= 2:\n        major_minor = f\"{parts[0]}.{parts[1]}\"\n        return CUDA_DRIVER_REQUIREMENTS.get(major_minor)\n    return None\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.get_nvidia_driver_version","title":"<code>get_nvidia_driver_version()</code>","text":"<p>Get NVIDIA driver version from nvidia-smi.</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def get_nvidia_driver_version() -&gt; Optional[str]:\n    \"\"\"Get NVIDIA driver version from nvidia-smi.\"\"\"\n    if not shutil.which(\"nvidia-smi\"):\n        return None\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n            capture_output=True,\n            text=True,\n            timeout=5,\n        )\n        if result.returncode == 0:\n            return result.stdout.strip().split(\"\\n\")[0]\n    except Exception:\n        pass\n    return None\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.get_package_info","title":"<code>get_package_info(name)</code>","text":"<p>Get package version, location, and install source.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Package name (e.g., \"sleap-nn\")</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dict with version, location, source, and editable fields.</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def get_package_info(name: str) -&gt; dict:\n    \"\"\"Get package version, location, and install source.\n\n    Args:\n        name: Package name (e.g., \"sleap-nn\")\n\n    Returns:\n        Dict with version, location, source, and editable fields.\n    \"\"\"\n    try:\n        dist = importlib.metadata.distribution(name)\n        version = dist.version\n\n        # Check for editable install and source via direct_url.json\n        is_editable = False\n        source = \"pip\"  # Default assumption\n        try:\n            direct_url_text = dist.read_text(\"direct_url.json\")\n            if direct_url_text:\n                direct_url = json.loads(direct_url_text)\n                is_editable = direct_url.get(\"dir_info\", {}).get(\"editable\", False)\n                if is_editable:\n                    source = \"editable\"\n                elif \"vcs_info\" in direct_url:\n                    source = \"git\"\n                elif direct_url.get(\"url\", \"\").startswith(\"file://\"):\n                    source = \"local\"\n        except FileNotFoundError:\n            pass\n\n        # Fallback: detect old-style editable installs (.egg-info not in site-packages)\n        if not is_editable and dist._path:\n            path_str = str(dist._path)\n            # Old-style editable: .egg-info in source dir, not site-packages\n            if \".egg-info\" in path_str and \"site-packages\" not in path_str:\n                is_editable = True\n                source = \"editable\"\n\n        # Check for conda install via INSTALLER file (only if not already known)\n        if source == \"pip\":\n            try:\n                installer = dist.read_text(\"INSTALLER\")\n                if installer and installer.strip() == \"conda\":\n                    source = \"conda\"\n            except FileNotFoundError:\n                pass\n\n        # Get location (after determining if editable, so we can use the right method)\n        location = _get_package_location(name, dist)\n\n        return {\n            \"version\": version,\n            \"location\": location,\n            \"source\": source,\n            \"editable\": is_editable,\n        }\n    except importlib.metadata.PackageNotFoundError:\n        return {\n            \"version\": \"not installed\",\n            \"location\": \"\",\n            \"source\": \"\",\n            \"editable\": False,\n        }\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.get_startup_info_string","title":"<code>get_startup_info_string()</code>","text":"<p>Get a concise system info string for startup logging.</p> <p>Returns:</p> Type Description <code>str</code> <p>Single-line string with key system info.</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def get_startup_info_string() -&gt; str:\n    \"\"\"Get a concise system info string for startup logging.\n\n    Returns:\n        Single-line string with key system info.\n    \"\"\"\n    import torch\n\n    from sleap_nn import __version__\n\n    parts = [f\"sleap-nn {__version__}\"]\n    parts.append(f\"Python {sys.version.split()[0]}\")\n    parts.append(f\"PyTorch {torch.__version__}\")\n\n    if torch.cuda.is_available():\n        parts.append(f\"CUDA {torch.version.cuda}\")\n        parts.append(f\"{torch.cuda.device_count()} GPU(s)\")\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        parts.append(\"MPS\")\n    else:\n        parts.append(\"CPU only\")\n\n    return \" | \".join(parts)\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.get_system_info_dict","title":"<code>get_system_info_dict()</code>","text":"<p>Get system information as a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with system info including Python version, platform, PyTorch version, CUDA availability, GPU details, and package versions.</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def get_system_info_dict() -&gt; dict:\n    \"\"\"Get system information as a dictionary.\n\n    Returns:\n        Dictionary with system info including Python version, platform,\n        PyTorch version, CUDA availability, GPU details, and package versions.\n    \"\"\"\n    import torch\n\n    info = {\n        \"python_version\": sys.version.split()[0],\n        \"platform\": platform.platform(),\n        \"pytorch_version\": torch.__version__,\n        \"cuda_available\": torch.cuda.is_available(),\n        \"cuda_version\": None,\n        \"cudnn_version\": None,\n        \"driver_version\": None,\n        \"driver_compatible\": None,\n        \"driver_min_required\": None,\n        \"gpu_count\": 0,\n        \"gpus\": [],\n        \"mps_available\": False,\n        \"accelerator\": \"cpu\",  # cpu, cuda, or mps\n        \"packages\": {},\n    }\n\n    # Driver version (check even if CUDA unavailable - old driver can cause this)\n    driver = get_nvidia_driver_version()\n    if driver:\n        info[\"driver_version\"] = driver\n\n    # CUDA details\n    if torch.cuda.is_available():\n        info[\"cuda_version\"] = torch.version.cuda\n        info[\"cudnn_version\"] = str(torch.backends.cudnn.version())\n        info[\"gpu_count\"] = torch.cuda.device_count()\n        info[\"accelerator\"] = \"cuda\"\n\n        # Check driver compatibility\n        if driver and info[\"cuda_version\"]:\n            is_compatible, min_required = check_driver_compatibility(\n                driver, info[\"cuda_version\"]\n            )\n            info[\"driver_compatible\"] = is_compatible\n            info[\"driver_min_required\"] = min_required\n\n        # GPU details\n        for i in range(torch.cuda.device_count()):\n            props = torch.cuda.get_device_properties(i)\n            info[\"gpus\"].append(\n                {\n                    \"id\": i,\n                    \"name\": props.name,\n                    \"compute_capability\": f\"{props.major}.{props.minor}\",\n                    \"memory_gb\": round(props.total_memory / (1024**3), 1),\n                }\n            )\n\n    # MPS (Apple Silicon)\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        info[\"mps_available\"] = True\n        info[\"accelerator\"] = \"mps\"\n        info[\"gpu_count\"] = 1\n\n    # Package versions\n    for pkg in PACKAGES:\n        info[\"packages\"][pkg] = get_package_info(pkg)\n\n    return info\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.parse_driver_version","title":"<code>parse_driver_version(version)</code>","text":"<p>Parse driver version string into comparable tuple.</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def parse_driver_version(version: str) -&gt; tuple[int, ...]:\n    \"\"\"Parse driver version string into comparable tuple.\"\"\"\n    try:\n        return tuple(int(x) for x in version.split(\".\"))\n    except ValueError:\n        return (0,)\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.print_system_info","title":"<code>print_system_info()</code>","text":"<p>Print comprehensive system diagnostics to console.</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def print_system_info() -&gt; None:\n    \"\"\"Print comprehensive system diagnostics to console.\"\"\"\n    from rich.console import Console\n    from rich.table import Table\n\n    console = Console()\n    info = get_system_info_dict()\n\n    # System info table (with GPU details integrated)\n    table = Table(title=\"System Information\", show_header=False)\n    table.add_column(\"Property\", style=\"cyan\")\n    table.add_column(\"Value\", style=\"white\")\n\n    table.add_row(\"Python\", info[\"python_version\"])\n    table.add_row(\"Platform\", info[\"platform\"])\n    table.add_row(\"PyTorch\", info[\"pytorch_version\"])\n\n    # GPU/Accelerator info\n    if info[\"accelerator\"] == \"cuda\":\n        table.add_row(\"Accelerator\", \"CUDA\")\n        table.add_row(\"CUDA version\", info[\"cuda_version\"] or \"N/A\")\n        table.add_row(\"cuDNN version\", info[\"cudnn_version\"] or \"N/A\")\n        table.add_row(\"Driver version\", info[\"driver_version\"] or \"N/A\")\n        table.add_row(\"GPU count\", str(info[\"gpu_count\"]))\n        # GPU details inline\n        for gpu in info[\"gpus\"]:\n            gpu_str = f\"{gpu['name']} ({gpu['memory_gb']} GB, compute {gpu['compute_capability']})\"\n            table.add_row(f\"GPU {gpu['id']}\", gpu_str)\n    elif info[\"accelerator\"] == \"mps\":\n        table.add_row(\"Accelerator\", \"MPS (Apple Silicon)\")\n    else:\n        table.add_row(\"Accelerator\", \"CPU only\")\n        # Show driver if present but CUDA unavailable (helps diagnose issues)\n        if info[\"driver_version\"]:\n            table.add_row(\"Driver version\", info[\"driver_version\"])\n\n    console.print(table)\n\n    # Package versions table\n    console.print()\n    pkg_table = Table(title=\"Package Versions\")\n    pkg_table.add_column(\"Package\", style=\"cyan\")\n    pkg_table.add_column(\"Version\", style=\"white\")\n    pkg_table.add_column(\"Source\", style=\"yellow\")\n    pkg_table.add_column(\"Location\", style=\"dim\")\n\n    for pkg, pkg_info in info[\"packages\"].items():\n        if pkg_info[\"version\"] == \"not installed\":\n            version_display = f\"[dim]{pkg_info['version']}[/dim]\"\n            pkg_table.add_row(pkg, version_display, \"\", \"\")\n        else:\n            location_display = _shorten_path(pkg_info[\"location\"])\n            pkg_table.add_row(\n                pkg, pkg_info[\"version\"], pkg_info[\"source\"], location_display\n            )\n\n    console.print(pkg_table)\n\n    # Actionable diagnostics\n    console.print()\n\n    # Driver compatibility check (CUDA only)\n    if info[\"accelerator\"] == \"cuda\" and info[\"driver_version\"]:\n        if info[\"driver_min_required\"]:\n            if info[\"driver_compatible\"]:\n                console.print(\n                    f\"[green]OK[/green] Driver is compatible: \"\n                    f\"{info['driver_version']} &gt;= {info['driver_min_required']} \"\n                    f\"(required for CUDA {info['cuda_version']})\"\n                )\n            else:\n                console.print(\n                    f\"[red]FAIL[/red] Driver is too old: \"\n                    f\"{info['driver_version']} &lt; {info['driver_min_required']} \"\n                    f\"(required for CUDA {info['cuda_version']})\"\n                )\n                console.print(\n                    \"  [yellow]Update your driver: https://www.nvidia.com/drivers[/yellow]\"\n                )\n        else:\n            # Unknown CUDA version, can't check compatibility\n            console.print(\n                f\"[yellow]![/yellow] Driver version: {info['driver_version']} \"\n                f\"(CUDA {info['cuda_version']} compatibility unknown)\"\n            )\n    elif info[\"accelerator\"] == \"cpu\" and info[\"driver_version\"]:\n        # Has driver but no CUDA - might be a problem\n        console.print(\n            f\"[yellow]![/yellow] NVIDIA driver found ({info['driver_version']}) \"\n            \"but CUDA is not available\"\n        )\n        console.print(\n            \"  [dim]This may indicate a driver/PyTorch version mismatch[/dim]\"\n        )\n\n    # GPU connection test\n    success, error = test_gpu_operations()\n    if info[\"accelerator\"] == \"cuda\":\n        if success:\n            console.print(\"[green]OK[/green] PyTorch can use GPU\")\n        else:\n            console.print(f\"[red]FAIL[/red] PyTorch cannot use GPU: {error}\")\n    elif info[\"accelerator\"] == \"mps\":\n        if success:\n            console.print(\"[green]OK[/green] PyTorch can use GPU\")\n        else:\n            console.print(f\"[red]FAIL[/red] PyTorch cannot use GPU: {error}\")\n    else:\n        console.print(\"[dim]--[/dim] No GPU available (using CPU)\")\n</code></pre>"},{"location":"api/system_info/#sleap_nn.system_info.test_gpu_operations","title":"<code>test_gpu_operations()</code>","text":"<p>Test that GPU tensor operations work.</p> <p>Returns:</p> Type Description <code>tuple[bool, Optional[str]]</code> <p>Tuple of (success, error_message).</p> Source code in <code>sleap_nn/system_info.py</code> <pre><code>def test_gpu_operations() -&gt; tuple[bool, Optional[str]]:\n    \"\"\"Test that GPU tensor operations work.\n\n    Returns:\n        Tuple of (success, error_message).\n    \"\"\"\n    import torch\n\n    if torch.cuda.is_available():\n        try:\n            x = torch.randn(100, 100, device=\"cuda\")\n            _ = torch.mm(x, x)\n            return True, None\n        except Exception as e:\n            return False, str(e)\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        try:\n            x = torch.randn(100, 100, device=\"mps\")\n            _ = torch.mm(x, x)\n            return True, None\n        except Exception as e:\n            return False, str(e)\n    return False, \"No GPU available\"\n</code></pre>"},{"location":"api/train/","title":"train","text":""},{"location":"api/train/#sleap_nn.train","title":"<code>sleap_nn.train</code>","text":"<p>Entry point for sleap_nn training.</p> <p>Functions:</p> Name Description <code>run_training</code> <p>Create ModelTrainer instance and start training.</p> <code>train</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p>"},{"location":"api/train/#sleap_nn.train.run_training","title":"<code>run_training(config, train_labels=None, val_labels=None)</code>","text":"<p>Create ModelTrainer instance and start training.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>Training configuration as a DictConfig.</p> required <code>train_labels</code> <code>Optional[List[Labels]]</code> <p>List of Labels objects for training.</p> <code>None</code> <code>val_labels</code> <code>Optional[List[Labels]]</code> <p>List of Labels objects for validation. If not provided, the labels will be loaded from paths in the config.</p> <code>None</code> Source code in <code>sleap_nn/train.py</code> <pre><code>def run_training(\n    config: DictConfig,\n    train_labels: Optional[List[sio.Labels]] = None,\n    val_labels: Optional[List[sio.Labels]] = None,\n):\n    \"\"\"Create ModelTrainer instance and start training.\n\n    Args:\n        config: Training configuration as a DictConfig.\n        train_labels: List of Labels objects for training.\n        val_labels: List of Labels objects for validation.\n            If not provided, the labels will be loaded from paths in the config.\n    \"\"\"\n    start_train_time = time()\n    start_timestamp = str(datetime.now())\n    logger.info(f\"Started training at: {start_timestamp}\")\n    logger.info(get_startup_info_string())\n\n    # provide the labels as the train labels, val labels will be split from the train labels\n    trainer = ModelTrainer.get_model_trainer_from_config(\n        config, train_labels=train_labels, val_labels=val_labels\n    )\n    trainer.train()\n\n    finish_timestamp = str(datetime.now())\n    total_elapsed = time() - start_train_time\n    logger.info(f\"Finished training at: {finish_timestamp}\")\n    logger.info(f\"Total training time: {total_elapsed} secs\")\n\n    rank = trainer.trainer.global_rank if trainer.trainer is not None else -1\n\n    logger.info(f\"Training Config: {OmegaConf.to_yaml(trainer.config)}\")\n\n    if rank in [0, -1]:\n        # run inference on val dataset\n        if trainer.config.trainer_config.save_ckpt:\n            data_paths = {}\n            run_path = (\n                Path(trainer.config.trainer_config.ckpt_dir)\n                / trainer.config.trainer_config.run_name\n            )\n            for index, _ in enumerate(trainer.train_labels):\n                logger.info(f\"Run path for index {index}: {run_path.as_posix()}\")\n                data_paths[f\"train.{index}\"] = (\n                    run_path / f\"labels_gt.train.{index}.slp\"\n                ).as_posix()\n                data_paths[f\"val.{index}\"] = (\n                    run_path / f\"labels_gt.val.{index}.slp\"\n                ).as_posix()\n\n            # Handle test_file_path as either a string or list of strings\n            test_file_path = OmegaConf.select(\n                config, \"data_config.test_file_path\", default=None\n            )\n            if test_file_path is not None:\n                # Normalize to list of strings\n                if isinstance(test_file_path, str):\n                    test_paths = [test_file_path]\n                else:\n                    test_paths = list(test_file_path)\n                # Add each test path to data_paths (always use index for consistency)\n                for idx, test_path in enumerate(test_paths):\n                    data_paths[f\"test.{idx}\"] = test_path\n\n            for d_name, path in data_paths.items():\n                # d_name is now in format: \"train.0\", \"val.0\", \"test.0\", etc.\n                pred_path = run_path / f\"labels_pr.{d_name}.slp\"\n                metrics_path = run_path / f\"metrics.{d_name}.npz\"\n\n                pred_labels = predict(\n                    data_path=path,\n                    model_paths=[run_path],\n                    peak_threshold=0.2,\n                    make_labels=True,\n                    device=str(trainer.trainer.strategy.root_device),\n                    output_path=pred_path,\n                    ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n                    ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n                )\n\n                if not len(pred_labels):\n                    logger.info(\n                        f\"Skipping eval on `{d_name}` dataset as there are no labeled frames...\"\n                    )\n                    continue  # skip if there are no labeled frames\n\n                # Run evaluation and save metrics\n                metrics = run_evaluation(\n                    ground_truth_path=path,\n                    predicted_path=pred_path.as_posix(),\n                    save_metrics=metrics_path.as_posix(),\n                )\n\n                logger.info(f\"---------Evaluation on `{d_name}` dataset---------\")\n                logger.info(f\"OKS mAP: {metrics['voc_metrics']['oks_voc.mAP']}\")\n                logger.info(f\"Average distance: {metrics['distance_metrics']['avg']}\")\n                logger.info(f\"p90 dist: {metrics['distance_metrics']['p90']}\")\n                logger.info(f\"p50 dist: {metrics['distance_metrics']['p50']}\")\n\n                # Log test metrics to wandb summary\n                if (\n                    d_name.startswith(\"test\")\n                    and trainer.config.trainer_config.use_wandb\n                ):\n                    import wandb\n\n                    if wandb.run is not None:\n                        summary_metrics = {\n                            f\"eval/{d_name}/mOKS\": metrics[\"mOKS\"][\"mOKS\"],\n                            f\"eval/{d_name}/oks_voc_mAP\": metrics[\"voc_metrics\"][\n                                \"oks_voc.mAP\"\n                            ],\n                            f\"eval/{d_name}/oks_voc_mAR\": metrics[\"voc_metrics\"][\n                                \"oks_voc.mAR\"\n                            ],\n                            f\"eval/{d_name}/mPCK\": metrics[\"pck_metrics\"][\"mPCK\"],\n                            f\"eval/{d_name}/PCK_5\": metrics[\"pck_metrics\"][\"PCK@5\"],\n                            f\"eval/{d_name}/PCK_10\": metrics[\"pck_metrics\"][\"PCK@10\"],\n                            f\"eval/{d_name}/distance_avg\": metrics[\"distance_metrics\"][\n                                \"avg\"\n                            ],\n                            f\"eval/{d_name}/distance_p50\": metrics[\"distance_metrics\"][\n                                \"p50\"\n                            ],\n                            f\"eval/{d_name}/distance_p95\": metrics[\"distance_metrics\"][\n                                \"p95\"\n                            ],\n                            f\"eval/{d_name}/distance_p99\": metrics[\"distance_metrics\"][\n                                \"p99\"\n                            ],\n                            f\"eval/{d_name}/visibility_precision\": metrics[\n                                \"visibility_metrics\"\n                            ][\"precision\"],\n                            f\"eval/{d_name}/visibility_recall\": metrics[\n                                \"visibility_metrics\"\n                            ][\"recall\"],\n                        }\n                        for key, value in summary_metrics.items():\n                            wandb.run.summary[key] = value\n\n            # Finish wandb run and cleanup after all evaluation is complete\n            if trainer.config.trainer_config.use_wandb:\n                import wandb\n                import shutil\n\n                if wandb.run is not None:\n                    wandb.finish()\n\n                # Delete local wandb logs if configured\n                wandb_config = trainer.config.trainer_config.wandb\n                should_delete_wandb_logs = wandb_config.delete_local_logs is True or (\n                    wandb_config.delete_local_logs is None\n                    and wandb_config.wandb_mode != \"offline\"\n                )\n                if should_delete_wandb_logs:\n                    wandb_dir = run_path / \"wandb\"\n                    if wandb_dir.exists():\n                        logger.info(\n                            f\"Deleting local wandb logs at {wandb_dir}... \"\n                            \"(set trainer_config.wandb.delete_local_logs=false to disable)\"\n                        )\n                        shutil.rmtree(wandb_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/train/#sleap_nn.train.train","title":"<code>train(train_labels_path=None, val_labels_path=None, validation_fraction=0.1, use_same_data_for_val=False, test_file_path=None, provider='LabelsReader', user_instances_only=True, data_pipeline_fw='torch_dataset', cache_img_path=None, use_existing_imgs=False, delete_cache_imgs_after_training=True, ensure_rgb=False, ensure_grayscale=False, scale=1.0, max_height=None, max_width=None, crop_size=None, min_crop_size=100, crop_padding=None, use_augmentations_train=True, intensity_aug=None, geometry_aug='rotation', init_weight='default', pretrained_backbone_weights=None, pretrained_head_weights=None, backbone_config='unet', head_configs=None, batch_size=1, shuffle_train=True, num_workers=0, ckpt_save_top_k=1, ckpt_save_last=None, trainer_num_devices=None, trainer_device_indices=None, trainer_accelerator='auto', enable_progress_bar=True, min_train_steps_per_epoch=200, train_steps_per_epoch=None, visualize_preds_during_training=False, keep_viz=False, max_epochs=10, seed=None, use_wandb=False, save_ckpt=False, ckpt_dir=None, run_name=None, resume_ckpt_path=None, wandb_entity=None, wandb_project=None, wandb_name=None, wandb_api_key=None, wandb_mode=None, wandb_save_viz_imgs_wandb=False, wandb_resume_prv_runid=None, wandb_group_name=None, wandb_delete_local_logs=None, optimizer='Adam', learning_rate=0.001, amsgrad=False, lr_scheduler=None, early_stopping=False, early_stopping_min_delta=0.0, early_stopping_patience=1, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, zmq_publish_port=None, zmq_controller_port=None, zmq_controller_timeout=10)</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to training data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>val_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to validation data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>validation_fraction</code> <code>float</code> <p>Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the <code>validation_labels</code> are already specified, this has no effect. Default: 0.1.</p> <code>0.1</code> <code>use_same_data_for_val</code> <code>bool</code> <p>If <code>True</code>, use the same data for both training and validation (train = val). Useful for intentional overfitting on small datasets. When enabled, <code>val_labels_path</code> and <code>validation_fraction</code> are ignored. Default: False.</p> <code>False</code> <code>test_file_path</code> <code>Optional[Union[str, List[str]]]</code> <p>Path or list of paths to test dataset(s) (<code>.slp</code> file(s) or <code>.mp4</code> file(s)). Note: This is used to get evaluation on test set after training is completed.</p> <code>None</code> <code>provider</code> <code>str</code> <p>Provider class to read the input sleap files. Only \"LabelsReader\" supported for the training pipeline. Default: \"LabelsReader\".</p> <code>'LabelsReader'</code> <code>user_instances_only</code> <code>bool</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used. Default: <code>True</code>.</p> <code>True</code> <code>data_pipeline_fw</code> <code>str</code> <p>Framework to create the data loaders. One of [<code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code>]. Default: \"torch_dataset\".</p> <code>'torch_dataset'</code> <code>cache_img_path</code> <code>Optional[str]</code> <p>Path to save <code>.jpg</code> images created with <code>torch_dataset_cache_img_disk</code> data pipeline framework. If <code>None</code>, the path provided in <code>trainer_config.save_ckpt</code> is used (else working dir is used). The <code>train_imgs</code> and <code>val_imgs</code> dirs are created inside this path. Default: None.</p> <code>None</code> <code>use_existing_imgs</code> <code>bool</code> <p>Use existing train and val images/ chunks in the <code>cache_img_path</code> for <code>torch_dataset_cache_img_disk</code> frameworks. If <code>True</code>, the <code>cache_img_path</code> should have <code>train_imgs</code> and <code>val_imgs</code> dirs. Default: False.</p> <code>False</code> <code>delete_cache_imgs_after_training</code> <code>bool</code> <p>If <code>False</code>, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. Default: True.</p> <code>True</code> <code>ensure_rgb</code> <code>bool</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to <code>True</code>, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: <code>False</code>.</p> <code>False</code> <code>ensure_grayscale</code> <code>bool</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: <code>False</code>.</p> <code>False</code> <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>1.0</code> <code>max_height</code> <code>Optional[int]</code> <p>Maximum height the original image should be resized and padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>Maximum width the original image should be resized and padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>crop_size</code> <code>Optional[int]</code> <p>Crop size of each instance for centered-instance model. If <code>None</code>, this would be automatically computed based on the largest instance in the <code>sio.Labels</code> file. If <code>scale</code> is provided, then the cropped image will be resized according to <code>scale</code>. Default: None.</p> <code>None</code> <code>min_crop_size</code> <code>Optional[int]</code> <p>Minimum crop size to be used if <code>crop_size</code> is <code>None</code>. Default: 100.</p> <code>100</code> <code>crop_padding</code> <code>Optional[int]</code> <p>Padding in pixels to add around instance bounding box when computing crop size. If <code>None</code>, padding is auto-computed based on augmentation settings. Only used when <code>crop_size</code> is <code>None</code>. Default: None.</p> <code>None</code> <code>use_augmentations_train</code> <code>bool</code> <p>True if the data augmentation should be applied to the training data, else False. Default: True.</p> <code>True</code> <code>intensity_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"] or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.IntensityConfig</code>. For eg: {             \"uniform_noise_min\": 1.0,             \"uniform_noise_p\": 1.0         }</p> <code>None</code> <code>geometry_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"]. or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.GeometryConfig</code>. For eg: {             \"rotation_min\": -45,             \"rotation_max\": 45,             \"affine_p\": 1.0         }</p> <code>'rotation'</code> <code>init_weight</code> <code>str</code> <p>model weights initialization method. \"default\" uses kaiming uniform initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".</p> <code>'default'</code> <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file with which the backbone is initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file with which the head layers are initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>backbone_config</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set, then pass a dictionary with the structure: {     \"unet((or) convnext (or)swint)\":         {(params in the corresponding architecture given in <code>sleap_nn.config.model_config.backbone_config</code>)         } }. For eg: {             \"unet\":                 {                     \"in_channels\": 3,                     \"filters\": 64,                     \"max_stride\": 32,                     \"output_stride\": 2                 }         }</p> <code>'unet'</code> <code>head_configs</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"]. The default <code>sigma</code> and <code>output_strides</code> are used if a string is passed. To set custom parameters, pass in a dictionary with the structure: {     \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):         {             \"confmaps\":                 {                     # params in the corresponding head type given in <code>sleap_nn.config.model_config.head_configs</code>                 },             \"pafs\":                 {                     # only for bottomup                 }         } }. For eg: {             \"single_instance\":                 {                     \"confmaps\":                         {                             \"part_names\": None,                             \"sigma\": 2.5,                             \"output_stride\": 2                         }                 }         }</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of samples per batch or batch size for training data. Default: 1.</p> <code>1</code> <code>shuffle_train</code> <code>bool</code> <p>True to have the train data reshuffled at every epoch. Default: True.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: 0.</p> <code>0</code> <code>ckpt_save_top_k</code> <code>int</code> <p>If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. Default: 1.</p> <code>1</code> <code>ckpt_save_last</code> <code>Optional[bool]</code> <p>When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. Default: None.</p> <code>None</code> <code>trainer_num_devices</code> <code>Optional[Union[str, int]]</code> <p>Number of devices to use or \"auto\" to let Lightning decide. If <code>None</code>, it defaults to <code>\"auto\"</code> when <code>trainer_device_indices</code> is also <code>None</code>, otherwise its value is inferred from trainer_device_indices. Default: None.</p> <code>None</code> <code>trainer_device_indices</code> <code>Optional[List[int]]</code> <p>List of device indices to use. For example, <code>[0, 1]</code> selects two devices and overrides <code>trainer_devices</code>, while <code>[2]</code> with <code>trainer_devices=2</code> still runs only on <code>device 2</code> (not two devices). If <code>None</code>, the number of devices is taken from <code>trainer_devices</code>, starting from index 0. Default: <code>None</code>.</p> <code>None</code> <code>trainer_accelerator</code> <code>str</code> <p>One of the (\"cpu\", \"gpu\", \"mps\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the <code>Trainer</code> to be connected to. Default: \"auto\".</p> <code>'auto'</code> <code>enable_progress_bar</code> <code>bool</code> <p>When True, enables printing the logs during training. Default: True.</p> <code>True</code> <code>min_train_steps_per_epoch</code> <code>int</code> <p>Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer <code>limit_train_batches</code> parameter of Torch <code>Trainer</code>. Default: 200.</p> <code>200</code> <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data or <code>min_train_steps_per_epoch</code>, whichever is largest. Default: <code>None</code>. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>None</code> <code>visualize_preds_during_training</code> <code>bool</code> <p>If set to <code>True</code>, sample predictions (keypoints  + confidence maps) are saved to <code>viz</code> folder in the ckpt dir.</p> <code>False</code> <code>keep_viz</code> <code>bool</code> <p>If set to <code>True</code>, the <code>viz</code> folder containing training visualizations will be kept after training completes. If <code>False</code>, the folder will be deleted. This parameter only has an effect when <code>visualize_preds_during_training</code> is <code>True</code>. Default: <code>False</code>.</p> <code>False</code> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs to run. Default: 10.</p> <code>10</code> <code>seed</code> <code>Optional[int]</code> <p>Seed value for the current experiment. If None, no seeding is applied. Default: None.</p> <code>None</code> <code>save_ckpt</code> <code>bool</code> <p>True to enable checkpointing. Default: False.</p> <code>False</code> <code>ckpt_dir</code> <code>Optional[str]</code> <p>Directory path where the <code>&lt;run_name&gt;</code> folder is created. If <code>None</code>, a new folder for the current run is created in the working dir. Default: <code>None</code></p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>Name of the current run. The ckpts will be created in <code>&lt;ckpt_dir&gt;/&lt;run_name&gt;</code>. If <code>None</code>, a run name is generated with <code>&lt;timestamp&gt;_&lt;head_name&gt;</code>. Default: <code>None</code></p> <code>None</code> <code>resume_ckpt_path</code> <code>Optional[str]</code> <p>Path to <code>.ckpt</code> file from which training is resumed. Default: None.</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>True to enable wandb logging. Default: False.</p> <code>False</code> <code>wandb_entity</code> <code>Optional[str]</code> <p>Entity of wandb project. Default: None. (The default entity in the user profile settings is used)</p> <code>None</code> <code>wandb_project</code> <code>Optional[str]</code> <p>Project name for the current wandb run. Default: None.</p> <code>None</code> <code>wandb_name</code> <code>Optional[str]</code> <p>Name of the current wandb run. Default: None.</p> <code>None</code> <code>wandb_api_key</code> <code>Optional[str]</code> <p>API key. The API key is masked when saved to config files. Default: None.</p> <code>None</code> <code>wandb_mode</code> <code>Optional[str]</code> <p>\"offline\" if only local logging is required. Default: None.</p> <code>None</code> <code>wandb_save_viz_imgs_wandb</code> <code>bool</code> <p>If set to <code>True</code>, sample predictions (keypoints + confidence maps) that are saved to local <code>viz</code> folder in the ckpt dir would also be uploaded to wandb. Default: False.</p> <code>False</code> <code>wandb_resume_prv_runid</code> <code>Optional[str]</code> <p>Previous run ID if training should be resumed from a previous ckpt. Default: None</p> <code>None</code> <code>wandb_group_name</code> <code>Optional[str]</code> <p>Group name for the wandb run. Default: None.</p> <code>None</code> <code>wandb_delete_local_logs</code> <code>Optional[bool]</code> <p>If True, delete local wandb logs folder after training. If False, keep the folder. If None (default), automatically delete if logging online (wandb_mode != \"offline\") and keep if logging offline. Default: None.</p> <code>None</code> <code>optimizer</code> <code>str</code> <p>Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".</p> <code>'Adam'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of type float. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>bool</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <code>lr_scheduler</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in <code>sleap_nn.config.trainer_config</code> are used). To use custom values, pass a dictionary with the structure in <code>sleap_nn.config.trainer_config.LRSchedulerConfig</code>. For eg, {             \"step_lr\":                 {                     (params in <code>sleap_nn.config.trainer_config.StepLRConfig</code>)                 }         }</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>True if early stopping should be enabled. Default: False.</p> <code>False</code> <code>early_stopping_min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. Default: 0.0.</p> <code>0.0</code> <code>early_stopping_patience</code> <code>int</code> <p>Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. Default: 1.</p> <code>1</code> <code>online_mining</code> <code>bool</code> <p>If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss.</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>float</code> <p>The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>int</code> <p>The minimum number of keypoints that will be considered as \"hard\", even if they are not below the <code>hard_to_easy_ratio</code>.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether.</p> <code>None</code> <code>loss_scale</code> <code>float</code> <p>Factor to scale the hard keypoint losses by for oks.</p> <code>5.0</code> <code>zmq_publish_port</code> <code>Optional[int]</code> <p>(int) Specifies the port to which the training logs (loss values) should be sent to.</p> <code>None</code> <code>zmq_controller_port</code> <code>Optional[int]</code> <p>(int) Specifies the port to listen to to stop the training (specific to SLEAP GUI).</p> <code>None</code> <code>zmq_controller_timeout</code> <code>int</code> <p>(int) Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed.</p> <code>10</code> Source code in <code>sleap_nn/train.py</code> <pre><code>def train(\n    train_labels_path: Optional[List[str]] = None,\n    val_labels_path: Optional[List[str]] = None,\n    validation_fraction: float = 0.1,\n    use_same_data_for_val: bool = False,\n    test_file_path: Optional[Union[str, List[str]]] = None,\n    provider: str = \"LabelsReader\",\n    user_instances_only: bool = True,\n    data_pipeline_fw: str = \"torch_dataset\",\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    delete_cache_imgs_after_training: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    scale: float = 1.0,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n    crop_size: Optional[int] = None,\n    min_crop_size: Optional[int] = 100,\n    crop_padding: Optional[int] = None,\n    use_augmentations_train: bool = True,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometry_aug: Optional[Union[str, List[str], Dict[str, Any]]] = \"rotation\",\n    init_weight: str = \"default\",\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    backbone_config: Union[str, Dict[str, Any]] = \"unet\",\n    head_configs: Union[str, Dict[str, Any]] = None,\n    batch_size: int = 1,\n    shuffle_train: bool = True,\n    num_workers: int = 0,\n    ckpt_save_top_k: int = 1,\n    ckpt_save_last: Optional[bool] = None,\n    trainer_num_devices: Optional[Union[str, int]] = None,\n    trainer_device_indices: Optional[List[int]] = None,\n    trainer_accelerator: str = \"auto\",\n    enable_progress_bar: bool = True,\n    min_train_steps_per_epoch: int = 200,\n    train_steps_per_epoch: Optional[int] = None,\n    visualize_preds_during_training: bool = False,\n    keep_viz: bool = False,\n    max_epochs: int = 10,\n    seed: Optional[int] = None,\n    use_wandb: bool = False,\n    save_ckpt: bool = False,\n    ckpt_dir: Optional[str] = None,\n    run_name: Optional[str] = None,\n    resume_ckpt_path: Optional[str] = None,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_name: Optional[str] = None,\n    wandb_api_key: Optional[str] = None,\n    wandb_mode: Optional[str] = None,\n    wandb_save_viz_imgs_wandb: bool = False,\n    wandb_resume_prv_runid: Optional[str] = None,\n    wandb_group_name: Optional[str] = None,\n    wandb_delete_local_logs: Optional[bool] = None,\n    optimizer: str = \"Adam\",\n    learning_rate: float = 1e-3,\n    amsgrad: bool = False,\n    lr_scheduler: Optional[Union[str, Dict[str, Any]]] = None,\n    early_stopping: bool = False,\n    early_stopping_min_delta: float = 0.0,\n    early_stopping_patience: int = 1,\n    online_mining: bool = False,\n    hard_to_easy_ratio: float = 2.0,\n    min_hard_keypoints: int = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: float = 5.0,\n    zmq_publish_port: Optional[int] = None,\n    zmq_controller_port: Optional[int] = None,\n    zmq_controller_timeout: int = 10,\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        train_labels_path: List of paths to training data (`.slp` file). Default: `None`\n        val_labels_path: List of paths to validation data (`.slp` file). Default: `None`\n        validation_fraction: Float between 0 and 1 specifying the fraction of the\n            training set to sample for generating the validation set. The remaining\n            labeled frames will be left in the training set. If the `validation_labels`\n            are already specified, this has no effect. Default: 0.1.\n        use_same_data_for_val: If `True`, use the same data for both training and\n            validation (train = val). Useful for intentional overfitting on small\n            datasets. When enabled, `val_labels_path` and `validation_fraction` are\n            ignored. Default: False.\n        test_file_path: Path or list of paths to test dataset(s) (`.slp` file(s) or `.mp4` file(s)).\n            Note: This is used to get evaluation on test set after training is completed.\n        provider: Provider class to read the input sleap files. Only \"LabelsReader\"\n            supported for the training pipeline. Default: \"LabelsReader\".\n        user_instances_only: `True` if only user labeled instances should be used for\n            training. If `False`, both user labeled and predicted instances would be used.\n            Default: `True`.\n        data_pipeline_fw: Framework to create the data loaders. One of [`torch_dataset`,\n            `torch_dataset_cache_img_memory`, `torch_dataset_cache_img_disk`]. Default: \"torch_dataset\".\n        cache_img_path: Path to save `.jpg` images created with `torch_dataset_cache_img_disk` data pipeline\n            framework. If `None`, the path provided in `trainer_config.save_ckpt` is used (else working dir is used). The `train_imgs` and `val_imgs` dirs are created inside this path. Default: None.\n        use_existing_imgs: Use existing train and val images/ chunks in the `cache_img_path` for `torch_dataset_cache_img_disk` frameworks. If `True`, the `cache_img_path` should have `train_imgs` and `val_imgs` dirs.\n            Default: False.\n        delete_cache_imgs_after_training: If `False`, the images (torch_dataset_cache_img_disk) are\n            retained after training. Else, the files are deleted. Default: True.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n            channel when this is set to `True`, then the images from single-channel\n            is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n            is set to True, then we convert the image to grayscale (single-channel)\n            image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        max_height: Maximum height the original image should be resized and padded to. If not provided, the\n            original image size will be retained. Default: None.\n        max_width: Maximum width the original image should be resized and padded to. If not provided, the\n            original image size will be retained. Default: None.\n        crop_size: Crop size of each instance for centered-instance model.\n            If `None`, this would be automatically computed based on the largest instance\n            in the `sio.Labels` file. If `scale` is provided, then the cropped image will be resized according to `scale`. Default: None.\n        min_crop_size: Minimum crop size to be used if `crop_size` is `None`. Default: 100.\n        crop_padding: Padding in pixels to add around instance bounding box when computing\n            crop size. If `None`, padding is auto-computed based on augmentation settings.\n            Only used when `crop_size` is `None`. Default: None.\n        use_augmentations_train: True if the data augmentation should be applied to the\n            training data, else False. Default: True.\n        intensity_aug: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"]\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.IntensityConfig`.\n            For eg: {\n                        \"uniform_noise_min\": 1.0,\n                        \"uniform_noise_p\": 1.0\n                    }\n        geometry_aug: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"].\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.GeometryConfig`.\n            For eg: {\n                        \"rotation_min\": -45,\n                        \"rotation_max\": 45,\n                        \"affine_p\": 1.0\n                    }\n        init_weight: model weights initialization method. \"default\" uses kaiming uniform\n            initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".\n        pretrained_backbone_weights: Path of the `ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file with which the backbone is\n            initialized. If `None`, random init is used. Default: None.\n        pretrained_head_weights: Path of the `ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file with which the head layers are\n            initialized. If `None`, random init is used. Default: None.\n        backbone_config: One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\",\n            \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\",\n            \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set,\n            then pass a dictionary with the structure:\n            {\n                \"unet((or) convnext (or)swint)\":\n                    {(params in the corresponding architecture given in `sleap_nn.config.model_config.backbone_config`)\n                    }\n            }.\n            For eg: {\n                        \"unet\":\n                            {\n                                \"in_channels\": 3,\n                                \"filters\": 64,\n                                \"max_stride\": 32,\n                                \"output_stride\": 2\n                            }\n                    }\n        head_configs: One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"].\n            The default `sigma` and `output_strides` are used if a string is passed. To\n            set custom parameters, pass in a dictionary with the structure:\n            {\n                \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):\n                    {\n                        \"confmaps\":\n                            {\n                                # params in the corresponding head type given in `sleap_nn.config.model_config.head_configs`\n                            },\n                        \"pafs\":\n                            {\n                                # only for bottomup\n                            }\n                    }\n            }.\n            For eg: {\n                        \"single_instance\":\n                            {\n                                \"confmaps\":\n                                    {\n                                        \"part_names\": None,\n                                        \"sigma\": 2.5,\n                                        \"output_stride\": 2\n                                    }\n                            }\n                    }\n        batch_size: Number of samples per batch or batch size for training data. Default: 1.\n        shuffle_train: True to have the train data reshuffled at every epoch. Default: True.\n        num_workers: Number of subprocesses to use for data loading. 0 means that the data\n            will be loaded in the main process. Default: 0.\n        ckpt_save_top_k: If save_top_k == k, the best k models according to the quantity\n            monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1,\n            all models are saved. Please note that the monitors are checked every every_n_epochs\n            epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an\n            epoch, the name of the saved file will be appended with a version count starting\n            with v1 unless enable_version_counter is set to False. Default: 1.\n        ckpt_save_last: When True, saves a last.ckpt whenever a checkpoint file gets saved.\n            On a local filesystem, this will be a symbolic link, and otherwise a copy of\n            the checkpoint file. This allows accessing the latest checkpoint in a deterministic\n            manner. Default: None.\n        trainer_num_devices: Number of devices to use or \"auto\" to let Lightning decide. If `None`, it defaults to `\"auto\"` when `trainer_device_indices` is also `None`, otherwise its value is inferred from trainer_device_indices. Default: None.\n        trainer_device_indices: List of device indices to use. For example, `[0, 1]` selects two devices and overrides `trainer_devices`, while `[2]` with `trainer_devices=2` still runs only on `device 2` (not two devices). If `None`, the number of devices is taken from `trainer_devices`, starting from index 0. Default: `None`.\n        trainer_accelerator: One of the (\"cpu\", \"gpu\", \"mps\", \"auto\"). \"auto\" recognises\n            the machine the model is running on and chooses the appropriate accelerator for\n            the `Trainer` to be connected to. Default: \"auto\".\n        enable_progress_bar: When True, enables printing the logs during training. Default: True.\n        min_train_steps_per_epoch: Minimum number of iterations in a single epoch. (Useful if model\n            is trained with very few data points). Refer `limit_train_batches` parameter\n            of Torch `Trainer`. Default: 200.\n        train_steps_per_epoch: Number of minibatches (steps) to train for in an epoch. If set to `None`,\n            this is set to the number of batches in the training data or `min_train_steps_per_epoch`,\n            whichever is largest. Default: `None`. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        visualize_preds_during_training: If set to `True`, sample predictions (keypoints  + confidence maps)\n            are saved to `viz` folder in the ckpt dir.\n        keep_viz: If set to `True`, the `viz` folder containing training visualizations will be kept after training completes. If `False`, the folder will be deleted. This parameter only has an effect when `visualize_preds_during_training` is `True`. Default: `False`.\n        max_epochs: Maximum number of epochs to run. Default: 10.\n        seed: Seed value for the current experiment. If None, no seeding is applied. Default: None.\n        save_ckpt: True to enable checkpointing. Default: False.\n        ckpt_dir: Directory path where the `&lt;run_name&gt;` folder is created. If `None`, a new folder for the current run is created in the working dir. **Default**: `None`\n        run_name: Name of the current run. The ckpts will be created in `&lt;ckpt_dir&gt;/&lt;run_name&gt;`. If `None`, a run name is generated with `&lt;timestamp&gt;_&lt;head_name&gt;`. **Default**: `None`\n        resume_ckpt_path: Path to `.ckpt` file from which training is resumed. Default: None.\n        use_wandb: True to enable wandb logging. Default: False.\n        wandb_entity: Entity of wandb project. Default: None.\n            (The default entity in the user profile settings is used)\n        wandb_project: Project name for the current wandb run. Default: None.\n        wandb_name: Name of the current wandb run. Default: None.\n        wandb_api_key: API key. The API key is masked when saved to config files. Default: None.\n        wandb_mode: \"offline\" if only local logging is required. Default: None.\n        wandb_save_viz_imgs_wandb: If set to `True`, sample predictions (keypoints + confidence maps) that are saved to local `viz` folder in the ckpt dir would also be uploaded to wandb. Default: False.\n        wandb_resume_prv_runid: Previous run ID if training should be resumed from a previous\n            ckpt. Default: None\n        wandb_group_name: Group name for the wandb run. Default: None.\n        wandb_delete_local_logs: If True, delete local wandb logs folder after training.\n            If False, keep the folder. If None (default), automatically delete if logging\n            online (wandb_mode != \"offline\") and keep if logging offline. Default: None.\n        optimizer: Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".\n        learning_rate: Learning rate of type float. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n        lr_scheduler: One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in\n            `sleap_nn.config.trainer_config` are used). To use custom values, pass a\n            dictionary with the structure in `sleap_nn.config.trainer_config.LRSchedulerConfig`.\n            For eg, {\n                        \"step_lr\":\n                            {\n                                (params in `sleap_nn.config.trainer_config.StepLRConfig`)\n                            }\n                    }\n        early_stopping: True if early stopping should be enabled. Default: False.\n        early_stopping_min_delta: Minimum change in the monitored quantity to qualify as\n            an improvement, i.e. an absolute change of less than or equal to min_delta,\n            will count as no improvement. Default: 0.0.\n        early_stopping_patience: Number of checks with no improvement after which training\n            will be stopped. Under the default configuration, one check happens after every\n            training epoch. Default: 1.\n        online_mining: If True, online hard keypoint mining (OHKM) will be enabled. When\n            this is enabled, the loss is computed per keypoint (or edge for PAFs) and\n            sorted from lowest (easy) to highest (hard). The hard keypoint loss will be\n            scaled to have a higher weight in the total loss, encouraging the training\n            to focus on tricky body parts that are more difficult to learn.\n            If False, no mining will be performed and all keypoints will be weighted\n            equally in the loss.\n        hard_to_easy_ratio: The minimum ratio of the individual keypoint loss with\n            respect to the lowest keypoint loss in order to be considered as \"hard\".\n            This helps to switch focus on across groups of keypoints during training.\n        min_hard_keypoints: The minimum number of keypoints that will be considered as\n            \"hard\", even if they are not below the `hard_to_easy_ratio`.\n        max_hard_keypoints: The maximum number of hard keypoints to apply scaling to.\n            This can help when there are few very easy keypoints which may skew the\n            ratio and result in loss scaling being applied to most keypoints, which can\n            reduce the impact of hard mining altogether.\n        loss_scale: Factor to scale the hard keypoint losses by for oks.\n        zmq_publish_port: (int) Specifies the port to which the training logs (loss values) should be sent to.\n        zmq_controller_port: (int) Specifies the port to listen to to stop the training (specific to SLEAP GUI).\n        zmq_controller_timeout: (int) Polling timeout in microseconds specified as an integer. This controls how long the poller\n            should wait to receive a response and should be set to a small value to minimize the impact on training speed.\n    \"\"\"\n    data_config = get_data_config(\n        train_labels_path=train_labels_path,\n        val_labels_path=val_labels_path,\n        validation_fraction=validation_fraction,\n        use_same_data_for_val=use_same_data_for_val,\n        test_file_path=test_file_path,\n        provider=provider,\n        user_instances_only=user_instances_only,\n        data_pipeline_fw=data_pipeline_fw,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        delete_cache_imgs_after_training=delete_cache_imgs_after_training,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        scale=scale,\n        max_height=max_height,\n        max_width=max_width,\n        crop_size=crop_size,\n        min_crop_size=min_crop_size,\n        crop_padding=crop_padding,\n        use_augmentations_train=use_augmentations_train,\n        intensity_aug=intensity_aug,\n        geometry_aug=geometry_aug,\n    )\n\n    model_config = get_model_config(\n        init_weight=init_weight,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n    )\n\n    trainer_config = get_trainer_config(\n        batch_size=batch_size,\n        shuffle_train=shuffle_train,\n        num_workers=num_workers,\n        ckpt_save_top_k=ckpt_save_top_k,\n        ckpt_save_last=ckpt_save_last,\n        trainer_num_devices=trainer_num_devices,\n        trainer_device_indices=trainer_device_indices,\n        trainer_accelerator=trainer_accelerator,\n        enable_progress_bar=enable_progress_bar,\n        min_train_steps_per_epoch=min_train_steps_per_epoch,\n        train_steps_per_epoch=train_steps_per_epoch,\n        visualize_preds_during_training=visualize_preds_during_training,\n        keep_viz=keep_viz,\n        max_epochs=max_epochs,\n        seed=seed,\n        use_wandb=use_wandb,\n        save_ckpt=save_ckpt,\n        ckpt_dir=ckpt_dir,\n        run_name=run_name,\n        resume_ckpt_path=resume_ckpt_path,\n        wandb_entity=wandb_entity,\n        wandb_project=wandb_project,\n        wandb_name=wandb_name,\n        wandb_api_key=wandb_api_key,\n        wandb_mode=wandb_mode,\n        wandb_save_viz_imgs_wandb=wandb_save_viz_imgs_wandb,\n        wandb_resume_prv_runid=wandb_resume_prv_runid,\n        wandb_group_name=wandb_group_name,\n        wandb_delete_local_logs=wandb_delete_local_logs,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n        lr_scheduler=lr_scheduler,\n        early_stopping=early_stopping,\n        early_stopping_min_delta=early_stopping_min_delta,\n        early_stopping_patience=early_stopping_patience,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        zmq_publish_port=zmq_publish_port,\n        zmq_controller_port=zmq_controller_port,\n        zmq_controller_timeout=zmq_controller_timeout,\n    )\n\n    # create omegaconf object\n    training_job_config = TrainingJobConfig(\n        data_config=data_config,\n        model_config=model_config,\n        trainer_config=trainer_config,\n    )\n    omegaconf_config = training_job_config.to_sleap_nn_cfg()\n\n    # run training\n    run_training(omegaconf_config.copy())\n</code></pre>"},{"location":"api/architectures/","title":"architectures","text":""},{"location":"api/architectures/#sleap_nn.architectures","title":"<code>sleap_nn.architectures</code>","text":"<p>Modules related to model architectures.</p> <p>Modules:</p> Name Description <code>common</code> <p>Common utilities for architecture and model building.</p> <code>convnext</code> <p>This module provides a generalized implementation of ConvNext.</p> <code>encoder_decoder</code> <p>Generic encoder-decoder fully convolutional backbones.</p> <code>heads</code> <p>Model head definitions for defining model output types.</p> <code>model</code> <p>This module defines the main SLEAP model class for defining a trainable model.</p> <code>swint</code> <p>This module provides a generalized implementation of SwinT.</p> <code>unet</code> <p>This module provides a generalized implementation of UNet.</p> <code>utils</code> <p>Miscellaneous utility functions for architectures and modeling.</p>"},{"location":"api/architectures/common/","title":"common","text":""},{"location":"api/architectures/common/#sleap_nn.architectures.common","title":"<code>sleap_nn.architectures.common</code>","text":"<p>Common utilities for architecture and model building.</p> <p>Classes:</p> Name Description <code>MaxPool2dWithSamePadding</code> <p>A MaxPool2d module with support for same padding.</p>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding","title":"<code>MaxPool2dWithSamePadding</code>","text":"<p>               Bases: <code>MaxPool2d</code></p> <p>A MaxPool2d module with support for same padding.</p> <p>This class extends the torch.nn.MaxPool2d module and adds the ability to perform 'same' padding, similar to 'same' padding in convolutional layers. When 'same' padding is specified, the input tensor is padded with zeros to ensure that the output spatial dimensions match the input spatial dimensions as closely as possible.</p> <p>Parameters:</p> Name Type Description Default <code>nn.MaxPool2d arguments</code> <p>Arguments that are passed to the parent torch.nn.MaxPool2d class.</p> required <p>Methods:</p> Name Description <code>forward</code> <p>torch.Tensor) -&gt; torch.Tensor: Forward pass through the MaxPool2dWithSamePadding module.</p> Note <p>The 'same' padding is applied only when self.padding is set to \"same\".</p> Example Source code in <code>sleap_nn/architectures/common.py</code> <pre><code>class MaxPool2dWithSamePadding(nn.MaxPool2d):\n    \"\"\"A MaxPool2d module with support for same padding.\n\n    This class extends the torch.nn.MaxPool2d module and adds the ability\n    to perform 'same' padding, similar to 'same' padding in convolutional\n    layers. When 'same' padding is specified, the input tensor is padded\n    with zeros to ensure that the output spatial dimensions match the input\n    spatial dimensions as closely as possible.\n\n    Args:\n        nn.MaxPool2d arguments: Arguments that are passed to the parent\n            torch.nn.MaxPool2d class.\n\n    Methods:\n        forward(x: torch.Tensor) -&gt; torch.Tensor:\n            Forward pass through the MaxPool2dWithSamePadding module.\n\n    Note:\n        The 'same' padding is applied only when self.padding is set to \"same\".\n\n    Example:\n        # Create an instance of MaxPool2dWithSamePadding\n        maxpool_layer = MaxPool2dWithSamePadding(kernel_size=3, stride=2, padding=\"same\")\n\n        # Perform a forward pass on an input tensor\n        input_tensor = torch.rand(1, 3, 32, 32)  # Example input tensor\n        output = maxpool_layer(input_tensor)  # Apply the MaxPool2d operation with same padding.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the MaxPool2dWithSamePadding module.\"\"\"\n        super().__init__(*args, **kwargs)\n\n    def _calc_same_pad(self, i: int, k: int, s: int, d: int) -&gt; int:\n        \"\"\"Calculate the required padding to achieve 'same' padding.\n\n        Args:\n            i (int): Input dimension (height or width).\n            k (int): Kernel size.\n            s (int): Stride.\n            d (int): Dilation.\n\n        Returns:\n            int: The calculated padding value.\n        \"\"\"\n        return int(\n            max(\n                (torch.ceil(torch.tensor(i / s)).item() - 1) * s + (k - 1) * d + 1 - i,\n                0,\n            )\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the MaxPool2dWithSamePadding module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the MaxPool2d operation.\n        \"\"\"\n        if self.padding == \"same\":\n            ih, iw = x.size()[-2:]\n\n            pad_h = self._calc_same_pad(\n                i=ih,\n                k=(\n                    self.kernel_size\n                    if type(self.kernel_size) is int\n                    else self.kernel_size[0]\n                ),\n                s=self.stride if type(self.stride) is int else self.stride[0],\n                d=self.dilation if type(self.dilation) is int else self.dilation[0],\n            )\n            pad_w = self._calc_same_pad(\n                i=iw,\n                k=(\n                    self.kernel_size\n                    if type(self.kernel_size) is int\n                    else self.kernel_size[1]\n                ),\n                s=self.stride if type(self.stride) is int else self.stride[1],\n                d=self.dilation if type(self.dilation) is int else self.dilation[1],\n            )\n\n            if pad_h &gt; 0 or pad_w &gt; 0:\n                x = F.pad(\n                    x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n                )\n            self.padding = 0\n\n        return F.max_pool2d(\n            x,\n            self.kernel_size,\n            self.stride,\n            self.padding,\n            self.dilation,\n            ceil_mode=self.ceil_mode,\n            return_indices=self.return_indices,\n        )\n</code></pre>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding--create-an-instance-of-maxpool2dwithsamepadding","title":"Create an instance of MaxPool2dWithSamePadding","text":"<p>maxpool_layer = MaxPool2dWithSamePadding(kernel_size=3, stride=2, padding=\"same\")</p>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding--perform-a-forward-pass-on-an-input-tensor","title":"Perform a forward pass on an input tensor","text":"<p>input_tensor = torch.rand(1, 3, 32, 32)  # Example input tensor output = maxpool_layer(input_tensor)  # Apply the MaxPool2d operation with same padding.</p>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the MaxPool2dWithSamePadding module.</p> Source code in <code>sleap_nn/architectures/common.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize the MaxPool2dWithSamePadding module.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/architectures/common/#sleap_nn.architectures.common.MaxPool2dWithSamePadding.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the MaxPool2dWithSamePadding module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the MaxPool2d operation.</p> Source code in <code>sleap_nn/architectures/common.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the MaxPool2dWithSamePadding module.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the MaxPool2d operation.\n    \"\"\"\n    if self.padding == \"same\":\n        ih, iw = x.size()[-2:]\n\n        pad_h = self._calc_same_pad(\n            i=ih,\n            k=(\n                self.kernel_size\n                if type(self.kernel_size) is int\n                else self.kernel_size[0]\n            ),\n            s=self.stride if type(self.stride) is int else self.stride[0],\n            d=self.dilation if type(self.dilation) is int else self.dilation[0],\n        )\n        pad_w = self._calc_same_pad(\n            i=iw,\n            k=(\n                self.kernel_size\n                if type(self.kernel_size) is int\n                else self.kernel_size[1]\n            ),\n            s=self.stride if type(self.stride) is int else self.stride[1],\n            d=self.dilation if type(self.dilation) is int else self.dilation[1],\n        )\n\n        if pad_h &gt; 0 or pad_w &gt; 0:\n            x = F.pad(\n                x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)\n            )\n        self.padding = 0\n\n    return F.max_pool2d(\n        x,\n        self.kernel_size,\n        self.stride,\n        self.padding,\n        self.dilation,\n        ceil_mode=self.ceil_mode,\n        return_indices=self.return_indices,\n    )\n</code></pre>"},{"location":"api/architectures/convnext/","title":"convnext","text":""},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext","title":"<code>sleap_nn.architectures.convnext</code>","text":"<p>This module provides a generalized implementation of ConvNext.</p> <p>See the <code>ConvNextWrapper</code> class docstring for more information.</p> <p>Classes:</p> Name Description <code>ConvNeXtEncoder</code> <p>ConvNext backbone for pose estimation.</p> <code>ConvNextWrapper</code> <p>ConvNext architecture for pose estimation.</p>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNeXtEncoder","title":"<code>ConvNeXtEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>ConvNext backbone for pose estimation.</p> <p>This class implements ConvNext from the <code>A ConvNet for the 2020s &lt;https://arxiv.org/abs/2201.03545&gt;</code> paper. Source: torchvision.models. This module serves as the backbone/ encoder architecture to extract features from the input image.</p> <p>Parameters:</p> Name Type Description Default <code>blocks (dict) </code> <p>Dictionary of depths and channels. Default is \"Tiny architecture\"</p> required <code>in_channels</code> <code>int</code> <p>Input number of channels. Default: 1.</p> <code>1</code> <code>stem_kernel</code> <code>int</code> <p>Size of the convolutional kernels in the stem layer.             Default is 4.</p> <code>4</code> <code>stem_stride</code> <code>int</code> <p>Convolutional stride in the stem layer. Default is 2.</p> <code>2</code> <code>stochastic_depth_prob</code> <code>float</code> <p>Stochastic depth rate. Default: 0.1.</p> <code>0.0</code> <code>layer_scale</code> <code>float</code> <p>Scale for Layer normalization layer. Default: 1e-6.</p> <code>1e-06</code> <code>block</code> <code>Module</code> <p>SwinTransformer Block. Default: None.</p> <code>None</code> <code>norm_layer</code> <code>Module</code> <p>Normalization layer. Default: None.</p> <code>None</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the ConvNext Encoder.</p> <code>forward</code> <p>Forward pass through the ConvNext encoder.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>class ConvNeXtEncoder(nn.Module):\n    \"\"\"ConvNext backbone for pose estimation.\n\n    This class implements ConvNext from the `A ConvNet for the 2020s &lt;https://arxiv.org/abs/2201.03545&gt;`\n    paper. Source: torchvision.models. This module serves as the backbone/ encoder\n    architecture to extract features from the input image.\n\n    Args:\n        blocks (dict) : Dictionary of depths and channels. Default is \"Tiny architecture\"\n                        {'depths': [3,3,9,3], 'channels':[96, 192, 384, 768]}\n        in_channels (int): Input number of channels. Default: 1.\n        stem_kernel (int): Size of the convolutional kernels in the stem layer.\n                        Default is 4.\n        stem_stride (int): Convolutional stride in the stem layer. Default is 2.\n        stochastic_depth_prob (float): Stochastic depth rate. Default: 0.1.\n        layer_scale (float): Scale for Layer normalization layer. Default: 1e-6.\n        block (nn.Module, optional): SwinTransformer Block. Default: None.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        blocks: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n        in_channels: int = 1,\n        stem_kernel: int = 4,\n        stem_stride: int = 2,\n        stochastic_depth_prob: float = 0.0,\n        layer_scale: float = 1e-6,\n        block: Optional[Callable[..., nn.Module]] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the ConvNext Encoder.\"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        depths, channels = blocks[\"depths\"], blocks[\"channels\"]\n        block_setting = [0] * len(depths)\n        for idx in range(len(depths)):\n            if idx == len(depths) - 1:\n                last = None\n            else:\n                last = channels[idx + 1]\n            block_setting[idx] = CNBlockConfig(channels[idx], last, depths[idx])\n        if block is None:\n            block = CNBlock\n\n        if norm_layer is None:\n            norm_layer = partial(LayerNorm2d, eps=1e-6)\n\n        layers: List[nn.Module] = []\n\n        # Stem\n        firstconv_output_channels = block_setting[0].input_channels\n        layers.append(\n            Conv2dNormActivation(\n                in_channels,\n                firstconv_output_channels,\n                kernel_size=stem_kernel,\n                stride=stem_stride,\n                padding=1,  ## 0 -&gt; 1\n                norm_layer=norm_layer,\n                activation_layer=None,\n                bias=True,\n            )\n        )\n\n        total_stage_blocks = sum(cnf.num_layers for cnf in block_setting)\n        stage_block_id = 0\n        for cnf in block_setting:\n            # Bottlenecks\n            stage: List[nn.Module] = []\n            for _ in range(cnf.num_layers):\n                # adjust stochastic depth probability based on the depth of the stage block\n                sd_prob = (\n                    stochastic_depth_prob * stage_block_id / (total_stage_blocks - 1.0)\n                )\n                stage.append(block(cnf.input_channels, layer_scale, sd_prob))\n                stage_block_id += 1\n            layers.append(nn.Sequential(*stage))\n            if cnf.out_channels is not None:\n                # Downsampling\n                layers.append(\n                    nn.Sequential(\n                        norm_layer(cnf.input_channels),\n                        nn.Conv2d(\n                            cnf.input_channels,\n                            cnf.out_channels,\n                            kernel_size=2,\n                            stride=2,\n                        ),\n                    )\n                )\n        self.features = nn.Sequential(*layers)\n\n    def _forward_impl(self, x: Tensor) -&gt; Tensor:\n        features_list = []\n        for l in self.features:\n            x = l(x)\n            features_list.append(x)\n        return features_list\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Forward pass through the ConvNext encoder.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Outputs a list of tensors from each stage after applying the ConvNext backbone.\n        \"\"\"\n        return self._forward_impl(x)\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNeXtEncoder.__init__","title":"<code>__init__(blocks={'depths': [3, 3, 9, 3], 'channels': [96, 192, 384, 768]}, in_channels=1, stem_kernel=4, stem_stride=2, stochastic_depth_prob=0.0, layer_scale=1e-06, block=None, norm_layer=None, **kwargs)</code>","text":"<p>Initialize the ConvNext Encoder.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def __init__(\n    self,\n    blocks: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n    in_channels: int = 1,\n    stem_kernel: int = 4,\n    stem_stride: int = 2,\n    stochastic_depth_prob: float = 0.0,\n    layer_scale: float = 1e-6,\n    block: Optional[Callable[..., nn.Module]] = None,\n    norm_layer: Optional[Callable[..., nn.Module]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the ConvNext Encoder.\"\"\"\n    super().__init__()\n    _log_api_usage_once(self)\n\n    depths, channels = blocks[\"depths\"], blocks[\"channels\"]\n    block_setting = [0] * len(depths)\n    for idx in range(len(depths)):\n        if idx == len(depths) - 1:\n            last = None\n        else:\n            last = channels[idx + 1]\n        block_setting[idx] = CNBlockConfig(channels[idx], last, depths[idx])\n    if block is None:\n        block = CNBlock\n\n    if norm_layer is None:\n        norm_layer = partial(LayerNorm2d, eps=1e-6)\n\n    layers: List[nn.Module] = []\n\n    # Stem\n    firstconv_output_channels = block_setting[0].input_channels\n    layers.append(\n        Conv2dNormActivation(\n            in_channels,\n            firstconv_output_channels,\n            kernel_size=stem_kernel,\n            stride=stem_stride,\n            padding=1,  ## 0 -&gt; 1\n            norm_layer=norm_layer,\n            activation_layer=None,\n            bias=True,\n        )\n    )\n\n    total_stage_blocks = sum(cnf.num_layers for cnf in block_setting)\n    stage_block_id = 0\n    for cnf in block_setting:\n        # Bottlenecks\n        stage: List[nn.Module] = []\n        for _ in range(cnf.num_layers):\n            # adjust stochastic depth probability based on the depth of the stage block\n            sd_prob = (\n                stochastic_depth_prob * stage_block_id / (total_stage_blocks - 1.0)\n            )\n            stage.append(block(cnf.input_channels, layer_scale, sd_prob))\n            stage_block_id += 1\n        layers.append(nn.Sequential(*stage))\n        if cnf.out_channels is not None:\n            # Downsampling\n            layers.append(\n                nn.Sequential(\n                    norm_layer(cnf.input_channels),\n                    nn.Conv2d(\n                        cnf.input_channels,\n                        cnf.out_channels,\n                        kernel_size=2,\n                        stride=2,\n                    ),\n                )\n            )\n    self.features = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNeXtEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the ConvNext encoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Outputs a list of tensors from each stage after applying the ConvNext backbone.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Forward pass through the ConvNext encoder.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Outputs a list of tensors from each stage after applying the ConvNext backbone.\n    \"\"\"\n    return self._forward_impl(x)\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper","title":"<code>ConvNextWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>ConvNext architecture for pose estimation.</p> <p>This class defines the ConvNext architecture for pose estimation, combining an ConvNext as the encoder and a decoder. The encoder extracts features from the input, while the decoder generates confidence maps based on the features.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"].</p> required <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>arch</code> <code>dict</code> <p>Dictionary of depths and channels. Default is \"Tiny architecture\"</p> <code>{'depths': [3, 3, 9, 3], 'channels': [96, 192, 384, 768]}</code> <code>{'depths'</code> <p>[3,3,9,3], 'channels':[96, 192, 384, 768]}</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>stem_patch_kernel</code> <code>int</code> <p>Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>4</code> <code>stem_patch_stride</code> <code>int</code> <p>Convolutional stride in the stem layer. Default is 2.</p> <code>2</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>16</code> for all convnext architectures.</p> <code>32</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the ConvNeXt architecture.</p> <code>from_config</code> <p>Create ConvNextWrapper from a config.</p> <p>Attributes:</p> Name Type Description <code>max_channels</code> <p>Returns the maximum channels of the ConvNext (last layer of the encoder).</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>class ConvNextWrapper(nn.Module):\n    \"\"\"ConvNext architecture for pose estimation.\n\n    This class defines the ConvNext architecture for pose estimation, combining an\n    ConvNext as the encoder and a decoder. The encoder extracts features from the input,\n    while the decoder generates confidence maps based on the features.\n\n    Args:\n        model_type: One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"].\n        output_stride: Minimum of the strides of the output heads. The input confidence map.\n        tensor is expected to be at the same stride.\n        in_channels: Number of input channels. Default is 1.\n        arch: Dictionary of depths and channels. Default is \"Tiny architecture\"\n        {'depths': [3,3,9,3], 'channels':[96, 192, 384, 768]}\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        stem_patch_kernel: Size of the convolutional kernels in the stem layer. Default is 4.\n        stem_patch_stride: Convolutional stride in the stem layer. Default is 2.\n        filters_rate: Factor to adjust the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n        max_stride: Factor by which input image size is reduced through the layers. This is always `16` for all convnext architectures.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        output_stride: int,\n        arch: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n        in_channels: int = 1,\n        kernel_size: int = 3,\n        stem_patch_kernel: int = 4,\n        stem_patch_stride: int = 2,\n        filters_rate: int = 2,\n        convs_per_block: int = 2,\n        up_interpolate: bool = True,\n        max_stride: int = 32,\n        block_contraction: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.filters_rate = filters_rate\n        self.block_contraction = block_contraction\n        arch_types = {\n            \"tiny\": {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n            \"small\": {\"depths\": [3, 3, 27, 3], \"channels\": [96, 192, 384, 768]},\n            \"base\": {\"depths\": [3, 3, 27, 3], \"channels\": [128, 256, 512, 1024]},\n            \"large\": {\"depths\": [3, 3, 27, 3], \"channels\": [192, 384, 768, 1536]},\n        }\n        if model_type in arch_types:\n            self.arch = arch_types[model_type]\n        elif arch is not None:\n            self.arch = arch\n        else:\n            self.arch = arch_types[\"tiny\"]\n\n        self.max_stride = (\n            stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * final_max_pool_stride\n        self.stem_blocks = 1  # 1 stem block + 3 down blocks in convnext\n\n        self.up_blocks = np.log2(\n            self.max_stride / (stem_patch_stride * output_stride)\n        ).astype(int) + np.log2(stem_patch_stride).astype(int)\n        self.convs_per_block = convs_per_block\n        self.stem_patch_kernel = stem_patch_kernel\n        self.stem_patch_stride = stem_patch_stride\n        self.output_stride = output_stride\n        self.up_interpolate = up_interpolate\n        self.down_blocks = len(self.arch[\"channels\"]) - 1\n\n        self.enc = ConvNeXtEncoder(\n            blocks=self.arch,\n            in_channels=in_channels,\n            stem_stride=stem_patch_stride,\n            stem_kernel=stem_patch_kernel,\n        )\n\n        # Add additional pooling layer after encoder\n        self.additional_pool = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=2, padding=\"same\"\n        )\n\n        # Create middle blocks\n        self.middle_blocks = nn.ModuleList()\n        # Get the last block filters from encoder\n        last_block_filters = self.arch[\"channels\"][-1]\n\n        if convs_per_block &gt; 1:\n            # Middle expansion block\n            middle_expand = SimpleConvBlock(\n                in_channels=last_block_filters,\n                pool=False,\n                pool_before_convs=False,\n                pooling_stride=2,\n                num_convs=convs_per_block - 1,\n                filters=int(last_block_filters * filters_rate),\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=\"convnext_middle_expand\",\n            )\n            self.middle_blocks.append(middle_expand)\n\n        # Middle contraction block\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            block_filters = int(last_block_filters)\n        else:\n            # Keep the block output filters the same\n            block_filters = int(last_block_filters * filters_rate)\n\n        middle_contract = SimpleConvBlock(\n            in_channels=int(last_block_filters * filters_rate),\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=1,\n            filters=block_filters,\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_contract\",\n        )\n        self.middle_blocks.append(middle_contract)\n\n        self.current_stride = (\n            self.stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * pool\n\n        # Calculate x_in_shape based on whether we have block contraction\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            x_in_shape = int(self.arch[\"channels\"][-1])\n        else:\n            # Keep the block output filters the same\n            x_in_shape = int(self.arch[\"channels\"][-1] * filters_rate)\n\n        # Encoder channels for skip connections (reversed to match decoder order)\n        # The forward pass uses enc_output[::2][::-1] for skip features\n        encoder_channels = self.arch[\"channels\"][::-1]\n\n        self.dec = Decoder(\n            x_in_shape=x_in_shape,\n            current_stride=self.current_stride,\n            filters=self.arch[\"channels\"][0],\n            up_blocks=self.up_blocks,\n            down_blocks=self.down_blocks,\n            filters_rate=filters_rate,\n            kernel_size=self.kernel_size,\n            stem_blocks=1,\n            block_contraction=self.block_contraction,\n            output_stride=self.output_stride,\n            up_interpolate=up_interpolate,\n            encoder_channels=encoder_channels,\n        )\n\n        if len(self.dec.decoder_stack):\n            self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n        else:\n            self.final_dec_channels = block_filters\n\n        self.decoder_stride_to_filters = self.dec.stride_to_filters\n\n    @property\n    def max_channels(self):\n        \"\"\"Returns the maximum channels of the ConvNext (last layer of the encoder).\"\"\"\n        return self.dec.x_in_shape\n\n    @classmethod\n    def from_config(cls, config: OmegaConf):\n        \"\"\"Create ConvNextWrapper from a config.\"\"\"\n        return cls(\n            in_channels=config.in_channels,\n            model_type=config.model_type,\n            arch=config.arch,\n            kernel_size=config.kernel_size,\n            filters_rate=config.filters_rate,\n            convs_per_block=config.convs_per_block,\n            up_interpolate=config.up_interpolate,\n            output_stride=config.output_stride,\n            stem_patch_kernel=config.stem_patch_kernel,\n            stem_patch_stride=config.stem_patch_stride,\n            max_stride=config.max_stride,\n            block_contraction=(\n                config.block_contraction\n                if hasattr(config, \"block_contraction\")\n                else False\n            ),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the ConvNeXt architecture.\n\n        Args:\n            x: Input tensor (Batch, Channels, Height, Width).\n\n        Returns:\n            x: Outputs a dictionary with `outputs` and `strides` containing the output\n            at different strides.\n        \"\"\"\n        enc_output = self.enc(x)\n        x, features = enc_output[-1], enc_output[::2]\n        features = features[::-1]\n\n        # Apply additional pooling layer\n        x = self.additional_pool(x)\n\n        # Process through middle blocks\n        middle_output = x\n        for middle_block in self.middle_blocks:\n            middle_output = middle_block(middle_output)\n\n        x = self.dec(middle_output, features)\n        x[\"middle_output\"] = middle_output\n        return x\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.max_channels","title":"<code>max_channels</code>  <code>property</code>","text":"<p>Returns the maximum channels of the ConvNext (last layer of the encoder).</p>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.__init__","title":"<code>__init__(model_type, output_stride, arch={'depths': [3, 3, 9, 3], 'channels': [96, 192, 384, 768]}, in_channels=1, kernel_size=3, stem_patch_kernel=4, stem_patch_stride=2, filters_rate=2, convs_per_block=2, up_interpolate=True, max_stride=32, block_contraction=False)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    output_stride: int,\n    arch: dict = {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n    in_channels: int = 1,\n    kernel_size: int = 3,\n    stem_patch_kernel: int = 4,\n    stem_patch_stride: int = 2,\n    filters_rate: int = 2,\n    convs_per_block: int = 2,\n    up_interpolate: bool = True,\n    max_stride: int = 32,\n    block_contraction: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.kernel_size = kernel_size\n    self.filters_rate = filters_rate\n    self.block_contraction = block_contraction\n    arch_types = {\n        \"tiny\": {\"depths\": [3, 3, 9, 3], \"channels\": [96, 192, 384, 768]},\n        \"small\": {\"depths\": [3, 3, 27, 3], \"channels\": [96, 192, 384, 768]},\n        \"base\": {\"depths\": [3, 3, 27, 3], \"channels\": [128, 256, 512, 1024]},\n        \"large\": {\"depths\": [3, 3, 27, 3], \"channels\": [192, 384, 768, 1536]},\n    }\n    if model_type in arch_types:\n        self.arch = arch_types[model_type]\n    elif arch is not None:\n        self.arch = arch\n    else:\n        self.arch = arch_types[\"tiny\"]\n\n    self.max_stride = (\n        stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * final_max_pool_stride\n    self.stem_blocks = 1  # 1 stem block + 3 down blocks in convnext\n\n    self.up_blocks = np.log2(\n        self.max_stride / (stem_patch_stride * output_stride)\n    ).astype(int) + np.log2(stem_patch_stride).astype(int)\n    self.convs_per_block = convs_per_block\n    self.stem_patch_kernel = stem_patch_kernel\n    self.stem_patch_stride = stem_patch_stride\n    self.output_stride = output_stride\n    self.up_interpolate = up_interpolate\n    self.down_blocks = len(self.arch[\"channels\"]) - 1\n\n    self.enc = ConvNeXtEncoder(\n        blocks=self.arch,\n        in_channels=in_channels,\n        stem_stride=stem_patch_stride,\n        stem_kernel=stem_patch_kernel,\n    )\n\n    # Add additional pooling layer after encoder\n    self.additional_pool = MaxPool2dWithSamePadding(\n        kernel_size=2, stride=2, padding=\"same\"\n    )\n\n    # Create middle blocks\n    self.middle_blocks = nn.ModuleList()\n    # Get the last block filters from encoder\n    last_block_filters = self.arch[\"channels\"][-1]\n\n    if convs_per_block &gt; 1:\n        # Middle expansion block\n        middle_expand = SimpleConvBlock(\n            in_channels=last_block_filters,\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=convs_per_block - 1,\n            filters=int(last_block_filters * filters_rate),\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_expand\",\n        )\n        self.middle_blocks.append(middle_expand)\n\n    # Middle contraction block\n    if self.block_contraction:\n        # Contract the channels with an exponent lower than the last encoder block\n        block_filters = int(last_block_filters)\n    else:\n        # Keep the block output filters the same\n        block_filters = int(last_block_filters * filters_rate)\n\n    middle_contract = SimpleConvBlock(\n        in_channels=int(last_block_filters * filters_rate),\n        pool=False,\n        pool_before_convs=False,\n        pooling_stride=2,\n        num_convs=1,\n        filters=block_filters,\n        kernel_size=kernel_size,\n        use_bias=True,\n        batch_norm=False,\n        activation=\"relu\",\n        prefix=\"convnext_middle_contract\",\n    )\n    self.middle_blocks.append(middle_contract)\n\n    self.current_stride = (\n        self.stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * pool\n\n    # Calculate x_in_shape based on whether we have block contraction\n    if self.block_contraction:\n        # Contract the channels with an exponent lower than the last encoder block\n        x_in_shape = int(self.arch[\"channels\"][-1])\n    else:\n        # Keep the block output filters the same\n        x_in_shape = int(self.arch[\"channels\"][-1] * filters_rate)\n\n    # Encoder channels for skip connections (reversed to match decoder order)\n    # The forward pass uses enc_output[::2][::-1] for skip features\n    encoder_channels = self.arch[\"channels\"][::-1]\n\n    self.dec = Decoder(\n        x_in_shape=x_in_shape,\n        current_stride=self.current_stride,\n        filters=self.arch[\"channels\"][0],\n        up_blocks=self.up_blocks,\n        down_blocks=self.down_blocks,\n        filters_rate=filters_rate,\n        kernel_size=self.kernel_size,\n        stem_blocks=1,\n        block_contraction=self.block_contraction,\n        output_stride=self.output_stride,\n        up_interpolate=up_interpolate,\n        encoder_channels=encoder_channels,\n    )\n\n    if len(self.dec.decoder_stack):\n        self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n    else:\n        self.final_dec_channels = block_filters\n\n    self.decoder_stride_to_filters = self.dec.stride_to_filters\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the ConvNeXt architecture.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor (Batch, Channels, Height, Width).</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tuple[List[Tensor], List]</code> <p>Outputs a dictionary with <code>outputs</code> and <code>strides</code> containing the output at different strides.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the ConvNeXt architecture.\n\n    Args:\n        x: Input tensor (Batch, Channels, Height, Width).\n\n    Returns:\n        x: Outputs a dictionary with `outputs` and `strides` containing the output\n        at different strides.\n    \"\"\"\n    enc_output = self.enc(x)\n    x, features = enc_output[-1], enc_output[::2]\n    features = features[::-1]\n\n    # Apply additional pooling layer\n    x = self.additional_pool(x)\n\n    # Process through middle blocks\n    middle_output = x\n    for middle_block in self.middle_blocks:\n        middle_output = middle_block(middle_output)\n\n    x = self.dec(middle_output, features)\n    x[\"middle_output\"] = middle_output\n    return x\n</code></pre>"},{"location":"api/architectures/convnext/#sleap_nn.architectures.convnext.ConvNextWrapper.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create ConvNextWrapper from a config.</p> Source code in <code>sleap_nn/architectures/convnext.py</code> <pre><code>@classmethod\ndef from_config(cls, config: OmegaConf):\n    \"\"\"Create ConvNextWrapper from a config.\"\"\"\n    return cls(\n        in_channels=config.in_channels,\n        model_type=config.model_type,\n        arch=config.arch,\n        kernel_size=config.kernel_size,\n        filters_rate=config.filters_rate,\n        convs_per_block=config.convs_per_block,\n        up_interpolate=config.up_interpolate,\n        output_stride=config.output_stride,\n        stem_patch_kernel=config.stem_patch_kernel,\n        stem_patch_stride=config.stem_patch_stride,\n        max_stride=config.max_stride,\n        block_contraction=(\n            config.block_contraction\n            if hasattr(config, \"block_contraction\")\n            else False\n        ),\n    )\n</code></pre>"},{"location":"api/architectures/encoder_decoder/","title":"encoder_decoder","text":""},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder","title":"<code>sleap_nn.architectures.encoder_decoder</code>","text":"<p>Generic encoder-decoder fully convolutional backbones.</p> <p>This module contains building blocks for creating encoder-decoder architectures of general form.</p> <p>The encoder branch of the network forms the initial multi-scale feature extraction via repeated blocks of convolutions and pooling steps.</p> <p>The decoder branch is then responsible for upsampling the low resolution feature maps to achieve the target output stride.</p> <p>This pattern is generalizable and describes most fully convolutional architectures. For example:     - simple convolutions with pooling form the structure in <code>LEAP CNN &lt;https://www.nature.com/articles/s41592-018-0234-5&gt;</code>;     - adding skip connections forms <code>U-Net &lt;https://arxiv.org/pdf/1505.04597.pdf&gt;</code>;     - using residual blocks with skip connections forms the base module in <code>stacked     hourglass &lt;https://arxiv.org/pdf/1603.06937.pdf&gt;</code>;     - using dense blocks with skip connections forms <code>FC-DenseNet &lt;https://arxiv.org/pdf/1611.09326.pdf&gt;</code>.</p> <p>This module implements blocks used in all of these variants on top of a generic base classes.</p> <p>See the <code>EncoderDecoder</code> base class for requirements for creating new architectures.</p> <p>Classes:</p> Name Description <code>Decoder</code> <p>Decoder module for the UNet architecture.</p> <code>Encoder</code> <p>Encoder module for a neural network architecture.</p> <code>SimpleConvBlock</code> <p>A simple convolutional block module.</p> <code>SimpleUpsamplingBlock</code> <p>A simple upsampling and refining block module.</p> <code>StemBlock</code> <p>Stem block module for initial feature extraction.</p>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Decoder","title":"<code>Decoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Decoder module for the UNet architecture.</p> <p>This class defines the decoder part of the UNet, which consists of a stack of upsampling and refining blocks for feature reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>x_in_shape</code> <code>int</code> <p>Number of input channels for the decoder's input.</p> required <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map</p> required <code>current_stride</code> <code>int</code> <p>Current stride value to adjust during upsampling.</p> required <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 64.</p> <code>64</code> <code>up_blocks</code> <code>int</code> <p>Number of upsampling blocks. Default is 4.</p> <code>4</code> <code>down_blocks</code> <code>int</code> <p>Number of downsampling blocks. Default is 3.</p> <code>3</code> <code>stem_blocks</code> <code>int</code> <p>If &gt;0, will create additional \"down\" blocks for initial downsampling. These will be configured identically to the down blocks below.</p> <code>0</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step. The original implementation does not do this, but the CARE implementation does.</p> <code>False</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the Decoder module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class Decoder(nn.Module):\n    \"\"\"Decoder module for the UNet architecture.\n\n    This class defines the decoder part of the UNet,\n    which consists of a stack of upsampling and refining blocks for feature reconstruction.\n\n    Args:\n        x_in_shape: Number of input channels for the decoder's input.\n        output_stride: Minimum of the strides of the output heads. The input confidence map\n        tensor is expected to be at the same stride.\n        current_stride: Current stride value to adjust during upsampling.\n        filters: Number of filters for the initial block. Default is 64.\n        up_blocks: Number of upsampling blocks. Default is 4.\n        down_blocks: Number of downsampling blocks. Default is 3.\n        stem_blocks: If &gt;0, will create additional \"down\" blocks for initial\n            downsampling. These will be configured identically to the down blocks below.\n        filters_rate: Factor to adjust the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step. The original implementation does not\n            do this, but the CARE implementation does.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        x_in_shape: int,\n        output_stride: int,\n        current_stride: int,\n        filters: int = 64,\n        up_blocks: int = 4,\n        down_blocks: int = 3,\n        stem_blocks: int = 0,\n        filters_rate: int = 2,\n        convs_per_block: int = 2,\n        kernel_size: int = 3,\n        block_contraction: bool = False,\n        up_interpolate: bool = True,\n        prefix: str = \"dec\",\n        encoder_channels: Optional[List[int]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.x_in_shape = x_in_shape\n        self.current_stride = current_stride\n        self.filters = filters\n        self.up_blocks = up_blocks\n        self.down_blocks = down_blocks\n        self.stem_blocks = stem_blocks\n        self.filters_rate = filters_rate\n        self.convs_per_block = convs_per_block\n        self.kernel_size = kernel_size\n        self.block_contraction = block_contraction\n        self.prefix = prefix\n        self.stride_to_filters = {}\n        self.encoder_channels = encoder_channels\n\n        self.current_strides = []\n        self.residuals = 0\n\n        self.decoder_stack = nn.ModuleList([])\n\n        self.stride_to_filters[current_stride] = x_in_shape\n\n        for block in range(up_blocks):\n            prev_block_filters = -1 if block == 0 else block_filters_out\n            block_filters_out = int(\n                filters\n                * (filters_rate ** max(0, down_blocks + self.stem_blocks - 1 - block))\n            )\n\n            if self.block_contraction:\n                block_filters_out = int(\n                    self.filters\n                    * (\n                        self.filters_rate\n                        ** (self.down_blocks + self.stem_blocks - 2 - block)\n                    )\n                )\n\n            next_stride = current_stride // 2\n\n            # Determine skip channels for this decoder block\n            # If encoder_channels provided, use actual encoder channels\n            # Otherwise fall back to computed filters (for UNet compatibility)\n            skip_channels = None\n            if encoder_channels is not None and block &lt; len(encoder_channels):\n                skip_channels = encoder_channels[block]\n\n            if self.stem_blocks &gt; 0 and block &gt;= down_blocks + self.stem_blocks:\n                # This accounts for the case where we dont have any more down block features to concatenate with.\n                # In this case, add a simple upsampling block with a conv layer and with no concatenation\n                self.decoder_stack.append(\n                    SimpleUpsamplingBlock(\n                        x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                        current_stride=current_stride,\n                        upsampling_stride=2,\n                        interp_method=\"bilinear\",\n                        refine_convs=1,\n                        refine_convs_filters=block_filters_out,\n                        refine_convs_kernel_size=self.kernel_size,\n                        refine_convs_batch_norm=False,\n                        up_interpolate=up_interpolate,\n                        transpose_convs_filters=block_filters_out,\n                        transpose_convs_batch_norm=False,\n                        feat_concat=False,\n                        prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                        skip_channels=skip_channels,\n                    )\n                )\n            else:\n                self.decoder_stack.append(\n                    SimpleUpsamplingBlock(\n                        x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                        current_stride=current_stride,\n                        upsampling_stride=2,\n                        interp_method=\"bilinear\",\n                        refine_convs=self.convs_per_block,\n                        refine_convs_filters=block_filters_out,\n                        refine_convs_kernel_size=self.kernel_size,\n                        refine_convs_batch_norm=False,\n                        up_interpolate=up_interpolate,\n                        transpose_convs_filters=block_filters_out,\n                        transpose_convs_batch_norm=False,\n                        prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                        skip_channels=skip_channels,\n                    )\n                )\n\n            self.stride_to_filters[next_stride] = block_filters_out\n\n            self.current_strides.append(next_stride)\n            current_stride = next_stride\n            self.residuals += 1\n\n    def forward(\n        self, x: torch.Tensor, features: List[torch.Tensor]\n    ) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the Decoder module.\n\n        Args:\n            x: Input tensor for the decoder.\n            features: List of feature tensors from different encoder levels.\n\n        Returns:\n            outputs: List of output tensors after applying the decoder operations.\n            current_strides: the current strides from the decoder blocks.\n        \"\"\"\n        outputs = {\n            \"outputs\": [],\n        }\n        outputs[\"intermediate_feat\"] = x\n        for i in range(len(self.decoder_stack)):\n            if i &lt; len(features):\n                x = self.decoder_stack[i](x, features[i])\n            else:\n                x = self.decoder_stack[i](x, None)\n            outputs[\"outputs\"].append(x)\n        outputs[\"strides\"] = self.current_strides\n\n        return outputs\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Decoder.__init__","title":"<code>__init__(x_in_shape, output_stride, current_stride, filters=64, up_blocks=4, down_blocks=3, stem_blocks=0, filters_rate=2, convs_per_block=2, kernel_size=3, block_contraction=False, up_interpolate=True, prefix='dec', encoder_channels=None)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    x_in_shape: int,\n    output_stride: int,\n    current_stride: int,\n    filters: int = 64,\n    up_blocks: int = 4,\n    down_blocks: int = 3,\n    stem_blocks: int = 0,\n    filters_rate: int = 2,\n    convs_per_block: int = 2,\n    kernel_size: int = 3,\n    block_contraction: bool = False,\n    up_interpolate: bool = True,\n    prefix: str = \"dec\",\n    encoder_channels: Optional[List[int]] = None,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.x_in_shape = x_in_shape\n    self.current_stride = current_stride\n    self.filters = filters\n    self.up_blocks = up_blocks\n    self.down_blocks = down_blocks\n    self.stem_blocks = stem_blocks\n    self.filters_rate = filters_rate\n    self.convs_per_block = convs_per_block\n    self.kernel_size = kernel_size\n    self.block_contraction = block_contraction\n    self.prefix = prefix\n    self.stride_to_filters = {}\n    self.encoder_channels = encoder_channels\n\n    self.current_strides = []\n    self.residuals = 0\n\n    self.decoder_stack = nn.ModuleList([])\n\n    self.stride_to_filters[current_stride] = x_in_shape\n\n    for block in range(up_blocks):\n        prev_block_filters = -1 if block == 0 else block_filters_out\n        block_filters_out = int(\n            filters\n            * (filters_rate ** max(0, down_blocks + self.stem_blocks - 1 - block))\n        )\n\n        if self.block_contraction:\n            block_filters_out = int(\n                self.filters\n                * (\n                    self.filters_rate\n                    ** (self.down_blocks + self.stem_blocks - 2 - block)\n                )\n            )\n\n        next_stride = current_stride // 2\n\n        # Determine skip channels for this decoder block\n        # If encoder_channels provided, use actual encoder channels\n        # Otherwise fall back to computed filters (for UNet compatibility)\n        skip_channels = None\n        if encoder_channels is not None and block &lt; len(encoder_channels):\n            skip_channels = encoder_channels[block]\n\n        if self.stem_blocks &gt; 0 and block &gt;= down_blocks + self.stem_blocks:\n            # This accounts for the case where we dont have any more down block features to concatenate with.\n            # In this case, add a simple upsampling block with a conv layer and with no concatenation\n            self.decoder_stack.append(\n                SimpleUpsamplingBlock(\n                    x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                    current_stride=current_stride,\n                    upsampling_stride=2,\n                    interp_method=\"bilinear\",\n                    refine_convs=1,\n                    refine_convs_filters=block_filters_out,\n                    refine_convs_kernel_size=self.kernel_size,\n                    refine_convs_batch_norm=False,\n                    up_interpolate=up_interpolate,\n                    transpose_convs_filters=block_filters_out,\n                    transpose_convs_batch_norm=False,\n                    feat_concat=False,\n                    prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                    skip_channels=skip_channels,\n                )\n            )\n        else:\n            self.decoder_stack.append(\n                SimpleUpsamplingBlock(\n                    x_in_shape=(x_in_shape if block == 0 else prev_block_filters),\n                    current_stride=current_stride,\n                    upsampling_stride=2,\n                    interp_method=\"bilinear\",\n                    refine_convs=self.convs_per_block,\n                    refine_convs_filters=block_filters_out,\n                    refine_convs_kernel_size=self.kernel_size,\n                    refine_convs_batch_norm=False,\n                    up_interpolate=up_interpolate,\n                    transpose_convs_filters=block_filters_out,\n                    transpose_convs_batch_norm=False,\n                    prefix=f\"{self.prefix}{block}_s{current_stride}_to_s{next_stride}\",\n                    skip_channels=skip_channels,\n                )\n            )\n\n        self.stride_to_filters[next_stride] = block_filters_out\n\n        self.current_strides.append(next_stride)\n        current_stride = next_stride\n        self.residuals += 1\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Decoder.forward","title":"<code>forward(x, features)</code>","text":"<p>Forward pass through the Decoder module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor for the decoder.</p> required <code>features</code> <code>List[Tensor]</code> <p>List of feature tensors from different encoder levels.</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>Tuple[List[Tensor], List]</code> <p>List of output tensors after applying the decoder operations. current_strides: the current strides from the decoder blocks.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, features: List[torch.Tensor]\n) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the Decoder module.\n\n    Args:\n        x: Input tensor for the decoder.\n        features: List of feature tensors from different encoder levels.\n\n    Returns:\n        outputs: List of output tensors after applying the decoder operations.\n        current_strides: the current strides from the decoder blocks.\n    \"\"\"\n    outputs = {\n        \"outputs\": [],\n    }\n    outputs[\"intermediate_feat\"] = x\n    for i in range(len(self.decoder_stack)):\n        if i &lt; len(features):\n            x = self.decoder_stack[i](x, features[i])\n        else:\n            x = self.decoder_stack[i](x, None)\n        outputs[\"outputs\"].append(x)\n    outputs[\"strides\"] = self.current_strides\n\n    return outputs\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Encoder","title":"<code>Encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Encoder module for a neural network architecture.</p> <p>This class defines the encoder part of a neural network architecture, which consists of a stack of convolutional blocks for feature extraction.</p> <p>The Encoder consists of a stack of SimpleConvBlocks designed for feature extraction.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 3.</p> <code>3</code> <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 64.</p> <code>64</code> <code>down_blocks</code> <code>int</code> <p>Number of downsampling blocks. Default is 4.</p> <code>4</code> <code>filters_rate</code> <code>Union[float, int]</code> <p>Factor to increase the number of filters per block. Default is 2.</p> <code>2</code> <code>current_stride</code> <code>int</code> <p>Initial stride for pooling operations. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>Union[int, Tuple[int, int]]</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the Encoder module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class Encoder(nn.Module):\n    \"\"\"Encoder module for a neural network architecture.\n\n    This class defines the encoder part of a neural network architecture,\n    which consists of a stack of convolutional blocks for feature extraction.\n\n    The Encoder consists of a stack of SimpleConvBlocks designed for feature extraction.\n\n    Args:\n        in_channels: Number of input channels. Default is 3.\n        filters: Number of filters for the initial block. Default is 64.\n        down_blocks: Number of downsampling blocks. Default is 4.\n        filters_rate: Factor to increase the number of filters per block. Default is 2.\n        current_stride: Initial stride for pooling operations. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        filters: int = 64,\n        down_blocks: int = 4,\n        filters_rate: Union[float, int] = 2,\n        current_stride: int = 2,\n        convs_per_block: int = 2,\n        kernel_size: Union[int, Tuple[int, int]] = 3,\n        stem_blocks: int = 0,\n        prefix: str = \"enc\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.filters = filters\n        self.down_blocks = down_blocks\n        self.filters_rate = filters_rate\n        self.current_stride = current_stride\n        self.convs_per_block = convs_per_block\n        self.kernel_size = kernel_size\n        self.stem_blocks = stem_blocks\n        self.prefix = prefix\n\n        self.encoder_stack = nn.ModuleList([])\n        block_filters = int(filters * (filters_rate ** (stem_blocks - 1)))\n        for block in range(down_blocks):\n            prev_block_filters = -1 if block + self.stem_blocks == 0 else block_filters\n            block_filters = int(filters * (filters_rate ** (block + self.stem_blocks)))\n\n            self.encoder_stack.append(\n                SimpleConvBlock(\n                    in_channels=(\n                        in_channels\n                        if block + self.stem_blocks == 0\n                        else prev_block_filters\n                    ),\n                    pool=(block + self.stem_blocks &gt; 0),\n                    pool_before_convs=True,\n                    pooling_stride=2,\n                    num_convs=convs_per_block,\n                    filters=block_filters,\n                    kernel_size=self.kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"{self.prefix}{block}\",\n                    name=f\"{self.prefix}{block}\",\n                )\n            )\n        after_block_filters = block_filters\n\n        # Add final pooling layer with proper naming\n        block += 1\n        final_pool_dict = OrderedDict()\n        final_pool_dict[f\"{self.prefix}{block}_last_pool\"] = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=2, padding=\"same\"\n        )\n        self.encoder_stack.append(nn.Sequential(final_pool_dict))\n\n        self.intermediate_features = {}\n        for i, block in enumerate(self.encoder_stack):\n            if isinstance(block, SimpleConvBlock) and block.pool:\n                current_stride *= block.pooling_stride\n\n            if current_stride not in self.intermediate_features.values():\n                self.intermediate_features[i] = current_stride\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the Encoder module.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the encoder operations.\n            list: List of intermediate feature tensors from different levels of the encoder.\n        \"\"\"\n        features = []\n\n        for i in range(len(self.encoder_stack)):\n            x = self.encoder_stack[i](x)\n\n            if i in self.intermediate_features.keys():\n                features.append(x)\n\n        return x, features[::-1]\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Encoder.__init__","title":"<code>__init__(in_channels=3, filters=64, down_blocks=4, filters_rate=2, current_stride=2, convs_per_block=2, kernel_size=3, stem_blocks=0, prefix='enc')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int = 3,\n    filters: int = 64,\n    down_blocks: int = 4,\n    filters_rate: Union[float, int] = 2,\n    current_stride: int = 2,\n    convs_per_block: int = 2,\n    kernel_size: Union[int, Tuple[int, int]] = 3,\n    stem_blocks: int = 0,\n    prefix: str = \"enc\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filters = filters\n    self.down_blocks = down_blocks\n    self.filters_rate = filters_rate\n    self.current_stride = current_stride\n    self.convs_per_block = convs_per_block\n    self.kernel_size = kernel_size\n    self.stem_blocks = stem_blocks\n    self.prefix = prefix\n\n    self.encoder_stack = nn.ModuleList([])\n    block_filters = int(filters * (filters_rate ** (stem_blocks - 1)))\n    for block in range(down_blocks):\n        prev_block_filters = -1 if block + self.stem_blocks == 0 else block_filters\n        block_filters = int(filters * (filters_rate ** (block + self.stem_blocks)))\n\n        self.encoder_stack.append(\n            SimpleConvBlock(\n                in_channels=(\n                    in_channels\n                    if block + self.stem_blocks == 0\n                    else prev_block_filters\n                ),\n                pool=(block + self.stem_blocks &gt; 0),\n                pool_before_convs=True,\n                pooling_stride=2,\n                num_convs=convs_per_block,\n                filters=block_filters,\n                kernel_size=self.kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=f\"{self.prefix}{block}\",\n                name=f\"{self.prefix}{block}\",\n            )\n        )\n    after_block_filters = block_filters\n\n    # Add final pooling layer with proper naming\n    block += 1\n    final_pool_dict = OrderedDict()\n    final_pool_dict[f\"{self.prefix}{block}_last_pool\"] = MaxPool2dWithSamePadding(\n        kernel_size=2, stride=2, padding=\"same\"\n    )\n    self.encoder_stack.append(nn.Sequential(final_pool_dict))\n\n    self.intermediate_features = {}\n    for i, block in enumerate(self.encoder_stack):\n        if isinstance(block, SimpleConvBlock) and block.pool:\n            current_stride *= block.pooling_stride\n\n        if current_stride not in self.intermediate_features.values():\n            self.intermediate_features[i] = current_stride\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.Encoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the Encoder module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the encoder operations. list: List of intermediate feature tensors from different levels of the encoder.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the Encoder module.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the encoder operations.\n        list: List of intermediate feature tensors from different levels of the encoder.\n    \"\"\"\n    features = []\n\n    for i in range(len(self.encoder_stack)):\n        x = self.encoder_stack[i](x)\n\n        if i in self.intermediate_features.keys():\n            features.append(x)\n\n    return x, features[::-1]\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleConvBlock","title":"<code>SimpleConvBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple convolutional block module.</p> <p>This class defines a convolutional block that consists of convolutional layers, optional pooling layers, batch normalization, and activation functions.</p> <p>The layers within the SimpleConvBlock are organized as follows:</p> <ol> <li>Optional max pooling (with same padding) layer (before convolutional layers).</li> <li>Convolutional layers with specified number of filters, kernel size, and activation.</li> <li>Optional batch normalization layer after each convolutional layer (if batch_norm is True).</li> <li>Activation function after each convolutional layer (ReLU, Sigmoid, Tanh, etc.).</li> <li>Optional max pooling (with same padding) layer (after convolutional layers).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>pool</code> <code>bool</code> <p>Whether to include pooling layers. Default is True.</p> <code>True</code> <code>pooling_stride</code> <code>int</code> <p>Stride for pooling layers. Default is 2.</p> <code>2</code> <code>pool_before_convs</code> <code>bool</code> <p>Whether to apply pooling before convolutional layers. Default is False.</p> <code>False</code> <code>num_convs</code> <code>int</code> <p>Number of convolutional layers. Default is 2.</p> <code>2</code> <code>filters</code> <code>int</code> <p>Number of filters for convolutional layers. Default is 32.</p> <code>32</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias in convolutional layers. Default is True.</p> <code>True</code> <code>batch_norm</code> <code>bool</code> <p>Whether to apply batch normalization. Default is False.</p> <code>False</code> <code>activation</code> <code>Text</code> <p>Activation function name. Default is \"relu\".</p> <code>'relu'</code> Note <p>The 'same' padding is applied using custom MaxPool2dWithSamePadding layers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SimpleConvBlock module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class SimpleConvBlock(nn.Module):\n    \"\"\"A simple convolutional block module.\n\n    This class defines a convolutional block that consists of convolutional layers,\n    optional pooling layers, batch normalization, and activation functions.\n\n    The layers within the SimpleConvBlock are organized as follows:\n\n    1. Optional max pooling (with same padding) layer (before convolutional layers).\n    2. Convolutional layers with specified number of filters, kernel size, and activation.\n    3. Optional batch normalization layer after each convolutional layer (if batch_norm is True).\n    4. Activation function after each convolutional layer (ReLU, Sigmoid, Tanh, etc.).\n    5. Optional max pooling (with same padding) layer (after convolutional layers).\n\n    Args:\n        in_channels: Number of input channels.\n        pool: Whether to include pooling layers. Default is True.\n        pooling_stride: Stride for pooling layers. Default is 2.\n        pool_before_convs: Whether to apply pooling before convolutional layers. Default is False.\n        num_convs: Number of convolutional layers. Default is 2.\n        filters: Number of filters for convolutional layers. Default is 32.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        use_bias: Whether to use bias in convolutional layers. Default is True.\n        batch_norm: Whether to apply batch normalization. Default is False.\n        activation: Activation function name. Default is \"relu\".\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n\n    Note:\n        The 'same' padding is applied using custom MaxPool2dWithSamePadding layers.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        pool: bool = True,\n        pooling_stride: int = 2,\n        pool_before_convs: bool = False,\n        num_convs: int = 2,\n        filters: int = 32,\n        kernel_size: int = 3,\n        use_bias: bool = True,\n        batch_norm: bool = False,\n        activation: Text = \"relu\",\n        prefix: Text = \"\",\n        name: Text = \"\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.pool = pool\n        self.pooling_stride = pooling_stride\n        self.pool_before_convs = pool_before_convs\n        self.num_convs = num_convs\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.use_bias = use_bias\n        self.batch_norm = batch_norm\n        self.activation = activation\n        self.prefix = prefix\n        self.name = name\n\n        self.blocks = OrderedDict()\n        if pool and pool_before_convs:\n            self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n                kernel_size=2, stride=pooling_stride, padding=\"same\"\n            )\n\n        for i in range(num_convs):\n            self.blocks[f\"{prefix}_conv{i}\"] = nn.Conv2d(\n                in_channels=in_channels if i == 0 else filters,\n                out_channels=filters,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=\"same\",\n                bias=use_bias,\n            )\n\n            if batch_norm:\n                self.blocks[f\"{prefix}_bn{i}\"] = nn.BatchNorm2d(filters)\n\n            self.blocks[f\"{prefix}_act{i}_{activation}\"] = get_act_fn(activation)\n\n        if pool and not pool_before_convs:\n            self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n                kernel_size=2, stride=pooling_stride, padding=\"same\"\n            )\n\n        self.blocks = nn.Sequential(self.blocks)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the SimpleConvBlock module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the convolutional block operations.\n        \"\"\"\n        for block in self.blocks:\n            x = block(x)\n        return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleConvBlock.__init__","title":"<code>__init__(in_channels, pool=True, pooling_stride=2, pool_before_convs=False, num_convs=2, filters=32, kernel_size=3, use_bias=True, batch_norm=False, activation='relu', prefix='', name='')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    pool: bool = True,\n    pooling_stride: int = 2,\n    pool_before_convs: bool = False,\n    num_convs: int = 2,\n    filters: int = 32,\n    kernel_size: int = 3,\n    use_bias: bool = True,\n    batch_norm: bool = False,\n    activation: Text = \"relu\",\n    prefix: Text = \"\",\n    name: Text = \"\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.pool = pool\n    self.pooling_stride = pooling_stride\n    self.pool_before_convs = pool_before_convs\n    self.num_convs = num_convs\n    self.filters = filters\n    self.kernel_size = kernel_size\n    self.use_bias = use_bias\n    self.batch_norm = batch_norm\n    self.activation = activation\n    self.prefix = prefix\n    self.name = name\n\n    self.blocks = OrderedDict()\n    if pool and pool_before_convs:\n        self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=pooling_stride, padding=\"same\"\n        )\n\n    for i in range(num_convs):\n        self.blocks[f\"{prefix}_conv{i}\"] = nn.Conv2d(\n            in_channels=in_channels if i == 0 else filters,\n            out_channels=filters,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=\"same\",\n            bias=use_bias,\n        )\n\n        if batch_norm:\n            self.blocks[f\"{prefix}_bn{i}\"] = nn.BatchNorm2d(filters)\n\n        self.blocks[f\"{prefix}_act{i}_{activation}\"] = get_act_fn(activation)\n\n    if pool and not pool_before_convs:\n        self.blocks[f\"{prefix}_pool\"] = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=pooling_stride, padding=\"same\"\n        )\n\n    self.blocks = nn.Sequential(self.blocks)\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleConvBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the SimpleConvBlock module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the convolutional block operations.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the SimpleConvBlock module.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the convolutional block operations.\n    \"\"\"\n    for block in self.blocks:\n        x = block(x)\n    return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleUpsamplingBlock","title":"<code>SimpleUpsamplingBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple upsampling and refining block module.</p> <p>This class defines an upsampling and refining block that consists of upsampling layers, convolutional layers for refinement, batch normalization, and activation functions.</p> <p>The block includes: 1. Upsampling layers with adjustable stride and interpolation method. 2. Refinement convolutional layers with customizable parameters. 3. BatchNormalization layers (if specified; can be before or after activation function). 4. Activation functions (default is ReLU) applied before or after BatchNormalization.</p> <p>Parameters:</p> Name Type Description Default <code>x_in_shape</code> <code>int</code> <p>Number of input channels for the feature map.</p> required <code>current_stride</code> <code>int</code> <p>Current stride value to adjust during upsampling.</p> required <code>upsampling_stride</code> <code>int</code> <p>Stride for upsampling. Default is 2.</p> <code>2</code> <code>interp_method</code> <code>Text</code> <p>Interpolation method for upsampling. Default is \"bilinear\".</p> <code>'bilinear'</code> <code>refine_convs</code> <code>int</code> <p>Number of convolutional layers for refinement. Default is 2.</p> <code>2</code> <code>refine_convs_filters</code> <code>int</code> <p>Number of filters for refinement convolutional layers. Default is 64.</p> <code>64</code> <code>refine_convs_kernel_size</code> <code>int</code> <p>Size of the refinement convolutional kernels. Default is 3.</p> <code>3</code> <code>refine_convs_use_bias</code> <code>bool</code> <p>Whether to use bias in refinement convolutional layers. Default is True.</p> <code>True</code> <code>refine_convs_batch_norm</code> <code>bool</code> <p>Whether to apply batch normalization. Default is True.</p> <code>False</code> <code>refine_convs_batch_norm_before_activation</code> <code>bool</code> <p>Whether to apply batch normalization before activation.</p> <code>True</code> <code>refine_convs_activation</code> <code>Text</code> <p>Activation function name. Default is \"relu\".</p> <code>'relu'</code> <code>transpose_convs_filters</code> <code>int</code> <p>Number of filters for Transpose convolutional layers. Default is 64.</p> <code>64</code> <code>transpose_convs_use_bias</code> <code>bool</code> <p>Whether to use bias in Transpose convolutional layers. Default is True.</p> <code>True</code> <code>transpose_convs_batch_norm</code> <code>bool</code> <p>Whether to apply batch normalization for Transpose Conv layers. Default is True.</p> <code>True</code> <code>transpose_convs_batch_norm_before_activation</code> <code>bool</code> <p>Whether to apply batch normalization before activation.</p> <code>True</code> <code>transpose_convs_activation</code> <code>Text</code> <p>Activation function name for Transpose Conv layers. Default is \"relu\".</p> <code>'relu'</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SimpleUpsamplingBlock module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class SimpleUpsamplingBlock(nn.Module):\n    \"\"\"A simple upsampling and refining block module.\n\n    This class defines an upsampling and refining block that consists of upsampling layers,\n    convolutional layers for refinement, batch normalization, and activation functions.\n\n    The block includes:\n    1. Upsampling layers with adjustable stride and interpolation method.\n    2. Refinement convolutional layers with customizable parameters.\n    3. BatchNormalization layers (if specified; can be before or after activation function).\n    4. Activation functions (default is ReLU) applied before or after BatchNormalization.\n\n    Args:\n        x_in_shape: Number of input channels for the feature map.\n        current_stride: Current stride value to adjust during upsampling.\n        upsampling_stride: Stride for upsampling. Default is 2.\n        interp_method: Interpolation method for upsampling. Default is \"bilinear\".\n        refine_convs: Number of convolutional layers for refinement. Default is 2.\n        refine_convs_filters: Number of filters for refinement convolutional layers. Default is 64.\n        refine_convs_kernel_size: Size of the refinement convolutional kernels. Default is 3.\n        refine_convs_use_bias: Whether to use bias in refinement convolutional layers. Default is True.\n        refine_convs_batch_norm: Whether to apply batch normalization. Default is True.\n        refine_convs_batch_norm_before_activation: Whether to apply batch normalization before activation.\n        refine_convs_activation: Activation function name. Default is \"relu\".\n        transpose_convs_filters: Number of filters for Transpose convolutional layers. Default is 64.\n        transpose_convs_use_bias: Whether to use bias in Transpose convolutional layers. Default is True.\n        transpose_convs_batch_norm: Whether to apply batch normalization for Transpose Conv layers. Default is True.\n        transpose_convs_batch_norm_before_activation: Whether to apply batch normalization before activation.\n        transpose_convs_activation: Activation function name for Transpose Conv layers. Default is \"relu\".\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        x_in_shape: int,\n        current_stride: int,\n        upsampling_stride: int = 2,\n        up_interpolate: bool = True,\n        interp_method: Text = \"bilinear\",\n        refine_convs: int = 2,\n        refine_convs_filters: int = 64,\n        refine_convs_kernel_size: int = 3,\n        refine_convs_use_bias: bool = True,\n        refine_convs_batch_norm: bool = False,\n        refine_convs_batch_norm_before_activation: bool = True,\n        refine_convs_activation: Text = \"relu\",\n        transpose_convs_filters: int = 64,\n        transpose_convs_kernel_size: int = 3,\n        transpose_convs_use_bias: bool = True,\n        transpose_convs_batch_norm: bool = True,\n        transpose_convs_batch_norm_before_activation: bool = True,\n        transpose_convs_activation: Text = \"relu\",\n        feat_concat: bool = True,\n        prefix: Text = \"\",\n        skip_channels: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        # Determine skip connection channels\n        # If skip_channels is provided, use it; otherwise fall back to refine_convs_filters\n        # This allows ConvNext/SwinT to specify actual encoder channels\n        self.skip_channels = (\n            skip_channels if skip_channels is not None else refine_convs_filters\n        )\n\n        self.x_in_shape = x_in_shape\n        self.current_stride = current_stride\n        self.upsampling_stride = upsampling_stride\n        self.interp_method = interp_method\n        self.refine_convs = refine_convs\n        self.refine_convs_filters = refine_convs_filters\n        self.refine_convs_kernel_size = refine_convs_kernel_size\n        self.refine_convs_use_bias = refine_convs_use_bias\n        self.refine_convs_batch_norm = refine_convs_batch_norm\n        self.refine_convs_batch_norm_before_activation = (\n            refine_convs_batch_norm_before_activation\n        )\n        self.refine_convs_activation = refine_convs_activation\n        self.up_interpolate = up_interpolate\n        self.feat_concat = feat_concat\n        self.prefix = prefix\n\n        self.blocks = OrderedDict()\n        if current_stride is not None:\n            # Append the strides to the block prefix.\n            new_stride = current_stride // upsampling_stride\n\n        # Upsample via interpolation.\n        if self.up_interpolate:\n            self.blocks[f\"{prefix}_interp_{interp_method}\"] = nn.Upsample(\n                scale_factor=upsampling_stride,\n                mode=interp_method,\n                align_corners=False,\n            )\n        else:\n            # Upsample via strided transposed convolution.\n            # The transpose conv should output the target number of filters\n            self.blocks[f\"{prefix}_trans_conv\"] = nn.ConvTranspose2d(\n                in_channels=x_in_shape,  # Input channels from the input tensor\n                out_channels=transpose_convs_filters,  # Output channels for the upsampled tensor\n                kernel_size=transpose_convs_kernel_size,\n                stride=upsampling_stride,\n                output_padding=1,\n                padding=1,\n                bias=transpose_convs_use_bias,\n            )\n            self.norm_act_layers = 1\n            if (\n                transpose_convs_batch_norm\n                and transpose_convs_batch_norm_before_activation\n            ):\n                self.blocks[f\"{prefix}_trans_conv_bn\"] = nn.BatchNorm2d(\n                    num_features=transpose_convs_filters\n                )\n                self.norm_act_layers += 1\n\n            self.blocks[f\"{prefix}_trans_conv_act_{transpose_convs_activation}\"] = (\n                get_act_fn(transpose_convs_activation)\n            )\n            self.norm_act_layers += 1\n\n            if (\n                transpose_convs_batch_norm\n                and not transpose_convs_batch_norm_before_activation\n            ):\n                self.blocks[f\"{prefix}_trans_conv_bn_after\"] = nn.BatchNorm2d(\n                    num_features=transpose_convs_filters\n                )\n                self.norm_act_layers += 1\n\n        # Add further convolutions to refine after upsampling and/or skip.\n        for i in range(refine_convs):\n            filters = refine_convs_filters\n            # For the first conv, calculate the actual input channels after concatenation\n            if i == 0:\n                if not self.feat_concat:\n                    first_conv_in_channels = refine_convs_filters\n                else:\n                    if self.up_interpolate:\n                        # With interpolation, input is x_in_shape + skip_channels\n                        # skip_channels may differ from refine_convs_filters for ConvNext/SwinT\n                        first_conv_in_channels = x_in_shape + self.skip_channels\n                    else:\n                        # With transpose conv, input is transpose_conv_output + skip_channels\n                        first_conv_in_channels = (\n                            self.skip_channels + transpose_convs_filters\n                        )\n            else:\n                if not self.feat_concat:\n                    first_conv_in_channels = refine_convs_filters\n                first_conv_in_channels = filters\n\n            self.blocks[f\"{prefix}_refine_conv{i}\"] = nn.Conv2d(\n                in_channels=int(first_conv_in_channels),\n                out_channels=int(filters),\n                kernel_size=refine_convs_kernel_size,\n                stride=1,\n                padding=\"same\",\n                bias=refine_convs_use_bias,\n            )\n\n            if refine_convs_batch_norm and refine_convs_batch_norm_before_activation:\n                self.blocks[f\"{prefix}_refine_conv{i}_bn\"] = nn.BatchNorm2d(\n                    num_features=refine_convs_filters\n                )\n\n            self.blocks[f\"{prefix}_refine_conv{i}_act_{refine_convs_activation}\"] = (\n                get_act_fn(refine_convs_activation)\n            )\n\n            if (\n                refine_convs_batch_norm\n                and not refine_convs_batch_norm_before_activation\n            ):\n                self.blocks[f\"{prefix}_refine_conv_bn_after{i}\"] = nn.BatchNorm2d(\n                    num_features=refine_convs_filters\n                )\n\n        self.blocks = nn.Sequential(self.blocks)\n\n    def forward(self, x: torch.Tensor, feature: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the SimpleUpsamplingBlock module.\n\n        Args:\n            x: Input tensor.\n            feature: Feature tensor to be concatenated with the upsampled tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the upsampling and refining operations.\n        \"\"\"\n        for idx, b in enumerate(self.blocks):\n            if (\n                not self.up_interpolate\n                and idx == self.norm_act_layers\n                and feature is not None\n            ):\n                x = torch.concat((feature, x), dim=1)\n            elif (\n                self.up_interpolate and idx == 1 and feature is not None\n            ):  # Right after upsampling or convtranspose2d.\n                x = torch.concat((feature, x), dim=1)\n            x = b(x)\n        return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleUpsamplingBlock.__init__","title":"<code>__init__(x_in_shape, current_stride, upsampling_stride=2, up_interpolate=True, interp_method='bilinear', refine_convs=2, refine_convs_filters=64, refine_convs_kernel_size=3, refine_convs_use_bias=True, refine_convs_batch_norm=False, refine_convs_batch_norm_before_activation=True, refine_convs_activation='relu', transpose_convs_filters=64, transpose_convs_kernel_size=3, transpose_convs_use_bias=True, transpose_convs_batch_norm=True, transpose_convs_batch_norm_before_activation=True, transpose_convs_activation='relu', feat_concat=True, prefix='', skip_channels=None)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    x_in_shape: int,\n    current_stride: int,\n    upsampling_stride: int = 2,\n    up_interpolate: bool = True,\n    interp_method: Text = \"bilinear\",\n    refine_convs: int = 2,\n    refine_convs_filters: int = 64,\n    refine_convs_kernel_size: int = 3,\n    refine_convs_use_bias: bool = True,\n    refine_convs_batch_norm: bool = False,\n    refine_convs_batch_norm_before_activation: bool = True,\n    refine_convs_activation: Text = \"relu\",\n    transpose_convs_filters: int = 64,\n    transpose_convs_kernel_size: int = 3,\n    transpose_convs_use_bias: bool = True,\n    transpose_convs_batch_norm: bool = True,\n    transpose_convs_batch_norm_before_activation: bool = True,\n    transpose_convs_activation: Text = \"relu\",\n    feat_concat: bool = True,\n    prefix: Text = \"\",\n    skip_channels: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    # Determine skip connection channels\n    # If skip_channels is provided, use it; otherwise fall back to refine_convs_filters\n    # This allows ConvNext/SwinT to specify actual encoder channels\n    self.skip_channels = (\n        skip_channels if skip_channels is not None else refine_convs_filters\n    )\n\n    self.x_in_shape = x_in_shape\n    self.current_stride = current_stride\n    self.upsampling_stride = upsampling_stride\n    self.interp_method = interp_method\n    self.refine_convs = refine_convs\n    self.refine_convs_filters = refine_convs_filters\n    self.refine_convs_kernel_size = refine_convs_kernel_size\n    self.refine_convs_use_bias = refine_convs_use_bias\n    self.refine_convs_batch_norm = refine_convs_batch_norm\n    self.refine_convs_batch_norm_before_activation = (\n        refine_convs_batch_norm_before_activation\n    )\n    self.refine_convs_activation = refine_convs_activation\n    self.up_interpolate = up_interpolate\n    self.feat_concat = feat_concat\n    self.prefix = prefix\n\n    self.blocks = OrderedDict()\n    if current_stride is not None:\n        # Append the strides to the block prefix.\n        new_stride = current_stride // upsampling_stride\n\n    # Upsample via interpolation.\n    if self.up_interpolate:\n        self.blocks[f\"{prefix}_interp_{interp_method}\"] = nn.Upsample(\n            scale_factor=upsampling_stride,\n            mode=interp_method,\n            align_corners=False,\n        )\n    else:\n        # Upsample via strided transposed convolution.\n        # The transpose conv should output the target number of filters\n        self.blocks[f\"{prefix}_trans_conv\"] = nn.ConvTranspose2d(\n            in_channels=x_in_shape,  # Input channels from the input tensor\n            out_channels=transpose_convs_filters,  # Output channels for the upsampled tensor\n            kernel_size=transpose_convs_kernel_size,\n            stride=upsampling_stride,\n            output_padding=1,\n            padding=1,\n            bias=transpose_convs_use_bias,\n        )\n        self.norm_act_layers = 1\n        if (\n            transpose_convs_batch_norm\n            and transpose_convs_batch_norm_before_activation\n        ):\n            self.blocks[f\"{prefix}_trans_conv_bn\"] = nn.BatchNorm2d(\n                num_features=transpose_convs_filters\n            )\n            self.norm_act_layers += 1\n\n        self.blocks[f\"{prefix}_trans_conv_act_{transpose_convs_activation}\"] = (\n            get_act_fn(transpose_convs_activation)\n        )\n        self.norm_act_layers += 1\n\n        if (\n            transpose_convs_batch_norm\n            and not transpose_convs_batch_norm_before_activation\n        ):\n            self.blocks[f\"{prefix}_trans_conv_bn_after\"] = nn.BatchNorm2d(\n                num_features=transpose_convs_filters\n            )\n            self.norm_act_layers += 1\n\n    # Add further convolutions to refine after upsampling and/or skip.\n    for i in range(refine_convs):\n        filters = refine_convs_filters\n        # For the first conv, calculate the actual input channels after concatenation\n        if i == 0:\n            if not self.feat_concat:\n                first_conv_in_channels = refine_convs_filters\n            else:\n                if self.up_interpolate:\n                    # With interpolation, input is x_in_shape + skip_channels\n                    # skip_channels may differ from refine_convs_filters for ConvNext/SwinT\n                    first_conv_in_channels = x_in_shape + self.skip_channels\n                else:\n                    # With transpose conv, input is transpose_conv_output + skip_channels\n                    first_conv_in_channels = (\n                        self.skip_channels + transpose_convs_filters\n                    )\n        else:\n            if not self.feat_concat:\n                first_conv_in_channels = refine_convs_filters\n            first_conv_in_channels = filters\n\n        self.blocks[f\"{prefix}_refine_conv{i}\"] = nn.Conv2d(\n            in_channels=int(first_conv_in_channels),\n            out_channels=int(filters),\n            kernel_size=refine_convs_kernel_size,\n            stride=1,\n            padding=\"same\",\n            bias=refine_convs_use_bias,\n        )\n\n        if refine_convs_batch_norm and refine_convs_batch_norm_before_activation:\n            self.blocks[f\"{prefix}_refine_conv{i}_bn\"] = nn.BatchNorm2d(\n                num_features=refine_convs_filters\n            )\n\n        self.blocks[f\"{prefix}_refine_conv{i}_act_{refine_convs_activation}\"] = (\n            get_act_fn(refine_convs_activation)\n        )\n\n        if (\n            refine_convs_batch_norm\n            and not refine_convs_batch_norm_before_activation\n        ):\n            self.blocks[f\"{prefix}_refine_conv_bn_after{i}\"] = nn.BatchNorm2d(\n                num_features=refine_convs_filters\n            )\n\n    self.blocks = nn.Sequential(self.blocks)\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.SimpleUpsamplingBlock.forward","title":"<code>forward(x, feature)</code>","text":"<p>Forward pass through the SimpleUpsamplingBlock module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>feature</code> <code>Tensor</code> <p>Feature tensor to be concatenated with the upsampled tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the upsampling and refining operations.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor, feature: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the SimpleUpsamplingBlock module.\n\n    Args:\n        x: Input tensor.\n        feature: Feature tensor to be concatenated with the upsampled tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the upsampling and refining operations.\n    \"\"\"\n    for idx, b in enumerate(self.blocks):\n        if (\n            not self.up_interpolate\n            and idx == self.norm_act_layers\n            and feature is not None\n        ):\n            x = torch.concat((feature, x), dim=1)\n        elif (\n            self.up_interpolate and idx == 1 and feature is not None\n        ):  # Right after upsampling or convtranspose2d.\n            x = torch.concat((feature, x), dim=1)\n        x = b(x)\n    return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.StemBlock","title":"<code>StemBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Stem block module for initial feature extraction.</p> <p>This class defines a stem block that consists of a stack of convolutional blocks for initial feature extraction before the main encoder. The stem blocks are typically used for initial downsampling and feature extraction.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 3.</p> <code>3</code> <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 64.</p> <code>64</code> <code>stem_blocks</code> <code>int</code> <p>Number of stem blocks. Default is 0.</p> <code>0</code> <code>filters_rate</code> <code>Union[float, int]</code> <p>Factor to increase the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 7.</p> <code>7</code> <code>prefix</code> <code>str</code> <p>Prefix for layer naming. Default is \"stem\".</p> <code>'stem'</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the StemBlock module.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>class StemBlock(nn.Module):\n    \"\"\"Stem block module for initial feature extraction.\n\n    This class defines a stem block that consists of a stack of convolutional blocks\n    for initial feature extraction before the main encoder. The stem blocks are typically\n    used for initial downsampling and feature extraction.\n\n    Args:\n        in_channels: Number of input channels. Default is 3.\n        filters: Number of filters for the initial block. Default is 64.\n        stem_blocks: Number of stem blocks. Default is 0.\n        filters_rate: Factor to increase the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 7.\n        prefix: Prefix for layer naming. Default is \"stem\".\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        filters: int = 64,\n        stem_blocks: int = 0,\n        filters_rate: Union[float, int] = 2,\n        convs_per_block: int = 2,\n        kernel_size: int = 7,\n        prefix: str = \"stem\",\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.filters = filters\n        self.stem_blocks = stem_blocks\n        self.filters_rate = filters_rate\n        self.convs_per_block = convs_per_block\n        self.kernel_size = kernel_size\n        self.prefix = prefix\n\n        self.stem_stack = nn.ModuleList([])\n\n        for block in range(self.stem_blocks):\n            prev_block_filters = in_channels if block == 0 else block_filters\n            block_filters = int(self.filters * (self.filters_rate**block))\n\n            self.stem_stack.append(\n                SimpleConvBlock(\n                    in_channels=prev_block_filters,\n                    pool=(block &gt; 0),\n                    pool_before_convs=True,\n                    pooling_stride=2,\n                    num_convs=convs_per_block,\n                    filters=block_filters,\n                    kernel_size=kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"{prefix}{block}\",\n                )\n            )\n\n        # Always finish with a pooling block to account for pooling before convs.\n        final_pool_dict = OrderedDict()\n        final_pool_dict[f\"{self.prefix}{block + 1}_last_pool\"] = (\n            MaxPool2dWithSamePadding(kernel_size=2, stride=2, padding=\"same\")\n        )\n        self.stem_stack.append(nn.Sequential(final_pool_dict))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the StemBlock module.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after applying the stem operations.\n        \"\"\"\n        for block in self.stem_stack:\n            x = block(x)\n        return x\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.StemBlock.__init__","title":"<code>__init__(in_channels=3, filters=64, stem_blocks=0, filters_rate=2, convs_per_block=2, kernel_size=7, prefix='stem')</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int = 3,\n    filters: int = 64,\n    stem_blocks: int = 0,\n    filters_rate: Union[float, int] = 2,\n    convs_per_block: int = 2,\n    kernel_size: int = 7,\n    prefix: str = \"stem\",\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.filters = filters\n    self.stem_blocks = stem_blocks\n    self.filters_rate = filters_rate\n    self.convs_per_block = convs_per_block\n    self.kernel_size = kernel_size\n    self.prefix = prefix\n\n    self.stem_stack = nn.ModuleList([])\n\n    for block in range(self.stem_blocks):\n        prev_block_filters = in_channels if block == 0 else block_filters\n        block_filters = int(self.filters * (self.filters_rate**block))\n\n        self.stem_stack.append(\n            SimpleConvBlock(\n                in_channels=prev_block_filters,\n                pool=(block &gt; 0),\n                pool_before_convs=True,\n                pooling_stride=2,\n                num_convs=convs_per_block,\n                filters=block_filters,\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=f\"{prefix}{block}\",\n            )\n        )\n\n    # Always finish with a pooling block to account for pooling before convs.\n    final_pool_dict = OrderedDict()\n    final_pool_dict[f\"{self.prefix}{block + 1}_last_pool\"] = (\n        MaxPool2dWithSamePadding(kernel_size=2, stride=2, padding=\"same\")\n    )\n    self.stem_stack.append(nn.Sequential(final_pool_dict))\n</code></pre>"},{"location":"api/architectures/encoder_decoder/#sleap_nn.architectures.encoder_decoder.StemBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the StemBlock module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after applying the stem operations.</p> Source code in <code>sleap_nn/architectures/encoder_decoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the StemBlock module.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor after applying the stem operations.\n    \"\"\"\n    for block in self.stem_stack:\n        x = block(x)\n    return x\n</code></pre>"},{"location":"api/architectures/heads/","title":"heads","text":""},{"location":"api/architectures/heads/#sleap_nn.architectures.heads","title":"<code>sleap_nn.architectures.heads</code>","text":"<p>Model head definitions for defining model output types.</p> <p>Classes:</p> Name Description <code>CenteredInstanceConfmapsHead</code> <p>Head for specifying centered instance confidence maps.</p> <code>CentroidConfmapsHead</code> <p>Head for specifying instance centroid confidence maps.</p> <code>ClassMapsHead</code> <p>Head for specifying class identity maps.</p> <code>ClassVectorsHead</code> <p>Head for specifying classification heads.</p> <code>Head</code> <p>Base class for model output heads.</p> <code>MultiInstanceConfmapsHead</code> <p>Head for specifying multi-instance confidence maps.</p> <code>OffsetRefinementHead</code> <p>Head for specifying offset refinement maps.</p> <code>PartAffinityFieldsHead</code> <p>Head for specifying multi-instance part affinity fields.</p> <code>SingleInstanceConfmapsHead</code> <p>Head for specifying single instance confidence maps.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead","title":"<code>CenteredInstanceConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying centered instance confidence maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>anchor_part</code> <p>Name of the part to use as an anchor node. If not specified, the bounding box centroid will be used.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class CenteredInstanceConfmapsHead(Head):\n    \"\"\"Head for specifying centered instance confidence maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        anchor_part: Name of the part to use as an anchor node. If not specified, the\n            bounding box centroid will be used.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        anchor_part: Optional[Text] = None,\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.anchor_part = anchor_part\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.part_names)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n    ) -&gt; \"CenteredInstanceConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.part_names is not None:\n            part_names = config.part_names\n        elif part_names is None:\n            message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            anchor_part=config.anchor_part,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead.__init__","title":"<code>__init__(part_names, anchor_part=None, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    anchor_part: Optional[Text] = None,\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.anchor_part = anchor_part\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CenteredInstanceConfmapsHead.from_config","title":"<code>from_config(config, part_names=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>CenteredInstanceConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n) -&gt; \"CenteredInstanceConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.part_names is not None:\n        part_names = config.part_names\n    elif part_names is None:\n        message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        anchor_part=config.anchor_part,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead","title":"<code>CentroidConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying instance centroid confidence maps.</p> <p>Attributes:</p> Name Type Description <code>anchor_part</code> <p>Name of the part to use as an anchor node. If not specified, the bounding box centroid will be used.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class CentroidConfmapsHead(Head):\n    \"\"\"Head for specifying instance centroid confidence maps.\n\n    Attributes:\n        anchor_part: Name of the part to use as an anchor node. If not specified, the\n            bounding box centroid will be used.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        anchor_part: Optional[Text] = None,\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.anchor_part = anchor_part\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return 1\n\n    @classmethod\n    def from_config(cls, config: DictConfig) -&gt; \"CentroidConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        return cls(\n            anchor_part=config.anchor_part,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead.__init__","title":"<code>__init__(anchor_part=None, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    anchor_part: Optional[Text] = None,\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.anchor_part = anchor_part\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.CentroidConfmapsHead.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <p>Returns:</p> Type Description <code>CentroidConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(cls, config: DictConfig) -&gt; \"CentroidConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    return cls(\n        anchor_part=config.anchor_part,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead","title":"<code>ClassMapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying class identity maps.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <p>List of string names of the classes.</p> <code>sigma</code> <p>Spread of the class maps around each node.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class ClassMapsHead(Head):\n    \"\"\"Head for specifying class identity maps.\n\n    Attributes:\n        classes: List of string names of the classes.\n        sigma: Spread of the class maps around each node.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        classes: List[Text],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.classes = classes\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.classes)\n\n    @property\n    def activation(self) -&gt; str:\n        \"\"\"Return the activation function of the head output layer.\"\"\"\n        return \"sigmoid\"\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        classes: Optional[List[Text]] = None,\n    ) -&gt; \"ClassMapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n            classes: List of string names of the classes that this head will predict.\n                This must be set if the `classes` attribute of the configuration is not\n                set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.classes is not None:\n            classes = config.classes\n        return cls(\n            classes=classes,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.activation","title":"<code>activation</code>  <code>property</code>","text":"<p>Return the activation function of the head output layer.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.__init__","title":"<code>__init__(classes, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    classes: List[Text],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.classes = classes\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassMapsHead.from_config","title":"<code>from_config(config, classes=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>classes</code> <p>List of string names of the classes that this head will predict. This must be set if the <code>classes</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>ClassMapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    classes: Optional[List[Text]] = None,\n) -&gt; \"ClassMapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n        classes: List of string names of the classes that this head will predict.\n            This must be set if the `classes` attribute of the configuration is not\n            set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.classes is not None:\n        classes = config.classes\n    return cls(\n        classes=classes,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead","title":"<code>ClassVectorsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying classification heads.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <p>List of string names of the classes.</p> <code>num_fc_layers</code> <p>Number of fully connected layers after flattening input features.</p> <code>num_fc_units</code> <p>Number of units (dimensions) in fully connected layers prior to classification output.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> <code>make_head</code> <p>Make head output tensor from input feature tensor.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class ClassVectorsHead(Head):\n    \"\"\"Head for specifying classification heads.\n\n    Attributes:\n        classes: List of string names of the classes.\n        num_fc_layers: Number of fully connected layers after flattening input features.\n        num_fc_units: Number of units (dimensions) in fully connected layers prior to\n            classification output.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        classes: List[Text],\n        num_fc_layers: int = 1,\n        num_fc_units: int = 64,\n        global_pool: bool = True,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.classes = classes\n        self.num_fc_layers = num_fc_layers\n        self.num_fc_units = num_fc_units\n        self.global_pool = global_pool\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.classes)\n\n    @property\n    def activation(self) -&gt; str:\n        \"\"\"Return the activation function of the head output layer.\"\"\"\n        return \"softmax\"\n\n    @property\n    def loss_function(self) -&gt; str:\n        \"\"\"Return the name of the loss function to use for this head.\"\"\"\n        return \"categorical_crossentropy\"\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        classes: Optional[List[Text]] = None,\n    ) -&gt; \"ClassVectorsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n            classes: List of string names of the classes that this head will predict.\n                This must be set if the `classes` attribute of the configuration is not\n                set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.classes is not None:\n            classes = config.classes\n        return cls(\n            classes=classes,\n            num_fc_layers=config.num_fc_layers,\n            num_fc_units=config.num_fc_units,\n            global_pool=config.global_pool,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n\n    def make_head(self, x_in: int) -&gt; nn.Sequential:\n        \"\"\"Make head output tensor from input feature tensor.\n\n        Args:\n            x_in: An int for the input shape after applying AdaptiveMaxPool2d on dim=1, assuming inputs of shape (B, C, H, W).\n\n        Returns:\n            A `nn.Sequential` with the correct shape for the head.\n        \"\"\"\n        from collections import OrderedDict\n\n        module_dict = OrderedDict()\n\n        if self.global_pool:\n            module_dict[f\"pre_classification_global_pool\"] = nn.AdaptiveMaxPool2d(1)\n\n        module_dict[f\"pre_classification_flatten\"] = nn.Flatten(start_dim=1)\n\n        for i in range(self.num_fc_layers):\n            if i == 0:\n                module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                    x_in, self.num_fc_units\n                )\n            else:\n                module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                    self.num_fc_units, self.num_fc_units\n                )\n            module_dict[f\"pre_classification{i}_relu\"] = get_act_fn(\"relu\")\n\n        module_dict[f\"ClassVectorsHead\"] = nn.Linear(self.num_fc_units, self.channels)\n        module_dict[f\"softmax\"] = get_act_fn(\"softmax\")\n\n        return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.activation","title":"<code>activation</code>  <code>property</code>","text":"<p>Return the activation function of the head output layer.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.loss_function","title":"<code>loss_function</code>  <code>property</code>","text":"<p>Return the name of the loss function to use for this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.__init__","title":"<code>__init__(classes, num_fc_layers=1, num_fc_units=64, global_pool=True, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    classes: List[Text],\n    num_fc_layers: int = 1,\n    num_fc_units: int = 64,\n    global_pool: bool = True,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.classes = classes\n    self.num_fc_layers = num_fc_layers\n    self.num_fc_units = num_fc_units\n    self.global_pool = global_pool\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.from_config","title":"<code>from_config(config, classes=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>classes</code> <p>List of string names of the classes that this head will predict. This must be set if the <code>classes</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>ClassVectorsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    classes: Optional[List[Text]] = None,\n) -&gt; \"ClassVectorsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n        classes: List of string names of the classes that this head will predict.\n            This must be set if the `classes` attribute of the configuration is not\n            set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.classes is not None:\n        classes = config.classes\n    return cls(\n        classes=classes,\n        num_fc_layers=config.num_fc_layers,\n        num_fc_units=config.num_fc_units,\n        global_pool=config.global_pool,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.ClassVectorsHead.make_head","title":"<code>make_head(x_in)</code>","text":"<p>Make head output tensor from input feature tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_in</code> <code>int</code> <p>An int for the input shape after applying AdaptiveMaxPool2d on dim=1, assuming inputs of shape (B, C, H, W).</p> required <p>Returns:</p> Type Description <code>Sequential</code> <p>A <code>nn.Sequential</code> with the correct shape for the head.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def make_head(self, x_in: int) -&gt; nn.Sequential:\n    \"\"\"Make head output tensor from input feature tensor.\n\n    Args:\n        x_in: An int for the input shape after applying AdaptiveMaxPool2d on dim=1, assuming inputs of shape (B, C, H, W).\n\n    Returns:\n        A `nn.Sequential` with the correct shape for the head.\n    \"\"\"\n    from collections import OrderedDict\n\n    module_dict = OrderedDict()\n\n    if self.global_pool:\n        module_dict[f\"pre_classification_global_pool\"] = nn.AdaptiveMaxPool2d(1)\n\n    module_dict[f\"pre_classification_flatten\"] = nn.Flatten(start_dim=1)\n\n    for i in range(self.num_fc_layers):\n        if i == 0:\n            module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                x_in, self.num_fc_units\n            )\n        else:\n            module_dict[f\"pre_classification{i}_fc\"] = nn.Linear(\n                self.num_fc_units, self.num_fc_units\n            )\n        module_dict[f\"pre_classification{i}_relu\"] = get_act_fn(\"relu\")\n\n    module_dict[f\"ClassVectorsHead\"] = nn.Linear(self.num_fc_units, self.channels)\n    module_dict[f\"softmax\"] = get_act_fn(\"softmax\")\n\n    return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head","title":"<code>Head</code>","text":"<p>Base class for model output heads.</p> <p>Attributes:</p> Name Type Description <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>make_head</code> <p>Make head output tensor from input feature tensor.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class Head:\n    \"\"\"Base class for model output heads.\n\n    Attributes:\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(self, output_stride: int = 1, loss_weight: float = 1.0) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        self.output_stride = output_stride\n        self.loss_weight = loss_weight\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the head.\"\"\"\n        return type(self).__name__\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        message = \"Subclasses must implement this method.\"\n        logger.error(message)\n        raise NotImplementedError(message)\n\n    @property\n    def activation(self) -&gt; str:\n        \"\"\"Return the activation function of the head output layer.\"\"\"\n        return \"identity\"\n\n    @property\n    def loss_function(self) -&gt; str:\n        \"\"\"Return the name of the loss function to use for this head.\"\"\"\n        return \"mse\"\n\n    def make_head(self, x_in: int) -&gt; nn.Sequential:\n        \"\"\"Make head output tensor from input feature tensor.\n\n        Args:\n            x_in: An int input for the input channels.\n\n        Returns:\n            A `nn.Sequential` with the correct shape for the head.\n        \"\"\"\n        module_dict = OrderedDict()\n        module_dict[self.name] = nn.Sequential(\n            nn.Conv2d(\n                in_channels=x_in,\n                out_channels=self.channels,\n                kernel_size=1,\n                stride=1,\n                padding=\"same\",\n            ),\n            get_act_fn(self.activation),\n        )\n\n        return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.activation","title":"<code>activation</code>  <code>property</code>","text":"<p>Return the activation function of the head output layer.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.loss_function","title":"<code>loss_function</code>  <code>property</code>","text":"<p>Return the name of the loss function to use for this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name of the head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.__init__","title":"<code>__init__(output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(self, output_stride: int = 1, loss_weight: float = 1.0) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    self.output_stride = output_stride\n    self.loss_weight = loss_weight\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.Head.make_head","title":"<code>make_head(x_in)</code>","text":"<p>Make head output tensor from input feature tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_in</code> <code>int</code> <p>An int input for the input channels.</p> required <p>Returns:</p> Type Description <code>Sequential</code> <p>A <code>nn.Sequential</code> with the correct shape for the head.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def make_head(self, x_in: int) -&gt; nn.Sequential:\n    \"\"\"Make head output tensor from input feature tensor.\n\n    Args:\n        x_in: An int input for the input channels.\n\n    Returns:\n        A `nn.Sequential` with the correct shape for the head.\n    \"\"\"\n    module_dict = OrderedDict()\n    module_dict[self.name] = nn.Sequential(\n        nn.Conv2d(\n            in_channels=x_in,\n            out_channels=self.channels,\n            kernel_size=1,\n            stride=1,\n            padding=\"same\",\n        ),\n        get_act_fn(self.activation),\n    )\n\n    return nn.Sequential(module_dict)\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead","title":"<code>MultiInstanceConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying multi-instance confidence maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class MultiInstanceConfmapsHead(Head):\n    \"\"\"Head for specifying multi-instance confidence maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.part_names)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n    ) -&gt; \"MultiInstanceConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.part_names is not None:\n            part_names = config.part_names\n        elif part_names is None:\n            message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead.__init__","title":"<code>__init__(part_names, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.MultiInstanceConfmapsHead.from_config","title":"<code>from_config(config, part_names=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>MultiInstanceConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n) -&gt; \"MultiInstanceConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.part_names is not None:\n        part_names = config.part_names\n    elif part_names is None:\n        message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead","title":"<code>OffsetRefinementHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying offset refinement maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>sigma_threshold</code> <p>Threshold of confidence map values to use for defining the boundary of the offset maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class OffsetRefinementHead(Head):\n    \"\"\"Head for specifying offset refinement maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        sigma_threshold: Threshold of confidence map values to use for defining the\n            boundary of the offset maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        sigma_threshold: float = 0.2,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.sigma_threshold = sigma_threshold\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return int(len(self.part_names) * 2)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n        sigma_threshold: float = 0.2,\n        loss_weight: float = 1.0,\n    ) -&gt; \"OffsetRefinementHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n            sigma_threshold: Minimum confidence map value below which offsets will be\n                replaced with zeros.\n            loss_weight: Weight of the loss associated with this head.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if hasattr(config, \"part_names\"):\n            if config.part_names is not None:\n                part_names = config.part_names\n        elif hasattr(config, \"anchor_part\"):\n            part_names = [config.anchor_part]\n        else:\n            message = \"Required attribute 'part_names' is missing in the configuration.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            output_stride=config.output_stride,\n            sigma_threshold=sigma_threshold,\n            loss_weight=loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead.__init__","title":"<code>__init__(part_names, sigma_threshold=0.2, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    sigma_threshold: float = 0.2,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.sigma_threshold = sigma_threshold\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.OffsetRefinementHead.from_config","title":"<code>from_config(config, part_names=None, sigma_threshold=0.2, loss_weight=1.0)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <code>sigma_threshold</code> <p>Minimum confidence map value below which offsets will be replaced with zeros.</p> <code>loss_weight</code> <p>Weight of the loss associated with this head.</p> <p>Returns:</p> Type Description <code>OffsetRefinementHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n    sigma_threshold: float = 0.2,\n    loss_weight: float = 1.0,\n) -&gt; \"OffsetRefinementHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n        sigma_threshold: Minimum confidence map value below which offsets will be\n            replaced with zeros.\n        loss_weight: Weight of the loss associated with this head.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if hasattr(config, \"part_names\"):\n        if config.part_names is not None:\n            part_names = config.part_names\n    elif hasattr(config, \"anchor_part\"):\n        part_names = [config.anchor_part]\n    else:\n        message = \"Required attribute 'part_names' is missing in the configuration.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        output_stride=config.output_stride,\n        sigma_threshold=sigma_threshold,\n        loss_weight=loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead","title":"<code>PartAffinityFieldsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying multi-instance part affinity fields.</p> <p>Attributes:</p> Name Type Description <code>edges</code> <p>List of tuples of <code>(source, destination)</code> node names.</p> <code>sigma</code> <p>Spread of the part affinity fields.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class PartAffinityFieldsHead(Head):\n    \"\"\"Head for specifying multi-instance part affinity fields.\n\n    Attributes:\n        edges: List of tuples of `(source, destination)` node names.\n        sigma: Spread of the part affinity fields.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        edges: Sequence[Tuple[Text, Text]],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.edges = edges\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return int(len(self.edges) * 2)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        edges: Optional[Sequence[Tuple[Text, Text]]] = None,\n    ) -&gt; \"PartAffinityFieldsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            edges: List of 2-tuples of the form `(source_node, destination_node)` that\n                define pairs of text names of the directed edges of the graph. This must\n                be set if the `edges` attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.edges is not None:\n            edges = config.edges\n        return cls(\n            edges=edges,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead.__init__","title":"<code>__init__(edges, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    edges: Sequence[Tuple[Text, Text]],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.edges = edges\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.PartAffinityFieldsHead.from_config","title":"<code>from_config(config, edges=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>edges</code> <p>List of 2-tuples of the form <code>(source_node, destination_node)</code> that define pairs of text names of the directed edges of the graph. This must be set if the <code>edges</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>PartAffinityFieldsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    edges: Optional[Sequence[Tuple[Text, Text]]] = None,\n) -&gt; \"PartAffinityFieldsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        edges: List of 2-tuples of the form `(source_node, destination_node)` that\n            define pairs of text names of the directed edges of the graph. This must\n            be set if the `edges` attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.edges is not None:\n        edges = config.edges\n    return cls(\n        edges=edges,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead","title":"<code>SingleInstanceConfmapsHead</code>","text":"<p>               Bases: <code>Head</code></p> <p>Head for specifying single instance confidence maps.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <p>List of strings specifying the part names associated with channels.</p> <code>sigma</code> <p>Spread of the confidence maps.</p> <code>output_stride</code> <p>Stride of the output head tensor. The input tensor is expected to be at the same stride.</p> <code>loss_weight</code> <p>Weight of the loss term for this head during optimization.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the object with the specified attributes.</p> <code>from_config</code> <p>Create this head from a set of configurations.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>class SingleInstanceConfmapsHead(Head):\n    \"\"\"Head for specifying single instance confidence maps.\n\n    Attributes:\n        part_names: List of strings specifying the part names associated with channels.\n        sigma: Spread of the confidence maps.\n        output_stride: Stride of the output head tensor. The input tensor is expected to\n            be at the same stride.\n        loss_weight: Weight of the loss term for this head during optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        part_names: List[Text],\n        sigma: float = 5.0,\n        output_stride: int = 1,\n        loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the object with the specified attributes.\"\"\"\n        super().__init__(output_stride, loss_weight)\n        self.part_names = part_names\n        self.sigma = sigma\n\n    @property\n    def channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the tensor output by this head.\"\"\"\n        return len(self.part_names)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: DictConfig,\n        part_names: Optional[List[Text]] = None,\n    ) -&gt; \"SingleInstanceConfmapsHead\":\n        \"\"\"Create this head from a set of configurations.\n\n        Attributes:\n            config: A `DictConfig` instance specifying the head\n                parameters.\n            part_names: Text name of the body parts (nodes) that the head will be\n                configured to produce. The number of parts determines the number of\n                channels in the output. This must be provided if the `part_names`\n                attribute of the configuration is not set.\n\n        Returns:\n            The instantiated head with the specified configuration options.\n        \"\"\"\n        if config.part_names is not None:\n            part_names = config.part_names\n        elif part_names is None:\n            message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n            logger.error(message)\n            raise ValueError(message)\n        return cls(\n            part_names=part_names,\n            sigma=config.sigma,\n            output_stride=config.output_stride,\n            loss_weight=config.loss_weight,\n        )\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead.channels","title":"<code>channels</code>  <code>property</code>","text":"<p>Return the number of channels in the tensor output by this head.</p>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead.__init__","title":"<code>__init__(part_names, sigma=5.0, output_stride=1, loss_weight=1.0)</code>","text":"<p>Initialize the object with the specified attributes.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>def __init__(\n    self,\n    part_names: List[Text],\n    sigma: float = 5.0,\n    output_stride: int = 1,\n    loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the object with the specified attributes.\"\"\"\n    super().__init__(output_stride, loss_weight)\n    self.part_names = part_names\n    self.sigma = sigma\n</code></pre>"},{"location":"api/architectures/heads/#sleap_nn.architectures.heads.SingleInstanceConfmapsHead.from_config","title":"<code>from_config(config, part_names=None)</code>  <code>classmethod</code>","text":"<p>Create this head from a set of configurations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A <code>DictConfig</code> instance specifying the head parameters.</p> <code>part_names</code> <p>Text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. This must be provided if the <code>part_names</code> attribute of the configuration is not set.</p> <p>Returns:</p> Type Description <code>SingleInstanceConfmapsHead</code> <p>The instantiated head with the specified configuration options.</p> Source code in <code>sleap_nn/architectures/heads.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    part_names: Optional[List[Text]] = None,\n) -&gt; \"SingleInstanceConfmapsHead\":\n    \"\"\"Create this head from a set of configurations.\n\n    Attributes:\n        config: A `DictConfig` instance specifying the head\n            parameters.\n        part_names: Text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of\n            channels in the output. This must be provided if the `part_names`\n            attribute of the configuration is not set.\n\n    Returns:\n        The instantiated head with the specified configuration options.\n    \"\"\"\n    if config.part_names is not None:\n        part_names = config.part_names\n    elif part_names is None:\n        message = \"Required attribute 'part_names' is missing in the configuration or in `from_config` input.\"\n        logger.error(message)\n        raise ValueError(message)\n    return cls(\n        part_names=part_names,\n        sigma=config.sigma,\n        output_stride=config.output_stride,\n        loss_weight=config.loss_weight,\n    )\n</code></pre>"},{"location":"api/architectures/model/","title":"model","text":""},{"location":"api/architectures/model/#sleap_nn.architectures.model","title":"<code>sleap_nn.architectures.model</code>","text":"<p>This module defines the main SLEAP model class for defining a trainable model.</p> <p>This is a higher level wrapper around <code>nn.Module</code> that holds all the configuration parameters required to construct the actual model. This allows for easy querying of the model configuration without actually instantiating the model itself.</p> <p>Classes:</p> Name Description <code>Model</code> <p>Model creates a model consisting of a backbone and head.</p> <p>Functions:</p> Name Description <code>get_backbone</code> <p>Get a backbone model <code>nn.Module</code> based on the provided name.</p> <code>get_head</code> <p>Get a head <code>nn.Module</code> based on the provided name.</p>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Module</code></p> <p>Model creates a model consisting of a backbone and head.</p> <p>Attributes:</p> Name Type Description <code>backbone_type</code> <p>Backbone type. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>backbone_config</code> <p>An <code>DictConfig</code> configuration dictionary for the model backbone.</p> <code>head_configs</code> <p>An <code>DictConfig</code> configuration dictionary for the model heads.</p> <code>model_type</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the backbone and head based on the backbone_config.</p> <code>forward</code> <p>Forward pass through the model.</p> <code>from_config</code> <p>Create the model from a config dictionary.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>class Model(nn.Module):\n    \"\"\"Model creates a model consisting of a backbone and head.\n\n    Attributes:\n        backbone_type: Backbone type. One of `unet`, `convnext` and `swint`.\n        backbone_config: An `DictConfig` configuration dictionary for the model backbone.\n        head_configs: An `DictConfig` configuration dictionary for the model heads.\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone_type: str,\n        backbone_config: DictConfig,\n        head_configs: DictConfig,\n        model_type: str,\n    ) -&gt; None:\n        \"\"\"Initialize the backbone and head based on the backbone_config.\"\"\"\n        super().__init__()\n        self.backbone_type = backbone_type\n        self.backbone_config = backbone_config\n        self.head_configs = head_configs\n\n        self.heads = get_head(model_type, self.head_configs)\n\n        output_strides = []\n        for head_type in head_configs:\n            head_config = head_configs[head_type]\n            output_strides.append(head_config.output_stride)\n\n        min_output_stride = min(output_strides)\n        min_output_stride = min(min_output_stride, self.backbone_config.output_stride)\n\n        self.backbone = get_backbone(\n            self.backbone_type,\n            backbone_config,\n        )\n\n        self.head_layers = nn.ModuleList([])\n        for head in self.heads:\n            if isinstance(head, ClassVectorsHead):\n                in_channels = int(self.backbone.middle_blocks[-1].filters)\n            else:\n                in_channels = self.backbone.decoder_stride_to_filters[\n                    head.output_stride\n                ]\n            self.head_layers.append(head.make_head(x_in=in_channels))\n\n    @classmethod\n    def from_config(\n        cls,\n        backbone_type: str,\n        backbone_config: DictConfig,\n        head_configs: DictConfig,\n        model_type: str,\n    ) -&gt; \"Model\":\n        \"\"\"Create the model from a config dictionary.\"\"\"\n        return cls(\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            model_type=model_type,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the model.\"\"\"\n        if x.shape[-3] != self.backbone_config.in_channels:\n            if x.shape[-3] == 1:\n                # convert grayscale to rgb\n                x = x.repeat(1, 3, 1, 1)\n            elif x.shape[-3] == 3:\n                # convert rgb to grayscale\n                x = F.rgb_to_grayscale(x, num_output_channels=1)\n\n        backbone_outputs = self.backbone(x)\n\n        outputs = {}\n        for head, head_layer in zip(self.heads, self.head_layers):\n            if not len(backbone_outputs[\"outputs\"]):\n                outputs[head.name] = head_layer(backbone_outputs[\"middle_output\"])\n            else:\n                if isinstance(head, ClassVectorsHead):\n                    backbone_out = backbone_outputs[\"intermediate_feat\"]\n                    outputs[head.name] = head_layer(backbone_out)\n                else:\n                    idx = backbone_outputs[\"strides\"].index(head.output_stride)\n                    outputs[head.name] = head_layer(backbone_outputs[\"outputs\"][idx])\n\n        return outputs\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model.__init__","title":"<code>__init__(backbone_type, backbone_config, head_configs, model_type)</code>","text":"<p>Initialize the backbone and head based on the backbone_config.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def __init__(\n    self,\n    backbone_type: str,\n    backbone_config: DictConfig,\n    head_configs: DictConfig,\n    model_type: str,\n) -&gt; None:\n    \"\"\"Initialize the backbone and head based on the backbone_config.\"\"\"\n    super().__init__()\n    self.backbone_type = backbone_type\n    self.backbone_config = backbone_config\n    self.head_configs = head_configs\n\n    self.heads = get_head(model_type, self.head_configs)\n\n    output_strides = []\n    for head_type in head_configs:\n        head_config = head_configs[head_type]\n        output_strides.append(head_config.output_stride)\n\n    min_output_stride = min(output_strides)\n    min_output_stride = min(min_output_stride, self.backbone_config.output_stride)\n\n    self.backbone = get_backbone(\n        self.backbone_type,\n        backbone_config,\n    )\n\n    self.head_layers = nn.ModuleList([])\n    for head in self.heads:\n        if isinstance(head, ClassVectorsHead):\n            in_channels = int(self.backbone.middle_blocks[-1].filters)\n        else:\n            in_channels = self.backbone.decoder_stride_to_filters[\n                head.output_stride\n            ]\n        self.head_layers.append(head.make_head(x_in=in_channels))\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the model.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the model.\"\"\"\n    if x.shape[-3] != self.backbone_config.in_channels:\n        if x.shape[-3] == 1:\n            # convert grayscale to rgb\n            x = x.repeat(1, 3, 1, 1)\n        elif x.shape[-3] == 3:\n            # convert rgb to grayscale\n            x = F.rgb_to_grayscale(x, num_output_channels=1)\n\n    backbone_outputs = self.backbone(x)\n\n    outputs = {}\n    for head, head_layer in zip(self.heads, self.head_layers):\n        if not len(backbone_outputs[\"outputs\"]):\n            outputs[head.name] = head_layer(backbone_outputs[\"middle_output\"])\n        else:\n            if isinstance(head, ClassVectorsHead):\n                backbone_out = backbone_outputs[\"intermediate_feat\"]\n                outputs[head.name] = head_layer(backbone_out)\n            else:\n                idx = backbone_outputs[\"strides\"].index(head.output_stride)\n                outputs[head.name] = head_layer(backbone_outputs[\"outputs\"][idx])\n\n    return outputs\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.Model.from_config","title":"<code>from_config(backbone_type, backbone_config, head_configs, model_type)</code>  <code>classmethod</code>","text":"<p>Create the model from a config dictionary.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    backbone_type: str,\n    backbone_config: DictConfig,\n    head_configs: DictConfig,\n    model_type: str,\n) -&gt; \"Model\":\n    \"\"\"Create the model from a config dictionary.\"\"\"\n    return cls(\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        model_type=model_type,\n    )\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.get_backbone","title":"<code>get_backbone(backbone, backbone_config)</code>","text":"<p>Get a backbone model <code>nn.Module</code> based on the provided name.</p> <p>This function returns an instance of a PyTorch <code>nn.Module</code> corresponding to the given backbone name.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>str</code> <p>Name of the backbone. Supported values are 'unet'.</p> required <code>backbone_config</code> <code>DictConfig</code> <p>A config for the backbone.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: An instance of the requested backbone model.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the provided backbone name is not one of the supported values.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def get_backbone(backbone: str, backbone_config: DictConfig) -&gt; nn.Module:\n    \"\"\"Get a backbone model `nn.Module` based on the provided name.\n\n    This function returns an instance of a PyTorch `nn.Module`\n    corresponding to the given backbone name.\n\n    Args:\n        backbone (str): Name of the backbone. Supported values are 'unet'.\n        backbone_config (DictConfig): A config for the backbone.\n\n    Returns:\n        nn.Module: An instance of the requested backbone model.\n\n    Raises:\n        KeyError: If the provided backbone name is not one of the supported values.\n    \"\"\"\n    backbones = {\"unet\": UNet, \"convnext\": ConvNextWrapper, \"swint\": SwinTWrapper}\n\n    if backbone not in backbones:\n        message = f\"Unsupported backbone: {backbone}. Supported backbones are: {', '.join(backbones.keys())}\"\n        logger.error(message)\n        raise KeyError(message)\n\n    backbone = backbones[backbone].from_config(backbone_config)\n\n    return backbone\n</code></pre>"},{"location":"api/architectures/model/#sleap_nn.architectures.model.get_head","title":"<code>get_head(model_type, head_config)</code>","text":"<p>Get a head <code>nn.Module</code> based on the provided name.</p> <p>This function returns an instance of a PyTorch <code>nn.Module</code> corresponding to the given head name.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Name of the head. Supported values are - 'single_instance' - 'centroid' - 'centered_instance' - 'bottomup' - 'multi_class_bottomup' - 'multi_class_topdown'</p> required <code>head_config</code> <code>DictConfig</code> <p>A config for the head.</p> required <p>Returns:</p> Type Description <code>Head</code> <p>nn.Module: An instance of the requested head.</p> Source code in <code>sleap_nn/architectures/model.py</code> <pre><code>def get_head(model_type: str, head_config: DictConfig) -&gt; Head:\n    \"\"\"Get a head `nn.Module` based on the provided name.\n\n    This function returns an instance of a PyTorch `nn.Module`\n    corresponding to the given head name.\n\n    Args:\n        model_type (str): Name of the head. Supported values are\n            - 'single_instance'\n            - 'centroid'\n            - 'centered_instance'\n            - 'bottomup'\n            - 'multi_class_bottomup'\n            - 'multi_class_topdown'\n        head_config (DictConfig): A config for the head.\n\n    Returns:\n        nn.Module: An instance of the requested head.\n    \"\"\"\n    heads = []\n    if model_type == \"single_instance\":\n        heads.append(SingleInstanceConfmapsHead(**head_config.confmaps))\n\n    elif model_type == \"centered_instance\":\n        heads.append(CenteredInstanceConfmapsHead(**head_config.confmaps))\n\n    elif model_type == \"centroid\":\n        heads.append(CentroidConfmapsHead(**head_config.confmaps))\n\n    elif model_type == \"bottomup\":\n        heads.append(MultiInstanceConfmapsHead(**head_config.confmaps))\n        heads.append(PartAffinityFieldsHead(**head_config.pafs))\n\n    elif model_type == \"multi_class_bottomup\":\n        heads.append(MultiInstanceConfmapsHead(**head_config.confmaps))\n        heads.append(ClassMapsHead(**head_config.class_maps))\n\n    elif model_type == \"multi_class_topdown\":\n        heads.append(CenteredInstanceConfmapsHead(**head_config.confmaps))\n        heads.append(ClassVectorsHead(**head_config.class_vectors))\n\n    else:\n        message = f\"{model_type} is not a defined model type. Please choose one of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\"\n        logger.error(message)\n        raise Exception(message)\n\n    return heads\n</code></pre>"},{"location":"api/architectures/swint/","title":"swint","text":""},{"location":"api/architectures/swint/#sleap_nn.architectures.swint","title":"<code>sleap_nn.architectures.swint</code>","text":"<p>This module provides a generalized implementation of SwinT.</p> <p>See the <code>SwinTWrapper</code> class docstring for more information.</p> <p>Classes:</p> Name Description <code>SwinTWrapper</code> <p>SwinT architecture for pose estimation.</p> <code>SwinTransformerEncoder</code> <p>SwinT backbone for pose estimation.</p>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper","title":"<code>SwinTWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>SwinT architecture for pose estimation.</p> <p>This class defines the SwinT architecture for pose estimation, combining an SwinT as the encoder and a decoder. The encoder extracts features from the input, while the decoder generates confidence maps based on the features.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\"].</p> required <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map.</p> required <code>patch_size</code> <code>List[int]</code> <p>Patch size. Default: [4,4]</p> <code>[4, 4]</code> <code>arch</code> <code>dict</code> <p>Dictionary of embed dimension, depths and number of heads in each layer.</p> <code>{'embed': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24]}</code> <code>{'embed'</code> <p>96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}</p> required <code>window_size</code> <code>List[int]</code> <p>Window size. Default: [7,7].</p> <code>[7, 7]</code> <code>stem_patch_stride</code> <code>int</code> <p>Stride for the patch. Default is 2.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 2.</p> <code>2</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>16</code> for all swint architectures.</p> <code>32</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SwinT architecture.</p> <code>from_config</code> <p>Create SwinTWrapper from a config.</p> <p>Attributes:</p> Name Type Description <code>max_channels</code> <p>Returns the maximum channels of the SwinT (last layer of the encoder).</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>class SwinTWrapper(nn.Module):\n    \"\"\"SwinT architecture for pose estimation.\n\n    This class defines the SwinT architecture for pose estimation, combining an\n    SwinT as the encoder and a decoder. The encoder extracts features from the input,\n    while the decoder generates confidence maps based on the features.\n\n    Args:\n        in_channels: Number of input channels. Default is 1.\n        model_type: One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\"].\n        output_stride: Minimum of the strides of the output heads. The input confidence map.\n        patch_size: Patch size. Default: [4,4]\n        arch: Dictionary of embed dimension, depths and number of heads in each layer.\n        Default is \"Tiny architecture\".\n        {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}\n        window_size: Window size. Default: [7,7].\n        stem_patch_stride: Stride for the patch. Default is 2.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        filters_rate: Factor to adjust the number of filters per block. Default is 2.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `16` for all swint architectures.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step.\n\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        output_stride: int,\n        in_channels: int = 1,\n        patch_size: List[int] = [4, 4],\n        arch: dict = {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n        window_size: List[int] = [7, 7],\n        stem_patch_stride: int = 2,\n        kernel_size: int = 3,\n        filters_rate: int = 2,\n        convs_per_block: int = 2,\n        up_interpolate: bool = True,\n        max_stride: int = 32,\n        block_contraction: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.patch_size = patch_size\n        self.kernel_size = kernel_size\n        self.filters_rate = filters_rate\n        self.block_contraction = block_contraction\n        arch_types = {\n            \"tiny\": {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n            \"small\": {\n                \"embed\": 96,\n                \"depths\": [2, 2, 18, 2],\n                \"num_heads\": [3, 6, 12, 24],\n            },\n            \"base\": {\n                \"embed\": 128,\n                \"depths\": [2, 2, 18, 2],\n                \"num_heads\": [4, 8, 16, 32],\n            },\n        }\n        if model_type in arch_types:\n            self.arch = arch_types[model_type]\n        elif arch is not None:\n            self.arch = arch\n        else:\n            self.arch = arch_types[\"tiny\"]\n\n        self.max_stride = (\n            stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * final_max_pool_stride\n        self.stem_blocks = 1  # 1 stem block + 3 down blocks in swint\n\n        self.up_blocks = np.log2(\n            self.max_stride / (stem_patch_stride * output_stride)\n        ).astype(int) + np.log2(stem_patch_stride).astype(int)\n        self.convs_per_block = convs_per_block\n        self.stem_patch_stride = stem_patch_stride\n        self.down_blocks = len(self.arch[\"depths\"]) - 1\n        self.enc = SwinTransformerEncoder(\n            in_channels=in_channels,\n            patch_size=patch_size,\n            embed_dim=self.arch[\"embed\"],\n            depths=self.arch[\"depths\"],\n            num_heads=self.arch[\"num_heads\"],\n            window_size=window_size,\n            stem_stride=stem_patch_stride,\n        )\n\n        self.additional_pool = MaxPool2dWithSamePadding(\n            kernel_size=2, stride=2, padding=\"same\"\n        )\n\n        # Create middle blocks\n        self.middle_blocks = nn.ModuleList()\n        # Get the last block filters from encoder\n        last_block_filters = self.arch[\"embed\"] * (2 ** (self.down_blocks))\n\n        if convs_per_block &gt; 1:\n            # Middle expansion block\n            middle_expand = SimpleConvBlock(\n                in_channels=last_block_filters,\n                pool=False,\n                pool_before_convs=False,\n                pooling_stride=2,\n                num_convs=convs_per_block - 1,\n                filters=int(last_block_filters * filters_rate),\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=\"convnext_middle_expand\",\n            )\n            self.middle_blocks.append(middle_expand)\n\n        # Middle contraction block\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            block_filters = int(last_block_filters)\n        else:\n            # Keep the block output filters the same\n            block_filters = int(last_block_filters * filters_rate)\n\n        middle_contract = SimpleConvBlock(\n            in_channels=int(last_block_filters * filters_rate),\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=1,\n            filters=block_filters,\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_contract\",\n        )\n        self.middle_blocks.append(middle_contract)\n\n        self.current_stride = (\n            self.stem_patch_stride * (2**3) * 2\n        )  # stem_stride * down_blocks_stride * final_max_pool_stride\n\n        # Encoder channels for skip connections (reversed to match decoder order)\n        # SwinT channels: embed * 2^i for each stage i, then reversed\n        num_stages = len(self.arch[\"depths\"])\n        encoder_channels = [\n            self.arch[\"embed\"] * (2 ** (num_stages - 1 - i)) for i in range(num_stages)\n        ]\n\n        self.dec = Decoder(\n            x_in_shape=block_filters,\n            current_stride=self.current_stride,\n            filters=self.arch[\"embed\"],\n            up_blocks=self.up_blocks,\n            down_blocks=self.down_blocks,\n            filters_rate=filters_rate,\n            kernel_size=self.kernel_size,\n            stem_blocks=self.stem_blocks,\n            block_contraction=self.block_contraction,\n            output_stride=output_stride,\n            up_interpolate=up_interpolate,\n            encoder_channels=encoder_channels,\n        )\n\n        if len(self.dec.decoder_stack):\n            self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n        else:\n            self.final_dec_channels = block_filters\n\n        self.decoder_stride_to_filters = self.dec.stride_to_filters\n\n    @property\n    def max_channels(self):\n        \"\"\"Returns the maximum channels of the SwinT (last layer of the encoder).\"\"\"\n        return self.dec.x_in_shape\n\n    @classmethod\n    def from_config(cls, config: OmegaConf):\n        \"\"\"Create SwinTWrapper from a config.\"\"\"\n        return cls(\n            in_channels=config.in_channels,\n            model_type=config.model_type,\n            arch=config.arch,\n            patch_size=(config.patch_size, config.patch_size),\n            window_size=(config.window_size, config.window_size),\n            kernel_size=config.kernel_size,\n            filters_rate=config.filters_rate,\n            convs_per_block=config.convs_per_block,\n            up_interpolate=config.up_interpolate,\n            output_stride=config.output_stride,\n            stem_patch_stride=config.stem_patch_stride,\n            max_stride=config.max_stride,\n            block_contraction=(\n                config.block_contraction\n                if hasattr(config, \"block_contraction\")\n                else False\n            ),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the SwinT architecture.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            x: Outputs a dictionary with `outputs` and `strides` containing the output\n            at different strides.\n        \"\"\"\n        enc_output = self.enc(x)\n        x, features = enc_output[-1], enc_output[::2]\n        features = features[::-1]\n\n        # Apply additional pooling layer\n        x = self.additional_pool(x)\n\n        # Process through middle blocks\n        middle_output = x\n        for middle_block in self.middle_blocks:\n            middle_output = middle_block(middle_output)\n\n        x = self.dec(middle_output, features)\n        x[\"middle_output\"] = middle_output\n        return x\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.max_channels","title":"<code>max_channels</code>  <code>property</code>","text":"<p>Returns the maximum channels of the SwinT (last layer of the encoder).</p>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.__init__","title":"<code>__init__(model_type, output_stride, in_channels=1, patch_size=[4, 4], arch={'embed': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24]}, window_size=[7, 7], stem_patch_stride=2, kernel_size=3, filters_rate=2, convs_per_block=2, up_interpolate=True, max_stride=32, block_contraction=False)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    output_stride: int,\n    in_channels: int = 1,\n    patch_size: List[int] = [4, 4],\n    arch: dict = {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n    window_size: List[int] = [7, 7],\n    stem_patch_stride: int = 2,\n    kernel_size: int = 3,\n    filters_rate: int = 2,\n    convs_per_block: int = 2,\n    up_interpolate: bool = True,\n    max_stride: int = 32,\n    block_contraction: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.patch_size = patch_size\n    self.kernel_size = kernel_size\n    self.filters_rate = filters_rate\n    self.block_contraction = block_contraction\n    arch_types = {\n        \"tiny\": {\"embed\": 96, \"depths\": [2, 2, 6, 2], \"num_heads\": [3, 6, 12, 24]},\n        \"small\": {\n            \"embed\": 96,\n            \"depths\": [2, 2, 18, 2],\n            \"num_heads\": [3, 6, 12, 24],\n        },\n        \"base\": {\n            \"embed\": 128,\n            \"depths\": [2, 2, 18, 2],\n            \"num_heads\": [4, 8, 16, 32],\n        },\n    }\n    if model_type in arch_types:\n        self.arch = arch_types[model_type]\n    elif arch is not None:\n        self.arch = arch\n    else:\n        self.arch = arch_types[\"tiny\"]\n\n    self.max_stride = (\n        stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * final_max_pool_stride\n    self.stem_blocks = 1  # 1 stem block + 3 down blocks in swint\n\n    self.up_blocks = np.log2(\n        self.max_stride / (stem_patch_stride * output_stride)\n    ).astype(int) + np.log2(stem_patch_stride).astype(int)\n    self.convs_per_block = convs_per_block\n    self.stem_patch_stride = stem_patch_stride\n    self.down_blocks = len(self.arch[\"depths\"]) - 1\n    self.enc = SwinTransformerEncoder(\n        in_channels=in_channels,\n        patch_size=patch_size,\n        embed_dim=self.arch[\"embed\"],\n        depths=self.arch[\"depths\"],\n        num_heads=self.arch[\"num_heads\"],\n        window_size=window_size,\n        stem_stride=stem_patch_stride,\n    )\n\n    self.additional_pool = MaxPool2dWithSamePadding(\n        kernel_size=2, stride=2, padding=\"same\"\n    )\n\n    # Create middle blocks\n    self.middle_blocks = nn.ModuleList()\n    # Get the last block filters from encoder\n    last_block_filters = self.arch[\"embed\"] * (2 ** (self.down_blocks))\n\n    if convs_per_block &gt; 1:\n        # Middle expansion block\n        middle_expand = SimpleConvBlock(\n            in_channels=last_block_filters,\n            pool=False,\n            pool_before_convs=False,\n            pooling_stride=2,\n            num_convs=convs_per_block - 1,\n            filters=int(last_block_filters * filters_rate),\n            kernel_size=kernel_size,\n            use_bias=True,\n            batch_norm=False,\n            activation=\"relu\",\n            prefix=\"convnext_middle_expand\",\n        )\n        self.middle_blocks.append(middle_expand)\n\n    # Middle contraction block\n    if self.block_contraction:\n        # Contract the channels with an exponent lower than the last encoder block\n        block_filters = int(last_block_filters)\n    else:\n        # Keep the block output filters the same\n        block_filters = int(last_block_filters * filters_rate)\n\n    middle_contract = SimpleConvBlock(\n        in_channels=int(last_block_filters * filters_rate),\n        pool=False,\n        pool_before_convs=False,\n        pooling_stride=2,\n        num_convs=1,\n        filters=block_filters,\n        kernel_size=kernel_size,\n        use_bias=True,\n        batch_norm=False,\n        activation=\"relu\",\n        prefix=\"convnext_middle_contract\",\n    )\n    self.middle_blocks.append(middle_contract)\n\n    self.current_stride = (\n        self.stem_patch_stride * (2**3) * 2\n    )  # stem_stride * down_blocks_stride * final_max_pool_stride\n\n    # Encoder channels for skip connections (reversed to match decoder order)\n    # SwinT channels: embed * 2^i for each stage i, then reversed\n    num_stages = len(self.arch[\"depths\"])\n    encoder_channels = [\n        self.arch[\"embed\"] * (2 ** (num_stages - 1 - i)) for i in range(num_stages)\n    ]\n\n    self.dec = Decoder(\n        x_in_shape=block_filters,\n        current_stride=self.current_stride,\n        filters=self.arch[\"embed\"],\n        up_blocks=self.up_blocks,\n        down_blocks=self.down_blocks,\n        filters_rate=filters_rate,\n        kernel_size=self.kernel_size,\n        stem_blocks=self.stem_blocks,\n        block_contraction=self.block_contraction,\n        output_stride=output_stride,\n        up_interpolate=up_interpolate,\n        encoder_channels=encoder_channels,\n    )\n\n    if len(self.dec.decoder_stack):\n        self.final_dec_channels = self.dec.decoder_stack[-1].refine_convs_filters\n    else:\n        self.final_dec_channels = block_filters\n\n    self.decoder_stride_to_filters = self.dec.stride_to_filters\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the SwinT architecture.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tuple[List[Tensor], List]</code> <p>Outputs a dictionary with <code>outputs</code> and <code>strides</code> containing the output at different strides.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the SwinT architecture.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        x: Outputs a dictionary with `outputs` and `strides` containing the output\n        at different strides.\n    \"\"\"\n    enc_output = self.enc(x)\n    x, features = enc_output[-1], enc_output[::2]\n    features = features[::-1]\n\n    # Apply additional pooling layer\n    x = self.additional_pool(x)\n\n    # Process through middle blocks\n    middle_output = x\n    for middle_block in self.middle_blocks:\n        middle_output = middle_block(middle_output)\n\n    x = self.dec(middle_output, features)\n    x[\"middle_output\"] = middle_output\n    return x\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTWrapper.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create SwinTWrapper from a config.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>@classmethod\ndef from_config(cls, config: OmegaConf):\n    \"\"\"Create SwinTWrapper from a config.\"\"\"\n    return cls(\n        in_channels=config.in_channels,\n        model_type=config.model_type,\n        arch=config.arch,\n        patch_size=(config.patch_size, config.patch_size),\n        window_size=(config.window_size, config.window_size),\n        kernel_size=config.kernel_size,\n        filters_rate=config.filters_rate,\n        convs_per_block=config.convs_per_block,\n        up_interpolate=config.up_interpolate,\n        output_stride=config.output_stride,\n        stem_patch_stride=config.stem_patch_stride,\n        max_stride=config.max_stride,\n        block_contraction=(\n            config.block_contraction\n            if hasattr(config, \"block_contraction\")\n            else False\n        ),\n    )\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTransformerEncoder","title":"<code>SwinTransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>SwinT backbone for pose estimation.</p> <p>This class implements ConvNext from the <code>\"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</code>https://arxiv.org/abs/2103.14030`paper. Source: torchvision.models. This module serves as the backbone/ encoder architecture to extract features from the input image.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>patch_size</code> <code>List[int]</code> <p>Patch size. Default: [4,4]</p> <code>[4, 4]</code> <code>embed_dim</code> <code>int</code> <p>Patch embedding dimension. Default: 96</p> <code>96</code> <code>depths</code> <code>List(int)</code> <p>Depth of each Swin Transformer layer. Default: [2,2,6,2].</p> <code>[2, 2, 6, 2]</code> <code>num_heads</code> <code>List(int)</code> <p>Number of attention heads in different layers.             Default: [3,6,12,24].</p> <code>[3, 6, 12, 24]</code> <code>window_size</code> <code>List[int]</code> <p>Window size. Default: [7,7].</p> <code>[7, 7]</code> <code>stem_stride</code> <code>int</code> <p>Stride for the patch. Default is 2.</p> <code>2</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4.0.</p> <code>4.0</code> <code>dropout</code> <code>float</code> <p>Dropout rate. Default: 0.0.</p> <code>0.0</code> <code>attention_dropout</code> <code>float</code> <p>Attention dropout rate. Default: 0.0.</p> <code>0.0</code> <code>stochastic_depth_prob</code> <code>float</code> <p>Stochastic depth rate. Default: 0.1.</p> <code>0.1</code> <code>num_classes</code> <code>int</code> <p>Number of classes for classification head. Default: 1000.</p> required <code>block</code> <code>Module</code> <p>SwinTransformer Block. Default: None.</p> required <code>norm_layer</code> <code>Module</code> <p>Normalization layer. Default: None.</p> <code>None</code> <code>downsample_layer</code> <code>Module</code> <p>Downsample layer (patch merging). Default: PatchMerging.</p> required <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the SwinT encoder.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>class SwinTransformerEncoder(nn.Module):\n    \"\"\"SwinT backbone for pose estimation.\n\n    This class implements ConvNext from the `\"Swin Transformer: Hierarchical Vision Transformer\n    using Shifted Windows `&lt;https://arxiv.org/abs/2103.14030&gt;`paper.\n    Source: torchvision.models. This module serves as the backbone/ encoder architecture\n    to extract features from the input image.\n\n    Args:\n        in_channels (int): Number of input channels. Default is 1.\n        patch_size (List[int]): Patch size. Default: [4,4]\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (List(int)): Depth of each Swin Transformer layer. Default: [2,2,6,2].\n        num_heads (List(int)): Number of attention heads in different layers.\n                        Default: [3,6,12,24].\n        window_size (List[int]): Window size. Default: [7,7].\n        stem_stride (int): Stride for the patch. Default is 2.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.\n        dropout (float): Dropout rate. Default: 0.0.\n        attention_dropout (float): Attention dropout rate. Default: 0.0.\n        stochastic_depth_prob (float): Stochastic depth rate. Default: 0.1.\n        num_classes (int): Number of classes for classification head. Default: 1000.\n        block (nn.Module, optional): SwinTransformer Block. Default: None.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None.\n        downsample_layer (nn.Module): Downsample layer (patch merging). Default: PatchMerging.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 1,\n        patch_size: List[int] = [4, 4],\n        embed_dim: int = 96,\n        depths: List[int] = [2, 2, 6, 2],\n        num_heads: List[int] = [3, 6, 12, 24],\n        window_size: List[int] = [7, 7],\n        stem_stride: int = 2,\n        mlp_ratio: float = 4.0,\n        dropout: float = 0.0,\n        attention_dropout: float = 0.0,\n        stochastic_depth_prob: float = 0.1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ):\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        block = SwinTransformerBlock  # or v2\n        downsample_layer = PatchMerging  # or v2\n\n        if not norm_layer:\n            norm_layer = partial(nn.LayerNorm, eps=1e-5)\n\n        layers: List[nn.Module] = []\n        # split image into non-overlapping patches\n        layers.append(\n            nn.Sequential(\n                nn.Conv2d(\n                    in_channels,\n                    embed_dim,\n                    kernel_size=(patch_size[0], patch_size[1]),\n                    stride=(stem_stride, stem_stride),\n                    padding=1,\n                ),\n                Permute([0, 2, 3, 1]),\n                norm_layer(embed_dim),\n            )\n        )\n\n        total_stage_blocks = sum(depths)\n        stage_block_id = 0\n        # build SwinTransformer blocks\n        for i_stage in range(len(depths)):\n            stage: List[nn.Module] = []\n            dim = embed_dim * 2**i_stage\n            for i_layer in range(depths[i_stage]):\n                # adjust stochastic depth probability based on the depth of the stage block\n                sd_prob = (\n                    stochastic_depth_prob\n                    * float(stage_block_id)\n                    / (total_stage_blocks - 1)\n                )\n                stage.append(\n                    block(\n                        dim,\n                        num_heads[i_stage],\n                        window_size=window_size,\n                        shift_size=[\n                            0 if i_layer % 2 == 0 else w // 2 for w in window_size\n                        ],\n                        mlp_ratio=mlp_ratio,\n                        dropout=dropout,\n                        attention_dropout=attention_dropout,\n                        stochastic_depth_prob=sd_prob,\n                        norm_layer=norm_layer,\n                    )\n                )\n                stage_block_id += 1\n            layers.append(nn.Sequential(*stage))\n            # add patch merging layer\n            if i_stage &lt; (len(depths) - 1):\n                layers.append(downsample_layer(dim, norm_layer))\n        self.features = nn.Sequential(*layers)\n\n        num_features = embed_dim * 2 ** (len(depths) - 1)\n        self.norm = norm_layer(num_features)\n        self.permute = Permute([0, 3, 1, 2])  # B H W C -&gt; B C H W\n\n    def forward(self, x):\n        \"\"\"Forward pass through the SwinT encoder.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Outputs a list of tensors from each stage after applying the SwinT backbone.\n        \"\"\"\n        features_list = []\n        for idx, l in enumerate(self.features):\n            x = l(x)\n            if idx == len(self.features) - 1:\n                x = self.norm(x)\n            features_list.append(self.permute(x))\n        return features_list\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTransformerEncoder.__init__","title":"<code>__init__(in_channels=1, patch_size=[4, 4], embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=[7, 7], stem_stride=2, mlp_ratio=4.0, dropout=0.0, attention_dropout=0.0, stochastic_depth_prob=0.1, norm_layer=None)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int = 1,\n    patch_size: List[int] = [4, 4],\n    embed_dim: int = 96,\n    depths: List[int] = [2, 2, 6, 2],\n    num_heads: List[int] = [3, 6, 12, 24],\n    window_size: List[int] = [7, 7],\n    stem_stride: int = 2,\n    mlp_ratio: float = 4.0,\n    dropout: float = 0.0,\n    attention_dropout: float = 0.0,\n    stochastic_depth_prob: float = 0.1,\n    norm_layer: Optional[Callable[..., nn.Module]] = None,\n):\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n    _log_api_usage_once(self)\n\n    block = SwinTransformerBlock  # or v2\n    downsample_layer = PatchMerging  # or v2\n\n    if not norm_layer:\n        norm_layer = partial(nn.LayerNorm, eps=1e-5)\n\n    layers: List[nn.Module] = []\n    # split image into non-overlapping patches\n    layers.append(\n        nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                embed_dim,\n                kernel_size=(patch_size[0], patch_size[1]),\n                stride=(stem_stride, stem_stride),\n                padding=1,\n            ),\n            Permute([0, 2, 3, 1]),\n            norm_layer(embed_dim),\n        )\n    )\n\n    total_stage_blocks = sum(depths)\n    stage_block_id = 0\n    # build SwinTransformer blocks\n    for i_stage in range(len(depths)):\n        stage: List[nn.Module] = []\n        dim = embed_dim * 2**i_stage\n        for i_layer in range(depths[i_stage]):\n            # adjust stochastic depth probability based on the depth of the stage block\n            sd_prob = (\n                stochastic_depth_prob\n                * float(stage_block_id)\n                / (total_stage_blocks - 1)\n            )\n            stage.append(\n                block(\n                    dim,\n                    num_heads[i_stage],\n                    window_size=window_size,\n                    shift_size=[\n                        0 if i_layer % 2 == 0 else w // 2 for w in window_size\n                    ],\n                    mlp_ratio=mlp_ratio,\n                    dropout=dropout,\n                    attention_dropout=attention_dropout,\n                    stochastic_depth_prob=sd_prob,\n                    norm_layer=norm_layer,\n                )\n            )\n            stage_block_id += 1\n        layers.append(nn.Sequential(*stage))\n        # add patch merging layer\n        if i_stage &lt; (len(depths) - 1):\n            layers.append(downsample_layer(dim, norm_layer))\n    self.features = nn.Sequential(*layers)\n\n    num_features = embed_dim * 2 ** (len(depths) - 1)\n    self.norm = norm_layer(num_features)\n    self.permute = Permute([0, 3, 1, 2])  # B H W C -&gt; B C H W\n</code></pre>"},{"location":"api/architectures/swint/#sleap_nn.architectures.swint.SwinTransformerEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the SwinT encoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <p>Outputs a list of tensors from each stage after applying the SwinT backbone.</p> Source code in <code>sleap_nn/architectures/swint.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass through the SwinT encoder.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Outputs a list of tensors from each stage after applying the SwinT backbone.\n    \"\"\"\n    features_list = []\n    for idx, l in enumerate(self.features):\n        x = l(x)\n        if idx == len(self.features) - 1:\n            x = self.norm(x)\n        features_list.append(self.permute(x))\n    return features_list\n</code></pre>"},{"location":"api/architectures/unet/","title":"unet","text":""},{"location":"api/architectures/unet/#sleap_nn.architectures.unet","title":"<code>sleap_nn.architectures.unet</code>","text":"<p>This module provides a generalized implementation of UNet.</p> <p>See the <code>UNet</code> class docstring for more information.</p> <p>Classes:</p> Name Description <code>UNet</code> <p>U-Net architecture for pose estimation.</p>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet","title":"<code>UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>U-Net architecture for pose estimation.</p> <p>This class defines the U-Net architecture for pose estimation, combining an encoder and a decoder. The encoder extracts features from the input, while the decoder generates confidence maps based on the features.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels. Default is 1.</p> <code>1</code> <code>output_stride</code> <code>int</code> <p>Minimum of the strides of the output heads. The input confidence map.</p> <code>2</code> <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernels. Default is 3.</p> <code>3</code> <code>stem_kernel_size</code> <code>int</code> <p>Kernle size for the stem blocks.</p> <code>7</code> <code>filters</code> <code>int</code> <p>Number of filters for the initial block. Default is 32.</p> <code>32</code> <code>filters_rate</code> <code>int</code> <p>Factor to adjust the number of filters per block. Default is 1.5.</p> <code>1.5</code> <code>down_blocks</code> <code>int</code> <p>Number of downsampling blocks. Default is 4.</p> <code>4</code> <code>up_blocks</code> <code>int</code> <p>Number of upsampling blocks in the decoder. Default is 3.</p> <code>3</code> <code>stem_blocks</code> <code>int</code> <p>If &gt;0, will create additional \"down\" blocks for initial downsampling. These will be configured identically to the down blocks below.</p> <code>0</code> <code>convs_per_block</code> <code>int</code> <p>Number of convolutional layers per block. Default is 2.</p> <code>2</code> <code>middle_block</code> <code>bool</code> <p>If True, add an additional block at the end of the encoder.</p> <code>True</code> <code>up_interpolate</code> <code>bool</code> <p>If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales.</p> <code>True</code> <code>block_contraction</code> <code>bool</code> <p>If True, reduces the number of filters at the end of middle and decoder blocks. This has the effect of introducing an additional bottleneck before each upsampling step. The original implementation does not do this, but the CARE implementation does.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class.</p> <code>forward</code> <p>Forward pass through the U-Net architecture.</p> <code>from_config</code> <p>Create UNet from a config.</p> <p>Attributes:</p> Name Type Description <code>max_channels</code> <p>Returns the maximum channels of the UNet (last layer of the encoder).</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>class UNet(nn.Module):\n    \"\"\"U-Net architecture for pose estimation.\n\n    This class defines the U-Net architecture for pose estimation, combining an\n    encoder and a decoder. The encoder extracts features from the input, while the\n    decoder generates confidence maps based on the features.\n\n    Args:\n        in_channels: Number of input channels. Default is 1.\n        output_stride: Minimum of the strides of the output heads. The input confidence map.\n        kernel_size: Size of the convolutional kernels. Default is 3.\n        stem_kernel_size: Kernle size for the stem blocks.\n        filters: Number of filters for the initial block. Default is 32.\n        filters_rate: Factor to adjust the number of filters per block. Default is 1.5.\n        down_blocks: Number of downsampling blocks. Default is 4.\n        up_blocks: Number of upsampling blocks in the decoder. Default is 3.\n        stem_blocks: If &gt;0, will create additional \"down\" blocks for initial\n            downsampling. These will be configured identically to the down blocks below.\n        convs_per_block: Number of convolutional layers per block. Default is 2.\n        middle_block: If True, add an additional block at the end of the encoder.\n        up_interpolate: If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales.\n        block_contraction: If True, reduces the number of filters at the end of middle\n            and decoder blocks. This has the effect of introducing an additional\n            bottleneck before each upsampling step. The original implementation does not\n            do this, but the CARE implementation does.\n\n    Attributes:\n        Inherits all attributes from torch.nn.Module.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_stride: int = 2,\n        in_channels: int = 1,\n        kernel_size: int = 3,\n        stem_kernel_size: int = 7,\n        filters: int = 32,\n        filters_rate: int = 1.5,\n        down_blocks: int = 4,\n        up_blocks: int = 3,\n        stem_blocks: int = 0,\n        convs_per_block: int = 2,\n        middle_block: bool = True,\n        up_interpolate: bool = True,\n        block_contraction: bool = False,\n        stacks: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the class.\"\"\"\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n        self.filters = filters\n        self.filters_rate = filters_rate\n        self.down_blocks = down_blocks\n        self.up_blocks = up_blocks\n        self.stem_blocks = stem_blocks\n        self.convs_per_block = convs_per_block\n        self.stem_kernel_size = stem_kernel_size\n        self.middle_block = middle_block\n        self.up_interpolate = up_interpolate\n        self.block_contraction = block_contraction\n        self.stacks = stacks\n\n        # Create stem block if stem_blocks &gt; 0\n        if self.stem_blocks &gt; 0:\n            self.stem = StemBlock(\n                in_channels=in_channels,\n                filters=filters,\n                stem_blocks=stem_blocks,\n                filters_rate=filters_rate,\n                convs_per_block=convs_per_block,\n                kernel_size=stem_kernel_size,\n                prefix=\"stem\",\n            )\n        else:\n            self.stem = None\n\n        # Initialize lists to store multiple encoders and decoders\n        self.encoders = nn.ModuleList()\n        self.decoders = nn.ModuleList()\n\n        for i in range(self.stacks):\n            # Create encoder for this stack\n            in_channels = (\n                int(self.filters * (self.filters_rate ** (self.stem_blocks)))\n                if self.stem_blocks &gt; 0\n                else in_channels\n            )\n            encoder = Encoder(\n                in_channels=in_channels,\n                filters=filters,\n                down_blocks=down_blocks,\n                filters_rate=filters_rate,\n                convs_per_block=convs_per_block,\n                kernel_size=kernel_size,\n                stem_blocks=stem_blocks,\n                prefix=f\"stack{i}_enc\",\n            )\n\n            # Create middle block separately (not part of encoder stack)\n            self.middle_blocks = nn.ModuleList()\n            # Get the last block filters from encoder\n            last_block_filters = int(\n                filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n            )\n            enc_num = len(encoder.encoder_stack)\n            if self.middle_block:\n                if convs_per_block &gt; 1:\n                    # Middle expansion block\n                    from sleap_nn.architectures.encoder_decoder import SimpleConvBlock\n\n                    middle_expand = SimpleConvBlock(\n                        in_channels=last_block_filters,\n                        pool=False,\n                        pool_before_convs=False,\n                        pooling_stride=2,\n                        num_convs=convs_per_block - 1,\n                        filters=int(\n                            filters * (filters_rate ** (down_blocks + stem_blocks))\n                        ),\n                        kernel_size=kernel_size,\n                        use_bias=True,\n                        batch_norm=False,\n                        activation=\"relu\",\n                        prefix=f\"stack{i}_enc{enc_num}_middle_expand\",\n                    )\n                    enc_num += 1\n                    self.middle_blocks.append(middle_expand)\n\n                # Middle contraction block\n                if self.block_contraction:\n                    # Contract the channels with an exponent lower than the last encoder block\n                    block_filters = int(last_block_filters)\n                else:\n                    # Keep the block output filters the same\n                    block_filters = int(\n                        filters * (filters_rate ** (down_blocks + stem_blocks))\n                    )\n\n                middle_contract = SimpleConvBlock(\n                    in_channels=int(\n                        filters * (filters_rate ** (down_blocks + stem_blocks))\n                    ),\n                    pool=False,\n                    pool_before_convs=False,\n                    pooling_stride=2,\n                    num_convs=1,\n                    filters=block_filters,\n                    kernel_size=kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"stack{i}_enc{enc_num}_middle_contract\",\n                )\n                enc_num += 1\n                self.middle_blocks.append(middle_contract)\n\n            self.encoders.append(encoder)\n\n            # Calculate current stride for this encoder\n            # Start with stem stride if stem blocks exist\n            current_stride = 2**self.stem_blocks if self.stem_blocks &gt; 0 else 1\n\n            # Add encoder strides\n            for block in encoder.encoder_stack:\n                if hasattr(block, \"pool\") and block.pool:\n                    current_stride *= block.pooling_stride\n\n            current_stride *= (\n                2  # for last pool layer MaxPool2dWithSamePadding in encoder\n            )\n\n            # Create decoder for this stack\n            if self.block_contraction:\n                # Contract the channels with an exponent lower than the last encoder block\n                x_in_shape = int(\n                    filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n                )\n            else:\n                # Keep the block output filters the same\n                x_in_shape = int(\n                    filters * (filters_rate ** (down_blocks + stem_blocks))\n                )\n            decoder = Decoder(\n                x_in_shape=x_in_shape,\n                current_stride=current_stride,\n                filters=filters,\n                up_blocks=up_blocks,\n                down_blocks=down_blocks,\n                filters_rate=filters_rate,\n                stem_blocks=stem_blocks,\n                output_stride=output_stride,\n                kernel_size=kernel_size,\n                block_contraction=self.block_contraction,\n                up_interpolate=up_interpolate,\n                prefix=f\"stack{i}_dec\",\n            )\n            self.decoders.append(decoder)\n\n        if len(self.decoders) and len(self.decoders[-1].decoder_stack):\n            self.final_dec_channels = (\n                self.decoders[-1].decoder_stack[-1].refine_convs_filters\n            )\n        else:\n            self.final_dec_channels = (\n                last_block_filters if not self.middle_block else block_filters\n            )\n\n        self.decoder_stride_to_filters = self.decoders[-1].stride_to_filters\n\n    @classmethod\n    def from_config(cls, config: OmegaConf):\n        \"\"\"Create UNet from a config.\"\"\"\n        stem_blocks = 0\n        if config.stem_stride is not None:\n            stem_blocks = np.log2(config.stem_stride).astype(int)\n        down_blocks = np.log2(config.max_stride).astype(int) - stem_blocks\n        up_blocks = (\n            np.log2(config.max_stride / config.output_stride).astype(int) + stem_blocks\n        )\n        return cls(\n            in_channels=config.in_channels,\n            kernel_size=config.kernel_size,\n            filters=config.filters,\n            filters_rate=config.filters_rate,\n            down_blocks=down_blocks,\n            up_blocks=up_blocks,\n            stem_blocks=stem_blocks,\n            convs_per_block=config.convs_per_block,\n            middle_block=config.middle_block,\n            up_interpolate=config.up_interpolate,\n            stacks=config.stacks,\n            output_stride=config.output_stride,\n        )\n\n    @property\n    def max_channels(self):\n        \"\"\"Returns the maximum channels of the UNet (last layer of the encoder).\"\"\"\n        return self.decoders[0].x_in_shape\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n        \"\"\"Forward pass through the U-Net architecture.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            x: Output a tensor after applying the U-Net operations.\n            current_strides: a list of the current strides from the decoder.\n        \"\"\"\n        # Process through stem block if it exists\n        stem_output = x\n        if self.stem is not None:\n            stem_output = self.stem(x)\n\n        # Process through all stacks\n        outputs = []\n        output = stem_output\n        for i in range(self.stacks):\n            # Get encoder and decoder for this stack\n            encoder = self.encoders[i]\n            decoder = self.decoders[i]\n\n            # Forward pass through encoder\n            encoded, features = encoder(output)\n\n            # Process through middle block if it exists\n            middle_output = encoded\n            if self.middle_block and hasattr(self, \"middle_blocks\"):\n                for middle_block in self.middle_blocks:\n                    middle_output = middle_block(middle_output)\n\n            if self.stem_blocks &gt; 0:\n                features.append(stem_output)\n\n            output = decoder(middle_output, features)\n            output[\"middle_output\"] = middle_output\n            outputs.append(output)\n\n        return outputs[-1]\n</code></pre>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.max_channels","title":"<code>max_channels</code>  <code>property</code>","text":"<p>Returns the maximum channels of the UNet (last layer of the encoder).</p>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.__init__","title":"<code>__init__(output_stride=2, in_channels=1, kernel_size=3, stem_kernel_size=7, filters=32, filters_rate=1.5, down_blocks=4, up_blocks=3, stem_blocks=0, convs_per_block=2, middle_block=True, up_interpolate=True, block_contraction=False, stacks=1)</code>","text":"<p>Initialize the class.</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>def __init__(\n    self,\n    output_stride: int = 2,\n    in_channels: int = 1,\n    kernel_size: int = 3,\n    stem_kernel_size: int = 7,\n    filters: int = 32,\n    filters_rate: int = 1.5,\n    down_blocks: int = 4,\n    up_blocks: int = 3,\n    stem_blocks: int = 0,\n    convs_per_block: int = 2,\n    middle_block: bool = True,\n    up_interpolate: bool = True,\n    block_contraction: bool = False,\n    stacks: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    super().__init__()\n\n    self.in_channels = in_channels\n    self.kernel_size = kernel_size\n    self.filters = filters\n    self.filters_rate = filters_rate\n    self.down_blocks = down_blocks\n    self.up_blocks = up_blocks\n    self.stem_blocks = stem_blocks\n    self.convs_per_block = convs_per_block\n    self.stem_kernel_size = stem_kernel_size\n    self.middle_block = middle_block\n    self.up_interpolate = up_interpolate\n    self.block_contraction = block_contraction\n    self.stacks = stacks\n\n    # Create stem block if stem_blocks &gt; 0\n    if self.stem_blocks &gt; 0:\n        self.stem = StemBlock(\n            in_channels=in_channels,\n            filters=filters,\n            stem_blocks=stem_blocks,\n            filters_rate=filters_rate,\n            convs_per_block=convs_per_block,\n            kernel_size=stem_kernel_size,\n            prefix=\"stem\",\n        )\n    else:\n        self.stem = None\n\n    # Initialize lists to store multiple encoders and decoders\n    self.encoders = nn.ModuleList()\n    self.decoders = nn.ModuleList()\n\n    for i in range(self.stacks):\n        # Create encoder for this stack\n        in_channels = (\n            int(self.filters * (self.filters_rate ** (self.stem_blocks)))\n            if self.stem_blocks &gt; 0\n            else in_channels\n        )\n        encoder = Encoder(\n            in_channels=in_channels,\n            filters=filters,\n            down_blocks=down_blocks,\n            filters_rate=filters_rate,\n            convs_per_block=convs_per_block,\n            kernel_size=kernel_size,\n            stem_blocks=stem_blocks,\n            prefix=f\"stack{i}_enc\",\n        )\n\n        # Create middle block separately (not part of encoder stack)\n        self.middle_blocks = nn.ModuleList()\n        # Get the last block filters from encoder\n        last_block_filters = int(\n            filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n        )\n        enc_num = len(encoder.encoder_stack)\n        if self.middle_block:\n            if convs_per_block &gt; 1:\n                # Middle expansion block\n                from sleap_nn.architectures.encoder_decoder import SimpleConvBlock\n\n                middle_expand = SimpleConvBlock(\n                    in_channels=last_block_filters,\n                    pool=False,\n                    pool_before_convs=False,\n                    pooling_stride=2,\n                    num_convs=convs_per_block - 1,\n                    filters=int(\n                        filters * (filters_rate ** (down_blocks + stem_blocks))\n                    ),\n                    kernel_size=kernel_size,\n                    use_bias=True,\n                    batch_norm=False,\n                    activation=\"relu\",\n                    prefix=f\"stack{i}_enc{enc_num}_middle_expand\",\n                )\n                enc_num += 1\n                self.middle_blocks.append(middle_expand)\n\n            # Middle contraction block\n            if self.block_contraction:\n                # Contract the channels with an exponent lower than the last encoder block\n                block_filters = int(last_block_filters)\n            else:\n                # Keep the block output filters the same\n                block_filters = int(\n                    filters * (filters_rate ** (down_blocks + stem_blocks))\n                )\n\n            middle_contract = SimpleConvBlock(\n                in_channels=int(\n                    filters * (filters_rate ** (down_blocks + stem_blocks))\n                ),\n                pool=False,\n                pool_before_convs=False,\n                pooling_stride=2,\n                num_convs=1,\n                filters=block_filters,\n                kernel_size=kernel_size,\n                use_bias=True,\n                batch_norm=False,\n                activation=\"relu\",\n                prefix=f\"stack{i}_enc{enc_num}_middle_contract\",\n            )\n            enc_num += 1\n            self.middle_blocks.append(middle_contract)\n\n        self.encoders.append(encoder)\n\n        # Calculate current stride for this encoder\n        # Start with stem stride if stem blocks exist\n        current_stride = 2**self.stem_blocks if self.stem_blocks &gt; 0 else 1\n\n        # Add encoder strides\n        for block in encoder.encoder_stack:\n            if hasattr(block, \"pool\") and block.pool:\n                current_stride *= block.pooling_stride\n\n        current_stride *= (\n            2  # for last pool layer MaxPool2dWithSamePadding in encoder\n        )\n\n        # Create decoder for this stack\n        if self.block_contraction:\n            # Contract the channels with an exponent lower than the last encoder block\n            x_in_shape = int(\n                filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n            )\n        else:\n            # Keep the block output filters the same\n            x_in_shape = int(\n                filters * (filters_rate ** (down_blocks + stem_blocks))\n            )\n        decoder = Decoder(\n            x_in_shape=x_in_shape,\n            current_stride=current_stride,\n            filters=filters,\n            up_blocks=up_blocks,\n            down_blocks=down_blocks,\n            filters_rate=filters_rate,\n            stem_blocks=stem_blocks,\n            output_stride=output_stride,\n            kernel_size=kernel_size,\n            block_contraction=self.block_contraction,\n            up_interpolate=up_interpolate,\n            prefix=f\"stack{i}_dec\",\n        )\n        self.decoders.append(decoder)\n\n    if len(self.decoders) and len(self.decoders[-1].decoder_stack):\n        self.final_dec_channels = (\n            self.decoders[-1].decoder_stack[-1].refine_convs_filters\n        )\n    else:\n        self.final_dec_channels = (\n            last_block_filters if not self.middle_block else block_filters\n        )\n\n    self.decoder_stride_to_filters = self.decoders[-1].stride_to_filters\n</code></pre>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the U-Net architecture.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tuple[List[Tensor], List]</code> <p>Output a tensor after applying the U-Net operations. current_strides: a list of the current strides from the decoder.</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[List[torch.Tensor], List]:\n    \"\"\"Forward pass through the U-Net architecture.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        x: Output a tensor after applying the U-Net operations.\n        current_strides: a list of the current strides from the decoder.\n    \"\"\"\n    # Process through stem block if it exists\n    stem_output = x\n    if self.stem is not None:\n        stem_output = self.stem(x)\n\n    # Process through all stacks\n    outputs = []\n    output = stem_output\n    for i in range(self.stacks):\n        # Get encoder and decoder for this stack\n        encoder = self.encoders[i]\n        decoder = self.decoders[i]\n\n        # Forward pass through encoder\n        encoded, features = encoder(output)\n\n        # Process through middle block if it exists\n        middle_output = encoded\n        if self.middle_block and hasattr(self, \"middle_blocks\"):\n            for middle_block in self.middle_blocks:\n                middle_output = middle_block(middle_output)\n\n        if self.stem_blocks &gt; 0:\n            features.append(stem_output)\n\n        output = decoder(middle_output, features)\n        output[\"middle_output\"] = middle_output\n        outputs.append(output)\n\n    return outputs[-1]\n</code></pre>"},{"location":"api/architectures/unet/#sleap_nn.architectures.unet.UNet.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create UNet from a config.</p> Source code in <code>sleap_nn/architectures/unet.py</code> <pre><code>@classmethod\ndef from_config(cls, config: OmegaConf):\n    \"\"\"Create UNet from a config.\"\"\"\n    stem_blocks = 0\n    if config.stem_stride is not None:\n        stem_blocks = np.log2(config.stem_stride).astype(int)\n    down_blocks = np.log2(config.max_stride).astype(int) - stem_blocks\n    up_blocks = (\n        np.log2(config.max_stride / config.output_stride).astype(int) + stem_blocks\n    )\n    return cls(\n        in_channels=config.in_channels,\n        kernel_size=config.kernel_size,\n        filters=config.filters,\n        filters_rate=config.filters_rate,\n        down_blocks=down_blocks,\n        up_blocks=up_blocks,\n        stem_blocks=stem_blocks,\n        convs_per_block=config.convs_per_block,\n        middle_block=config.middle_block,\n        up_interpolate=config.up_interpolate,\n        stacks=config.stacks,\n        output_stride=config.output_stride,\n    )\n</code></pre>"},{"location":"api/architectures/utils/","title":"utils","text":""},{"location":"api/architectures/utils/#sleap_nn.architectures.utils","title":"<code>sleap_nn.architectures.utils</code>","text":"<p>Miscellaneous utility functions for architectures and modeling.</p> <p>Functions:</p> Name Description <code>get_act_fn</code> <p>Get an instance of an activation function module based on the provided name.</p> <code>get_children_layers</code> <p>Recursively retrieves a flattened list of all children modules and submodules within the given model.</p>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_act_fn","title":"<code>get_act_fn(activation)</code>","text":"<p>Get an instance of an activation function module based on the provided name.</p> <p>This function returns an instance of a PyTorch activation function module corresponding to the given activation function name.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>str</code> <p>Name of the activation function. Supported values are 'relu', 'sigmoid', 'tanh', 'softmax', and 'identity'.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: An instance of the requested activation function module.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the provided activation function name is not one of the supported values.</p> Example Source code in <code>sleap_nn/architectures/utils.py</code> <pre><code>def get_act_fn(activation: str) -&gt; nn.Module:\n    \"\"\"Get an instance of an activation function module based on the provided name.\n\n    This function returns an instance of a PyTorch activation function module\n    corresponding to the given activation function name.\n\n    Args:\n        activation (str): Name of the activation function. Supported values are 'relu', 'sigmoid', 'tanh', 'softmax', and 'identity'.\n\n    Returns:\n        nn.Module: An instance of the requested activation function module.\n\n    Raises:\n        KeyError: If the provided activation function name is not one of the supported values.\n\n    Example:\n        # Get an instance of the ReLU activation function\n        relu_fn = get_act_fn('relu')\n\n        # Apply the activation function to an input tensor\n        input_tensor = torch.randn(1, 64, 64)\n        output = relu_fn(input_tensor)\n    \"\"\"\n    activations = {\n        \"relu\": nn.ReLU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"tanh\": nn.Tanh(),\n        \"softmax\": nn.Softmax(dim=-1),\n        \"identity\": nn.Identity(),\n    }\n\n    if activation not in activations:\n        message = f\"Unsupported activation function: {activation}. Supported activations are: {', '.join(activations.keys())}\"\n        logger.error(message)\n        raise KeyError(message)\n\n    return activations[activation]\n</code></pre>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_act_fn--get-an-instance-of-the-relu-activation-function","title":"Get an instance of the ReLU activation function","text":"<p>relu_fn = get_act_fn('relu')</p>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_act_fn--apply-the-activation-function-to-an-input-tensor","title":"Apply the activation function to an input tensor","text":"<p>input_tensor = torch.randn(1, 64, 64) output = relu_fn(input_tensor)</p>"},{"location":"api/architectures/utils/#sleap_nn.architectures.utils.get_children_layers","title":"<code>get_children_layers(model)</code>","text":"<p>Recursively retrieves a flattened list of all children modules and submodules within the given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to extract children from.</p> required <p>Returns:</p> Type Description <p>list of nn.Module: A flattened list containing all children modules and submodules.</p> Source code in <code>sleap_nn/architectures/utils.py</code> <pre><code>def get_children_layers(model: torch.nn.Module):\n    \"\"\"Recursively retrieves a flattened list of all children modules and submodules within the given model.\n\n    Args:\n        model: The PyTorch model to extract children from.\n\n    Returns:\n        list of nn.Module: A flattened list containing all children modules and submodules.\n    \"\"\"\n    children = list(model.children())\n    flattened_children = []\n    if len(children) == 0:\n        return model\n    else:\n        for child in children:\n            try:\n                flattened_children.extend(get_children_layers(child))\n            except TypeError:\n                flattened_children.append(get_children_layers(child))\n    return flattened_children\n</code></pre>"},{"location":"api/config/","title":"config","text":""},{"location":"api/config/#sleap_nn.config","title":"<code>sleap_nn.config</code>","text":"<p>Configuration modules for sleap-nn.</p> <p>Modules:</p> Name Description <code>data_config</code> <p>Serializable configuration classes for specifying all data configuration parameters.</p> <code>get_config</code> <p>This module contains functions to get the configuration for the data, model, and trainer.</p> <code>model_config</code> <p>Serializable configuration classes for specifying all model config parameters.</p> <code>trainer_config</code> <p>Serializable configuration classes for specifying all trainer config parameters.</p> <code>training_job_config</code> <p>Serializable configuration classes for specifying all training job parameters.</p> <code>utils</code> <p>Utilities for config building and validation.</p>"},{"location":"api/config/data_config/","title":"data_config","text":""},{"location":"api/config/data_config/#sleap_nn.config.data_config","title":"<code>sleap_nn.config.data_config</code>","text":"<p>Serializable configuration classes for specifying all data configuration parameters.</p> <p>These configuration classes are intended to specify all the parameters required to initialize the data config.</p> <p>Classes:</p> Name Description <code>AugmentationConfig</code> <p>Configuration of Augmentation.</p> <code>DataConfig</code> <p>Data configuration.</p> <code>GeometricConfig</code> <p>Configuration of Geometric (Optional).</p> <code>IntensityConfig</code> <p>Configuration of Intensity (Optional).</p> <code>PreprocessingConfig</code> <p>Configuration of Preprocessing.</p> <p>Functions:</p> Name Description <code>data_mapper</code> <p>Maps the legacy data configuration to the new data configuration.</p> <code>validate_proportion</code> <p>General Proportion Validation.</p> <code>validate_test_file_path</code> <p>Validate test_file_path to accept str or List[str].</p>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.AugmentationConfig","title":"<code>AugmentationConfig</code>","text":"<p>Configuration of Augmentation.</p> <p>Attributes:</p> Name Type Description <code>intensity</code> <code>Optional[IntensityConfig]</code> <p>Configuration options for intensity-based augmentations like brightness, contrast, etc. If None, no intensity augmentations will be applied.</p> <code>geometric</code> <code>Optional[GeometricConfig]</code> <p>Configuration options for geometric augmentations like rotation, scaling, translation etc. If None, no geometric augmentations will be applied.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass AugmentationConfig:\n    \"\"\"Configuration of Augmentation.\n\n    Attributes:\n        intensity: Configuration options for intensity-based augmentations like brightness, contrast, etc. If None, no intensity augmentations will be applied.\n        geometric: Configuration options for geometric augmentations like rotation, scaling, translation etc. If None, no geometric augmentations will be applied.\n    \"\"\"\n\n    intensity: Optional[IntensityConfig] = None\n    geometric: Optional[GeometricConfig] = None\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.DataConfig","title":"<code>DataConfig</code>","text":"<p>Data configuration.</p> <p>Attributes:</p> Name Type Description <code>train_labels_path</code> <code>Optional[List[str]]</code> <p>(List[str]) List of paths to training data (<code>.slp</code> file(s)). Default: <code>None</code>.</p> <code>val_labels_path</code> <code>Optional[List[str]]</code> <p>(List[str]) List of paths to validation data (<code>.slp</code> file(s)). Default: <code>None</code>.</p> <code>validation_fraction</code> <code>float</code> <p>(float) Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the <code>validation_labels</code> are already specified, this has no effect. Default: <code>0.1</code>.</p> <code>use_same_data_for_val</code> <code>bool</code> <p>(bool) If <code>True</code>, use the same data for both training and validation (train = val). Useful for intentional overfitting on small datasets. When enabled, <code>val_labels_path</code> and <code>validation_fraction</code> are ignored. Default: <code>False</code>.</p> <code>test_file_path</code> <code>Optional[Any]</code> <p>(str or List[str]) Path or list of paths to test dataset(s) (<code>.slp</code> file(s) or <code>.mp4</code> file(s)). Note: This is used only with CLI to get evaluation on test set after training is completed. Default: <code>None</code>.</p> <code>provider</code> <code>str</code> <p>(str) Provider class to read the input sleap files. Only \"LabelsReader\" is currently supported for the training pipeline. Default: <code>\"LabelsReader\"</code>.</p> <code>user_instances_only</code> <code>bool</code> <p>(bool) <code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used. Default: <code>True</code>.</p> <code>data_pipeline_fw</code> <code>str</code> <p>(str) Framework to create the data loaders. One of [<code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code>]. Default: <code>\"torch_dataset\"</code>. (Note: When using <code>torch_dataset</code>, <code>num_workers</code> in <code>trainer_config</code> should be set to 0 as multiprocessing doesn't work with pickling video backends.)</p> <code>cache_img_path</code> <code>Optional[str]</code> <p>(str) Path to save <code>.jpg</code> images created with <code>torch_dataset_cache_img_disk</code> data pipeline framework. If <code>None</code>, the path provided in <code>trainer_config.save_ckpt</code> is used. The <code>train_imgs</code> and <code>val_imgs</code> dirs are created inside this path. Default: <code>None</code>.</p> <code>use_existing_imgs</code> <code>bool</code> <p>(bool) Use existing train and val images/ chunks in the <code>cache_img_path</code> for <code>torch_dataset_cache_img_disk</code> frameworks. If <code>True</code>, the <code>cache_img_path</code> should have <code>train_imgs</code> and <code>val_imgs</code> dirs. Default: <code>False</code>.</p> <code>delete_cache_imgs_after_training</code> <code>bool</code> <p>(bool) If <code>False</code>, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. Default: <code>True</code>.</p> <code>parallel_caching</code> <code>bool</code> <p>(bool) If <code>True</code>, use parallel processing to cache images (significantly faster for large datasets). Default: <code>True</code>.</p> <code>cache_workers</code> <code>int</code> <p>(int) Number of worker threads for parallel caching. If 0, uses min(4, cpu_count). Default: <code>0</code>.</p> <code>preprocessing</code> <code>PreprocessingConfig</code> <p>Configuration options related to data preprocessing.</p> <code>use_augmentations_train</code> <code>bool</code> <p>(bool) True if the data augmentation should be applied to the training data, else False. Default: <code>True</code>.</p> <code>augmentation_config</code> <code>Optional[AugmentationConfig]</code> <p>Configurations related to augmentation. (only if <code>use_augmentations_train</code> is <code>True</code>)</p> <code>skeletons</code> <code>Optional[list]</code> <p>skeleton configuration for the <code>.slp</code> file. This will be pulled from the train dataset and saved to the <code>training_config.yaml</code></p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass DataConfig:\n    \"\"\"Data configuration.\n\n    Attributes:\n        train_labels_path: (List[str]) List of paths to training data (`.slp` file(s)). *Default*: `None`.\n        val_labels_path: (List[str]) List of paths to validation data (`.slp` file(s)). *Default*: `None`.\n        validation_fraction: (float) Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the `validation_labels` are already specified, this has no effect. *Default*: `0.1`.\n        use_same_data_for_val: (bool) If `True`, use the same data for both training and validation (train = val). Useful for intentional overfitting on small datasets. When enabled, `val_labels_path` and `validation_fraction` are ignored. *Default*: `False`.\n        test_file_path: (str or List[str]) Path or list of paths to test dataset(s) (`.slp` file(s) or `.mp4` file(s)). *Note*: This is used only with CLI to get evaluation on test set after training is completed. *Default*: `None`.\n        provider: (str) Provider class to read the input sleap files. Only \"LabelsReader\" is currently supported for the training pipeline. *Default*: `\"LabelsReader\"`.\n        user_instances_only: (bool) `True` if only user labeled instances should be used for training. If `False`, both user labeled and predicted instances would be used. *Default*: `True`.\n        data_pipeline_fw: (str) Framework to create the data loaders. One of [`torch_dataset`, `torch_dataset_cache_img_memory`, `torch_dataset_cache_img_disk`]. *Default*: `\"torch_dataset\"`. (Note: When using `torch_dataset`, `num_workers` in `trainer_config` should be set to 0 as multiprocessing doesn't work with pickling video backends.)\n        cache_img_path: (str) Path to save `.jpg` images created with `torch_dataset_cache_img_disk` data pipeline framework. If `None`, the path provided in `trainer_config.save_ckpt` is used. The `train_imgs` and `val_imgs` dirs are created inside this path. *Default*: `None`.\n        use_existing_imgs: (bool) Use existing train and val images/ chunks in the `cache_img_path` for `torch_dataset_cache_img_disk` frameworks. If `True`, the `cache_img_path` should have `train_imgs` and `val_imgs` dirs. *Default*: `False`.\n        delete_cache_imgs_after_training: (bool) If `False`, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. *Default*: `True`.\n        parallel_caching: (bool) If `True`, use parallel processing to cache images (significantly faster for large datasets). *Default*: `True`.\n        cache_workers: (int) Number of worker threads for parallel caching. If 0, uses min(4, cpu_count). *Default*: `0`.\n        preprocessing: Configuration options related to data preprocessing.\n        use_augmentations_train: (bool) True if the data augmentation should be applied to the training data, else False. *Default*: `True`.\n        augmentation_config: Configurations related to augmentation. (only if `use_augmentations_train` is `True`)\n        skeletons: skeleton configuration for the `.slp` file. This will be pulled from the train dataset and saved to the `training_config.yaml`\n    \"\"\"\n\n    train_labels_path: Optional[List[str]] = None\n    val_labels_path: Optional[List[str]] = None  # TODO : revisit MISSING!\n    validation_fraction: float = 0.1\n    use_same_data_for_val: bool = False\n    test_file_path: Optional[Any] = field(\n        default=None, validator=validate_test_file_path\n    )\n    provider: str = \"LabelsReader\"\n    user_instances_only: bool = True\n    data_pipeline_fw: str = \"torch_dataset\"\n    cache_img_path: Optional[str] = None\n    use_existing_imgs: bool = False\n    delete_cache_imgs_after_training: bool = True\n    parallel_caching: bool = True\n    cache_workers: int = 0\n    preprocessing: PreprocessingConfig = field(factory=PreprocessingConfig)\n    use_augmentations_train: bool = True\n    augmentation_config: Optional[AugmentationConfig] = field(\n        factory=lambda: AugmentationConfig(geometric=GeometricConfig())\n    )\n    skeletons: Optional[list] = None\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.GeometricConfig","title":"<code>GeometricConfig</code>","text":"<p>Configuration of Geometric (Optional).</p> <p>Attributes:</p> Name Type Description <code>rotation_min</code> <code>float</code> <p>(float) Minimum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. Default: <code>-15.0</code>.</p> <code>rotation_max</code> <code>float</code> <p>(float) Maximum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. Default: <code>15.0</code>.</p> <code>rotation_p</code> <code>Optional[float]</code> <p>(float, optional) Probability of applying random rotation independently. If set, rotation is applied separately from scale/translate. If <code>None</code>, falls back to <code>affine_p</code> for bundled behavior. Default: <code>1.0</code>.</p> <code>scale_min</code> <code>float</code> <p>(float) Minimum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. Default: <code>0.9</code>.</p> <code>scale_max</code> <code>float</code> <p>(float) Maximum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. Default: <code>1.1</code>.</p> <code>scale_p</code> <code>Optional[float]</code> <p>(float, optional) Probability of applying random scaling independently. If set, scaling is applied separately from rotation/translate. If <code>None</code>, falls back to <code>affine_p</code> for bundled behavior. Default: <code>1.0</code>.</p> <code>translate_width</code> <code>float</code> <p>(float) Maximum absolute fraction for horizontal translation. For example, if translate_width=a, then horizontal shift is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a. Will not translate by default. Default: <code>0.0</code>.</p> <code>translate_height</code> <code>float</code> <p>(float) Maximum absolute fraction for vertical translation. For example, if translate_height=a, then vertical shift is randomly sampled in the range -img_height * a &lt; dy &lt; img_height * a. Will not translate by default. Default: <code>0.0</code>.</p> <code>translate_p</code> <code>Optional[float]</code> <p>(float, optional) Probability of applying random translation independently. If set, translation is applied separately from rotation/scale. If <code>None</code>, falls back to <code>affine_p</code> for bundled behavior. Default: <code>None</code>.</p> <code>affine_p</code> <code>float</code> <p>(float) Probability of applying random affine transformations (rotation, scale, translate bundled together). Used for backwards compatibility when individual <code>*_p</code> params are not set. Default: <code>0.0</code>.</p> <code>erase_scale_min</code> <code>float</code> <p>(float) Minimum value of range of proportion of erased area against input image. Default: <code>0.0001</code>.</p> <code>erase_scale_max</code> <code>float</code> <p>(float) Maximum value of range of proportion of erased area against input image. Default: <code>0.01</code>.</p> <code>erase_ratio_min</code> <code>float</code> <p>(float) Minimum value of range of aspect ratio of erased area. Default: <code>1.0</code>.</p> <code>erase_ratio_max</code> <code>float</code> <p>(float) Maximum value of range of aspect ratio of erased area. Default: <code>1.0</code>.</p> <code>erase_p</code> <code>float</code> <p>(float) Probability of applying random erase. Default: <code>1.0</code>.</p> <code>mixup_lambda_min</code> <code>float</code> <p>(float) Minimum mixup strength value. Default: <code>0.01</code>.</p> <code>mixup_lambda_max</code> <code>float</code> <p>(float) Maximum mixup strength value. Default: <code>0.05</code>.</p> <code>mixup_p</code> <code>float</code> <p>(float) Probability of applying random mixup v2. Default: <code>0.0</code>.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass GeometricConfig:\n    \"\"\"Configuration of Geometric (Optional).\n\n    Attributes:\n        rotation_min: (float) Minimum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. *Default*: `-15.0`.\n        rotation_max: (float) Maximum rotation angle in degrees. A random angle in (rotation_min, rotation_max) will be sampled and applied to both images and keypoints. Set to 0 to disable rotation augmentation. *Default*: `15.0`.\n        rotation_p: (float, optional) Probability of applying random rotation independently. If set, rotation is applied separately from scale/translate. If `None`, falls back to `affine_p` for bundled behavior. *Default*: `1.0`.\n        scale_min: (float) Minimum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. *Default*: `0.9`.\n        scale_max: (float) Maximum scaling factor. If scale_min and scale_max are provided, the scale is randomly sampled from the range scale_min &lt;= scale &lt;= scale_max for isotropic scaling. *Default*: `1.1`.\n        scale_p: (float, optional) Probability of applying random scaling independently. If set, scaling is applied separately from rotation/translate. If `None`, falls back to `affine_p` for bundled behavior. *Default*: `1.0`.\n        translate_width: (float) Maximum absolute fraction for horizontal translation. For example, if translate_width=a, then horizontal shift is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a. Will not translate by default. *Default*: `0.0`.\n        translate_height: (float) Maximum absolute fraction for vertical translation. For example, if translate_height=a, then vertical shift is randomly sampled in the range -img_height * a &lt; dy &lt; img_height * a. Will not translate by default. *Default*: `0.0`.\n        translate_p: (float, optional) Probability of applying random translation independently. If set, translation is applied separately from rotation/scale. If `None`, falls back to `affine_p` for bundled behavior. *Default*: `None`.\n        affine_p: (float) Probability of applying random affine transformations (rotation, scale, translate bundled together). Used for backwards compatibility when individual `*_p` params are not set. *Default*: `0.0`.\n        erase_scale_min: (float) Minimum value of range of proportion of erased area against input image. *Default*: `0.0001`.\n        erase_scale_max: (float) Maximum value of range of proportion of erased area against input image. *Default*: `0.01`.\n        erase_ratio_min: (float) Minimum value of range of aspect ratio of erased area. *Default*: `1.0`.\n        erase_ratio_max: (float) Maximum value of range of aspect ratio of erased area. *Default*: `1.0`.\n        erase_p: (float) Probability of applying random erase. *Default*: `1.0`.\n        mixup_lambda_min: (float) Minimum mixup strength value. *Default*: `0.01`.\n        mixup_lambda_max: (float) Maximum mixup strength value. *Default*: `0.05`.\n        mixup_p: (float) Probability of applying random mixup v2. *Default*: `0.0`.\n    \"\"\"\n\n    rotation_min: float = field(default=-15.0, validator=validators.ge(-180))\n    rotation_max: float = field(default=15.0, validator=validators.le(180))\n    rotation_p: Optional[float] = field(default=1.0)\n    scale_min: float = field(default=0.9, validator=validators.ge(0))\n    scale_max: float = field(default=1.1, validator=validators.ge(0))\n    scale_p: Optional[float] = field(default=1.0)\n    translate_width: float = 0.0\n    translate_height: float = 0.0\n    translate_p: Optional[float] = field(default=None)\n    affine_p: float = field(default=0.0, validator=validate_proportion)\n    erase_scale_min: float = 0.0001\n    erase_scale_max: float = 0.01\n    erase_ratio_min: float = 1.0\n    erase_ratio_max: float = 1.0\n    erase_p: float = field(default=0.0, validator=validate_proportion)\n    mixup_lambda_min: float = field(default=0.01, validator=validators.ge(0))\n    mixup_lambda_max: float = field(default=0.05, validator=validators.le(1))\n    mixup_p: float = field(default=0.0, validator=validate_proportion)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.IntensityConfig","title":"<code>IntensityConfig</code>","text":"<p>Configuration of Intensity (Optional).</p> <p>Attributes:</p> Name Type Description <code>uniform_noise_min</code> <code>float</code> <p>(float) Minimum value for uniform noise (uniform_noise_min &gt;=0). Default: <code>0.0</code>.</p> <code>uniform_noise_max</code> <code>float</code> <p>(float) Maximum value for uniform noise (uniform_noise_max &lt;&gt;=1). Default: <code>1.0</code>.</p> <code>uniform_noise_p</code> <code>float</code> <p>(float) Probability of applying random uniform noise. Default: <code>0.0</code>.</p> <code>gaussian_noise_mean</code> <code>float</code> <p>(float) The mean of the gaussian noise distribution. Default: <code>0.0</code>.</p> <code>gaussian_noise_std</code> <code>float</code> <p>(float) The standard deviation of the gaussian noise distribution. Default: <code>1.0</code>.</p> <code>gaussian_noise_p</code> <code>float</code> <p>(float) Probability of applying random gaussian noise. Default: <code>0.0</code>.</p> <code>contrast_min</code> <code>float</code> <p>(float) Minimum contrast factor to apply. Default: <code>0.9</code>.</p> <code>contrast_max</code> <code>float</code> <p>(float) Maximum contrast factor to apply. Default: <code>1.1</code>.</p> <code>contrast_p</code> <code>float</code> <p>(float) Probability of applying random contrast. Default: <code>0.0</code>.</p> <code>brightness_min</code> <code>float</code> <p>(float) Minimum brightness factor to apply. Default: <code>1.0</code>.</p> <code>brightness_max</code> <code>float</code> <p>(float) Maximum brightness factor to apply. Default: <code>1.0</code>.</p> <code>brightness_p</code> <code>float</code> <p>(float) Probability of applying random brightness. Default: <code>0.0</code>.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass IntensityConfig:\n    \"\"\"Configuration of Intensity (Optional).\n\n    Attributes:\n        uniform_noise_min: (float) Minimum value for uniform noise (uniform_noise_min &gt;=0). *Default*: `0.0`.\n        uniform_noise_max: (float) Maximum value for uniform noise (uniform_noise_max &lt;&gt;=1). *Default*: `1.0`.\n        uniform_noise_p: (float) Probability of applying random uniform noise. *Default*: `0.0`.\n        gaussian_noise_mean: (float) The mean of the gaussian noise distribution. *Default*: `0.0`.\n        gaussian_noise_std: (float) The standard deviation of the gaussian noise distribution. *Default*: `1.0`.\n        gaussian_noise_p: (float) Probability of applying random gaussian noise. *Default*: `0.0`.\n        contrast_min: (float) Minimum contrast factor to apply. *Default*: `0.9`.\n        contrast_max: (float) Maximum contrast factor to apply. *Default*: `1.1`.\n        contrast_p: (float) Probability of applying random contrast. *Default*: `0.0`.\n        brightness_min: (float) Minimum brightness factor to apply. *Default*: `1.0`.\n        brightness_max: (float) Maximum brightness factor to apply. *Default*: `1.0`.\n        brightness_p: (float) Probability of applying random brightness. *Default*: `0.0`.\n    \"\"\"\n\n    uniform_noise_min: float = field(default=0.0, validator=validators.ge(0))\n    uniform_noise_max: float = field(default=1.0, validator=validators.le(1))\n    uniform_noise_p: float = field(default=0.0, validator=validate_proportion)\n    gaussian_noise_mean: float = 0.0\n    gaussian_noise_std: float = 1.0\n    gaussian_noise_p: float = field(default=0.0, validator=validate_proportion)\n    contrast_min: float = field(default=0.9, validator=validators.ge(0))\n    contrast_max: float = field(default=1.1, validator=validators.ge(0))\n    contrast_p: float = field(default=0.0, validator=validate_proportion)\n    brightness_min: float = field(default=1.0, validator=validators.ge(0))\n    brightness_max: float = field(default=1.0, validator=validators.le(2))\n    brightness_p: float = field(default=0.0, validator=validate_proportion)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.PreprocessingConfig","title":"<code>PreprocessingConfig</code>","text":"<p>Configuration of Preprocessing.</p> <p>Attributes:</p> Name Type Description <code>ensure_rgb</code> <code>bool</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to <code>True</code>, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: <code>False</code>.</p> <code>ensure_grayscale</code> <code>bool</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: <code>False</code>.</p> <code>max_height</code> <code>Optional[int]</code> <p>(int) Maximum height the original image should be resized and padded to. If not provided, the original image size will be retained. Default: <code>None</code>.</p> <code>max_width</code> <code>Optional[int]</code> <p>(int) Maximum width the original image should be resized and padded to. If not provided, the original image size will be retained. Default: <code>None</code>.</p> <code>scale</code> <code>float</code> <p>(float) Factor to resize the image dimensions by, specified as a float. Default: <code>1.0</code>.</p> <code>crop_size</code> <code>Optional[int]</code> <p>(int) Crop size of each instance for centered-instance model. If <code>None</code>, this would be automatically computed based on the largest instance in the <code>sio.Labels</code> file. If <code>scale</code> is provided, then the cropped image will be resized according to <code>scale</code>.Default: <code>None</code>.</p> <code>min_crop_size</code> <code>Optional[int]</code> <p>(int) Minimum crop size to be used if <code>crop_size</code> is <code>None</code>. Default: <code>100</code>.</p> <code>crop_padding</code> <code>Optional[int]</code> <p>(int) Padding in pixels to add around the instance bounding box when computing crop size. If <code>None</code>, padding is auto-computed based on augmentation settings (rotation/scale). Only used when <code>crop_size</code> is <code>None</code>. Default: <code>None</code>.</p> <p>Methods:</p> Name Description <code>validate_scale</code> <p>Scale Validation.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>@define\nclass PreprocessingConfig:\n    \"\"\"Configuration of Preprocessing.\n\n    Attributes:\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one channel when this is set to `True`, then the images from single-channel is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. *Default*: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this is set to True, then we convert the image to grayscale (single-channel) image. If the source image has only one channel and this is set to False, then we retain the single channel input. *Default*: `False`.\n        max_height: (int) Maximum height the original image should be resized and padded to. If not provided, the original image size will be retained. *Default*: `None`.\n        max_width: (int) Maximum width the original image should be resized and padded to. If not provided, the original image size will be retained. *Default*: `None`.\n        scale: (float) Factor to resize the image dimensions by, specified as a float. *Default*: `1.0`.\n        crop_size: (int) Crop size of each instance for centered-instance model. If `None`, this would be automatically computed based on the largest instance in the `sio.Labels` file.\n            If `scale` is provided, then the cropped image will be resized according to `scale`.*Default*: `None`.\n        min_crop_size: (int) Minimum crop size to be used if `crop_size` is `None`. *Default*: `100`.\n        crop_padding: (int) Padding in pixels to add around the instance bounding box when computing crop size.\n            If `None`, padding is auto-computed based on augmentation settings (rotation/scale).\n            Only used when `crop_size` is `None`. *Default*: `None`.\n    \"\"\"\n\n    ensure_rgb: bool = False\n    ensure_grayscale: bool = False\n    max_height: Optional[int] = None\n    max_width: Optional[int] = None\n    scale: float = field(\n        default=1.0, validator=lambda instance, attr, value: instance.validate_scale()\n    )\n    crop_size: Optional[int] = None\n    min_crop_size: Optional[int] = 100  # to help app work in case of error\n    crop_padding: Optional[int] = None\n\n    def validate_scale(self):\n        \"\"\"Scale Validation.\n\n        Ensures PreprocessingConfig's scale is a float&gt;=0 or list of floats&gt;=0\n        \"\"\"\n        if isinstance(self.scale, float) and self.scale &gt;= 0:\n            return\n        if isinstance(self.scale, list) and all(\n            isinstance(x, float) and x &gt;= 0 for x in self.scale\n        ):\n            return\n        message = \"PreprocessingConfig's scale must be a float or a list of floats.\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.PreprocessingConfig.validate_scale","title":"<code>validate_scale()</code>","text":"<p>Scale Validation.</p> <p>Ensures PreprocessingConfig's scale is a float&gt;=0 or list of floats&gt;=0</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>def validate_scale(self):\n    \"\"\"Scale Validation.\n\n    Ensures PreprocessingConfig's scale is a float&gt;=0 or list of floats&gt;=0\n    \"\"\"\n    if isinstance(self.scale, float) and self.scale &gt;= 0:\n        return\n    if isinstance(self.scale, list) and all(\n        isinstance(x, float) and x &gt;= 0 for x in self.scale\n    ):\n        return\n    message = \"PreprocessingConfig's scale must be a float or a list of floats.\"\n    logger.error(message)\n    raise ValueError(message)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.data_mapper","title":"<code>data_mapper(legacy_config)</code>","text":"<p>Maps the legacy data configuration to the new data configuration.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_config</code> <code>dict</code> <p>A dictionary containing the legacy data configuration.</p> required <p>Returns:</p> Type Description <code>DataConfig</code> <p>An instance of <code>DataConfig</code> with the mapped configuration.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>def data_mapper(legacy_config: dict) -&gt; DataConfig:\n    \"\"\"Maps the legacy data configuration to the new data configuration.\n\n    Args:\n        legacy_config: A dictionary containing the legacy data configuration.\n\n    Returns:\n        An instance of `DataConfig` with the mapped configuration.\n    \"\"\"\n    legacy_config_data = legacy_config.get(\"data\", {})\n    legacy_config_optimization = legacy_config.get(\"optimization\", {})\n    train_labels_path = legacy_config_data.get(\"labels\", {}).get(\n        \"training_labels\", None\n    )\n    val_labels_path = legacy_config_data.get(\"labels\", {}).get(\n        \"validation_labels\", None\n    )\n\n    # get skeleton(s)\n    json_skeletons = legacy_config_data.get(\"labels\", {}).get(\"skeletons\", None)\n    skeletons_list = None\n    if json_skeletons is not None:\n        skeletons_list = []\n        skeletons = SkeletonDecoder().decode(json_skeletons)\n        skeletons = yaml.safe_load(SkeletonYAMLEncoder().encode(skeletons))\n        for skl_name in skeletons.keys():\n            skl = skeletons[skl_name]\n            skl[\"name\"] = skl_name\n            skeletons_list.append(skl)\n\n    data_cfg_args = {}\n    preprocessing_args = {}\n    intensity_args = {}\n    geometric_args = {}\n\n    if train_labels_path is not None:\n        data_cfg_args[\"train_labels_path\"] = [train_labels_path]\n    if val_labels_path is not None:\n        data_cfg_args[\"val_labels_path\"] = [val_labels_path]\n    if (\n        legacy_config_data.get(\"labels\", {}).get(\"validation_fraction\", None)\n        is not None\n    ):\n        data_cfg_args[\"validation_fraction\"] = legacy_config_data[\"labels\"][\n            \"validation_fraction\"\n        ]\n    if legacy_config_data.get(\"labels\", {}).get(\"test_labels\", None) is not None:\n        data_cfg_args[\"test_file_path\"] = legacy_config_data[\"labels\"][\"test_labels\"]\n\n    # preprocessing\n    if legacy_config_data.get(\"preprocessing\", {}).get(\"ensure_rgb\", None) is not None:\n        preprocessing_args[\"ensure_rgb\"] = legacy_config_data[\"preprocessing\"][\n            \"ensure_rgb\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"ensure_grayscale\", None)\n        is not None\n    ):\n        preprocessing_args[\"ensure_grayscale\"] = legacy_config_data[\"preprocessing\"][\n            \"ensure_grayscale\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"target_height\", None)\n        is not None\n    ):\n        preprocessing_args[\"max_height\"] = legacy_config_data[\"preprocessing\"][\n            \"target_height\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"target_width\", None)\n        is not None\n    ):\n        preprocessing_args[\"max_width\"] = legacy_config_data[\"preprocessing\"][\n            \"target_width\"\n        ]\n    if (\n        legacy_config_data.get(\"preprocessing\", {}).get(\"input_scaling\", None)\n        is not None\n    ):\n        preprocessing_args[\"scale\"] = legacy_config_data[\"preprocessing\"][\n            \"input_scaling\"\n        ]\n    if (\n        legacy_config_data.get(\"instance_cropping\", {}).get(\"crop_size\", None)\n        is not None\n    ):\n        size = legacy_config_data[\"instance_cropping\"][\"crop_size\"]\n        preprocessing_args[\"crop_size\"] = size\n\n    # augmentation\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"uniform_noise_min_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"uniform_noise_min\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"uniform_noise_min_val\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"uniform_noise_max_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"uniform_noise_max\"] = min(\n            legacy_config_optimization[\"augmentation_config\"][\"uniform_noise_max_val\"],\n            1.0,\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"uniform_noise\", None\n        )\n        is not None\n    ):\n        intensity_args[\"uniform_noise_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"uniform_noise\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"gaussian_noise_mean\", None\n        )\n        is not None\n    ):\n        intensity_args[\"gaussian_noise_mean\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"gaussian_noise_mean\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"gaussian_noise_stddev\", None\n        )\n        is not None\n    ):\n        intensity_args[\"gaussian_noise_std\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"gaussian_noise_stddev\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"gaussian_noise\", None\n        )\n        is not None\n    ):\n        intensity_args[\"gaussian_noise_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"gaussian_noise\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"contrast_min_gamma\", None\n        )\n        is not None\n    ):\n        intensity_args[\"contrast_min\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"contrast_min_gamma\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"contrast_max_gamma\", None\n        )\n        is not None\n    ):\n        intensity_args[\"contrast_max\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"contrast_max_gamma\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"contrast\", None)\n        is not None\n    ):\n        intensity_args[\"contrast_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"contrast\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"brightness_min_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"brightness_min\"] = min(\n            legacy_config_optimization[\"augmentation_config\"][\"brightness_min_val\"], 2.0\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"brightness_max_val\", None\n        )\n        is not None\n    ):\n        intensity_args[\"brightness_max\"] = min(\n            legacy_config_optimization[\"augmentation_config\"][\"brightness_max_val\"], 2.0\n        )  # kornia brightness_max can only be 2.0\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"brightness\", None\n        )\n        is not None\n    ):\n        intensity_args[\"brightness_p\"] = float(\n            legacy_config_optimization[\"augmentation_config\"][\"brightness\"]\n        )\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"rotation_min_angle\", None\n        )\n        is not None\n    ):\n        geometric_args[\"rotation_min\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"rotation_min_angle\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\n            \"rotation_max_angle\", None\n        )\n        is not None\n    ):\n        geometric_args[\"rotation_max\"] = legacy_config_optimization[\n            \"augmentation_config\"\n        ][\"rotation_max_angle\"]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"scale_min\", None)\n        is not None\n    ):\n        geometric_args[\"scale_min\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_min\"\n        ]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"scale_max\", None)\n        is not None\n    ):\n        geometric_args[\"scale_max\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_max\"\n        ]\n\n    if (\n        legacy_config_optimization.get(\"augmentation_config\", {}).get(\"scale\", None)\n        is not None\n    ):\n        geometric_args[\"scale_min\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_min\"\n        ]\n        geometric_args[\"scale_max\"] = legacy_config_optimization[\"augmentation_config\"][\n            \"scale_max\"\n        ]\n\n    geometric_args[\"affine_p\"] = (\n        1.0\n        if any(\n            [\n                legacy_config_optimization.get(\"augmentation_config\", {}).get(\n                    \"rotate\", False\n                ),\n                legacy_config_optimization.get(\"augmentation_config\", {}).get(\n                    \"scale\", False\n                ),\n            ]\n        )\n        else 0.0\n    )\n\n    data_cfg_args[\"preprocessing\"] = PreprocessingConfig(**preprocessing_args)\n    data_cfg_args[\"augmentation_config\"] = AugmentationConfig(\n        intensity=IntensityConfig(**intensity_args),\n        geometric=GeometricConfig(**geometric_args),\n    )\n\n    data_cfg_args[\"skeletons\"] = (\n        skeletons_list\n        if skeletons_list is not None and len(skeletons_list) &gt; 0\n        else None\n    )\n\n    return DataConfig(**data_cfg_args)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.validate_proportion","title":"<code>validate_proportion(instance, attribute, value)</code>","text":"<p>General Proportion Validation.</p> <p>Ensures all proportions are a 0&lt;=float&lt;=1.0</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>def validate_proportion(instance, attribute, value):\n    \"\"\"General Proportion Validation.\n\n    Ensures all proportions are a 0&lt;=float&lt;=1.0\n    \"\"\"\n    if not (0.0 &lt;= value &lt;= 1.0):\n        message = f\"{attribute.name} must be between 0.0 and 1.0, got {value}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/data_config/#sleap_nn.config.data_config.validate_test_file_path","title":"<code>validate_test_file_path(instance, attribute, value)</code>","text":"<p>Validate test_file_path to accept str or List[str].</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <p>The instance being validated.</p> required <code>attribute</code> <p>The attribute being validated.</p> required <code>value</code> <p>The value to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If value is not None, str, or list of strings.</p> Source code in <code>sleap_nn/config/data_config.py</code> <pre><code>def validate_test_file_path(instance, attribute, value):\n    \"\"\"Validate test_file_path to accept str or List[str].\n\n    Args:\n        instance: The instance being validated.\n        attribute: The attribute being validated.\n        value: The value to validate.\n\n    Raises:\n        ValueError: If value is not None, str, or list of strings.\n    \"\"\"\n    if value is None:\n        return\n    if isinstance(value, str):\n        return\n    if isinstance(value, (list, tuple)) and all(isinstance(p, str) for p in value):\n        return\n    message = f\"{attribute.name} must be a string or list of strings, got {type(value).__name__}\"\n    logger.error(message)\n    raise ValueError(message)\n</code></pre>"},{"location":"api/config/get_config/","title":"get_config","text":""},{"location":"api/config/get_config/#sleap_nn.config.get_config","title":"<code>sleap_nn.config.get_config</code>","text":"<p>This module contains functions to get the configuration for the data, model, and trainer.</p> <p>Functions:</p> Name Description <code>get_aug_config</code> <p>Create an augmentation configuration for training data.</p> <code>get_backbone_config</code> <p>Create a backbone configuration for neural network architecture.</p> <code>get_data_config</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p> <code>get_head_configs</code> <p>Create head configurations for pose estimation model outputs.</p> <code>get_model_config</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p> <code>get_trainer_config</code> <p>Train a pose-estimation model with SLEAP-NN framework.</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config","title":"<code>get_aug_config(intensity_aug=None, geometric_aug=None)</code>","text":"<p>Create an augmentation configuration for training data.</p> <p>This method creates an <code>AugmentationConfig</code> object based on the user-provided parameters for intensity and geometric augmentations. The function supports both string-based preset configurations and custom dictionary-based configurations.</p> <p>Parameters:</p> Name Type Description Default <code>intensity_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>Intensity augmentation configuration. Can be: - String: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom configuration matching <code>IntensityConfig</code> structure - None: No intensity augmentation applied</p> <code>None</code> <code>geometric_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>Geometric augmentation configuration. Can be: - String: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom configuration matching <code>GeometricConfig</code> structure - None: No geometric augmentation applied</p> <code>None</code> <p>Returns:</p> Name Type Description <code>AugmentationConfig</code> <p>Configured augmentation object with intensity and geometric settings.</p> <p>Examples:</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config--string-based-configuration","title":"String-based configuration","text":"<p>aug_config = get_aug_config(\"contrast\", \"rotation\")</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config--list-based-configuration","title":"List-based configuration","text":"<p>aug_config = get_aug_config([\"contrast\", \"brightness\"], [\"scale\", \"translate\"])</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_aug_config--dictionary-based-configuration","title":"Dictionary-based configuration","text":"<p>intensity_dict = {     \"uniform_noise_min\": 0.0,     \"uniform_noise_max\": 0.1,     \"uniform_noise_p\": 0.5,     \"contrast_p\": 1.0 } geometric_dict = {     \"rotation\": 15.0,     \"scale\": (0.9, 1.1),     \"affine_p\": 1.0 } aug_config = get_aug_config(intensity_dict, geometric_dict)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid augmentation options are provided.</p> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_aug_config(\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n):\n    \"\"\"Create an augmentation configuration for training data.\n\n    This method creates an `AugmentationConfig` object based on the user-provided parameters\n    for intensity and geometric augmentations. The function supports both string-based\n    preset configurations and custom dictionary-based configurations.\n\n    Args:\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"]\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom configuration matching `IntensityConfig` structure\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"]\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom configuration matching `GeometricConfig` structure\n            - None: No geometric augmentation applied\n\n    Returns:\n        AugmentationConfig: Configured augmentation object with intensity and geometric settings.\n\n    Examples:\n        # String-based configuration\n        aug_config = get_aug_config(\"contrast\", \"rotation\")\n\n        # List-based configuration\n        aug_config = get_aug_config([\"contrast\", \"brightness\"], [\"scale\", \"translate\"])\n\n        # Dictionary-based configuration\n        intensity_dict = {\n            \"uniform_noise_min\": 0.0,\n            \"uniform_noise_max\": 0.1,\n            \"uniform_noise_p\": 0.5,\n            \"contrast_p\": 1.0\n        }\n        geometric_dict = {\n            \"rotation\": 15.0,\n            \"scale\": (0.9, 1.1),\n            \"affine_p\": 1.0\n        }\n        aug_config = get_aug_config(intensity_dict, geometric_dict)\n\n    Raises:\n        ValueError: If invalid augmentation options are provided.\n    \"\"\"\n    aug_config = AugmentationConfig(\n        intensity=IntensityConfig(), geometric=GeometricConfig()\n    )\n    if isinstance(intensity_aug, str) or isinstance(intensity_aug, list):\n        if isinstance(intensity_aug, str):\n            intensity_aug = [intensity_aug]\n\n        for i in intensity_aug:\n            if i == \"uniform_noise\":\n                aug_config.intensity.uniform_noise_p = 1.0\n            elif i == \"gaussian_noise\":\n                aug_config.intensity.gaussian_noise_p = 1.0\n            elif i == \"contrast\":\n                aug_config.intensity.contrast_p = 1.0\n            elif i == \"brightness\":\n                aug_config.intensity.brightness_p = 1.0\n            else:\n                raise ValueError(\n                    f\"`{intensity_aug}` is not a valid intensity augmentation option. Please use one of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\"\n                )\n\n    elif isinstance(intensity_aug, dict):\n        aug_config.intensity = IntensityConfig(**intensity_aug)\n\n    if isinstance(geometric_aug, str) or isinstance(geometric_aug, list):\n        if isinstance(geometric_aug, str):\n            geometric_aug = [geometric_aug]\n\n        for g in geometric_aug:\n            if g == \"rotation\":\n                # Use new independent rotation probability\n                aug_config.geometric.rotation_p = 1.0\n            elif g == \"scale\":\n                # Use new independent scale probability\n                aug_config.geometric.scale_min = 0.9\n                aug_config.geometric.scale_max = 1.1\n                aug_config.geometric.scale_p = 1.0\n            elif g == \"translate\":\n                # Use new independent translate probability\n                aug_config.geometric.translate_height = 0.2\n                aug_config.geometric.translate_width = 0.2\n                aug_config.geometric.translate_p = 1.0\n            elif g == \"erase_scale\":\n                aug_config.geometric.erase_p = 1.0\n            elif g == \"mixup\":\n                aug_config.geometric.mixup_p = 1.0\n            else:\n                raise ValueError(\n                    f\"`{geometric_aug}` is not a valid geometric augmentation option. Please use one of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\"\n                )\n\n    elif isinstance(geometric_aug, dict):\n        aug_config.geometric = GeometricConfig(**geometric_aug)\n\n    return aug_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_backbone_config","title":"<code>get_backbone_config(backbone_cfg)</code>","text":"<p>Create a backbone configuration for neural network architecture.</p> <p>This method creates a <code>BackboneConfig</code> object based on the user-provided parameters for the neural network backbone architecture. The function supports both string-based preset configurations and custom dictionary-based configurations for UNet, ConvNeXt, and Swin Transformer architectures.</p> <p>Parameters:</p> Name Type Description Default <code>backbone_cfg</code> <code>Union[str, Dict[str, Any]]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary.</p> required <p>Returns:</p> Name Type Description <code>BackboneConfig</code> <p>Configured backbone object with architecture-specific settings.</p> <p>Examples:</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_backbone_config--string-based-configuration","title":"String-based configuration","text":"<p>backbone_config = get_backbone_config(\"unet\") backbone_config = get_backbone_config(\"convnext_tiny\") backbone_config = get_backbone_config(\"swint_base\")</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_backbone_config--dictionary-based-configuration","title":"Dictionary-based configuration","text":"<p>unet_dict = {     \"unet\": {         \"in_channels\": 3,         \"filters\": 64,         \"max_stride\": 32,         \"output_stride\": 2,         \"kernel_size\": 3,         \"filters_rate\": 2.0     } } backbone_config = get_backbone_config(unet_dict)</p> <p>convnext_dict = {     \"convnext\": {         \"model_type\": \"tiny\",         \"in_channels\": 3,         \"pre_trained_weights\": \"ConvNeXt_Tiny_Weights\"     } } backbone_config = get_backbone_config(convnext_dict)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid backbone type is provided.</p> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_backbone_config(backbone_cfg: Union[str, Dict[str, Any]]):\n    \"\"\"Create a backbone configuration for neural network architecture.\n\n    This method creates a `BackboneConfig` object based on the user-provided parameters\n    for the neural network backbone architecture. The function supports both string-based\n    preset configurations and custom dictionary-based configurations for UNet, ConvNeXt,\n    and Swin Transformer architectures.\n\n    Args:\n        backbone_cfg: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n\n    Returns:\n        BackboneConfig: Configured backbone object with architecture-specific settings.\n\n    Examples:\n        # String-based configuration\n        backbone_config = get_backbone_config(\"unet\")\n        backbone_config = get_backbone_config(\"convnext_tiny\")\n        backbone_config = get_backbone_config(\"swint_base\")\n\n        # Dictionary-based configuration\n        unet_dict = {\n            \"unet\": {\n                \"in_channels\": 3,\n                \"filters\": 64,\n                \"max_stride\": 32,\n                \"output_stride\": 2,\n                \"kernel_size\": 3,\n                \"filters_rate\": 2.0\n            }\n        }\n        backbone_config = get_backbone_config(unet_dict)\n\n        convnext_dict = {\n            \"convnext\": {\n                \"model_type\": \"tiny\",\n                \"in_channels\": 3,\n                \"pre_trained_weights\": \"ConvNeXt_Tiny_Weights\"\n            }\n        }\n        backbone_config = get_backbone_config(convnext_dict)\n\n    Raises:\n        ValueError: If invalid backbone type is provided.\n    \"\"\"\n    backbone_config = BackboneConfig()\n    unet_config_mapper = {\n        \"unet\": UNetConfig(),\n        \"unet_medium_rf\": UNetMediumRFConfig(),\n        \"unet_large_rf\": UNetLargeRFConfig(),\n    }\n    convnext_config_mapper = {\n        \"convnext\": ConvNextConfig(),\n        \"convnext_tiny\": ConvNextConfig(),\n        \"convnext_small\": ConvNextSmallConfig(),\n        \"convnext_base\": ConvNextBaseConfig(),\n        \"convnext_large\": ConvNextLargeConfig(),\n    }\n    swint_config_mapper = {\n        \"swint\": SwinTConfig(),\n        \"swint_tiny\": SwinTConfig(),\n        \"swint_small\": SwinTSmallConfig(),\n        \"swint_base\": SwinTBaseConfig(),\n    }\n    if isinstance(backbone_cfg, str):\n        if backbone_cfg.startswith(\"unet\"):\n            backbone_config.unet = unet_config_mapper[backbone_cfg]\n        elif backbone_cfg.startswith(\"convnext\"):\n            backbone_config.convnext = convnext_config_mapper[backbone_cfg]\n        elif backbone_cfg.startswith(\"swint\"):\n            backbone_config.swint = swint_config_mapper[backbone_cfg]\n        else:\n            raise ValueError(\n                f\"{backbone_cfg} is not a valid backbone. Please choose one of ['unet', 'unet_medium_rf', 'unet_large_rf', 'convnext', 'convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large', 'swint', 'swint_tiny', 'swint_small', 'swint_base']\"\n            )\n\n    elif isinstance(backbone_cfg, dict):\n        backbone_config = BackboneConfig()\n        if \"unet\" in backbone_cfg:\n            backbone_config.unet = UNetConfig(**backbone_cfg[\"unet\"])\n        elif \"convnext\" in backbone_cfg:\n            backbone_config.convnext = ConvNextConfig(**backbone_cfg[\"convnext\"])\n        elif \"swint\" in backbone_cfg:\n            backbone_config.swint = SwinTConfig(**backbone_cfg[\"swint\"])\n\n    return backbone_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_data_config","title":"<code>get_data_config(train_labels_path=None, val_labels_path=None, validation_fraction=0.1, use_same_data_for_val=False, test_file_path=None, provider='LabelsReader', user_instances_only=True, data_pipeline_fw='torch_dataset', cache_img_path=None, use_existing_imgs=False, delete_cache_imgs_after_training=True, ensure_rgb=False, ensure_grayscale=False, scale=1.0, max_height=None, max_width=None, crop_size=None, min_crop_size=100, crop_padding=None, use_augmentations_train=True, intensity_aug=None, geometry_aug='rotation')</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to training data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>val_labels_path</code> <code>Optional[List[str]]</code> <p>List of paths to validation data (<code>.slp</code> file). Default: <code>None</code></p> <code>None</code> <code>validation_fraction</code> <code>float</code> <p>Float between 0 and 1 specifying the fraction of the training set to sample for generating the validation set. The remaining labeled frames will be left in the training set. If the <code>validation_labels</code> are already specified, this has no effect. Default: 0.1.</p> <code>0.1</code> <code>use_same_data_for_val</code> <code>bool</code> <p>If <code>True</code>, use the same data for both training and validation (train = val). Useful for intentional overfitting on small datasets. When enabled, <code>val_labels_path</code> and <code>validation_fraction</code> are ignored. Default: False.</p> <code>False</code> <code>test_file_path</code> <code>Optional[Union[str, List[str]]]</code> <p>Path or list of paths to test dataset(s) (<code>.slp</code> file(s) or <code>.mp4</code> file(s)). Note: This is used to get evaluation on test set after training is completed.</p> <code>None</code> <code>provider</code> <code>str</code> <p>Provider class to read the input sleap files. Only \"LabelsReader\" supported for the training pipeline. Default: \"LabelsReader\".</p> <code>'LabelsReader'</code> <code>user_instances_only</code> <code>bool</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used. Default: <code>True</code>.</p> <code>True</code> <code>data_pipeline_fw</code> <code>str</code> <p>Framework to create the data loaders. One of [<code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code>]. Default: \"torch_dataset\".</p> <code>'torch_dataset'</code> <code>cache_img_path</code> <code>Optional[str]</code> <p>Path to save <code>.jpg</code> images created with <code>torch_dataset_cache_img_disk</code> data pipeline framework. If <code>None</code>, the path provided in <code>trainer_config.save_ckpt</code> is used (else working dir is used). The <code>train_imgs</code> and <code>val_imgs</code> dirs are created inside this path. Default: None.</p> <code>None</code> <code>use_existing_imgs</code> <code>bool</code> <p>Use existing train and val images/ chunks in the <code>cache_img_path</code> for <code>torch_dataset_cache_img_disk</code> frameworks. If <code>True</code>, the <code>cache_img_path</code> should have <code>train_imgs</code> and <code>val_imgs</code> dirs. Default: False.</p> <code>False</code> <code>delete_cache_imgs_after_training</code> <code>bool</code> <p>If <code>False</code>, the images (torch_dataset_cache_img_disk) are retained after training. Else, the files are deleted. Default: True.</p> <code>True</code> <code>ensure_rgb</code> <code>bool</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>False</code> <code>is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> required <code>ensure_grayscale</code> <code>bool</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>False</code> <code>image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> required <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>1.0</code> <code>max_height</code> <code>Optional[int]</code> <p>Maximum height the original image should be resized and padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>Maximum width the original image should be resized and padded to. If not provided, the original image size will be retained. Default: None.</p> <code>None</code> <code>crop_size</code> <code>Optional[int]</code> <p>Crop size of each instance for centered-instance model. If <code>None</code>, this would be automatically computed based on the largest instance in the <code>sio.Labels</code> file. If <code>scale</code> is provided, then the cropped image will be resized according to <code>scale</code>. Default: None.</p> <code>None</code> <code>min_crop_size</code> <code>Optional[int]</code> <p>Minimum crop size to be used if <code>crop_size</code> is <code>None</code>. Default: 100.</p> <code>100</code> <code>crop_padding</code> <code>Optional[int]</code> <p>Padding in pixels to add around instance bounding box when computing crop size. If <code>None</code>, padding is auto-computed based on augmentation settings. Only used when <code>crop_size</code> is <code>None</code>. Default: None.</p> <code>None</code> <code>use_augmentations_train</code> <code>bool</code> <p>True if the data augmentation should be applied to the training data, else False. Default: True.</p> <code>True</code> <code>intensity_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"] or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.IntensityConfig</code>. For eg: {             \"uniform_noise_min\": 1.0,             \"uniform_noise_p\": 1.0         }</p> <code>None</code> <code>geometry_aug</code> <code>Optional[Union[str, List[str], Dict[str, Any]]]</code> <p>One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"]. or list of strings from the above allowed values. To have custom values, pass a dict with the structure in <code>sleap_nn.config.data_config.GeometryConfig</code>. For eg: {             \"rotation_min\": -45,             \"rotation_max\": 45,             \"affine_p\": 1.0         }</p> <code>'rotation'</code> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_data_config(\n    train_labels_path: Optional[List[str]] = None,\n    val_labels_path: Optional[List[str]] = None,\n    validation_fraction: float = 0.1,\n    use_same_data_for_val: bool = False,\n    test_file_path: Optional[Union[str, List[str]]] = None,\n    provider: str = \"LabelsReader\",\n    user_instances_only: bool = True,\n    data_pipeline_fw: str = \"torch_dataset\",\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    delete_cache_imgs_after_training: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    scale: float = 1.0,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n    crop_size: Optional[int] = None,\n    min_crop_size: Optional[int] = 100,\n    crop_padding: Optional[int] = None,\n    use_augmentations_train: bool = True,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometry_aug: Optional[Union[str, List[str], Dict[str, Any]]] = \"rotation\",\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        train_labels_path: List of paths to training data (`.slp` file). Default: `None`\n        val_labels_path: List of paths to validation data (`.slp` file). Default: `None`\n        validation_fraction: Float between 0 and 1 specifying the fraction of the\n            training set to sample for generating the validation set. The remaining\n            labeled frames will be left in the training set. If the `validation_labels`\n            are already specified, this has no effect. Default: 0.1.\n        use_same_data_for_val: If `True`, use the same data for both training and\n            validation (train = val). Useful for intentional overfitting on small\n            datasets. When enabled, `val_labels_path` and `validation_fraction` are\n            ignored. Default: False.\n        test_file_path: Path or list of paths to test dataset(s) (`.slp` file(s) or `.mp4` file(s)).\n            Note: This is used to get evaluation on test set after training is completed.\n        provider: Provider class to read the input sleap files. Only \"LabelsReader\"\n            supported for the training pipeline. Default: \"LabelsReader\".\n        user_instances_only: `True` if only user labeled instances should be used for\n            training. If `False`, both user labeled and predicted instances would be used.\n            Default: `True`.\n        data_pipeline_fw: Framework to create the data loaders. One of [`torch_dataset`,\n            `torch_dataset_cache_img_memory`, `torch_dataset_cache_img_disk`]. Default: \"torch_dataset\".\n        cache_img_path: Path to save `.jpg` images created with `torch_dataset_cache_img_disk` data pipeline\n            framework. If `None`, the path provided in `trainer_config.save_ckpt` is used (else working dir is used). The `train_imgs` and `val_imgs` dirs are created inside this path. Default: None.\n        use_existing_imgs: Use existing train and val images/ chunks in the `cache_img_path` for `torch_dataset_cache_img_disk` frameworks. If `True`, the `cache_img_path` should have `train_imgs` and `val_imgs` dirs.\n            Default: False.\n        delete_cache_imgs_after_training: If `False`, the images (torch_dataset_cache_img_disk) are\n            retained after training. Else, the files are deleted. Default: True.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        max_height: Maximum height the original image should be resized and padded to. If not provided, the\n            original image size will be retained. Default: None.\n        max_width: Maximum width the original image should be resized and padded to. If not provided, the\n            original image size will be retained. Default: None.\n        crop_size: Crop size of each instance for centered-instance model.\n            If `None`, this would be automatically computed based on the largest instance\n            in the `sio.Labels` file. If `scale` is provided, then the cropped image will be resized according to `scale`. Default: None.\n        min_crop_size: Minimum crop size to be used if `crop_size` is `None`. Default: 100.\n        crop_padding: Padding in pixels to add around instance bounding box when computing\n            crop size. If `None`, padding is auto-computed based on augmentation settings.\n            Only used when `crop_size` is `None`. Default: None.\n        use_augmentations_train: True if the data augmentation should be applied to the\n            training data, else False. Default: True.\n        intensity_aug: One of [\"uniform_noise\", \"gaussian_noise\", \"contrast\", \"brightness\"]\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.IntensityConfig`.\n            For eg: {\n                        \"uniform_noise_min\": 1.0,\n                        \"uniform_noise_p\": 1.0\n                    }\n        geometry_aug: One of [\"rotation\", \"scale\", \"translate\", \"erase_scale\", \"mixup\"].\n            or list of strings from the above allowed values. To have custom values, pass\n            a dict with the structure in `sleap_nn.config.data_config.GeometryConfig`.\n            For eg: {\n                        \"rotation_min\": -45,\n                        \"rotation_max\": 45,\n                        \"affine_p\": 1.0\n                    }\n    \"\"\"\n    preprocessing_config = PreprocessingConfig(\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        max_height=max_height,\n        max_width=max_width,\n        scale=scale,\n        crop_size=crop_size,\n        min_crop_size=min_crop_size,\n        crop_padding=crop_padding,\n    )\n    augmentation_config = None\n    if use_augmentations_train:\n        augmentation_config = get_aug_config(\n            intensity_aug=intensity_aug, geometric_aug=geometry_aug\n        )\n\n    # construct data config\n    data_config = DataConfig(\n        train_labels_path=train_labels_path,\n        val_labels_path=val_labels_path,\n        validation_fraction=validation_fraction,\n        use_same_data_for_val=use_same_data_for_val,\n        test_file_path=test_file_path,\n        provider=provider,\n        user_instances_only=user_instances_only,\n        data_pipeline_fw=data_pipeline_fw,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        delete_cache_imgs_after_training=delete_cache_imgs_after_training,\n        preprocessing=preprocessing_config,\n        use_augmentations_train=use_augmentations_train,\n        augmentation_config=augmentation_config,\n    )\n\n    return data_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_head_configs","title":"<code>get_head_configs(head_cfg)</code>","text":"<p>Create head configurations for pose estimation model outputs.</p> <p>This method creates a <code>HeadConfig</code> object based on the user-provided parameters for the pose estimation model head layers. The function supports both string-based preset configurations and custom dictionary-based configurations for different model types including Single Instance, Centroid, Centered Instance, Bottom-Up, and Multi-Class variants.</p> <p>Parameters:</p> Name Type Description Default <code>head_cfg</code> <code>Union[str, Dict[str, Any]]</code> <p>Head configuration. Can be: - String: One of the preset head types:     - [\"single_instance\", \"centroid\", \"centered_instance\", \"bottomup\", \"multi_class_bottomup\", \"multi_class_topdown\"] - Dictionary: Custom configuration with structure:     {         \"single_instance\": {             \"confmaps\": {SingleInstanceConfMapsConfig parameters}         },         \"centroid\": {             \"confmaps\": {CentroidConfMapsConfig parameters}         },         \"centered_instance\": {             \"confmaps\": {CenteredInstanceConfMapsConfig parameters}         },         \"bottomup\": {             \"confmaps\": {BottomUpConfMapsConfig parameters},             \"pafs\": {PAFConfig parameters}         },         \"multi_class_bottomup\": {             \"confmaps\": {BottomUpConfMapsConfig parameters},             \"class_maps\": {ClassMapConfig parameters}         },         \"multi_class_topdown\": {             \"confmaps\": {CenteredInstanceConfMapsConfig parameters},             \"class_vectors\": {ClassVectorsConfig parameters}         }     }     Only one head type should be specified in the dictionary.</p> required <p>Returns:</p> Name Type Description <code>HeadConfig</code> <p>Configured head object with model-specific settings.</p> <p>Examples:</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_head_configs--string-based-configuration","title":"String-based configuration","text":"<p>head_configs = get_head_configs(\"single_instance\") head_configs = get_head_configs(\"bottomup\") head_configs = get_head_configs(\"multi_class_topdown\")</p>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_head_configs--dictionary-based-configuration","title":"Dictionary-based configuration","text":"<p>single_instance_dict = {     \"single_instance\": {         \"confmaps\": {             \"part_names\": [\"head\", \"tail\"],             \"sigma\": 2.5,             \"output_stride\": 2         }     } } head_configs = get_head_configs(single_instance_dict)</p> <p>bottomup_dict = {     \"bottomup\": {         \"confmaps\": {             \"part_names\": [\"head\", \"tail\"],             \"sigma\": 5.0,             \"output_stride\": 4,             \"loss_weight\": 1.0         },         \"pafs\": {             \"edges\": [(\"head\", \"tail\")],             \"sigma\": 15.0,             \"output_stride\": 4,             \"loss_weight\": 1.0         }     } } head_configs = get_head_configs(bottomup_dict)</p> <p>multi_class_dict = {     \"multi_class_topdown\": {         \"confmaps\": {             \"part_names\": [\"head\", \"tail\"],             \"sigma\": 5.0,             \"output_stride\": 16,             \"loss_weight\": 1.0         },         \"class_vectors\": {             \"classes\": None,  # Auto-inferred from track names             \"num_fc_layers\": 1,             \"num_fc_units\": 64,             \"output_stride\": 16,             \"loss_weight\": 1.0         }     } } head_configs = get_head_configs(multi_class_dict)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid head type is provided.</p> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_head_configs(head_cfg: Union[str, Dict[str, Any]]):\n    \"\"\"Create head configurations for pose estimation model outputs.\n\n    This method creates a `HeadConfig` object based on the user-provided parameters\n    for the pose estimation model head layers. The function supports both string-based\n    preset configurations and custom dictionary-based configurations for different model\n    types including Single Instance, Centroid, Centered Instance, Bottom-Up, and\n    Multi-Class variants.\n\n    Args:\n        head_cfg: Head configuration. Can be:\n            - String: One of the preset head types:\n                - [\"single_instance\", \"centroid\", \"centered_instance\", \"bottomup\", \"multi_class_bottomup\", \"multi_class_topdown\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"single_instance\": {\n                        \"confmaps\": {SingleInstanceConfMapsConfig parameters}\n                    },\n                    \"centroid\": {\n                        \"confmaps\": {CentroidConfMapsConfig parameters}\n                    },\n                    \"centered_instance\": {\n                        \"confmaps\": {CenteredInstanceConfMapsConfig parameters}\n                    },\n                    \"bottomup\": {\n                        \"confmaps\": {BottomUpConfMapsConfig parameters},\n                        \"pafs\": {PAFConfig parameters}\n                    },\n                    \"multi_class_bottomup\": {\n                        \"confmaps\": {BottomUpConfMapsConfig parameters},\n                        \"class_maps\": {ClassMapConfig parameters}\n                    },\n                    \"multi_class_topdown\": {\n                        \"confmaps\": {CenteredInstanceConfMapsConfig parameters},\n                        \"class_vectors\": {ClassVectorsConfig parameters}\n                    }\n                }\n                Only one head type should be specified in the dictionary.\n\n    Returns:\n        HeadConfig: Configured head object with model-specific settings.\n\n    Examples:\n        # String-based configuration\n        head_configs = get_head_configs(\"single_instance\")\n        head_configs = get_head_configs(\"bottomup\")\n        head_configs = get_head_configs(\"multi_class_topdown\")\n\n        # Dictionary-based configuration\n        single_instance_dict = {\n            \"single_instance\": {\n                \"confmaps\": {\n                    \"part_names\": [\"head\", \"tail\"],\n                    \"sigma\": 2.5,\n                    \"output_stride\": 2\n                }\n            }\n        }\n        head_configs = get_head_configs(single_instance_dict)\n\n        bottomup_dict = {\n            \"bottomup\": {\n                \"confmaps\": {\n                    \"part_names\": [\"head\", \"tail\"],\n                    \"sigma\": 5.0,\n                    \"output_stride\": 4,\n                    \"loss_weight\": 1.0\n                },\n                \"pafs\": {\n                    \"edges\": [(\"head\", \"tail\")],\n                    \"sigma\": 15.0,\n                    \"output_stride\": 4,\n                    \"loss_weight\": 1.0\n                }\n            }\n        }\n        head_configs = get_head_configs(bottomup_dict)\n\n        multi_class_dict = {\n            \"multi_class_topdown\": {\n                \"confmaps\": {\n                    \"part_names\": [\"head\", \"tail\"],\n                    \"sigma\": 5.0,\n                    \"output_stride\": 16,\n                    \"loss_weight\": 1.0\n                },\n                \"class_vectors\": {\n                    \"classes\": None,  # Auto-inferred from track names\n                    \"num_fc_layers\": 1,\n                    \"num_fc_units\": 64,\n                    \"output_stride\": 16,\n                    \"loss_weight\": 1.0\n                }\n            }\n        }\n        head_configs = get_head_configs(multi_class_dict)\n\n    Raises:\n        ValueError: If invalid head type is provided.\n    \"\"\"\n    head_configs = HeadConfig()\n    if isinstance(head_cfg, str):\n        if head_cfg == \"centered_instance\":\n            head_configs.centered_instance = CenteredInstanceConfig(\n                confmaps=CenteredInstanceConfMapsConfig\n            )\n        elif head_cfg == \"single_instance\":\n            head_configs.single_instance = SingleInstanceConfig(\n                confmaps=SingleInstanceConfMapsConfig\n            )\n        elif head_cfg == \"centroid\":\n            head_configs.centroid = CentroidConfig(confmaps=CentroidConfMapsConfig)\n        elif head_cfg == \"bottomup\":\n            head_configs.bottomup = BottomUpConfig(\n                confmaps=BottomUpConfMapsConfig, pafs=PAFConfig\n            )\n        elif head_cfg == \"multi_class_bottomup\":\n            head_configs.multi_class_bottomup = BottomUpMultiClassConfig(\n                confmaps=BottomUpConfMapsConfig, class_maps=ClassMapConfig\n            )\n        elif head_cfg == \"multi_class_topdown\":\n            head_configs.multi_class_topdown = TopDownCenteredInstanceMultiClassConfig(\n                confmaps=CenteredInstanceConfMapsConfig,\n                class_vectors=ClassVectorsConfig,\n            )\n        else:\n            raise ValueError(\n                f\"{head_cfg} is not a valid head type. Please choose one of ['bottomup', 'centered_instance', 'centroid', 'single_instance', 'multi_class_bottomup', 'multi_class_topdown']\"\n            )\n\n    elif isinstance(head_cfg, dict):\n        head_configs = HeadConfig()\n        if \"single_instance\" in head_cfg and head_cfg[\"single_instance\"] is not None:\n            head_configs.single_instance = SingleInstanceConfig(\n                confmaps=SingleInstanceConfMapsConfig(\n                    **head_cfg[\"single_instance\"][\"confmaps\"]\n                )\n            )\n        elif \"centroid\" in head_cfg and head_cfg[\"centroid\"] is not None:\n            head_configs.centroid = CentroidConfig(\n                confmaps=CentroidConfMapsConfig(**head_cfg[\"centroid\"][\"confmaps\"])\n            )\n        elif (\n            \"centered_instance\" in head_cfg\n            and head_cfg[\"centered_instance\"] is not None\n        ):\n            head_configs.centered_instance = CenteredInstanceConfig(\n                confmaps=CenteredInstanceConfMapsConfig(\n                    **head_cfg[\"centered_instance\"][\"confmaps\"]\n                )\n            )\n        elif \"bottomup\" in head_cfg and head_cfg[\"bottomup\"] is not None:\n            head_configs.bottomup = BottomUpConfig(\n                confmaps=BottomUpConfMapsConfig(\n                    **head_cfg[\"bottomup\"][\"confmaps\"],\n                ),\n                pafs=PAFConfig(**head_cfg[\"bottomup\"][\"pafs\"]),\n            )\n        elif (\n            \"multi_class_bottomup\" in head_cfg\n            and head_cfg[\"multi_class_bottomup\"] is not None\n        ):\n            head_configs.multi_class_bottomup = BottomUpMultiClassConfig(\n                confmaps=BottomUpConfMapsConfig(\n                    **head_cfg[\"multi_class_bottomup\"][\"confmaps\"]\n                ),\n                class_maps=ClassMapConfig(\n                    **head_cfg[\"multi_class_bottomup\"][\"class_maps\"]\n                ),\n            )\n        elif (\n            \"multi_class_topdown\" in head_cfg\n            and head_cfg[\"multi_class_topdown\"] is not None\n        ):\n            head_configs.multi_class_topdown = TopDownCenteredInstanceMultiClassConfig(\n                confmaps=CenteredInstanceConfMapsConfig(\n                    **head_cfg[\"multi_class_topdown\"][\"confmaps\"]\n                ),\n                class_vectors=ClassVectorsConfig(\n                    **head_cfg[\"multi_class_topdown\"][\"class_vectors\"]\n                ),\n            )\n\n    return head_configs\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_model_config","title":"<code>get_model_config(init_weight='default', pretrained_backbone_weights=None, pretrained_head_weights=None, backbone_config='unet', head_configs=None)</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>init_weight</code> <code>str</code> <p>model weights initialization method. \"default\" uses kaiming uniform initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".</p> <code>'default'</code> <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file with which the backbone is initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file with which the head layers are initialized. If <code>None</code>, random init is used. Default: None.</p> <code>None</code> <code>backbone_config</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set, then pass a dictionary with the structure: {     \"unet((or) convnext (or)swint)\":         {(params in the corresponding architecture given in <code>sleap_nn.config.model_config.backbone_config</code>)         } }. For eg: {             \"unet\":                 {                     \"in_channels\": 3,                     \"filters\": 64,                     \"max_stride\": 32,                     \"output_stride\": 2                 }         }</p> <code>'unet'</code> <code>head_configs</code> <code>Union[str, Dict[str, Any]]</code> <p>One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"]. The default <code>sigma</code> and <code>output_strides</code> are used if a string is passed. To set custom parameters, pass in a dictionary with the structure: {     \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):         {             \"confmaps\":                 {                     # params in the corresponding head type given in <code>sleap_nn.config.model_config.head_configs</code>                 },             \"pafs\":                 {                     # only for bottomup                 }         } }. For eg: {             \"single_instance\":                 {                     \"confmaps\":                         {                             \"part_names\": None,                             \"sigma\": 2.5,                             \"output_stride\": 2                         }                 }         }</p> <code>None</code> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_model_config(\n    init_weight: str = \"default\",\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    backbone_config: Union[str, Dict[str, Any]] = \"unet\",\n    head_configs: Union[str, Dict[str, Any]] = None,\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        init_weight: model weights initialization method. \"default\" uses kaiming uniform\n            initialization and \"xavier\" uses Xavier initialization method. Default: \"default\".\n        pretrained_backbone_weights: Path of the `ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file with which the backbone is\n            initialized. If `None`, random init is used. Default: None.\n        pretrained_head_weights: Path of the `ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file with which the head layers are\n            initialized. If `None`, random init is used. Default: None.\n        backbone_config: One of [\"unet\", \"unet_medium_rf\", \"unet_large_rf\", \"convnext\",\n            \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\", \"swint\",\n            \"swint_tiny\", \"swint_small\", \"swint_base\"]. If custom values need to be set,\n            then pass a dictionary with the structure:\n            {\n                \"unet((or) convnext (or)swint)\":\n                    {(params in the corresponding architecture given in `sleap_nn.config.model_config.backbone_config`)\n                    }\n            }.\n            For eg: {\n                        \"unet\":\n                            {\n                                \"in_channels\": 3,\n                                \"filters\": 64,\n                                \"max_stride\": 32,\n                                \"output_stride\": 2\n                            }\n                    }\n        head_configs: One of [\"bottomup\", \"centered_instance\", \"centroid\", \"single_instance\", \"multi_class_bottomup\", \"multi_class_topdown\"].\n            The default `sigma` and `output_strides` are used if a string is passed. To\n            set custom parameters, pass in a dictionary with the structure:\n            {\n                \"bottomup\" (or \"centroid\" or \"single_instance\" or \"centered_instance\" or \"multi_class_bottomup\" or \"multi_class_topdown\"):\n                    {\n                        \"confmaps\":\n                            {\n                                # params in the corresponding head type given in `sleap_nn.config.model_config.head_configs`\n                            },\n                        \"pafs\":\n                            {\n                                # only for bottomup\n                            }\n                    }\n            }.\n            For eg: {\n                        \"single_instance\":\n                            {\n                                \"confmaps\":\n                                    {\n                                        \"part_names\": None,\n                                        \"sigma\": 2.5,\n                                        \"output_stride\": 2\n                                    }\n                            }\n                    }\n\n    \"\"\"\n    backbone_config = get_backbone_config(backbone_cfg=backbone_config)\n    head_configs = get_head_configs(head_cfg=head_configs)\n    model_config = ModelConfig(\n        init_weights=init_weight,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n    )\n    return model_config\n</code></pre>"},{"location":"api/config/get_config/#sleap_nn.config.get_config.get_trainer_config","title":"<code>get_trainer_config(batch_size=1, shuffle_train=True, num_workers=0, ckpt_save_top_k=1, ckpt_save_last=None, trainer_num_devices=None, trainer_device_indices=None, trainer_accelerator='auto', enable_progress_bar=True, min_train_steps_per_epoch=200, train_steps_per_epoch=None, visualize_preds_during_training=False, keep_viz=False, max_epochs=10, seed=None, use_wandb=False, save_ckpt=False, ckpt_dir=None, run_name=None, resume_ckpt_path=None, wandb_entity=None, wandb_project=None, wandb_name=None, wandb_api_key=None, wandb_mode=None, wandb_save_viz_imgs_wandb=False, wandb_resume_prv_runid=None, wandb_group_name=None, wandb_delete_local_logs=None, optimizer='Adam', learning_rate=0.001, amsgrad=False, lr_scheduler=None, early_stopping=False, early_stopping_min_delta=0.0, early_stopping_patience=1, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, zmq_publish_port=None, zmq_controller_port=None, zmq_controller_timeout=10)</code>","text":"<p>Train a pose-estimation model with SLEAP-NN framework.</p> <p>This method creates a config object based on the parameters provided by the user, and starts training by passing this config to the <code>ModelTrainer</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples per batch or batch size for training data. Default: 4.</p> <code>1</code> <code>shuffle_train</code> <code>bool</code> <p>True to have the train data reshuffled at every epoch. Default: True.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: 0.</p> <code>0</code> <code>ckpt_save_top_k</code> <code>int</code> <p>If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. Default: 1.</p> <code>1</code> <code>ckpt_save_last</code> <code>Optional[bool]</code> <p>When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. Default: False.</p> <code>None</code> <code>trainer_num_devices</code> <code>Optional[Union[str, int]]</code> <p>Number of devices to use or \"auto\" to let Lightning decide. If <code>None</code>, it defaults to <code>\"auto\"</code> when <code>trainer_device_indices</code> is also <code>None</code>, otherwise its value is inferred from trainer_device_indices. Default: None.</p> <code>None</code> <code>trainer_device_indices</code> <code>Optional[List[int]]</code> <p>List of device indices to use. For example, <code>[0, 1]</code> selects two devices and overrides <code>trainer_devices</code>, while <code>[2]</code> with <code>trainer_devices=2</code> still runs only on <code>device 2</code> (not two devices). If <code>None</code>, the number of devices is taken from <code>trainer_devices</code>, starting from index 0. Default: <code>None</code>.</p> <code>None</code> <code>trainer_accelerator</code> <code>str</code> <p>One of the (\"cpu\", \"gpu\", \"mps\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the <code>Trainer</code> to be connected to. Default: \"auto\".</p> <code>'auto'</code> <code>enable_progress_bar</code> <code>bool</code> <p>When True, enables printing the logs during training. Default: False.</p> <code>True</code> <code>min_train_steps_per_epoch</code> <code>int</code> <p>Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer <code>limit_train_batches</code> parameter of Torch <code>Trainer</code>. Default: 200.</p> <code>200</code> <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data or <code>min_train_steps_per_epoch</code>, whichever is largest. Default: <code>None</code>. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>None</code> <code>visualize_preds_during_training</code> <code>bool</code> <p>If set to <code>True</code>, sample predictions (keypoints  + confidence maps) are saved to <code>viz</code> folder in the ckpt dir.</p> <code>False</code> <code>keep_viz</code> <code>bool</code> <p>If set to <code>True</code>, the <code>viz</code> folder will be kept after training. If <code>False</code>, the <code>viz</code> folder will be deleted after training. Only applies when <code>visualize_preds_during_training</code> is <code>True</code>.</p> <code>False</code> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs to run. Default: 100.</p> <code>10</code> <code>seed</code> <code>Optional[int]</code> <p>Seed value for the current experiment. If None, no seeding is applied. Default: None.</p> <code>None</code> <code>save_ckpt</code> <code>bool</code> <p>True to enable checkpointing. Default: False.</p> <code>False</code> <code>ckpt_dir</code> <code>Optional[str]</code> <p>Directory path where the <code>&lt;run_name&gt;</code> folder is created. If <code>None</code>, a new folder for the current run is created in the working dir. Default: <code>None</code></p> <code>None</code> <code>run_name</code> <code>Optional[str]</code> <p>Name of the current run. The ckpts will be created in <code>&lt;ckpt_dir&gt;/&lt;run_name&gt;</code>. If <code>None</code>, a run name is generated with <code>&lt;timestamp&gt;_&lt;head_name&gt;</code>. Default: None.</p> <code>None</code> <code>resume_ckpt_path</code> <code>Optional[str]</code> <p>Path to <code>.ckpt</code> file from which training is resumed. Default: None.</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>True to enable wandb logging. Default: False.</p> <code>False</code> <code>wandb_entity</code> <code>Optional[str]</code> <p>Entity of wandb project. Default: None. (The default entity in the user profile settings is used)</p> <code>None</code> <code>wandb_project</code> <code>Optional[str]</code> <p>Project name for the current wandb run. Default: None.</p> <code>None</code> <code>wandb_name</code> <code>Optional[str]</code> <p>Name of the current wandb run. Default: None.</p> <code>None</code> <code>wandb_api_key</code> <code>Optional[str]</code> <p>API key. The API key is masked when saved to config files. Default: None.</p> <code>None</code> <code>wandb_mode</code> <code>Optional[str]</code> <p>\"offline\" if only local logging is required. Default: None.</p> <code>None</code> <code>wandb_save_viz_imgs_wandb</code> <code>bool</code> <p>If set to <code>True</code>, sample predictions (keypoints + confidence maps) that are saved to local <code>viz</code> folder in the ckpt dir would also be uploaded to wandb. Default: False.</p> <code>False</code> <code>wandb_resume_prv_runid</code> <code>Optional[str]</code> <p>Previous run ID if training should be resumed from a previous ckpt. Default: None</p> <code>None</code> <code>wandb_group_name</code> <code>Optional[str]</code> <p>Group name for the wandb run. Default: None.</p> <code>None</code> <code>wandb_delete_local_logs</code> <code>Optional[bool]</code> <p>If True, delete local wandb logs folder after training. If False, keep the folder. If None (default), automatically delete if logging online (wandb_mode != \"offline\") and keep if logging offline. Default: None.</p> <code>None</code> <code>optimizer</code> <code>str</code> <p>Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".</p> <code>'Adam'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate of type float. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>bool</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <code>lr_scheduler</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in <code>sleap_nn.config.trainer_config</code> are used). To use custom values, pass a dictionary with the structure in <code>sleap_nn.config.trainer_config.LRSchedulerConfig</code>. For eg, {             \"step_lr\":                 {                     (params in <code>sleap_nn.config.trainer_config.StepLRConfig</code>)                 }         }</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>True if early stopping should be enabled. Default: False.</p> <code>False</code> <code>early_stopping_min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. Default: 0.0.</p> <code>0.0</code> <code>early_stopping_patience</code> <code>int</code> <p>Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. Default: 1.</p> <code>1</code> <code>online_mining</code> <code>bool</code> <p>If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss.</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>float</code> <p>The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>int</code> <p>The minimum number of keypoints that will be considered as \"hard\", even if they are not below the <code>hard_to_easy_ratio</code>.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether.</p> <code>None</code> <code>loss_scale</code> <code>float</code> <p>Factor to scale the hard keypoint losses by for oks.</p> <code>5.0</code> <code>zmq_publish_port</code> <code>Optional[int]</code> <p>(int) Specifies the port to which the training logs (loss values) should be sent to.</p> <code>None</code> <code>zmq_controller_port</code> <code>Optional[int]</code> <p>(int) Specifies the port to listen to to stop the training (specific to SLEAP GUI).</p> <code>None</code> <code>zmq_controller_timeout</code> <code>int</code> <p>(int) Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed.</p> <code>10</code> Source code in <code>sleap_nn/config/get_config.py</code> <pre><code>def get_trainer_config(\n    batch_size: int = 1,\n    shuffle_train: bool = True,\n    num_workers: int = 0,\n    ckpt_save_top_k: int = 1,\n    ckpt_save_last: Optional[bool] = None,\n    trainer_num_devices: Optional[Union[str, int]] = None,\n    trainer_device_indices: Optional[List[int]] = None,\n    trainer_accelerator: str = \"auto\",\n    enable_progress_bar: bool = True,\n    min_train_steps_per_epoch: int = 200,\n    train_steps_per_epoch: Optional[int] = None,\n    visualize_preds_during_training: bool = False,\n    keep_viz: bool = False,\n    max_epochs: int = 10,\n    seed: Optional[int] = None,\n    use_wandb: bool = False,\n    save_ckpt: bool = False,\n    ckpt_dir: Optional[str] = None,\n    run_name: Optional[str] = None,\n    resume_ckpt_path: Optional[str] = None,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_name: Optional[str] = None,\n    wandb_api_key: Optional[str] = None,\n    wandb_mode: Optional[str] = None,\n    wandb_save_viz_imgs_wandb: bool = False,\n    wandb_resume_prv_runid: Optional[str] = None,\n    wandb_group_name: Optional[str] = None,\n    wandb_delete_local_logs: Optional[bool] = None,\n    optimizer: str = \"Adam\",\n    learning_rate: float = 1e-3,\n    amsgrad: bool = False,\n    lr_scheduler: Optional[Union[str, Dict[str, Any]]] = None,\n    early_stopping: bool = False,\n    early_stopping_min_delta: float = 0.0,\n    early_stopping_patience: int = 1,\n    online_mining: bool = False,\n    hard_to_easy_ratio: float = 2.0,\n    min_hard_keypoints: int = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: float = 5.0,\n    zmq_publish_port: Optional[int] = None,\n    zmq_controller_port: Optional[int] = None,\n    zmq_controller_timeout: int = 10,\n):\n    \"\"\"Train a pose-estimation model with SLEAP-NN framework.\n\n    This method creates a config object based on the parameters provided by the user,\n    and starts training by passing this config to the `ModelTrainer` class.\n\n    Args:\n        batch_size: Number of samples per batch or batch size for training data. Default: 4.\n        shuffle_train: True to have the train data reshuffled at every epoch. Default: True.\n        num_workers: Number of subprocesses to use for data loading. 0 means that the data\n            will be loaded in the main process. Default: 0.\n        ckpt_save_top_k: If save_top_k == k, the best k models according to the quantity\n            monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1,\n            all models are saved. Please note that the monitors are checked every every_n_epochs\n            epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an\n            epoch, the name of the saved file will be appended with a version count starting\n            with v1 unless enable_version_counter is set to False. Default: 1.\n        ckpt_save_last: When True, saves a last.ckpt whenever a checkpoint file gets saved.\n            On a local filesystem, this will be a symbolic link, and otherwise a copy of\n            the checkpoint file. This allows accessing the latest checkpoint in a deterministic\n            manner. Default: False.\n        trainer_num_devices: Number of devices to use or \"auto\" to let Lightning decide. If `None`, it defaults to `\"auto\"` when `trainer_device_indices` is also `None`, otherwise its value is inferred from trainer_device_indices. Default: None.\n        trainer_device_indices: List of device indices to use. For example, `[0, 1]` selects two devices and overrides `trainer_devices`, while `[2]` with `trainer_devices=2` still runs only on `device 2` (not two devices). If `None`, the number of devices is taken from `trainer_devices`, starting from index 0. Default: `None`.\n        trainer_accelerator: One of the (\"cpu\", \"gpu\", \"mps\", \"auto\"). \"auto\" recognises\n            the machine the model is running on and chooses the appropriate accelerator for\n            the `Trainer` to be connected to. Default: \"auto\".\n        enable_progress_bar: When True, enables printing the logs during training. Default: False.\n        min_train_steps_per_epoch: Minimum number of iterations in a single epoch. (Useful if model\n            is trained with very few data points). Refer `limit_train_batches` parameter\n            of Torch `Trainer`. Default: 200.\n        train_steps_per_epoch: Number of minibatches (steps) to train for in an epoch. If set to `None`,\n            this is set to the number of batches in the training data or `min_train_steps_per_epoch`,\n            whichever is largest. Default: `None`. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        visualize_preds_during_training: If set to `True`, sample predictions (keypoints  + confidence maps)\n            are saved to `viz` folder in the ckpt dir.\n        keep_viz: If set to `True`, the `viz` folder will be kept after training. If `False`, the `viz` folder\n            will be deleted after training. Only applies when `visualize_preds_during_training` is `True`.\n        max_epochs: Maximum number of epochs to run. Default: 100.\n        seed: Seed value for the current experiment. If None, no seeding is applied. Default: None.\n        save_ckpt: True to enable checkpointing. Default: False.\n        ckpt_dir: Directory path where the `&lt;run_name&gt;` folder is created. If `None`, a new folder for the current run is created in the working dir. **Default**: `None`\n        run_name: Name of the current run. The ckpts will be created in `&lt;ckpt_dir&gt;/&lt;run_name&gt;`. If `None`, a run name is generated with `&lt;timestamp&gt;_&lt;head_name&gt;`. Default: None.\n        resume_ckpt_path: Path to `.ckpt` file from which training is resumed. Default: None.\n        use_wandb: True to enable wandb logging. Default: False.\n        wandb_entity: Entity of wandb project. Default: None.\n            (The default entity in the user profile settings is used)\n        wandb_project: Project name for the current wandb run. Default: None.\n        wandb_name: Name of the current wandb run. Default: None.\n        wandb_api_key: API key. The API key is masked when saved to config files. Default: None.\n        wandb_mode: \"offline\" if only local logging is required. Default: None.\n        wandb_save_viz_imgs_wandb: If set to `True`, sample predictions (keypoints + confidence maps) that are saved to local `viz` folder in the ckpt dir would also be uploaded to wandb. Default: False.\n        wandb_resume_prv_runid: Previous run ID if training should be resumed from a previous\n            ckpt. Default: None\n        wandb_group_name: Group name for the wandb run. Default: None.\n        wandb_delete_local_logs: If True, delete local wandb logs folder after training.\n            If False, keep the folder. If None (default), automatically delete if logging\n            online (wandb_mode != \"offline\") and keep if logging offline. Default: None.\n        optimizer: Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: \"Adam\".\n        learning_rate: Learning rate of type float. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n        lr_scheduler: One of [\"step_lr\", \"reduce_lr_on_plateau\"] (the default values in\n            `sleap_nn.config.trainer_config` are used). To use custom values, pass a\n            dictionary with the structure in `sleap_nn.config.trainer_config.LRSchedulerConfig`.\n            For eg, {\n                        \"step_lr\":\n                            {\n                                (params in `sleap_nn.config.trainer_config.StepLRConfig`)\n                            }\n                    }\n        early_stopping: True if early stopping should be enabled. Default: False.\n        early_stopping_min_delta: Minimum change in the monitored quantity to qualify as\n            an improvement, i.e. an absolute change of less than or equal to min_delta,\n            will count as no improvement. Default: 0.0.\n        early_stopping_patience: Number of checks with no improvement after which training\n            will be stopped. Under the default configuration, one check happens after every\n            training epoch. Default: 1.\n        online_mining: If True, online hard keypoint mining (OHKM) will be enabled. When\n            this is enabled, the loss is computed per keypoint (or edge for PAFs) and\n            sorted from lowest (easy) to highest (hard). The hard keypoint loss will be\n            scaled to have a higher weight in the total loss, encouraging the training\n            to focus on tricky body parts that are more difficult to learn.\n            If False, no mining will be performed and all keypoints will be weighted\n            equally in the loss.\n        hard_to_easy_ratio: The minimum ratio of the individual keypoint loss with\n            respect to the lowest keypoint loss in order to be considered as \"hard\".\n            This helps to switch focus on across groups of keypoints during training.\n        min_hard_keypoints: The minimum number of keypoints that will be considered as\n            \"hard\", even if they are not below the `hard_to_easy_ratio`.\n        max_hard_keypoints: The maximum number of hard keypoints to apply scaling to.\n            This can help when there are few very easy keypoints which may skew the\n            ratio and result in loss scaling being applied to most keypoints, which can\n            reduce the impact of hard mining altogether.\n        loss_scale: Factor to scale the hard keypoint losses by for oks.\n        zmq_publish_port: (int) Specifies the port to which the training logs (loss values) should be sent to.\n        zmq_controller_port: (int) Specifies the port to listen to to stop the training (specific to SLEAP GUI).\n        zmq_controller_timeout: (int) Polling timeout in microseconds specified as an integer. This controls how long the poller\n            should wait to receive a response and should be set to a small value to minimize the impact on training speed.\n    \"\"\"\n    # constrict trainer config\n    train_dataloader_cfg = TrainDataLoaderConfig(\n        batch_size=batch_size, shuffle=shuffle_train, num_workers=num_workers\n    )\n    val_dataloader_cfg = ValDataLoaderConfig(\n        batch_size=batch_size, shuffle=False, num_workers=num_workers\n    )\n\n    lr_scheduler_cfg = LRSchedulerConfig()\n    if isinstance(lr_scheduler, str):\n        if lr_scheduler == \"step_lr\":\n            lr_scheduler_cfg.step_lr = StepLRConfig()\n        elif lr_scheduler == \"reduce_lr_on_plateau\":\n            lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig()\n        else:\n            message = f\"{lr_scheduler} is not a valid scheduler. Please choose one of ['step_lr', 'reduce_lr_on_plateau']\"\n            logger.error(message)\n            raise ValueError(message)\n    elif isinstance(lr_scheduler, dict):\n        if lr_scheduler is None:\n            lr_scheduler = {\n                \"step_lr\": None,\n                \"reduce_lr_on_plateau\": None,\n            }\n        for k, v in lr_scheduler.items():\n            if v is not None:\n                if k == \"step_lr\":\n                    lr_scheduler_cfg.step_lr = StepLRConfig(**v)\n                    break\n                elif k == \"reduce_lr_on_plateau\":\n                    lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig(**v)\n                    break\n\n    trainer_config = TrainerConfig(\n        train_data_loader=train_dataloader_cfg,\n        val_data_loader=val_dataloader_cfg,\n        model_ckpt=ModelCkptConfig(\n            save_top_k=ckpt_save_top_k, save_last=ckpt_save_last\n        ),\n        trainer_devices=trainer_num_devices,\n        trainer_device_indices=trainer_device_indices,\n        trainer_accelerator=trainer_accelerator,\n        enable_progress_bar=enable_progress_bar,\n        min_train_steps_per_epoch=min_train_steps_per_epoch,\n        train_steps_per_epoch=train_steps_per_epoch,\n        visualize_preds_during_training=visualize_preds_during_training,\n        keep_viz=keep_viz,\n        max_epochs=max_epochs,\n        seed=seed,\n        use_wandb=use_wandb,\n        wandb=WandBConfig(\n            entity=wandb_entity,\n            project=wandb_project,\n            name=wandb_name,\n            api_key=wandb_api_key,\n            wandb_mode=wandb_mode,\n            save_viz_imgs_wandb=wandb_save_viz_imgs_wandb,\n            prv_runid=wandb_resume_prv_runid,\n            group=wandb_group_name,\n            delete_local_logs=wandb_delete_local_logs,\n        ),\n        save_ckpt=save_ckpt,\n        ckpt_dir=ckpt_dir,\n        run_name=run_name,\n        resume_ckpt_path=resume_ckpt_path,\n        optimizer_name=optimizer,\n        optimizer=OptimizerConfig(lr=learning_rate, amsgrad=amsgrad),\n        lr_scheduler=lr_scheduler_cfg,\n        early_stopping=EarlyStoppingConfig(\n            min_delta=early_stopping_min_delta,\n            patience=early_stopping_patience,\n            stop_training_on_plateau=early_stopping,\n        ),\n        online_hard_keypoint_mining=HardKeypointMiningConfig(\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n        ),\n        zmq=ZMQConfig(\n            controller_port=zmq_controller_port,\n            controller_polling_timeout=zmq_controller_timeout,\n            publish_port=zmq_publish_port,\n        ),\n    )\n    return trainer_config\n</code></pre>"},{"location":"api/config/model_config/","title":"model_config","text":""},{"location":"api/config/model_config/#sleap_nn.config.model_config","title":"<code>sleap_nn.config.model_config</code>","text":"<p>Serializable configuration classes for specifying all model config parameters.</p> <p>These configuration classes are intended to specify all the parameters required to initialize the model config.</p> <p>Classes:</p> Name Description <code>BackboneConfig</code> <p>Configurations related to model backbone configuration.</p> <code>BottomUpConfMapsConfig</code> <p>Bottomup configuration map.</p> <code>BottomUpConfig</code> <p>bottomup head_config.</p> <code>BottomUpMultiClassConfig</code> <p>Head config for BottomUp Id models.</p> <code>CenteredInstanceConfMapsConfig</code> <p>Centered Instance configuration map.</p> <code>CenteredInstanceConfig</code> <p>centered_instance head_config.</p> <code>CentroidConfMapsConfig</code> <p>Centroid configuration map.</p> <code>CentroidConfig</code> <p>centroid head_config.</p> <code>ClassMapConfig</code> <p>Class map head config.</p> <code>ClassVectorsConfig</code> <p>Configurations for class vectors heads.</p> <code>ConvNextBaseConfig</code> <p>Convnext configuration for backbone.</p> <code>ConvNextConfig</code> <p>Convnext configuration for backbone.</p> <code>ConvNextLargeConfig</code> <p>Convnext configuration for backbone.</p> <code>ConvNextSmallConfig</code> <p>Convnext configuration for backbone.</p> <code>HeadConfig</code> <p>Configurations related to the model output head type.</p> <code>ModelConfig</code> <p>Configurations related to model architecture.</p> <code>PAFConfig</code> <p>PAF configuration map.</p> <code>SingleInstanceConfMapsConfig</code> <p>Single Instance configuration map.</p> <code>SingleInstanceConfig</code> <p>single instance head_config.</p> <code>SwinTBaseConfig</code> <p>SwinT configuration for backbone.</p> <code>SwinTConfig</code> <p>SwinT configuration (tiny) for backbone.</p> <code>SwinTSmallConfig</code> <p>SwinT configuration (small) for backbone.</p> <code>TopDownCenteredInstanceMultiClassConfig</code> <p>Head config for TopDown centered instance ID models.</p> <code>UNetConfig</code> <p>UNet config for backbone.</p> <code>UNetLargeRFConfig</code> <p>UNet config for backbone with large receptive field.</p> <code>UNetMediumRFConfig</code> <p>UNet config for backbone with medium receptive field.</p> <p>Functions:</p> Name Description <code>model_mapper</code> <p>Map the legacy model configuration to the new model configuration.</p>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BackboneConfig","title":"<code>BackboneConfig</code>","text":"<p>Configurations related to model backbone configuration.</p> <p>Attributes:</p> Name Type Description <code>unet</code> <code>Optional[UNetConfig]</code> <p>An instance of <code>UNetConfig</code>.</p> <code>convnext</code> <code>Optional[ConvNextConfig]</code> <p>An instance of <code>ConvNextConfig</code>.</p> <code>swint</code> <code>Optional[SwinTConfig]</code> <p>An instance of <code>SwinTConfig</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@oneof\n@define\nclass BackboneConfig:\n    \"\"\"Configurations related to model backbone configuration.\n\n    Attributes:\n        unet: An instance of `UNetConfig`.\n        convnext: An instance of `ConvNextConfig`.\n        swint: An instance of `SwinTConfig`.\n    \"\"\"\n\n    unet: Optional[UNetConfig] = None\n    convnext: Optional[ConvNextConfig] = None\n    swint: Optional[SwinTConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BottomUpConfMapsConfig","title":"<code>BottomUpConfMapsConfig</code>","text":"<p>Bottomup configuration map.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>Optional[List[str]]</code> <p>(List[str]) None if nodes from sio.Labels file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>Optional[float]</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass BottomUpConfMapsConfig:\n    \"\"\"Bottomup configuration map.\n\n    Attributes:\n        part_names: (List[str]) None if nodes from sio.Labels file can be used directly.\n            Else provide text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of channels\n            in the output. If not specified, all body parts in the skeleton will be used.\n            This config does not apply for 'PartAffinityFieldsHead'.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn\n            as they have a lower density within the image space. Larger values are easier\n            to learn but are less precise with respect to the peak coordinate. This spread\n            is in units of pixels of the model input image, i.e., the image resolution\n            after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    part_names: Optional[List[str]] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n    loss_weight: Optional[float] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BottomUpConfig","title":"<code>BottomUpConfig</code>","text":"<p>bottomup head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass BottomUpConfig:\n    \"\"\"bottomup head_config.\"\"\"\n\n    confmaps: Optional[BottomUpConfMapsConfig] = None\n    pafs: Optional[PAFConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.BottomUpMultiClassConfig","title":"<code>BottomUpMultiClassConfig</code>","text":"<p>Head config for BottomUp Id models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass BottomUpMultiClassConfig:\n    \"\"\"Head config for BottomUp Id models.\"\"\"\n\n    confmaps: Optional[BottomUpConfMapsConfig] = None\n    class_maps: Optional[ClassMapConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CenteredInstanceConfMapsConfig","title":"<code>CenteredInstanceConfMapsConfig</code>","text":"<p>Centered Instance configuration map.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>Optional[List[str]]</code> <p>(List[str]) None if nodes from sio.Labels file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'.</p> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) Node name to use as the anchor point. If None, the midpoint of the bounding box of all visible instance points will be used as the anchor. The bounding box midpoint will also be used if the anchor part is specified but not visible in the instance. Setting a reliable anchor point can significantly improve topdown model accuracy as they benefit from a consistent geometry of the body parts relative to the center of the image. Default is None.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>float</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CenteredInstanceConfMapsConfig:\n    \"\"\"Centered Instance configuration map.\n\n    Attributes:\n        part_names: (List[str]) None if nodes from sio.Labels file can be used directly.\n            Else provide text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of channels\n            in the output. If not specified, all body parts in the skeleton will be used.\n            This config does not apply for 'PartAffinityFieldsHead'.\n        anchor_part: (str) Node name to use as the anchor point. If None, the midpoint of the\n            bounding box of all visible instance points will be used as the anchor. The bounding\n            box midpoint will also be used if the anchor part is specified but not visible in the\n            instance. Setting a reliable anchor point can significantly improve topdown model\n            accuracy as they benefit from a consistent geometry of the body parts relative to the\n            center of the image. Default is None.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn\n            as they have a lower density within the image space. Larger values are\n            easier to learn but are less precise with respect to the peak coordinate.\n            This spread is in units of pixels of the model input image, i.e., the image\n            resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    part_names: Optional[List[str]] = None\n    anchor_part: Optional[str] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n    loss_weight: float = 1.0\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CenteredInstanceConfig","title":"<code>CenteredInstanceConfig</code>","text":"<p>centered_instance head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CenteredInstanceConfig:\n    \"\"\"centered_instance head_config.\"\"\"\n\n    confmaps: Optional[CenteredInstanceConfMapsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CentroidConfMapsConfig","title":"<code>CentroidConfMapsConfig</code>","text":"<p>Centroid configuration map.</p> <p>Attributes:</p> Name Type Description <code>anchor_part</code> <code>Optional[str]</code> <p>(str) Node name to use as the anchor point. If None, the midpoint of the bounding box of all visible instance points will be used as the anchor. The bounding box midpoint will also be used if the anchor part is specified but not visible in the instance. Setting a reliable anchor point can significantly improve topdown model accuracy as they benefit from a consistent geometry of the body parts relative to the center of the image. Default is None.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CentroidConfMapsConfig:\n    \"\"\"Centroid configuration map.\n\n    Attributes:\n        anchor_part: (str) Node name to use as the anchor point. If None, the midpoint of the\n            bounding box of all visible instance points will be used as the anchor. The bounding\n            box midpoint will also be used if the anchor part is specified but not visible in the\n            instance. Setting a reliable anchor point can significantly improve topdown model\n            accuracy as they benefit from a consistent geometry of the body parts relative to the\n            center of the image. Default is None.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn as\n            they have a lower density within the image space. Larger values are easier to\n            learn but are less precise with respect to the peak coordinate. This spread is\n            in units of pixels of the model input image, i.e., the image resolution after\n            any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n    \"\"\"\n\n    anchor_part: Optional[str] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.CentroidConfig","title":"<code>CentroidConfig</code>","text":"<p>centroid head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass CentroidConfig:\n    \"\"\"centroid head_config.\"\"\"\n\n    confmaps: Optional[CentroidConfMapsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ClassMapConfig","title":"<code>ClassMapConfig</code>","text":"<p>Class map head config.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>Optional[List[str]]</code> <p>(List[str]) List of class (track) names. Default is <code>None</code>. When <code>None</code>, these are inferred from the track names in the labels file.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>Optional[float]</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ClassMapConfig:\n    \"\"\"Class map head config.\n\n    Attributes:\n        classes: (List[str]) List of class (track) names. Default is `None`. When `None`, these are inferred from the track names in the labels file.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as\n            a scalar float. Smaller values are more precise but may be difficult to\n            learn as they have a lower density within the image space. Larger values\n            are easier to learn but are less precise with respect to the peak\n            coordinate. This spread is in units of pixels of the model input image,\n            i.e., the image resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to\n            the input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the\n            input. Increasing this value can considerably speed up model performance\n            and decrease memory requirements, at the cost of decreased spatial\n            resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    classes: Optional[List[str]] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n    loss_weight: Optional[float] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ClassVectorsConfig","title":"<code>ClassVectorsConfig</code>","text":"<p>Configurations for class vectors heads.</p> <p>These heads are used in top-down multi-instance models that classify detected points using a fixed set of learned classes (e.g., animal identities).</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>Optional[List[str]]</code> <p>List of string names of the classes that this head will predict.</p> <code>num_fc_layers</code> <code>int</code> <p>Number of fully-connected layers before the classification output layer. These can help in transforming general image features into classification-specific features.</p> <code>num_fc_units</code> <code>int</code> <p>Number of units (dimensions) in the fully-connected layers before classification. Increasing this can improve the representational capacity in the pre-classification layers.</p> <code>output_stride</code> <code>int</code> <p>(Ideally this should be same as the backbone's maxstride). The stride of the output class maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in maps that are 0.5x the size of the input. This should be the same size as the confidence maps they are associated with.</p> <code>loss_weight</code> <code>float</code> <p>Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ClassVectorsConfig:\n    \"\"\"Configurations for class vectors heads.\n\n    These heads are used in top-down multi-instance models that classify detected\n    points using a fixed set of learned classes (e.g., animal identities).\n\n    Attributes:\n        classes: List of string names of the classes that this head will predict.\n        num_fc_layers: Number of fully-connected layers before the classification output\n            layer. These can help in transforming general image features into\n            classification-specific features.\n        num_fc_units: Number of units (dimensions) in the fully-connected layers before\n            classification. Increasing this can improve the representational capacity in\n            the pre-classification layers.\n        output_stride: (Ideally this should be same as the backbone's maxstride).\n            The stride of the output class maps relative to the input image.\n            This is the reciprocal of the resolution, e.g., an output stride of 2\n            results in maps that are 0.5x the size of the input. This should be the same\n            size as the confidence maps they are associated with.\n        loss_weight: Scalar float used to weigh the loss term for this head during\n            training. Increase this to encourage the optimization to focus on improving\n            this specific output in multi-head models.\n    \"\"\"\n\n    classes: Optional[List[str]] = None\n    num_fc_layers: int = 1\n    num_fc_units: int = 64\n    global_pool: bool = True\n    output_stride: int = 1\n    loss_weight: float = 1.0\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextBaseConfig","title":"<code>ConvNextBaseConfig</code>","text":"<p>               Bases: <code>ConvNextConfig</code></p> <p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3]. channels: (List(int)) Number of channels in each block. Default:     [96, 192, 384, 768].</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default is 2.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default is 1.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default is 3.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default is 2.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default is 2.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: True.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextBaseConfig(ConvNextConfig):\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type\n            is provided)\n            depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3].\n            channels: (List(int)) Number of channels in each block. Default:\n                [96, 192, 384, 768].\n        model_type: (str) One of the ConvNext architecture types:\n            [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer.\n            Default is 4.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. Default is 2.\n        in_channels: (int) Number of input channels. Default is 1.\n        kernel_size: (int) Size of the convolutional kernels. Default is 3.\n        filters_rate: (float) Factor to adjust the number of filters per block.\n            Default is 2.\n        convs_per_block: (int) Number of convolutional layers per block. Default is 2.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales. Default: True.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `32` for all convnext architectures.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"base\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextBaseConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextConfig","title":"<code>ConvNextConfig</code>","text":"<p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List[int]) Number of layers in each block. Default: <code>[3, 3, 9, 3]</code>. channels: (List[int]) Number of channels in each block. Default: <code>[96, 192, 384, 768]</code>.</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: <code>\"tiny\"</code>.</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default: <code>2</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures. Default: <code>32</code>.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextConfig:\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type is provided)\n            depths: (List[int]) Number of layers in each block. *Default*: `[3, 3, 9, 3]`.\n            channels: (List[int]) Number of channels in each block. *Default*: `[96, 192, 384, 768]`.\n        model_type: (str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. *Default*: `\"tiny\"`.\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer. *Default*: `4`.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. *Default*: `2`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all convnext architectures. *Default*: `32`.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"tiny\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextLargeConfig","title":"<code>ConvNextLargeConfig</code>","text":"<p>               Bases: <code>ConvNextConfig</code></p> <p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3]. channels: (List(int)) Number of channels in each block. Default:     [96, 192, 384, 768].</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default is 2.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default is 1.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default is 3.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default is 2.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default is 2.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: True.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextLargeConfig(ConvNextConfig):\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type\n            is provided)\n            depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3].\n            channels: (List(int)) Number of channels in each block. Default:\n                [96, 192, 384, 768].\n        model_type: (str) One of the ConvNext architecture types:\n            [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer.\n            Default is 4.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. Default is 2.\n        in_channels: (int) Number of input channels. Default is 1.\n        kernel_size: (int) Size of the convolutional kernels. Default is 3.\n        filters_rate: (float) Factor to adjust the number of filters per block.\n            Default is 2.\n        convs_per_block: (int) Number of convolutional layers per block. Default is 2.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales. Default: True.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `32` for all convnext architectures.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"large\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextLargeConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextSmallConfig","title":"<code>ConvNextSmallConfig</code>","text":"<p>               Bases: <code>ConvNextConfig</code></p> <p>Convnext configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\", \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].</p> <code>arch</code> <code>Optional[dict]</code> <p>(Default is Tiny architecture config. No need to provide if model_type is provided) depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3]. channels: (List(int)) Number of channels in each block. Default:     [96, 192, 384, 768].</p> <code>model_type</code> <code>str</code> <p>(str) One of the ConvNext architecture types: [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".</p> <code>stem_patch_kernel</code> <code>int</code> <p>(int) Size of the convolutional kernels in the stem layer. Default is 4.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Convolutional stride in the stem layer. Default is 2.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default is 1.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default is 3.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default is 2.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default is 2.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: True.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>max_stride</code> <code>int</code> <p>Factor by which input image size is reduced through the layers. This is always <code>32</code> for all convnext architectures.</p> <p>Methods:</p> Name Description <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ConvNextSmallConfig(ConvNextConfig):\n    \"\"\"Convnext configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            ConvNext backbones. For ConvNext, one of [\"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\", \"ConvNeXt_Small_Weights\", \"ConvNeXt_Large_Weights\"].\n        arch: (Default is Tiny architecture config. No need to provide if model_type\n            is provided)\n            depths: (List(int)) Number of layers in each block. Default: [3, 3, 9, 3].\n            channels: (List(int)) Number of channels in each block. Default:\n                [96, 192, 384, 768].\n        model_type: (str) One of the ConvNext architecture types:\n            [\"tiny\", \"small\", \"base\", \"large\"]. Default: \"tiny\".\n        stem_patch_kernel: (int) Size of the convolutional kernels in the stem layer.\n            Default is 4.\n        stem_patch_stride: (int) Convolutional stride in the stem layer. Default is 2.\n        in_channels: (int) Number of input channels. Default is 1.\n        kernel_size: (int) Size of the convolutional kernels. Default is 3.\n        filters_rate: (float) Factor to adjust the number of filters per block.\n            Default is 2.\n        convs_per_block: (int) Number of convolutional layers per block. Default is 2.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed\n            convolutions for upsampling. Interpolation is faster but transposed\n            convolutions may be able to learn richer or more complex upsampling to\n            recover details from higher scales. Default: True.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output stride\n            of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n        max_stride: Factor by which input image size is reduced through the layers.\n            This is always `32` for all convnext architectures.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = \"small\"  # Options: tiny, small, base, large\n    arch: Optional[dict] = None\n    stem_patch_kernel: int = 4\n    stem_patch_stride: int = 2\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n    max_stride: int = 32\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        convnext_weights are one of\n        (\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        convnext_weights = [\n            \"ConvNeXt_Base_Weights\",\n            \"ConvNeXt_Tiny_Weights\",\n            \"ConvNeXt_Small_Weights\",\n            \"ConvNeXt_Large_Weights\",\n        ]\n\n        if value not in convnext_weights:\n            message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ConvNextSmallConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: convnext_weights are one of (     \"ConvNeXt_Base_Weights\",     \"ConvNeXt_Tiny_Weights\",     \"ConvNeXt_Small_Weights\",     \"ConvNeXt_Large_Weights\", )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    convnext_weights are one of\n    (\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    convnext_weights = [\n        \"ConvNeXt_Base_Weights\",\n        \"ConvNeXt_Tiny_Weights\",\n        \"ConvNeXt_Small_Weights\",\n        \"ConvNeXt_Large_Weights\",\n    ]\n\n    if value not in convnext_weights:\n        message = f\"Invalid pre-trained weights for ConvNext. Must be one of {convnext_weights}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.HeadConfig","title":"<code>HeadConfig</code>","text":"<p>Configurations related to the model output head type.</p> <p>Only one attribute of this class can be set, which defines the model output type.</p> <p>Attributes:</p> Name Type Description <code>single_instance</code> <code>Optional[SingleInstanceConfig]</code> <p>An instance of <code>SingleInstanceConfmapsHeadConfig</code>.</p> <code>centroid</code> <code>Optional[CentroidConfig]</code> <p>An instance of <code>CentroidsHeadConfig</code>.</p> <code>centered_instance</code> <code>Optional[CenteredInstanceConfig]</code> <p>An instance of <code>CenteredInstanceConfmapsHeadConfig</code>.</p> <code>bottomup</code> <code>Optional[BottomUpConfig]</code> <p>An instance of <code>BottomUpConfig</code>.</p> <code>multi_class_bottomup</code> <code>Optional[BottomUpMultiClassConfig]</code> <p>An instance of <code>BottomUpMultiClassConfig</code>.</p> <code>multi_class_topdown</code> <code>Optional[TopDownCenteredInstanceMultiClassConfig]</code> <p>An instance of <code>TopDownCenteredInstanceMultiClassConfig</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@oneof\n@define\nclass HeadConfig:\n    \"\"\"Configurations related to the model output head type.\n\n    Only one attribute of this class can be set, which defines the model output type.\n\n    Attributes:\n        single_instance: An instance of `SingleInstanceConfmapsHeadConfig`.\n        centroid: An instance of `CentroidsHeadConfig`.\n        centered_instance: An instance of `CenteredInstanceConfmapsHeadConfig`.\n        bottomup: An instance of `BottomUpConfig`.\n        multi_class_bottomup: An instance of `BottomUpMultiClassConfig`.\n        multi_class_topdown: An instance of `TopDownCenteredInstanceMultiClassConfig`.\n    \"\"\"\n\n    single_instance: Optional[SingleInstanceConfig] = None\n    centroid: Optional[CentroidConfig] = None\n    centered_instance: Optional[CenteredInstanceConfig] = None\n    bottomup: Optional[BottomUpConfig] = None\n    multi_class_bottomup: Optional[BottomUpMultiClassConfig] = None\n    multi_class_topdown: Optional[TopDownCenteredInstanceMultiClassConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>Configurations related to model architecture.</p> <p>Attributes:</p> Name Type Description <code>init_weights</code> <code>str</code> <p>(str) model weights initialization method. \"default\" uses kaiming uniform initialization and \"xavier\" uses Xavier initialization method.</p> <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file with which the backbone is initialized. If <code>None</code>, random init is used.</p> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path of the <code>ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file with which the head layers are initialized. If <code>None</code>, random init is used.</p> <code>backbone_config</code> <code>BackboneConfig</code> <p>initialize either UNetConfig, ConvNextConfig, or SwinTConfig based on input from backbone_type</p> <code>head_configs</code> <code>HeadConfig</code> <p>(Dict) Dictionary with the following keys having head configs for the model to be trained. Note: Configs should be provided only for the model to train and others should be None</p> <code>total_params</code> <code>Optional[int]</code> <p>(int) Total number of parameters in the model. This is automatically computed when the training starts.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass ModelConfig:\n    \"\"\"Configurations related to model architecture.\n\n    Attributes:\n        init_weights: (str) model weights initialization method. \"default\" uses kaiming\n            uniform initialization and \"xavier\" uses Xavier initialization method.\n        pretrained_backbone_weights: Path of the `ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file with which the backbone\n            is initialized. If `None`, random init is used.\n        pretrained_head_weights: Path of the `ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file with which the head layers\n            are initialized. If `None`, random init is used.\n        backbone_config: initialize either UNetConfig, ConvNextConfig, or SwinTConfig\n            based on input from backbone_type\n        head_configs: (Dict) Dictionary with the following keys having head configs for\n            the model to be trained. Note: Configs should be provided only for the model\n            to train and others should be None\n        total_params: (int) Total number of parameters in the model. This is automatically\n            computed when the training starts.\n    \"\"\"\n\n    init_weights: str = \"default\"\n    pretrained_backbone_weights: Optional[str] = None\n    pretrained_head_weights: Optional[str] = None\n    backbone_config: BackboneConfig = field(factory=BackboneConfig)\n    head_configs: HeadConfig = field(factory=HeadConfig)\n    total_params: Optional[int] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.PAFConfig","title":"<code>PAFConfig</code>","text":"<p>PAF configuration map.</p> <p>Attributes:</p> Name Type Description <code>edges</code> <code>Optional[List[List[str]]]</code> <p>(List[str]) None if edges from sio.Labels file can be used directly. Note: Only for 'PartAffinityFieldsHead'. List of indices (src, dest) that form an edge.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> <code>loss_weight</code> <code>Optional[float]</code> <p>(float) Scalar float used to weigh the loss term for this head during training. Increase this to encourage the optimization to focus on improving this specific output in multi-head models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass PAFConfig:\n    \"\"\"PAF configuration map.\n\n    Attributes:\n        edges: (List[str]) None if edges from sio.Labels file can be used directly.\n            Note: Only for 'PartAffinityFieldsHead'. List of indices (src, dest) that\n            form an edge.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as\n            a scalar float. Smaller values are more precise but may be difficult to\n            learn as they have a lower density within the image space. Larger values\n            are easier to learn but are less precise with respect to the peak\n            coordinate. This spread is in units of pixels of the model input image,\n            i.e., the image resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to\n            the input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the\n            input. Increasing this value can considerably speed up model performance\n            and decrease memory requirements, at the cost of decreased spatial\n            resolution.\n        loss_weight: (float) Scalar float used to weigh the loss term for this head\n            during training. Increase this to encourage the optimization to focus on\n            improving this specific output in multi-head models.\n    \"\"\"\n\n    edges: Optional[List[List[str]]] = None\n    sigma: float = 15.0\n    output_stride: int = 1\n    loss_weight: Optional[float] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SingleInstanceConfMapsConfig","title":"<code>SingleInstanceConfMapsConfig</code>","text":"<p>Single Instance configuration map.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>Optional[List[str]]</code> <p>(List[str]) None if nodes from sio.Labels file can be used directly. Else provide text name of the body parts (nodes) that the head will be configured to produce. The number of parts determines the number of channels in the output. If not specified, all body parts in the skeleton will be used. This config does not apply for 'PartAffinityFieldsHead'.</p> <code>sigma</code> <code>float</code> <p>(float) Spread of the Gaussian distribution of the confidence maps as a scalar float. Smaller values are more precise but may be difficult to learn as they have a lower density within the image space. Larger values are easier to learn but are less precise with respect to the peak coordinate. This spread is in units of pixels of the model input image, i.e., the image resolution after any input scaling is applied.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SingleInstanceConfMapsConfig:\n    \"\"\"Single Instance configuration map.\n\n    Attributes:\n        part_names: (List[str]) None if nodes from sio.Labels file can be used directly.\n            Else provide text name of the body parts (nodes) that the head will be\n            configured to produce. The number of parts determines the number of channels\n            in the output. If not specified, all body parts in the skeleton will be used.\n            This config does not apply for 'PartAffinityFieldsHead'.\n        sigma: (float) Spread of the Gaussian distribution of the confidence maps as a\n            scalar float. Smaller values are more precise but may be difficult to learn\n            as they have a lower density within the image space. Larger values are\n            easier to learn but are less precise with respect to the peak coordinate.\n            This spread is in units of pixels of the model input image,\n            i.e., the image resolution after any input scaling is applied.\n        output_stride: (int) The stride of the output confidence maps relative to the\n            input image. This is the reciprocal of the resolution, e.g., an output\n            stride of 2 results in confidence maps that are 0.5x the size of the input.\n            Increasing this value can considerably speed up model performance and\n            decrease memory requirements, at the cost of decreased spatial resolution.\n    \"\"\"\n\n    part_names: Optional[List[str]] = None\n    sigma: float = 5.0\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SingleInstanceConfig","title":"<code>SingleInstanceConfig</code>","text":"<p>single instance head_config.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SingleInstanceConfig:\n    \"\"\"single instance head_config.\"\"\"\n\n    confmaps: Optional[SingleInstanceConfMapsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTBaseConfig","title":"<code>SwinTBaseConfig</code>","text":"<p>               Bases: <code>SwinTConfig</code></p> <p>SwinT configuration for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].</p> <code>model_type</code> <code>str</code> <p>(str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. Default: <code>\"base\"</code>.</p> <code>arch</code> <code>Optional[dict]</code> <p>Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. Default: <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all swint architectures. Default: <code>32</code>.</p> <code>patch_size</code> <code>int</code> <p>(int) Patch size for the stem layer of SwinT. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Stride for the patch. Default: <code>2</code>.</p> <code>window_size</code> <code>int</code> <p>(int) Window size. Default: <code>7</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <p>Methods:</p> Name Description <code>validate_model_type</code> <p>Validate model_type.</p> <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SwinTBaseConfig(SwinTConfig):\n    \"\"\"SwinT configuration for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].\n        model_type: (str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. *Default*: `\"base\"`.\n        arch: Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. *Default*: `None`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all swint architectures. *Default*: `32`.\n        patch_size: (int) Patch size for the stem layer of SwinT. *Default*: `4`.\n        stem_patch_stride: (int) Stride for the patch. *Default*: `2`.\n        window_size: (int) Window size. *Default*: `7`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = field(\n        default=\"base\",\n        validator=lambda instance, attr, value: instance.validate_model_type(value),\n    )\n    arch: Optional[dict] = None\n    max_stride: int = 32\n    patch_size: int = 4\n    stem_patch_stride: int = 2\n    window_size: int = 7\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n\n    def validate_model_type(self, value):\n        \"\"\"Validate model_type.\n\n        Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n        \"\"\"\n        valid_types = [\"tiny\", \"small\", \"base\"]\n        if value not in valid_types:\n            message = f\"Invalid model_type. Must be one of {valid_types}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        swint_weights are one of\n        (\n            \"Swin_T_Weights\",\n            \"Swin_S_Weights\",\n            \"Swin_B_Weights\"\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n        if value not in swint_weights:\n            message = (\n                f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTBaseConfig.validate_model_type","title":"<code>validate_model_type(value)</code>","text":"<p>Validate model_type.</p> <p>Ensure model_type is one of \"tiny\", \"small\", or \"base\".</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_model_type(self, value):\n    \"\"\"Validate model_type.\n\n    Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n    \"\"\"\n    valid_types = [\"tiny\", \"small\", \"base\"]\n    if value not in valid_types:\n        message = f\"Invalid model_type. Must be one of {valid_types}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTBaseConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: swint_weights are one of (     \"Swin_T_Weights\",     \"Swin_S_Weights\",     \"Swin_B_Weights\" )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    swint_weights are one of\n    (\n        \"Swin_T_Weights\",\n        \"Swin_S_Weights\",\n        \"Swin_B_Weights\"\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n    if value not in swint_weights:\n        message = (\n            f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTConfig","title":"<code>SwinTConfig</code>","text":"<p>SwinT configuration (tiny) for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].</p> <code>model_type</code> <code>str</code> <p>(str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. Default: <code>\"tiny\"</code>.</p> <code>arch</code> <code>Optional[dict]</code> <p>Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. Default: <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all swint architectures. Default: <code>32</code>.</p> <code>patch_size</code> <code>int</code> <p>(int) Patch size for the stem layer of SwinT. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Stride for the patch. Default: <code>2</code>.</p> <code>window_size</code> <code>int</code> <p>(int) Window size. Default: <code>7</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <p>Methods:</p> Name Description <code>validate_model_type</code> <p>Validate model_type.</p> <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SwinTConfig:\n    \"\"\"SwinT configuration (tiny) for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].\n        model_type: (str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. *Default*: `\"tiny\"`.\n        arch: Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. *Default*: `None`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all swint architectures. *Default*: `32`.\n        patch_size: (int) Patch size for the stem layer of SwinT. *Default*: `4`.\n        stem_patch_stride: (int) Stride for the patch. *Default*: `2`.\n        window_size: (int) Window size. *Default*: `7`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = field(\n        default=\"tiny\",\n        validator=lambda instance, attr, value: instance.validate_model_type(value),\n    )\n    arch: Optional[dict] = None\n    max_stride: int = 32\n    patch_size: int = 4\n    stem_patch_stride: int = 2\n    window_size: int = 7\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n\n    def validate_model_type(self, value):\n        \"\"\"Validate model_type.\n\n        Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n        \"\"\"\n        valid_types = [\"tiny\", \"small\", \"base\"]\n        if value not in valid_types:\n            message = f\"Invalid model_type. Must be one of {valid_types}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        swint_weights are one of\n        (\n            \"Swin_T_Weights\",\n            \"Swin_S_Weights\",\n            \"Swin_B_Weights\"\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n        if value not in swint_weights:\n            message = (\n                f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTConfig.validate_model_type","title":"<code>validate_model_type(value)</code>","text":"<p>Validate model_type.</p> <p>Ensure model_type is one of \"tiny\", \"small\", or \"base\".</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_model_type(self, value):\n    \"\"\"Validate model_type.\n\n    Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n    \"\"\"\n    valid_types = [\"tiny\", \"small\", \"base\"]\n    if value not in valid_types:\n        message = f\"Invalid model_type. Must be one of {valid_types}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: swint_weights are one of (     \"Swin_T_Weights\",     \"Swin_S_Weights\",     \"Swin_B_Weights\" )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    swint_weights are one of\n    (\n        \"Swin_T_Weights\",\n        \"Swin_S_Weights\",\n        \"Swin_B_Weights\"\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n    if value not in swint_weights:\n        message = (\n            f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTSmallConfig","title":"<code>SwinTSmallConfig</code>","text":"<p>               Bases: <code>SwinTConfig</code></p> <p>SwinT configuration (small) for backbone.</p> <p>Attributes:</p> Name Type Description <code>pre_trained_weights</code> <code>Optional[str]</code> <p>(str) Pretrained weights file name supported only for SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].</p> <code>model_type</code> <code>str</code> <p>(str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. Default: <code>\"small\"</code>.</p> <code>arch</code> <code>Optional[dict]</code> <p>Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. Default: <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Factor by which input image size is reduced through the layers. This is always <code>32</code> for all swint architectures. Default: <code>32</code>.</p> <code>patch_size</code> <code>int</code> <p>(int) Patch size for the stem layer of SwinT. Default: <code>4</code>.</p> <code>stem_patch_stride</code> <code>int</code> <p>(int) Stride for the patch. Default: <code>2</code>.</p> <code>window_size</code> <code>int</code> <p>(int) Window size. Default: <code>7</code>.</p> <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> <p>Methods:</p> Name Description <code>validate_model_type</code> <p>Validate model_type.</p> <code>validate_pre_trained_weights</code> <p>Validate pre_trained_weights.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass SwinTSmallConfig(SwinTConfig):\n    \"\"\"SwinT configuration (small) for backbone.\n\n    Attributes:\n        pre_trained_weights: (str) Pretrained weights file name supported only for\n            SwinT backbones. For SwinT, one of [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"].\n        model_type: (str) One of the SwinT architecture types: [\"tiny\", \"small\", \"base\"]. *Default*: `\"small\"`.\n        arch: Dictionary of embed dimension, depths and number of heads in each layer. Default is \"Tiny architecture\". {'embed': 96, 'depths': [2,2,6,2], 'channels':[3, 6, 12, 24]}. *Default*: `None`.\n        max_stride: (int) Factor by which input image size is reduced through the layers. This is always `32` for all swint architectures. *Default*: `32`.\n        patch_size: (int) Patch size for the stem layer of SwinT. *Default*: `4`.\n        stem_patch_stride: (int) Stride for the patch. *Default*: `2`.\n        window_size: (int) Window size. *Default*: `7`.\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    pre_trained_weights: Optional[str] = field(\n        default=None,\n        validator=lambda instance, attr, value: instance.validate_pre_trained_weights(\n            value\n        ),\n    )\n    model_type: str = field(\n        default=\"small\",\n        validator=lambda instance, attr, value: instance.validate_model_type(value),\n    )\n    arch: Optional[dict] = None\n    max_stride: int = 32\n    patch_size: int = 4\n    stem_patch_stride: int = 2\n    window_size: int = 7\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters_rate: float = 2\n    convs_per_block: int = 2\n    up_interpolate: bool = True\n    output_stride: int = 1\n\n    def validate_model_type(self, value):\n        \"\"\"Validate model_type.\n\n        Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n        \"\"\"\n        valid_types = [\"tiny\", \"small\", \"base\"]\n        if value not in valid_types:\n            message = f\"Invalid model_type. Must be one of {valid_types}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    def validate_pre_trained_weights(self, value):\n        \"\"\"Validate pre_trained_weights.\n\n        Check:\n        swint_weights are one of\n        (\n            \"Swin_T_Weights\",\n            \"Swin_S_Weights\",\n            \"Swin_B_Weights\"\n        )\n        \"\"\"\n        if value is None:\n            return\n\n        swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n        if value not in swint_weights:\n            message = (\n                f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTSmallConfig.validate_model_type","title":"<code>validate_model_type(value)</code>","text":"<p>Validate model_type.</p> <p>Ensure model_type is one of \"tiny\", \"small\", or \"base\".</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_model_type(self, value):\n    \"\"\"Validate model_type.\n\n    Ensure model_type is one of \"tiny\", \"small\", or \"base\".\n    \"\"\"\n    valid_types = [\"tiny\", \"small\", \"base\"]\n    if value not in valid_types:\n        message = f\"Invalid model_type. Must be one of {valid_types}\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.SwinTSmallConfig.validate_pre_trained_weights","title":"<code>validate_pre_trained_weights(value)</code>","text":"<p>Validate pre_trained_weights.</p> <p>Check: swint_weights are one of (     \"Swin_T_Weights\",     \"Swin_S_Weights\",     \"Swin_B_Weights\" )</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def validate_pre_trained_weights(self, value):\n    \"\"\"Validate pre_trained_weights.\n\n    Check:\n    swint_weights are one of\n    (\n        \"Swin_T_Weights\",\n        \"Swin_S_Weights\",\n        \"Swin_B_Weights\"\n    )\n    \"\"\"\n    if value is None:\n        return\n\n    swint_weights = [\"Swin_T_Weights\", \"Swin_S_Weights\", \"Swin_B_Weights\"]\n\n    if value not in swint_weights:\n        message = (\n            f\"Invalid pre-trained weights for SwinT. Must be one of {swint_weights}\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.TopDownCenteredInstanceMultiClassConfig","title":"<code>TopDownCenteredInstanceMultiClassConfig</code>","text":"<p>Head config for TopDown centered instance ID models.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass TopDownCenteredInstanceMultiClassConfig:\n    \"\"\"Head config for TopDown centered instance ID models.\"\"\"\n\n    confmaps: Optional[CenteredInstanceConfMapsConfig] = None\n    class_vectors: Optional[ClassVectorsConfig] = None\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.UNetConfig","title":"<code>UNetConfig</code>","text":"<p>UNet config for backbone.</p> <p>Attributes:</p> Name Type Description <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters</code> <code>int</code> <p>(int) Base number of filters in the network. Default: <code>32</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>1.5</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Scalar integer specifying the maximum stride that the image must be divisible by. Default: <code>16</code>.</p> <code>stem_stride</code> <code>Optional[int]</code> <p>(int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. Default: <code>None</code>.</p> <code>middle_block</code> <code>bool</code> <p>(bool) If True, add an additional block at the end of the encoder. Default: <code>True</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>stacks</code> <code>int</code> <p>(int) Number of upsampling blocks in the decoder. Default: <code>1</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass UNetConfig:\n    \"\"\"UNet config for backbone.\n\n    Attributes:\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters: (int) Base number of filters in the network. *Default*: `32`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `1.5`.\n        max_stride: (int) Scalar integer specifying the maximum stride that the image must be divisible by. *Default*: `16`.\n        stem_stride: (int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. *Default*: `None`.\n        middle_block: (bool) If True, add an additional block at the end of the encoder. *Default*: `True`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        stacks: (int) Number of upsampling blocks in the decoder. *Default*: `1`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters: int = 32\n    filters_rate: float = 1.5\n    max_stride: int = 16\n    stem_stride: Optional[int] = None\n    middle_block: bool = True\n    up_interpolate: bool = True\n    stacks: int = 1\n    convs_per_block: int = 2\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.UNetLargeRFConfig","title":"<code>UNetLargeRFConfig</code>","text":"<p>               Bases: <code>UNetConfig</code></p> <p>UNet config for backbone with large receptive field.</p> <p>Attributes:</p> Name Type Description <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters</code> <code>int</code> <p>(int) Base number of filters in the network. Default: <code>24</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>1.5</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Scalar integer specifying the maximum stride that the image must be divisible by. Default: <code>32</code>.</p> <code>stem_stride</code> <code>Optional[int]</code> <p>(int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. Default: <code>None</code>.</p> <code>middle_block</code> <code>bool</code> <p>(bool) If True, add an additional block at the end of the encoder. Default: <code>True</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>stacks</code> <code>int</code> <p>(int) Number of upsampling blocks in the decoder. Default: <code>1</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass UNetLargeRFConfig(UNetConfig):\n    \"\"\"UNet config for backbone with large receptive field.\n\n    Attributes:\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters: (int) Base number of filters in the network. *Default*: `24`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `1.5`.\n        max_stride: (int) Scalar integer specifying the maximum stride that the image must be divisible by. *Default*: `32`.\n        stem_stride: (int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. *Default*: `None`.\n        middle_block: (bool) If True, add an additional block at the end of the encoder. *Default*: `True`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        stacks: (int) Number of upsampling blocks in the decoder. *Default*: `1`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters: int = 24\n    filters_rate: float = 1.5\n    max_stride: int = 32\n    stem_stride: Optional[int] = None\n    middle_block: bool = True\n    up_interpolate: bool = True\n    stacks: int = 1\n    convs_per_block: int = 2\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.UNetMediumRFConfig","title":"<code>UNetMediumRFConfig</code>","text":"<p>               Bases: <code>UNetConfig</code></p> <p>UNet config for backbone with medium receptive field.</p> <p>Attributes:</p> Name Type Description <code>in_channels</code> <code>int</code> <p>(int) Number of input channels. Default: <code>1</code>.</p> <code>kernel_size</code> <code>int</code> <p>(int) Size of the convolutional kernels. Default: <code>3</code>.</p> <code>filters</code> <code>int</code> <p>(int) Base number of filters in the network. Default: <code>32</code>.</p> <code>filters_rate</code> <code>float</code> <p>(float) Factor to adjust the number of filters per block. Default: <code>2</code>.</p> <code>max_stride</code> <code>int</code> <p>(int) Scalar integer specifying the maximum stride that the image must be divisible by. Default: <code>16</code>.</p> <code>stem_stride</code> <code>Optional[int]</code> <p>(int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. Default: <code>None</code>.</p> <code>middle_block</code> <code>bool</code> <p>(bool) If True, add an additional block at the end of the encoder. Default: <code>True</code>.</p> <code>up_interpolate</code> <code>bool</code> <p>(bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. Default: <code>True</code>.</p> <code>stacks</code> <code>int</code> <p>(int) Number of upsampling blocks in the decoder. Default: <code>1</code>.</p> <code>convs_per_block</code> <code>int</code> <p>(int) Number of convolutional layers per block. Default: <code>2</code>.</p> <code>output_stride</code> <code>int</code> <p>(int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. Default: <code>1</code>.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>@define\nclass UNetMediumRFConfig(UNetConfig):\n    \"\"\"UNet config for backbone with medium receptive field.\n\n    Attributes:\n        in_channels: (int) Number of input channels. *Default*: `1`.\n        kernel_size: (int) Size of the convolutional kernels. *Default*: `3`.\n        filters: (int) Base number of filters in the network. *Default*: `32`.\n        filters_rate: (float) Factor to adjust the number of filters per block. *Default*: `2`.\n        max_stride: (int) Scalar integer specifying the maximum stride that the image must be divisible by. *Default*: `16`.\n        stem_stride: (int) If not None, will create additional \"down\" blocks for initial downsampling based on the stride. These will be configured identically to the down blocks below. *Default*: `None`.\n        middle_block: (bool) If True, add an additional block at the end of the encoder. *Default*: `True`.\n        up_interpolate: (bool) If True, use bilinear interpolation instead of transposed convolutions for upsampling. Interpolation is faster but transposed convolutions may be able to learn richer or more complex upsampling to recover details from higher scales. *Default*: `True`.\n        stacks: (int) Number of upsampling blocks in the decoder. *Default*: `1`.\n        convs_per_block: (int) Number of convolutional layers per block. *Default*: `2`.\n        output_stride: (int) The stride of the output confidence maps relative to the input image. This is the reciprocal of the resolution, e.g., an output stride of 2 results in confidence maps that are 0.5x the size of the input. Increasing this value can considerably speed up model performance and decrease memory requirements, at the cost of decreased spatial resolution. *Default*: `1`.\n    \"\"\"\n\n    in_channels: int = 1\n    kernel_size: int = 3\n    filters: int = 32\n    filters_rate: float = 2\n    max_stride: int = 16\n    stem_stride: Optional[int] = None\n    middle_block: bool = True\n    up_interpolate: bool = True\n    stacks: int = 1\n    convs_per_block: int = 2\n    output_stride: int = 1\n</code></pre>"},{"location":"api/config/model_config/#sleap_nn.config.model_config.model_mapper","title":"<code>model_mapper(legacy_config)</code>","text":"<p>Map the legacy model configuration to the new model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_config</code> <code>dict</code> <p>A dictionary containing the legacy model configuration.</p> required <p>Returns:</p> Type Description <code>ModelConfig</code> <p>An instance of <code>ModelConfig</code> with the mapped configuration.</p> Source code in <code>sleap_nn/config/model_config.py</code> <pre><code>def model_mapper(legacy_config: dict) -&gt; ModelConfig:\n    \"\"\"Map the legacy model configuration to the new model configuration.\n\n    Args:\n        legacy_config: A dictionary containing the legacy model configuration.\n\n    Returns:\n        An instance of `ModelConfig` with the mapped configuration.\n    \"\"\"\n    legacy_config_model = legacy_config.get(\"model\", {})\n    backbone_cfg_args = {}\n    head_cfg_args = {}\n    if legacy_config_model.get(\"backbone\", {}).get(\"unet\", None) is not None:\n        backbone_cfg_args[\"unet\"] = UNetConfig(\n            filters=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"filters\", 32),\n            filters_rate=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"filters_rate\", 1.5),\n            max_stride=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"max_stride\", 16),\n            stem_stride=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"stem_stride\", 16),\n            middle_block=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"middle_block\", True),\n            up_interpolate=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"up_interpolate\", True),\n            stacks=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"stacks\", 1),\n            output_stride=legacy_config_model.get(\"backbone\", {})\n            .get(\"unet\", {})\n            .get(\"output_stride\", 1),\n        )\n\n    backbone_cfg = BackboneConfig(**backbone_cfg_args)\n\n    if legacy_config_model.get(\"heads\", {}).get(\"single_instance\", None) is not None:\n        head_cfg_args[\"single_instance\"] = SingleInstanceConfig(\n            confmaps=SingleInstanceConfMapsConfig(\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"single_instance\", {})\n                .get(\"part_names\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"single_instance\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"single_instance\", {})\n                .get(\"output_stride\", 1),\n            )\n        )\n    if legacy_config_model.get(\"heads\", {}).get(\"centroid\", None) is not None:\n        head_cfg_args[\"centroid\"] = CentroidConfig(\n            confmaps=CentroidConfMapsConfig(\n                anchor_part=legacy_config_model.get(\"heads\", {})\n                .get(\"centroid\", {})\n                .get(\"anchor_part\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"centroid\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"centroid\", {})\n                .get(\"output_stride\", 1),\n            )\n        )\n    if legacy_config_model.get(\"heads\", {}).get(\"centered_instance\", None) is not None:\n        head_cfg_args[\"centered_instance\"] = CenteredInstanceConfig(\n            confmaps=CenteredInstanceConfMapsConfig(\n                anchor_part=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"anchor_part\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"output_stride\", 1),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"centered_instance\", {})\n                .get(\"part_names\", None),\n            )\n        )\n    if legacy_config_model.get(\"heads\", {}).get(\"multi_instance\", None) is not None:\n        head_cfg_args[\"bottomup\"] = BottomUpConfig(\n            confmaps=BottomUpConfMapsConfig(\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"loss_weight\", 1.0),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"output_stride\", 1),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"confmaps\", {})\n                .get(\"part_names\", None),\n            ),\n            pafs=PAFConfig(\n                edges=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"edges\", None),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"sigma\", 15.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"output_stride\", 1),\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_instance\", {})\n                .get(\"pafs\", {})\n                .get(\"loss_weight\", 1.0),\n            ),\n        )\n    if (\n        legacy_config_model.get(\"heads\", {}).get(\"multi_class_bottomup\", None)\n        is not None\n    ):\n        head_cfg_args[\"multi_class_bottomup\"] = BottomUpMultiClassConfig(\n            confmaps=BottomUpConfMapsConfig(\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"loss_weight\", 1.0),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"output_stride\", 1),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"confmaps\", {})\n                .get(\"part_names\", None),\n            ),\n            class_maps=ClassMapConfig(\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"sigma\", 15.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"output_stride\", 1),\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"loss_weight\", 1.0),\n                classes=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_bottomup\", {})\n                .get(\"class_maps\", {})\n                .get(\"classes\", None),\n            ),\n        )\n\n    if (\n        legacy_config_model.get(\"heads\", {}).get(\"multi_class_topdown\", None)\n        is not None\n    ):\n        head_cfg_args[\"multi_class_topdown\"] = TopDownCenteredInstanceMultiClassConfig(\n            confmaps=CenteredInstanceConfMapsConfig(\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"loss_weight\", 1.0),\n                sigma=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"sigma\", 5.0),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"output_stride\", 1),\n                anchor_part=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"anchor_part\", None),\n                part_names=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"confmaps\", {})\n                .get(\"part_names\", None),\n            ),\n            class_vectors=ClassVectorsConfig(\n                classes=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"classes\", None),\n                num_fc_layers=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"num_fc_layers\", 2),\n                num_fc_units=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"num_fc_units\", 1024),\n                global_pool=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"global_pool\", True),\n                output_stride=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"output_stride\", 1),\n                loss_weight=legacy_config_model.get(\"heads\", {})\n                .get(\"multi_class_topdown\", {})\n                .get(\"class_vectors\", {})\n                .get(\"loss_weight\", 1.0),\n            ),\n        )\n\n    head_cfg = HeadConfig(**head_cfg_args)\n\n    trained_weights_path = legacy_config_model.get(\"base_checkpoint\", None)\n\n    return ModelConfig(\n        backbone_config=backbone_cfg,\n        head_configs=head_cfg,\n        pretrained_backbone_weights=trained_weights_path,\n        pretrained_head_weights=trained_weights_path,\n    )\n</code></pre>"},{"location":"api/config/trainer_config/","title":"trainer_config","text":""},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config","title":"<code>sleap_nn.config.trainer_config</code>","text":"<p>Serializable configuration classes for specifying all trainer config parameters.</p> <p>These configuration classes are intended to specify all the parameters required to initialize the trainer config.</p> <p>Classes:</p> Name Description <code>CosineAnnealingWarmupConfig</code> <p>Configuration for Cosine Annealing with Linear Warmup scheduler.</p> <code>DataLoaderConfig</code> <p>Train DataLoaderConfig.</p> <code>EarlyStoppingConfig</code> <p>Configuration for early_stopping.</p> <code>EvalConfig</code> <p>Configuration for epoch-end evaluation.</p> <code>HardKeypointMiningConfig</code> <p>Configuration for online hard keypoint mining.</p> <code>LRSchedulerConfig</code> <p>Configuration for lr_scheduler.</p> <code>LinearWarmupLinearDecayConfig</code> <p>Configuration for Linear Warmup + Linear Decay scheduler.</p> <code>ModelCkptConfig</code> <p>Configuration for model checkpoint.</p> <code>OptimizerConfig</code> <p>Configuration for optimizer.</p> <code>ReduceLROnPlateauConfig</code> <p>Configuration for ReduceLROnPlateau scheduler.</p> <code>StepLRConfig</code> <p>Configuration for StepLR scheduler.</p> <code>TrainDataLoaderConfig</code> <p>Train DataLoaderConfig.</p> <code>TrainerConfig</code> <p>Configuration for trainer.</p> <code>ValDataLoaderConfig</code> <p>Validation DataLoaderConfig.</p> <code>WandBConfig</code> <p>Configuration for WandB.</p> <code>ZMQConfig</code> <p>Configuration of ZeroMQ-based monitoring of the training.</p> <p>Functions:</p> Name Description <code>trainer_mapper</code> <p>Map the legacy trainer configuration to the new trainer configuration.</p>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.CosineAnnealingWarmupConfig","title":"<code>CosineAnnealingWarmupConfig</code>","text":"<p>Configuration for Cosine Annealing with Linear Warmup scheduler.</p> <p>The learning rate increases linearly during warmup, then decreases following a cosine curve to the minimum value.</p> <p>Attributes:</p> Name Type Description <code>warmup_epochs</code> <code>int</code> <p>(int) Number of epochs for linear warmup phase. Default: <code>5</code>.</p> <code>max_epochs</code> <code>Optional[int]</code> <p>(int) Total number of training epochs. Will be overridden by trainer's max_epochs if not specified. Default: <code>None</code>.</p> <code>warmup_start_lr</code> <code>float</code> <p>(float) Learning rate at start of warmup. Default: <code>0.0</code>.</p> <code>eta_min</code> <code>float</code> <p>(float) Minimum learning rate at end of cosine decay. Default: <code>0.0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass CosineAnnealingWarmupConfig:\n    \"\"\"Configuration for Cosine Annealing with Linear Warmup scheduler.\n\n    The learning rate increases linearly during warmup, then decreases following\n    a cosine curve to the minimum value.\n\n    Attributes:\n        warmup_epochs: (int) Number of epochs for linear warmup phase. *Default*: `5`.\n        max_epochs: (int) Total number of training epochs. Will be overridden by\n            trainer's max_epochs if not specified. *Default*: `None`.\n        warmup_start_lr: (float) Learning rate at start of warmup. *Default*: `0.0`.\n        eta_min: (float) Minimum learning rate at end of cosine decay. *Default*: `0.0`.\n    \"\"\"\n\n    warmup_epochs: int = field(default=5, validator=validators.ge(0))\n    max_epochs: Optional[int] = None\n    warmup_start_lr: float = field(default=0.0, validator=validators.ge(0))\n    eta_min: float = field(default=0.0, validator=validators.ge(0))\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.DataLoaderConfig","title":"<code>DataLoaderConfig</code>","text":"<p>Train DataLoaderConfig.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch or batch size for training/validation data. Default: <code>4</code>.</p> <code>shuffle</code> <code>bool</code> <p>(bool) True to have the data reshuffled at every epoch. Default: <code>False</code>.</p> <code>num_workers</code> <code>int</code> <p>(int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: <code>0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass DataLoaderConfig:\n    \"\"\"Train DataLoaderConfig.\n\n    Attributes:\n        batch_size: (int) Number of samples per batch or batch size for training/validation data. *Default*: `4`.\n        shuffle: (bool) True to have the data reshuffled at every epoch. *Default*: `False`.\n        num_workers: (int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. *Default*: `0`.\n    \"\"\"\n\n    batch_size: int = 4\n    shuffle: bool = False\n    num_workers: int = 0\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.EarlyStoppingConfig","title":"<code>EarlyStoppingConfig</code>","text":"<p>Configuration for early_stopping.</p> <p>Attributes:</p> Name Type Description <code>stop_training_on_plateau</code> <code>bool</code> <p>(bool) True if early stopping should be enabled. Default: <code>True</code>.</p> <code>min_delta</code> <code>float</code> <p>(float) Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. Default: <code>1e-8</code>.</p> <code>patience</code> <code>int</code> <p>(int) Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. Default: <code>10</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass EarlyStoppingConfig:\n    \"\"\"Configuration for early_stopping.\n\n    Attributes:\n        stop_training_on_plateau: (bool) True if early stopping should be enabled. *Default*: `True`.\n        min_delta: (float) Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement. *Default*: `1e-8`.\n        patience: (int) Number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. *Default*: `10`.\n    \"\"\"\n\n    min_delta: float = field(default=1e-8, validator=validators.ge(0))\n    patience: int = field(default=10, validator=validators.ge(0))\n    stop_training_on_plateau: bool = True\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.EvalConfig","title":"<code>EvalConfig</code>","text":"<p>Configuration for epoch-end evaluation.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>(bool) Enable epoch-end evaluation metrics. Default: <code>False</code>.</p> <code>frequency</code> <code>int</code> <p>(int) Evaluate every N epochs. Default: <code>1</code>.</p> <code>oks_stddev</code> <code>float</code> <p>(float) OKS standard deviation for evaluation. Default: <code>0.025</code>.</p> <code>oks_scale</code> <code>Optional[float]</code> <p>(float) OKS scale override. If None, uses default. Default: <code>None</code>.</p> <code>match_threshold</code> <code>float</code> <p>(float) Maximum distance in pixels for centroid matching. Only used for centroid model evaluation. Default: <code>50.0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass EvalConfig:\n    \"\"\"Configuration for epoch-end evaluation.\n\n    Attributes:\n        enabled: (bool) Enable epoch-end evaluation metrics. *Default*: `False`.\n        frequency: (int) Evaluate every N epochs. *Default*: `1`.\n        oks_stddev: (float) OKS standard deviation for evaluation. *Default*: `0.025`.\n        oks_scale: (float) OKS scale override. If None, uses default. *Default*: `None`.\n        match_threshold: (float) Maximum distance in pixels for centroid matching.\n            Only used for centroid model evaluation. *Default*: `50.0`.\n    \"\"\"\n\n    enabled: bool = False\n    frequency: int = field(default=1, validator=validators.ge(1))\n    oks_stddev: float = field(default=0.025, validator=validators.gt(0))\n    oks_scale: Optional[float] = None\n    match_threshold: float = field(default=50.0, validator=validators.gt(0))\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.HardKeypointMiningConfig","title":"<code>HardKeypointMiningConfig</code>","text":"<p>Configuration for online hard keypoint mining.</p> <p>Attributes:</p> Name Type Description <code>online_mining</code> <code>bool</code> <p>If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss. Default: <code>False</code>.</p> <code>hard_to_easy_ratio</code> <code>float</code> <p>The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training. Default: <code>2.0</code>.</p> <code>min_hard_keypoints</code> <code>int</code> <p>The minimum number of keypoints that will be considered as \"hard\", even if they are not below the <code>hard_to_easy_ratio</code>. Default: <code>2</code>.</p> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether. Default: <code>None</code>.</p> <code>loss_scale</code> <code>float</code> <p>Factor to scale the hard keypoint losses by. Default: <code>5.0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass HardKeypointMiningConfig:\n    \"\"\"Configuration for online hard keypoint mining.\n\n    Attributes:\n        online_mining: If True, online hard keypoint mining (OHKM) will be enabled. When this is enabled, the loss is computed per keypoint (or edge for PAFs) and sorted from lowest (easy) to highest (hard). The hard keypoint loss will be scaled to have a higher weight in the total loss, encouraging the training to focus on tricky body parts that are more difficult to learn. If False, no mining will be performed and all keypoints will be weighted equally in the loss. *Default*: `False`.\n        hard_to_easy_ratio: The minimum ratio of the individual keypoint loss with respect to the lowest keypoint loss in order to be considered as \"hard\". This helps to switch focus on across groups of keypoints during training. *Default*: `2.0`.\n        min_hard_keypoints: The minimum number of keypoints that will be considered as \"hard\", even if they are not below the `hard_to_easy_ratio`. *Default*: `2`.\n        max_hard_keypoints: The maximum number of hard keypoints to apply scaling to. This can help when there are few very easy keypoints which may skew the ratio and result in loss scaling being applied to most keypoints, which can reduce the impact of hard mining altogether. *Default*: `None`.\n        loss_scale: Factor to scale the hard keypoint losses by. *Default*: `5.0`.\n    \"\"\"\n\n    online_mining: bool = False\n    hard_to_easy_ratio: float = 2.0\n    min_hard_keypoints: int = 2\n    max_hard_keypoints: Optional[int] = None\n    loss_scale: float = 5.0\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.LRSchedulerConfig","title":"<code>LRSchedulerConfig</code>","text":"<p>Configuration for lr_scheduler.</p> <p>Only one scheduler should be configured at a time. If multiple are set, priority order is: cosine_annealing_warmup &gt; linear_warmup_linear_decay &gt; step_lr &gt; reduce_lr_on_plateau.</p> <p>Attributes:</p> Name Type Description <code>step_lr</code> <code>Optional[StepLRConfig]</code> <p>Configuration for StepLR scheduler.</p> <code>reduce_lr_on_plateau</code> <code>Optional[ReduceLROnPlateauConfig]</code> <p>Configuration for ReduceLROnPlateau scheduler.</p> <code>cosine_annealing_warmup</code> <code>Optional[CosineAnnealingWarmupConfig]</code> <p>Configuration for Cosine Annealing with Linear Warmup scheduler.</p> <code>linear_warmup_linear_decay</code> <code>Optional[LinearWarmupLinearDecayConfig]</code> <p>Configuration for Linear Warmup + Linear Decay scheduler.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass LRSchedulerConfig:\n    \"\"\"Configuration for lr_scheduler.\n\n    Only one scheduler should be configured at a time. If multiple are set,\n    priority order is: cosine_annealing_warmup &gt; linear_warmup_linear_decay &gt;\n    step_lr &gt; reduce_lr_on_plateau.\n\n    Attributes:\n        step_lr: Configuration for StepLR scheduler.\n        reduce_lr_on_plateau: Configuration for ReduceLROnPlateau scheduler.\n        cosine_annealing_warmup: Configuration for Cosine Annealing with Linear Warmup scheduler.\n        linear_warmup_linear_decay: Configuration for Linear Warmup + Linear Decay scheduler.\n    \"\"\"\n\n    step_lr: Optional[StepLRConfig] = None\n    reduce_lr_on_plateau: Optional[ReduceLROnPlateauConfig] = field(\n        factory=ReduceLROnPlateauConfig\n    )\n    cosine_annealing_warmup: Optional[CosineAnnealingWarmupConfig] = None\n    linear_warmup_linear_decay: Optional[LinearWarmupLinearDecayConfig] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.LinearWarmupLinearDecayConfig","title":"<code>LinearWarmupLinearDecayConfig</code>","text":"<p>Configuration for Linear Warmup + Linear Decay scheduler.</p> <p>The learning rate increases linearly during warmup, then decreases linearly to the end learning rate.</p> <p>Attributes:</p> Name Type Description <code>warmup_epochs</code> <code>int</code> <p>(int) Number of epochs for linear warmup phase. Default: <code>5</code>.</p> <code>max_epochs</code> <code>Optional[int]</code> <p>(int) Total number of training epochs. Will be overridden by trainer's max_epochs if not specified. Default: <code>None</code>.</p> <code>warmup_start_lr</code> <code>float</code> <p>(float) Learning rate at start of warmup. Default: <code>0.0</code>.</p> <code>end_lr</code> <code>float</code> <p>(float) Learning rate at end of training. Default: <code>0.0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass LinearWarmupLinearDecayConfig:\n    \"\"\"Configuration for Linear Warmup + Linear Decay scheduler.\n\n    The learning rate increases linearly during warmup, then decreases linearly\n    to the end learning rate.\n\n    Attributes:\n        warmup_epochs: (int) Number of epochs for linear warmup phase. *Default*: `5`.\n        max_epochs: (int) Total number of training epochs. Will be overridden by\n            trainer's max_epochs if not specified. *Default*: `None`.\n        warmup_start_lr: (float) Learning rate at start of warmup. *Default*: `0.0`.\n        end_lr: (float) Learning rate at end of training. *Default*: `0.0`.\n    \"\"\"\n\n    warmup_epochs: int = field(default=5, validator=validators.ge(0))\n    max_epochs: Optional[int] = None\n    warmup_start_lr: float = field(default=0.0, validator=validators.ge(0))\n    end_lr: float = field(default=0.0, validator=validators.ge(0))\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ModelCkptConfig","title":"<code>ModelCkptConfig</code>","text":"<p>Configuration for model checkpoint.</p> <p>Any parameters from Lightning's ModelCheckpoint could be used.</p> <p>Attributes:</p> Name Type Description <code>save_top_k</code> <code>int</code> <p>(int) If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. Default: <code>1</code>.</p> <code>save_last</code> <code>Optional[bool]</code> <p>(bool) When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. Default: <code>None</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass ModelCkptConfig:\n    \"\"\"Configuration for model checkpoint.\n\n    Any parameters from Lightning's ModelCheckpoint could be used.\n\n    Attributes:\n        save_top_k: (int) If save_top_k == k, the best k models according to the quantity monitored will be saved. If save_top_k == 0, no models are saved. If save_top_k == -1, all models are saved. Please note that the monitors are checked every every_n_epochs epochs. if save_top_k &gt;= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v1 unless enable_version_counter is set to False. *Default*: `1`.\n        save_last: (bool) When True, saves a last.ckpt whenever a checkpoint file gets saved. On a local filesystem, this will be a symbolic link, and otherwise a copy of the checkpoint file. This allows accessing the latest checkpoint in a deterministic manner. *Default*: `None`.\n    \"\"\"\n\n    save_top_k: int = 1\n    save_last: Optional[bool] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.OptimizerConfig","title":"<code>OptimizerConfig</code>","text":"<p>Configuration for optimizer.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>(float) Learning rate of type float. Default: <code>1e-4</code>.</p> <code>amsgrad</code> <code>bool</code> <p>(bool) Enable AMSGrad with the optimizer. Default: <code>False</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass OptimizerConfig:\n    \"\"\"Configuration for optimizer.\n\n    Attributes:\n        lr: (float) Learning rate of type float. *Default*: `1e-4`.\n        amsgrad: (bool) Enable AMSGrad with the optimizer. *Default*: `False`.\n    \"\"\"\n\n    lr: float = field(default=1e-4, validator=validators.gt(0))\n    amsgrad: bool = False\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ReduceLROnPlateauConfig","title":"<code>ReduceLROnPlateauConfig</code>","text":"<p>Configuration for ReduceLROnPlateau scheduler.</p> <p>Attributes:</p> Name Type Description <code>threshold</code> <code>float</code> <p>(float) Threshold for measuring the new optimum, to only focus on significant changes. Default: <code>1e-6</code>.</p> <code>threshold_mode</code> <code>str</code> <p>(str) One of \"rel\", \"abs\". In rel mode, dynamic_threshold = best * ( 1 + threshold ) in max mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: <code>\"abs\"</code>.</p> <code>cooldown</code> <code>int</code> <p>(int) Number of epochs to wait before resuming normal operation after lr has been reduced. Default: <code>3</code>.</p> <code>patience</code> <code>int</code> <p>(int) Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the third epoch if the loss still hasn't improved then. Default: <code>5</code>.</p> <code>factor</code> <code>float</code> <p>(float) Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: <code>0.5</code>.</p> <code>min_lr</code> <code>Any</code> <p>(float or List[float]) A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: <code>1e-8</code>.</p> <p>Methods:</p> Name Description <code>validate_min_lr</code> <p>min_lr Validation.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass ReduceLROnPlateauConfig:\n    \"\"\"Configuration for ReduceLROnPlateau scheduler.\n\n    Attributes:\n        threshold: (float) Threshold for measuring the new optimum, to only focus on significant changes. *Default*: `1e-6`.\n        threshold_mode: (str) One of \"rel\", \"abs\". In rel mode, dynamic_threshold = best * ( 1 + threshold ) in max mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. *Default*: `\"abs\"`.\n        cooldown: (int) Number of epochs to wait before resuming normal operation after lr has been reduced. *Default*: `3`.\n        patience: (int) Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the third epoch if the loss still hasn't improved then. *Default*: `5`.\n        factor: (float) Factor by which the learning rate will be reduced. new_lr = lr * factor. *Default*: `0.5`.\n        min_lr: (float or List[float]) A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. *Default*: `1e-8`.\n    \"\"\"\n\n    threshold: float = 1e-6\n    threshold_mode: str = \"abs\"\n    cooldown: int = 3\n    patience: int = 5\n    factor: float = 0.5\n    min_lr: Any = field(\n        default=1e-8, validator=lambda instance, attr, value: instance.validate_min_lr()\n    )\n\n    def validate_min_lr(self):\n        \"\"\"min_lr Validation.\n\n        Ensures min_lr is a float&gt;=0 or list of floats&gt;=0\n        \"\"\"\n        if isinstance(self.min_lr, float) and self.min_lr &gt;= 0:\n            return\n        if isinstance(self.min_lr, list) and all(\n            isinstance(x, float) and x &gt;= 0 for x in self.min_lr\n        ):\n            return\n        message = \"min_lr must be a float or a list of floats.\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ReduceLROnPlateauConfig.validate_min_lr","title":"<code>validate_min_lr()</code>","text":"<p>min_lr Validation.</p> <p>Ensures min_lr is a float&gt;=0 or list of floats&gt;=0</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>def validate_min_lr(self):\n    \"\"\"min_lr Validation.\n\n    Ensures min_lr is a float&gt;=0 or list of floats&gt;=0\n    \"\"\"\n    if isinstance(self.min_lr, float) and self.min_lr &gt;= 0:\n        return\n    if isinstance(self.min_lr, list) and all(\n        isinstance(x, float) and x &gt;= 0 for x in self.min_lr\n    ):\n        return\n    message = \"min_lr must be a float or a list of floats.\"\n    logger.error(message)\n    raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.StepLRConfig","title":"<code>StepLRConfig</code>","text":"<p>Configuration for StepLR scheduler.</p> <p>Attributes:</p> Name Type Description <code>step_size</code> <code>int</code> <p>(int) Period of learning rate decay. If step_size=10, then every 10 epochs, learning rate will be reduced by a factor of gamma. Default: <code>10</code>.</p> <code>gamma</code> <code>float</code> <p>(float) Multiplicative factor of learning rate decay. Default: <code>0.1</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass StepLRConfig:\n    \"\"\"Configuration for StepLR scheduler.\n\n    Attributes:\n        step_size: (int) Period of learning rate decay. If step_size=10, then every 10 epochs, learning rate will be reduced by a factor of gamma. *Default*: `10`.\n        gamma: (float) Multiplicative factor of learning rate decay. *Default*: `0.1`.\n    \"\"\"\n\n    step_size: int = field(default=10, validator=validators.gt(0))\n    gamma: float = 0.1\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.TrainDataLoaderConfig","title":"<code>TrainDataLoaderConfig</code>","text":"<p>               Bases: <code>DataLoaderConfig</code></p> <p>Train DataLoaderConfig.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch or batch size for training/validation data. Default: <code>4</code>.</p> <code>shuffle</code> <code>bool</code> <p>(bool) True to have the data reshuffled at every epoch. Default: <code>True</code>.</p> <code>num_workers</code> <code>int</code> <p>(int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: <code>0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass TrainDataLoaderConfig(DataLoaderConfig):\n    \"\"\"Train DataLoaderConfig.\n\n    Attributes:\n        batch_size: (int) Number of samples per batch or batch size for training/validation data. *Default*: `4`.\n        shuffle: (bool) True to have the data reshuffled at every epoch. *Default*: `True`.\n        num_workers: (int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. *Default*: `0`.\n    \"\"\"\n\n    shuffle: bool = True\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.TrainerConfig","title":"<code>TrainerConfig</code>","text":"<p>Configuration for trainer.</p> <p>Attributes:</p> Name Type Description <code>train_data_loader</code> <code>TrainDataLoaderConfig</code> <p>(Note: Any parameters from Torch's DataLoader could be used.)</p> <code>val_data_loader</code> <code>ValDataLoaderConfig</code> <p>(Similar to train_data_loader)</p> <code>model_ckpt</code> <code>ModelCkptConfig</code> <p>(Note: Any parameters from Lightning's ModelCheckpoint could be used.)</p> <code>trainer_num_devices</code> <code>ModelCkptConfig</code> <p>(int) Number of devices to use or \"auto\" to let Lightning decide. If <code>None</code>, it defaults to <code>\"auto\"</code> when <code>trainer_device_indices</code> is also <code>None</code>, otherwise its value is inferred from trainer_device_indices. Default: None.</p> <code>trainer_device_indices</code> <code>Optional[List[int]]</code> <p>(list) List of device indices to use. For example, <code>[0, 1]</code> selects two devices and overrides <code>trainer_devices</code>, while <code>[2]</code> with <code>trainer_devices=2</code> still runs only on <code>device 2</code> (not two devices). If <code>None</code>, the number of devices is taken from <code>trainer_devices</code>, starting from index 0. Default: <code>None</code>.</p> <code>trainer_accelerator</code> <code>str</code> <p>(str) One of the (\"cpu\", \"gpu\", \"mps\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the Trainer to be connected to. Default: <code>\"auto\"</code>.</p> <code>profiler</code> <code>Optional[str]</code> <p>(str) Profiler for pytorch Trainer. One of [\"advanced\", \"passthrough\", \"pytorch\", \"simple\"]. Default: <code>None</code>.</p> <code>trainer_strategy</code> <code>str</code> <p>(str) Training strategy, one of [\"auto\", \"ddp\", \"fsdp\", \"ddp_find_unused_parameters_false\", \"ddp_find_unused_parameters_true\", ...]. This supports any training strategy that is supported by <code>lightning.Trainer</code>. Default: <code>\"auto\"</code>.</p> <code>enable_progress_bar</code> <code>bool</code> <p>(bool) When True, enables printing the logs during training. Default: <code>True</code>.</p> <code>min_train_steps_per_epoch</code> <code>int</code> <p>(int) Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer limit_train_batches parameter of Torch Trainer. Default: <code>200</code>.</p> <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>(int) Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data or <code>min_train_steps_per_epoch</code>, whichever is largest. Default: <code>None</code>. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>visualize_preds_during_training</code> <code>bool</code> <p>(bool) If set to <code>True</code>, sample predictions (keypoints + confidence maps) are saved to <code>viz</code> folder in the ckpt dir and in wandb table. Default: <code>False</code>.</p> <code>keep_viz</code> <code>bool</code> <p>(bool) If set to <code>True</code>, the <code>viz</code> folder will be kept after training. If <code>False</code>, the <code>viz</code> folder will be deleted after training. Only applies when <code>visualize_preds_during_training</code> is <code>True</code>. Default: <code>False</code>.</p> <code>max_epochs</code> <code>int</code> <p>(int) Maximum number of epochs to run. Default: <code>100</code>.</p> <code>seed</code> <code>Optional[int]</code> <p>(int) Seed value for the current experiment. If None, no seeding is applied. Default: <code>None</code>.</p> <code>use_wandb</code> <code>bool</code> <p>(bool) True to enable wandb logging. Default: <code>False</code>.</p> <code>save_ckpt</code> <code>bool</code> <p>(bool) True to enable checkpointing. Default: <code>False</code>.</p> <code>ckpt_dir</code> <code>Optional[str]</code> <p>(str) Directory path where the <code>&lt;run_name&gt;</code> folder is created. If <code>None</code>, a new folder for the current run is created in the working dir. Default: <code>None</code></p> <code>run_name</code> <code>Optional[str]</code> <p>(str) Name of the current run. The ckpts will be created in <code>&lt;ckpt_dir&gt;/&lt;run_name&gt;</code>. If <code>None</code>, a run name is generated with <code>&lt;timestamp&gt;_&lt;head_name&gt;</code>. Default: <code>None</code>.</p> <code>resume_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file from which training is resumed. Default: <code>None</code>.</p> <code>wandb</code> <code>WandBConfig</code> <p>(Only if use_wandb is True, else skip this)</p> <code>optimizer_name</code> <code>str</code> <p>(str) Optimizer to be used. One of [\"Adam\", \"AdamW\"]. Default: <code>\"Adam\"</code>.</p> <code>optimizer</code> <code>OptimizerConfig</code> <p>create an optimizer configuration</p> <code>lr_scheduler</code> <code>Optional[LRSchedulerConfig]</code> <p>create an lr_scheduler configuration</p> <code>early_stopping</code> <code>EarlyStoppingConfig</code> <p>create an early_stopping configuration</p> <code>zmq</code> <code>Optional[ZMQConfig]</code> <p>Zmq config with publish and controller port addresses.</p> <p>Methods:</p> Name Description <code>validate_optimizer_name</code> <p>Validate that optimizer_name is one of the allowed values.</p> <code>validate_trainer_devices</code> <p>Validate the value of trainer_devices.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass TrainerConfig:\n    \"\"\"Configuration for trainer.\n\n    Attributes:\n        train_data_loader: (Note: Any parameters from Torch's DataLoader could be used.)\n        val_data_loader: (Similar to train_data_loader)\n        model_ckpt: (Note: Any parameters from Lightning's ModelCheckpoint could be used.)\n        trainer_num_devices: (int) Number of devices to use or \"auto\" to let Lightning decide. If `None`, it defaults to `\"auto\"` when `trainer_device_indices` is also `None`, otherwise its value is inferred from trainer_device_indices. Default: None.\n        trainer_device_indices: (list) List of device indices to use. For example, `[0, 1]` selects two devices and overrides `trainer_devices`, while `[2]` with `trainer_devices=2` still runs only on `device 2` (not two devices). If `None`, the number of devices is taken from `trainer_devices`, starting from index 0. Default: `None`.\n        trainer_accelerator: (str) One of the (\"cpu\", \"gpu\", \"mps\", \"auto\"). \"auto\" recognises the machine the model is running on and chooses the appropriate accelerator for the Trainer to be connected to. *Default*: `\"auto\"`.\n        profiler: (str) Profiler for pytorch Trainer. One of [\"advanced\", \"passthrough\", \"pytorch\", \"simple\"]. *Default*: `None`.\n        trainer_strategy: (str) Training strategy, one of [\"auto\", \"ddp\", \"fsdp\", \"ddp_find_unused_parameters_false\", \"ddp_find_unused_parameters_true\", ...]. This supports any training strategy that is supported by `lightning.Trainer`. *Default*: `\"auto\"`.\n        enable_progress_bar: (bool) When True, enables printing the logs during training. *Default*: `True`.\n        min_train_steps_per_epoch: (int) Minimum number of iterations in a single epoch. (Useful if model is trained with very few data points). Refer limit_train_batches parameter of Torch Trainer. *Default*: `200`.\n        train_steps_per_epoch: (int) Number of minibatches (steps) to train for in an epoch. If set to `None`, this is set to the number of batches in the training data or `min_train_steps_per_epoch`, whichever is largest. *Default*: `None`. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        visualize_preds_during_training: (bool) If set to `True`, sample predictions (keypoints + confidence maps) are saved to `viz` folder in the ckpt dir and in wandb table. *Default*: `False`.\n        keep_viz: (bool) If set to `True`, the `viz` folder will be kept after training. If `False`, the `viz` folder will be deleted after training. Only applies when `visualize_preds_during_training` is `True`. *Default*: `False`.\n        max_epochs: (int) Maximum number of epochs to run. *Default*: `100`.\n        seed: (int) Seed value for the current experiment. If None, no seeding is applied. *Default*: `None`.\n        use_wandb: (bool) True to enable wandb logging. *Default*: `False`.\n        save_ckpt: (bool) True to enable checkpointing. *Default*: `False`.\n        ckpt_dir: (str) Directory path where the `&lt;run_name&gt;` folder is created. If `None`, a new folder for the current run is created in the working dir. **Default**: `None`\n        run_name: (str) Name of the current run. The ckpts will be created in `&lt;ckpt_dir&gt;/&lt;run_name&gt;`. If `None`, a run name is generated with `&lt;timestamp&gt;_&lt;head_name&gt;`. *Default*: `None`.\n        resume_ckpt_path: (str) Path to `.ckpt` file from which training is resumed. *Default*: `None`.\n        wandb: (Only if use_wandb is True, else skip this)\n        optimizer_name: (str) Optimizer to be used. One of [\"Adam\", \"AdamW\"]. *Default*: `\"Adam\"`.\n        optimizer: create an optimizer configuration\n        lr_scheduler: create an lr_scheduler configuration\n        early_stopping: create an early_stopping configuration\n        zmq: Zmq config with publish and controller port addresses.\n    \"\"\"\n\n    train_data_loader: TrainDataLoaderConfig = field(factory=TrainDataLoaderConfig)\n    val_data_loader: ValDataLoaderConfig = field(factory=ValDataLoaderConfig)\n    model_ckpt: ModelCkptConfig = field(factory=ModelCkptConfig)\n    trainer_devices: Optional[Any] = field(\n        default=None,\n        validator=lambda inst, attr, val: TrainerConfig.validate_trainer_devices(val),\n    )\n    trainer_device_indices: Optional[List[int]] = None\n    trainer_accelerator: str = \"auto\"\n    profiler: Optional[str] = None\n    trainer_strategy: str = \"auto\"\n    enable_progress_bar: bool = True\n    min_train_steps_per_epoch: int = 200\n    train_steps_per_epoch: Optional[int] = None\n    visualize_preds_during_training: bool = False\n    keep_viz: bool = False\n    max_epochs: int = 100\n    seed: Optional[int] = None\n    use_wandb: bool = False\n    save_ckpt: bool = False\n    ckpt_dir: Optional[str] = \".\"\n    run_name: Optional[str] = None\n    resume_ckpt_path: Optional[str] = None\n    wandb: WandBConfig = field(factory=WandBConfig)\n    optimizer_name: str = field(\n        default=\"Adam\",\n        validator=lambda inst, attr, val: TrainerConfig.validate_optimizer_name(val),\n    )\n    optimizer: OptimizerConfig = field(factory=OptimizerConfig)\n    lr_scheduler: Optional[LRSchedulerConfig] = field(factory=LRSchedulerConfig)\n    early_stopping: EarlyStoppingConfig = field(factory=EarlyStoppingConfig)\n    online_hard_keypoint_mining: Optional[HardKeypointMiningConfig] = field(\n        factory=HardKeypointMiningConfig\n    )\n    zmq: Optional[ZMQConfig] = field(factory=ZMQConfig)  # Required for SLEAP GUI\n    eval: EvalConfig = field(factory=EvalConfig)  # Epoch-end evaluation config\n\n    @staticmethod\n    def validate_optimizer_name(value):\n        \"\"\"Validate that optimizer_name is one of the allowed values.\"\"\"\n        if value not in [\"Adam\", \"AdamW\"]:\n            message = \"optimizer_name must be one of: Adam, AdamW\"\n            logger.error(message)\n            raise ValueError(message)\n        return True\n\n    @staticmethod\n    def validate_trainer_devices(value):\n        \"\"\"Validate the value of trainer_devices.\"\"\"\n        if value is None:\n            return\n        if isinstance(value, int) and value &gt;= 0:\n            return\n        if isinstance(value, str) and value == \"auto\":\n            return\n        message = \"trainer_devices must be an integer &gt;= 0, or the string 'auto'.\"\n        logger.error(message)\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.TrainerConfig.validate_optimizer_name","title":"<code>validate_optimizer_name(value)</code>  <code>staticmethod</code>","text":"<p>Validate that optimizer_name is one of the allowed values.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@staticmethod\ndef validate_optimizer_name(value):\n    \"\"\"Validate that optimizer_name is one of the allowed values.\"\"\"\n    if value not in [\"Adam\", \"AdamW\"]:\n        message = \"optimizer_name must be one of: Adam, AdamW\"\n        logger.error(message)\n        raise ValueError(message)\n    return True\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.TrainerConfig.validate_trainer_devices","title":"<code>validate_trainer_devices(value)</code>  <code>staticmethod</code>","text":"<p>Validate the value of trainer_devices.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@staticmethod\ndef validate_trainer_devices(value):\n    \"\"\"Validate the value of trainer_devices.\"\"\"\n    if value is None:\n        return\n    if isinstance(value, int) and value &gt;= 0:\n        return\n    if isinstance(value, str) and value == \"auto\":\n        return\n    message = \"trainer_devices must be an integer &gt;= 0, or the string 'auto'.\"\n    logger.error(message)\n    raise ValueError(message)\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ValDataLoaderConfig","title":"<code>ValDataLoaderConfig</code>","text":"<p>               Bases: <code>DataLoaderConfig</code></p> <p>Validation DataLoaderConfig.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch or batch size for training/validation data. Default: <code>4</code>.</p> <code>shuffle</code> <code>bool</code> <p>(bool) True to have the data reshuffled at every epoch. Default: <code>False</code>.</p> <code>num_workers</code> <code>int</code> <p>(int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default: <code>0</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass ValDataLoaderConfig(DataLoaderConfig):\n    \"\"\"Validation DataLoaderConfig.\n\n    Attributes:\n        batch_size: (int) Number of samples per batch or batch size for training/validation data. *Default*: `4`.\n        shuffle: (bool) True to have the data reshuffled at every epoch. *Default*: `False`.\n        num_workers: (int) Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. *Default*: `0`.\n    \"\"\"\n\n    shuffle: bool = False\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.WandBConfig","title":"<code>WandBConfig</code>","text":"<p>Configuration for WandB.</p> <p>Only if use_wandb is True, else skip this</p> <p>Attributes:</p> Name Type Description <code>entity</code> <code>Optional[str]</code> <p>(str) Entity of wandb project. Default: <code>None</code>.</p> <code>project</code> <code>Optional[str]</code> <p>(str) Project name for the wandb project. Default: <code>None</code>.</p> <code>name</code> <code>Optional[str]</code> <p>(str) Name of the current run. Default: <code>None</code>.</p> <code>save_viz_imgs_wandb</code> <code>bool</code> <p>(bool) If set to <code>True</code>, sample predictions (keypoints + confidence maps) that are saved to local <code>viz</code> folder in the ckpt dir would also be uploaded to wandb. Default: <code>False</code>.</p> <code>api_key</code> <code>Optional[str]</code> <p>(str) API key. The API key is masked when saved to config files. Default: <code>None</code>.</p> <code>wandb_mode</code> <code>Optional[str]</code> <p>(str) \"offline\" if only local logging is required. Default: <code>\"None\"</code>.</p> <code>prv_runid</code> <code>Optional[str]</code> <p>(str) Previous run ID if training should be resumed from a previous ckpt. Default: <code>None</code>.</p> <code>group</code> <code>Optional[str]</code> <p>(str) Group for wandb logging. Default: <code>None</code>.</p> <code>current_run_id</code> <code>Optional[str]</code> <p>(str) Run ID for the current model training. (stored once the training starts). Default: <code>None</code>.</p> <code>viz_enabled</code> <code>bool</code> <p>(bool) If True, log pre-rendered matplotlib images to wandb. Default: <code>True</code>.</p> <code>viz_boxes</code> <code>bool</code> <p>(bool) If True, log interactive keypoint boxes. Default: <code>False</code>.</p> <code>viz_masks</code> <code>bool</code> <p>(bool) If True, log confidence map overlay masks. Default: <code>False</code>.</p> <code>viz_box_size</code> <code>float</code> <p>(float) Size of keypoint boxes in pixels (for viz_boxes). Default: <code>5.0</code>.</p> <code>viz_confmap_threshold</code> <code>float</code> <p>(float) Threshold for confidence map masks (for viz_masks). Default: <code>0.1</code>.</p> <code>log_viz_table</code> <code>bool</code> <p>(bool) If True, also log images to a wandb.Table for backwards compatibility. Default: <code>False</code>.</p> <code>delete_local_logs</code> <code>Optional[bool]</code> <p>(bool, optional) If True, delete local wandb logs folder after training. If False, keep the folder. If None (default), automatically delete if logging online (wandb_mode != \"offline\") and keep if logging offline. Default: <code>None</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass WandBConfig:\n    \"\"\"Configuration for WandB.\n\n    Only if use_wandb is True, else skip this\n\n    Attributes:\n        entity: (str) Entity of wandb project. *Default*: `None`.\n        project: (str) Project name for the wandb project. *Default*: `None`.\n        name: (str) Name of the current run. *Default*: `None`.\n        save_viz_imgs_wandb: (bool) If set to `True`, sample predictions (keypoints + confidence maps) that are saved to local `viz` folder in the ckpt dir would also be uploaded to wandb. *Default*: `False`.\n        api_key: (str) API key. The API key is masked when saved to config files. *Default*: `None`.\n        wandb_mode: (str) \"offline\" if only local logging is required. *Default*: `\"None\"`.\n        prv_runid: (str) Previous run ID if training should be resumed from a previous ckpt. *Default*: `None`.\n        group: (str) Group for wandb logging. *Default*: `None`.\n        current_run_id: (str) Run ID for the current model training. (stored once the training starts). *Default*: `None`.\n        viz_enabled: (bool) If True, log pre-rendered matplotlib images to wandb. *Default*: `True`.\n        viz_boxes: (bool) If True, log interactive keypoint boxes. *Default*: `False`.\n        viz_masks: (bool) If True, log confidence map overlay masks. *Default*: `False`.\n        viz_box_size: (float) Size of keypoint boxes in pixels (for viz_boxes). *Default*: `5.0`.\n        viz_confmap_threshold: (float) Threshold for confidence map masks (for viz_masks). *Default*: `0.1`.\n        log_viz_table: (bool) If True, also log images to a wandb.Table for backwards compatibility. *Default*: `False`.\n        delete_local_logs: (bool, optional) If True, delete local wandb logs folder after\n            training. If False, keep the folder. If None (default), automatically delete\n            if logging online (wandb_mode != \"offline\") and keep if logging offline.\n            *Default*: `None`.\n    \"\"\"\n\n    entity: Optional[str] = None\n    project: Optional[str] = None\n    name: Optional[str] = None\n    save_viz_imgs_wandb: bool = False\n    api_key: Optional[str] = None\n    wandb_mode: Optional[str] = None\n    prv_runid: Optional[str] = None\n    group: Optional[str] = None\n    current_run_id: Optional[str] = None\n    viz_enabled: bool = True\n    viz_boxes: bool = False\n    viz_masks: bool = False\n    viz_box_size: float = 5.0\n    viz_confmap_threshold: float = 0.1\n    log_viz_table: bool = False\n    delete_local_logs: Optional[bool] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.ZMQConfig","title":"<code>ZMQConfig</code>","text":"<p>Configuration of ZeroMQ-based monitoring of the training.</p> <p>Attributes:</p> Name Type Description <code>controller_port</code> <code>Optional[int]</code> <p>Port number of the endpoint to listen for command messages from. \"tcp://tcp://127.0.0.1:{port_number}\". Set to <code>None</code> to disable log publishing. Default: <code>None</code>.</p> <code>controller_polling_timeout</code> <code>int</code> <p>Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed. Default: <code>10</code>.</p> <code>publish_port</code> <code>Optional[int]</code> <p>Port number of the endpoint to publish updates to. \"tcp://tcp://127.0.0.1:{port_number}\". Set to <code>None</code> to disable log publishing. Default: <code>None</code>.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>@define\nclass ZMQConfig:\n    \"\"\"Configuration of ZeroMQ-based monitoring of the training.\n\n    Attributes:\n        controller_port: Port number of the endpoint to listen for command messages from. \"tcp://tcp://127.0.0.1:{port_number}\". Set to `None` to disable log publishing. *Default*: `None`.\n        controller_polling_timeout: Polling timeout in microseconds specified as an integer. This controls how long the poller should wait to receive a response and should be set to a small value to minimize the impact on training speed. *Default*: `10`.\n        publish_port: Port number of the endpoint to publish updates to. \"tcp://tcp://127.0.0.1:{port_number}\". Set to `None` to disable log publishing. *Default*: `None`.\n    \"\"\"\n\n    controller_port: Optional[int] = None\n    controller_polling_timeout: int = 10\n    publish_port: Optional[int] = None\n</code></pre>"},{"location":"api/config/trainer_config/#sleap_nn.config.trainer_config.trainer_mapper","title":"<code>trainer_mapper(legacy_config)</code>","text":"<p>Map the legacy trainer configuration to the new trainer configuration.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_config</code> <code>dict</code> <p>A dictionary containing the legacy trainer configuration.</p> required <p>Returns:</p> Type Description <code>TrainerConfig</code> <p>An instance of <code>TrainerConfig</code> with the mapped configuration.</p> Source code in <code>sleap_nn/config/trainer_config.py</code> <pre><code>def trainer_mapper(legacy_config: dict) -&gt; TrainerConfig:\n    \"\"\"Map the legacy trainer configuration to the new trainer configuration.\n\n    Args:\n        legacy_config: A dictionary containing the legacy trainer configuration.\n\n    Returns:\n        An instance of `TrainerConfig` with the mapped configuration.\n    \"\"\"\n    legacy_config_optimization = legacy_config.get(\"optimization\", {})\n    legacy_config_outputs = legacy_config.get(\"outputs\", {})\n    run_name = legacy_config_outputs.get(\"run_name\", None)\n    run_name = run_name if run_name is not None else \"\"\n    run_name_prefix = legacy_config_outputs.get(\"run_name_prefix\", \"\")\n    run_name_suffix = legacy_config_outputs.get(\"run_name_suffix\", \"\")\n    run_name = (\n        run_name_prefix\n        if run_name_prefix is not None\n        else \"\" + run_name + run_name_suffix if run_name_prefix is not None else \"\"\n    )\n    run_name = None if run_name == \"\" else run_name\n\n    trainer_cfg_args = {}\n    train_dataloader_cfg_args = {}\n    val_dataloader_cfg_args = {}\n    model_ckpt_cfg_args = {}\n    optimizer_cfg_args = {}\n    lr_scheduler_cfg_args = {}\n    reduce_lr_on_plateau_cfg_args = {}\n    early_stopping_cfg_args = {}\n    zmq_cfg_args = {}\n    online_hard_keypoint_mining_cfg_args = {}\n\n    # train dataloader\n    if legacy_config_optimization.get(\"batch_size\", None) is not None:\n        train_dataloader_cfg_args[\"batch_size\"] = legacy_config_optimization[\n            \"batch_size\"\n        ]\n\n    if legacy_config_optimization.get(\"online_shuffling\", None) is not None:\n        train_dataloader_cfg_args[\"shuffle\"] = legacy_config_optimization[\n            \"online_shuffling\"\n        ]\n\n    if legacy_config_optimization.get(\"num_workers\", None) is not None:\n        train_dataloader_cfg_args[\"num_workers\"] = legacy_config_optimization[\n            \"num_workers\"\n        ]\n\n    trainer_cfg_args[\"train_data_loader\"] = TrainDataLoaderConfig(\n        **train_dataloader_cfg_args\n    )\n\n    # val dataloader\n    if legacy_config_optimization.get(\"batch_size\", None) is not None:\n        val_dataloader_cfg_args[\"batch_size\"] = legacy_config_optimization[\"batch_size\"]\n\n    if legacy_config_optimization.get(\"num_workers\", None) is not None:\n        val_dataloader_cfg_args[\"num_workers\"] = legacy_config_optimization[\n            \"num_workers\"\n        ]\n\n    trainer_cfg_args[\"val_data_loader\"] = ValDataLoaderConfig(**val_dataloader_cfg_args)\n\n    # model ckpt\n    if (\n        legacy_config_outputs.get(\"checkpointing\", {}).get(\"latest_model\", None)\n        is not None\n    ):\n        model_ckpt_cfg_args[\"save_last\"] = legacy_config_outputs[\"checkpointing\"][\n            \"latest_model\"\n        ]\n\n    trainer_cfg_args[\"model_ckpt\"] = ModelCkptConfig(**model_ckpt_cfg_args)\n\n    if legacy_config_outputs.get(\"save_visualizations\", None) is not None:\n        trainer_cfg_args[\"visualize_preds_during_training\"] = legacy_config_outputs[\n            \"save_visualizations\"\n        ]\n\n    # Handle legacy delete_viz_images parameter\n    if legacy_config_outputs.get(\"keep_viz_images\", None) is not None:\n        trainer_cfg_args[\"keep_viz\"] = legacy_config_outputs[\"keep_viz_images\"]\n\n    if legacy_config_optimization.get(\"epochs\", None) is not None:\n        trainer_cfg_args[\"max_epochs\"] = legacy_config_optimization[\"epochs\"]\n\n    if legacy_config_optimization.get(\"min_batches_per_epoch\", None) is not None:\n        trainer_cfg_args[\"min_train_steps_per_epoch\"] = legacy_config_optimization[\n            \"min_batches_per_epoch\"\n        ]\n\n    if legacy_config_optimization.get(\"batches_per_epoch\", None) is not None:\n        trainer_cfg_args[\"train_steps_per_epoch\"] = legacy_config_optimization[\n            \"batches_per_epoch\"\n        ]\n\n    trainer_cfg_args[\"save_ckpt\"] = True\n    trainer_cfg_args[\"ckpt_dir\"] = (\n        Path(legacy_config_outputs.get(\"runs_folder\", \".\"))\n    ).as_posix()\n    trainer_cfg_args[\"run_name\"] = run_name\n\n    trainer_cfg_args[\"optimizer_name\"] = re.sub(\n        r\"^[a-z]\",\n        lambda x: x.group().upper(),\n        legacy_config_optimization.get(\"optimizer\", \"adam\"),\n    )\n    if legacy_config_optimization.get(\"initial_learning_rate\", None) is not None:\n        optimizer_cfg_args[\"lr\"] = legacy_config_optimization[\"initial_learning_rate\"]\n\n    trainer_cfg_args[\"optimizer\"] = OptimizerConfig(**optimizer_cfg_args)\n\n    if (\n        legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n            \"reduce_on_plateau\", None\n        )\n        is not None\n    ):\n        if legacy_config_optimization[\"learning_rate_schedule\"][\"reduce_on_plateau\"]:\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"plateau_min_delta\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"threshold\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"plateau_min_delta\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"plateau_cooldown\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"cooldown\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"plateau_cooldown\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"reduction_factor\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"factor\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"reduction_factor\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"plateau_patience\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"patience\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"plateau_patience\"]\n\n            if (\n                legacy_config_optimization.get(\"learning_rate_schedule\", {}).get(\n                    \"min_learning_rate\", None\n                )\n                is not None\n            ):\n                reduce_lr_on_plateau_cfg_args[\"min_lr\"] = legacy_config_optimization[\n                    \"learning_rate_schedule\"\n                ][\"min_learning_rate\"]\n\n            lr_scheduler_cfg_args[\"reduce_lr_on_plateau\"] = ReduceLROnPlateauConfig(\n                **reduce_lr_on_plateau_cfg_args\n            )\n\n    trainer_cfg_args[\"lr_scheduler\"] = LRSchedulerConfig(**lr_scheduler_cfg_args)\n\n    if (\n        legacy_config_optimization.get(\"early_stopping\", {}).get(\n            \"stop_training_on_plateau\", None\n        )\n        is not None\n    ):\n        early_stopping_cfg_args[\"stop_training_on_plateau\"] = (\n            legacy_config_optimization[\"early_stopping\"][\"stop_training_on_plateau\"]\n        )\n        if (\n            legacy_config_optimization.get(\"early_stopping\", {}).get(\n                \"plateau_min_delta\", None\n            )\n            is not None\n        ):\n            early_stopping_cfg_args[\"min_delta\"] = legacy_config_optimization[\n                \"early_stopping\"\n            ][\"plateau_min_delta\"]\n\n        if (\n            legacy_config_optimization.get(\"early_stopping\", {}).get(\n                \"plateau_patience\", None\n            )\n            is not None\n        ):\n            early_stopping_cfg_args[\"patience\"] = legacy_config_optimization[\n                \"early_stopping\"\n            ][\"plateau_patience\"]\n\n    trainer_cfg_args[\"early_stopping\"] = EarlyStoppingConfig(**early_stopping_cfg_args)\n\n    if (\n        legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n            \"online_mining\", None\n        )\n        is not None\n    ):\n        if legacy_config_optimization[\"hard_keypoint_mining\"][\"online_mining\"]:\n            online_hard_keypoint_mining_cfg_args[\"online_mining\"] = True\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"hard_to_easy_ratio\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"hard_to_easy_ratio\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"hard_to_easy_ratio\"]\n            )\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"min_hard_keypoints\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"min_hard_keypoints\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"min_hard_keypoints\"]\n            )\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"max_hard_keypoints\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"max_hard_keypoints\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"max_hard_keypoints\"]\n            )\n\n        if (\n            legacy_config_optimization.get(\"hard_keypoint_mining\", {}).get(\n                \"loss_scale\", None\n            )\n            is not None\n        ):\n            online_hard_keypoint_mining_cfg_args[\"loss_scale\"] = (\n                legacy_config_optimization[\"hard_keypoint_mining\"][\"loss_scale\"]\n            )\n\n    trainer_cfg_args[\"online_hard_keypoint_mining\"] = HardKeypointMiningConfig(\n        **online_hard_keypoint_mining_cfg_args\n    )\n\n    if (\n        legacy_config_outputs.get(\"zmq\", {}).get(\"subscribe_to_controller\", None)\n        is not None\n    ):\n        zmq_cfg_args[\"controller_port\"] = int(\n            legacy_config_outputs[\"zmq\"][\"controller_address\"].split(\":\")[-1]\n        )\n\n    if legacy_config_outputs.get(\"zmq\", {}).get(\"publish_updates\", None) is not None:\n        zmq_cfg_args[\"publish_port\"] = int(\n            legacy_config_outputs[\"zmq\"][\"publish_address\"].split(\":\")[-1]\n        )\n\n    if (\n        legacy_config_outputs.get(\"zmq\", {}).get(\"controller_polling_timeout\", None)\n        is not None\n    ):\n        zmq_cfg_args[\"controller_polling_timeout\"] = legacy_config_outputs[\"zmq\"][\n            \"controller_polling_timeout\"\n        ]\n\n    trainer_cfg_args[\"zmq\"] = ZMQConfig(**zmq_cfg_args)\n\n    return TrainerConfig(**trainer_cfg_args)\n</code></pre>"},{"location":"api/config/training_job_config/","title":"training_job_config","text":""},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config","title":"<code>sleap_nn.config.training_job_config</code>","text":"<p>Serializable configuration classes for specifying all training job parameters.</p> <p>These configuration classes are intended to specify all the parameters required to run a training job or perform inference from a serialized one.</p> <p>They are explicitly not intended to implement any of the underlying functionality that they parametrize. This serves two purposes:</p> <pre><code>1. Parameter specification through simple attributes. These can be read/edited by a\n    human, as well as easily be serialized/deserialized to/from simple dictionaries\n    and YAML.\n\n2. Decoupling from the implementation. This makes it easier to design functional\n    modules with attributes/parameters that contain objects that may not be easily\n    serializable or may implement additional logic that relies on runtime\n    information or other parameters.\n</code></pre> <p>In general, classes that implement the actual functionality related to these configuration classes should provide a classmethod for instantiation from the configuration class instances. This makes it easier to implement other logic not related to the high level parameters at creation time.</p> <p>Conveniently, this format also provides a single location where all user-facing parameters are aggregated and documented for end users (as opposed to developers).</p> <p>Classes:</p> Name Description <code>TrainingJobConfig</code> <p>Configuration of a training job.</p> <p>Functions:</p> Name Description <code>check_must_be_set</code> <p>Check that all required fields are set in the BackboneConfig and HeadConfig.</p> <code>verify_training_cfg</code> <p>Get sleap-nn training config from a DictConfig object.</p>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig","title":"<code>TrainingJobConfig</code>","text":"<p>Configuration of a training job.</p> <p>Attributes:</p> Name Type Description <code>data_config</code> <code>DataConfig</code> <p>Configuration options related to the training data.</p> <code>model_config</code> <code>ModelConfig</code> <p>Configuration options related to the model architecture.</p> <code>trainer_config</code> <code>TrainerConfig</code> <p>Configuration ooptions related to model training.</p> <code>outputs</code> <code>TrainerConfig</code> <p>Configuration options related to outputs during training.</p> <code>name</code> <code>Optional[Text]</code> <p>Optional name for this configuration profile.</p> <code>description</code> <code>Optional[Text]</code> <p>Optional description of the configuration.</p> <code>sleap_nn_version</code> <code>Optional[Text]</code> <p>Version of SLEAP that generated this configuration.</p> <code>filename</code> <code>Optional[Text]</code> <p>Path to this config file if it was loaded from disk.</p> <p>Methods:</p> Name Description <code>load_sleap_config</code> <p>Load a SLEAP configuration from a JSON file and convert it to OmegaConf.</p> <code>load_sleap_config_from_json</code> <p>Load a SLEAP configuration from a JSON string and convert it to OmegaConf.</p> <code>to_sleap_nn_cfg</code> <p>Convert the attrs class to OmegaConf object.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>@define\nclass TrainingJobConfig:\n    \"\"\"Configuration of a training job.\n\n    Attributes:\n        data_config: Configuration options related to the training data.\n        model_config: Configuration options related to the model architecture.\n        trainer_config: Configuration ooptions related to model training.\n        outputs: Configuration options related to outputs during training.\n        name: Optional name for this configuration profile.\n        description: Optional description of the configuration.\n        sleap_nn_version: Version of SLEAP that generated this configuration.\n        filename: Path to this config file if it was loaded from disk.\n    \"\"\"\n\n    data_config: DataConfig = field(factory=DataConfig)\n    model_config: ModelConfig = field(factory=ModelConfig)\n    trainer_config: TrainerConfig = field(factory=TrainerConfig)\n    name: Optional[Text] = \"\"\n    description: Optional[Text] = \"\"\n    sleap_nn_version: Optional[Text] = sleap_nn.__version__\n    filename: Optional[Text] = \"\"\n\n    def to_sleap_nn_cfg(self) -&gt; DictConfig:\n        \"\"\"Convert the attrs class to OmegaConf object.\"\"\"\n        config = OmegaConf.structured(self)\n        OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n        return config\n\n    @classmethod\n    def load_sleap_config(cls, json_file_path: str) -&gt; OmegaConf:\n        \"\"\"Load a SLEAP configuration from a JSON file and convert it to OmegaConf.\n\n        Args:\n            cls: The class to instantiate with the loaded configuration.\n            json_file_path: Path to a JSON file containing the SLEAP configuration.\n\n        Returns:\n            An OmegaConf instance with the loaded configuration.\n        \"\"\"\n        with open(json_file_path, \"r\") as f:\n            old_config = json.load(f)\n\n        return cls.load_sleap_config_from_json(old_config)\n\n    @classmethod\n    def load_sleap_config_from_json(cls, json_str: str) -&gt; OmegaConf:\n        \"\"\"Load a SLEAP configuration from a JSON string and convert it to OmegaConf.\n\n        Args:\n            cls: The class to instantiate with the loaded configuration.\n            json_str: JSON-formatted string containing the SLEAP configuration.\n\n        Returns:\n            An OmegaConf instance with the loaded configuration.\n        \"\"\"\n        data_config = data_mapper(json_str)\n        model_config = model_mapper(json_str)\n        trainer_config = trainer_mapper(json_str)\n\n        config = cls(\n            data_config=data_config,\n            model_config=model_config,\n            trainer_config=trainer_config,\n        )\n\n        schema = OmegaConf.structured(config)\n        config_omegaconf = OmegaConf.merge(schema, OmegaConf.create(asdict(config)))\n        OmegaConf.to_container(config_omegaconf, resolve=True, throw_on_missing=True)\n\n        return config_omegaconf\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig.load_sleap_config","title":"<code>load_sleap_config(json_file_path)</code>  <code>classmethod</code>","text":"<p>Load a SLEAP configuration from a JSON file and convert it to OmegaConf.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class to instantiate with the loaded configuration.</p> required <code>json_file_path</code> <code>str</code> <p>Path to a JSON file containing the SLEAP configuration.</p> required <p>Returns:</p> Type Description <code>OmegaConf</code> <p>An OmegaConf instance with the loaded configuration.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>@classmethod\ndef load_sleap_config(cls, json_file_path: str) -&gt; OmegaConf:\n    \"\"\"Load a SLEAP configuration from a JSON file and convert it to OmegaConf.\n\n    Args:\n        cls: The class to instantiate with the loaded configuration.\n        json_file_path: Path to a JSON file containing the SLEAP configuration.\n\n    Returns:\n        An OmegaConf instance with the loaded configuration.\n    \"\"\"\n    with open(json_file_path, \"r\") as f:\n        old_config = json.load(f)\n\n    return cls.load_sleap_config_from_json(old_config)\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig.load_sleap_config_from_json","title":"<code>load_sleap_config_from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Load a SLEAP configuration from a JSON string and convert it to OmegaConf.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class to instantiate with the loaded configuration.</p> required <code>json_str</code> <code>str</code> <p>JSON-formatted string containing the SLEAP configuration.</p> required <p>Returns:</p> Type Description <code>OmegaConf</code> <p>An OmegaConf instance with the loaded configuration.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>@classmethod\ndef load_sleap_config_from_json(cls, json_str: str) -&gt; OmegaConf:\n    \"\"\"Load a SLEAP configuration from a JSON string and convert it to OmegaConf.\n\n    Args:\n        cls: The class to instantiate with the loaded configuration.\n        json_str: JSON-formatted string containing the SLEAP configuration.\n\n    Returns:\n        An OmegaConf instance with the loaded configuration.\n    \"\"\"\n    data_config = data_mapper(json_str)\n    model_config = model_mapper(json_str)\n    trainer_config = trainer_mapper(json_str)\n\n    config = cls(\n        data_config=data_config,\n        model_config=model_config,\n        trainer_config=trainer_config,\n    )\n\n    schema = OmegaConf.structured(config)\n    config_omegaconf = OmegaConf.merge(schema, OmegaConf.create(asdict(config)))\n    OmegaConf.to_container(config_omegaconf, resolve=True, throw_on_missing=True)\n\n    return config_omegaconf\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.TrainingJobConfig.to_sleap_nn_cfg","title":"<code>to_sleap_nn_cfg()</code>","text":"<p>Convert the attrs class to OmegaConf object.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>def to_sleap_nn_cfg(self) -&gt; DictConfig:\n    \"\"\"Convert the attrs class to OmegaConf object.\"\"\"\n    config = OmegaConf.structured(self)\n    OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n    return config\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.check_must_be_set","title":"<code>check_must_be_set(config)</code>","text":"<p>Check that all required fields are set in the BackboneConfig and HeadConfig.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>def check_must_be_set(config: DictConfig) -&gt; None:\n    \"\"\"Check that all required fields are set in the BackboneConfig and HeadConfig.\"\"\"\n    backbone_config = config.model_config.backbone_config\n    head_config = config.model_config.head_configs\n\n    backbone_attributes = [k for k, v in backbone_config.items() if v is not None]\n\n    head_config_attributes = [k for k, v in head_config.items() if v is not None]\n\n    if len(backbone_attributes) == 0:\n        message = \"BackboneConfig: At least one attribute of this class must be set.\"\n        raise ValueError(message)\n\n    if len(head_config_attributes) == 0:\n        message = \"HeadConfig: At least one attribute of this class must be set.\"\n        raise ValueError(message)\n</code></pre>"},{"location":"api/config/training_job_config/#sleap_nn.config.training_job_config.verify_training_cfg","title":"<code>verify_training_cfg(cfg)</code>","text":"<p>Get sleap-nn training config from a DictConfig object.</p> Source code in <code>sleap_nn/config/training_job_config.py</code> <pre><code>def verify_training_cfg(cfg: DictConfig) -&gt; DictConfig:\n    \"\"\"Get sleap-nn training config from a DictConfig object.\"\"\"\n    schema = OmegaConf.structured(TrainingJobConfig())\n    config = OmegaConf.merge(schema, cfg)\n    OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n\n    # Verify configs with @oneof class is valid\n    _ = OmegaConf.to_object(config)\n\n    # Verify required fields are set\n    check_must_be_set(config)\n    return config\n</code></pre>"},{"location":"api/config/utils/","title":"utils","text":""},{"location":"api/config/utils/#sleap_nn.config.utils","title":"<code>sleap_nn.config.utils</code>","text":"<p>Utilities for config building and validation.</p> <p>Functions:</p> Name Description <code>check_output_strides</code> <p>Check max_stride and output_stride in backbone_config with head_config.</p> <code>get_backbone_type_from_cfg</code> <p>Return the backbone type from the config. One of [unet, swint, convnext].</p> <code>get_model_type_from_cfg</code> <p>Return the model type from the config. One of [single_instance, centroid, centered_instance, bottomup].</p> <code>get_output_strides_from_heads</code> <p>Get list of output strides from head configs.</p> <code>oneof</code> <p>Ensure that the decorated attrs class only has a single attribute set.</p>"},{"location":"api/config/utils/#sleap_nn.config.utils.check_output_strides","title":"<code>check_output_strides(config)</code>","text":"<p>Check max_stride and output_stride in backbone_config with head_config.</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def check_output_strides(config: OmegaConf) -&gt; OmegaConf:\n    \"\"\"Check max_stride and output_stride in backbone_config with head_config.\"\"\"\n    output_strides = get_output_strides_from_heads(config.model_config.head_configs)\n    backbone_type = get_backbone_type_from_cfg(config)\n    if output_strides:\n        config.model_config.backbone_config[f\"{backbone_type}\"][\"output_stride\"] = min(\n            output_strides\n        )\n        if config.model_config.backbone_config[f\"{backbone_type}\"][\"max_stride\"] &lt; max(\n            output_strides\n        ):\n            config.model_config.backbone_config[f\"{backbone_type}\"][\"max_stride\"] = max(\n                output_strides\n            )\n\n    model_type = get_model_type_from_cfg(config)\n    if model_type == \"multi_class_topdown\":\n        config.model_config.head_configs.multi_class_topdown.class_vectors.output_stride = config.model_config.backbone_config[\n            f\"{backbone_type}\"\n        ][\n            \"max_stride\"\n        ]\n    return config\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.get_backbone_type_from_cfg","title":"<code>get_backbone_type_from_cfg(config)</code>","text":"<p>Return the backbone type from the config. One of [unet, swint, convnext].</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def get_backbone_type_from_cfg(config: DictConfig):\n    \"\"\"Return the backbone type from the config. One of [unet, swint, convnext].\"\"\"\n    backbone_type = None\n    for k, v in config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n    return backbone_type\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.get_model_type_from_cfg","title":"<code>get_model_type_from_cfg(config)</code>","text":"<p>Return the model type from the config. One of [single_instance, centroid, centered_instance, bottomup].</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def get_model_type_from_cfg(config: DictConfig):\n    \"\"\"Return the model type from the config. One of [single_instance, centroid, centered_instance, bottomup].\"\"\"\n    model_type = None\n    for k, v in config.model_config.head_configs.items():\n        if v is not None:\n            model_type = k\n            break\n    return model_type\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.get_output_strides_from_heads","title":"<code>get_output_strides_from_heads(head_configs)</code>","text":"<p>Get list of output strides from head configs.</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def get_output_strides_from_heads(head_configs: DictConfig):\n    \"\"\"Get list of output strides from head configs.\"\"\"\n    output_strides_from_heads = []\n    for head_type in head_configs:\n        if head_configs[head_type] is not None:\n            for head_layer in head_configs[head_type]:\n                output_strides_from_heads.append(\n                    head_configs[head_type][head_layer][\"output_stride\"]\n                )\n    return output_strides_from_heads\n</code></pre>"},{"location":"api/config/utils/#sleap_nn.config.utils.oneof","title":"<code>oneof(attrs_cls, must_be_set=False)</code>","text":"<p>Ensure that the decorated attrs class only has a single attribute set.</p> <p>This decorator is inspired by the <code>oneof</code> protobuffer field behavior.</p> <p>Parameters:</p> Name Type Description Default <code>attrs_cls</code> <p>An attrs decorated class.</p> required <code>must_be_set</code> <code>bool</code> <p>If True, raise an error if none of the attributes are set. If not, error will only be raised if more than one attribute is set.</p> <code>False</code> <p>Returns:</p> Type Description <p>The <code>attrs_cls</code> with an <code>__init__</code> method that checks for the number of attributes that are set.</p> Source code in <code>sleap_nn/config/utils.py</code> <pre><code>def oneof(attrs_cls, must_be_set: bool = False):\n    \"\"\"Ensure that the decorated attrs class only has a single attribute set.\n\n    This decorator is inspired by the `oneof` protobuffer field behavior.\n\n    Args:\n        attrs_cls: An attrs decorated class.\n        must_be_set: If True, raise an error if none of the attributes are set. If not,\n            error will only be raised if more than one attribute is set.\n\n    Returns:\n        The `attrs_cls` with an `__init__` method that checks for the number of\n        attributes that are set.\n    \"\"\"\n    # Check if the class is an attrs class at all.\n    if not hasattr(attrs_cls, \"__attrs_attrs__\"):\n        message = \"Classes decorated with oneof must also be attr.s decorated.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    # Pull out attrs generated class attributes.\n    attribs = attrs_cls.__attrs_attrs__\n    init_fn = attrs_cls.__init__\n\n    # Define a new __init__ function that wraps the attrs generated one.\n    def new_init_fn(self, *args, **kwargs):\n        # Execute the standard attrs-generated __init__.\n        init_fn(self, *args, **kwargs)\n\n        # Check for attribs with set values.\n        attribs_with_value = [\n            attrib for attrib in attribs if getattr(self, attrib.name) is not None\n        ]\n\n        class_name = self.__class__.__name__\n\n        if len(attribs_with_value) &gt; 1:\n            # Raise error if more than one attribute is set.\n            message = (\n                f\"{class_name}: Only one attribute of this class can be set (not None).\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        if len(attribs_with_value) == 0 and must_be_set:\n            # Raise error if none are set.\n            message = f\"{class_name}: At least one attribute of this class must be set.\"\n            logger.error(message)\n            raise ValueError(message)\n\n    # Replace with wrapped __init__.\n    attrs_cls.__init__ = new_init_fn\n\n    # Define convenience method for getting the set attribute.\n    def which_oneof_attrib_name(self):\n        attribs_with_value = [\n            attrib for attrib in attribs if getattr(self, attrib.name) is not None\n        ]\n        class_name = self.__class__.__name__\n\n        if len(attribs_with_value) &gt; 1:\n            # Raise error if more than one attribute is set.\n            message = (\n                f\"{class_name}: Only one attribute of this class can be set (not None).\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        if len(attribs_with_value) == 0:\n            if must_be_set:\n                # Raise error if none are set.\n                message = (\n                    f\"{class_name}: At least one attribute of this class must be set.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n            else:\n                return None\n\n        return attribs_with_value[0].name\n\n    def which_oneof(self):\n        attrib_name = self.which_oneof_attrib_name()\n\n        if attrib_name is None:\n            return None\n\n        return getattr(self, attrib_name)\n\n    attrs_cls.which_oneof_attrib_name = which_oneof_attrib_name\n    attrs_cls.which_oneof = which_oneof\n\n    return attrs_cls\n</code></pre>"},{"location":"api/data/","title":"data","text":""},{"location":"api/data/#sleap_nn.data","title":"<code>sleap_nn.data</code>","text":"<p>Modules related to data loading and processing.</p> <p>Modules:</p> Name Description <code>augmentation</code> <p>This module implements data pipeline blocks for augmentation operations.</p> <code>confidence_maps</code> <p>Generate confidence maps.</p> <code>custom_datasets</code> <p>Custom <code>torch.utils.data.Dataset</code>s for different model types.</p> <code>edge_maps</code> <p>Transformers for generating edge confidence maps and part affinity fields.</p> <code>identity</code> <p>Utilities for generating data for track identity models.</p> <code>instance_centroids</code> <p>Handle calculation of instance centroids.</p> <code>instance_cropping</code> <p>Handle cropping of instances.</p> <code>normalization</code> <p>This module implements data pipeline blocks for normalization operations.</p> <code>providers</code> <p>This module implements pipeline blocks for reading input data such as labels.</p> <code>resizing</code> <p>This module implements image resizing and padding.</p> <code>skia_augmentation</code> <p>Skia-based augmentation functions that operate on uint8 tensors.</p> <code>utils</code> <p>Miscellaneous utility functions for data processing.</p>"},{"location":"api/data/augmentation/","title":"augmentation","text":""},{"location":"api/data/augmentation/#sleap_nn.data.augmentation","title":"<code>sleap_nn.data.augmentation</code>","text":"<p>This module implements data pipeline blocks for augmentation operations.</p> <p>Uses Skia (skia-python) for ~1.5x faster augmentation compared to Kornia.</p> <p>Functions:</p> Name Description <code>apply_geometric_augmentation</code> <p>Apply geometric augmentation on image and instances.</p> <code>apply_intensity_augmentation</code> <p>Apply intensity augmentation on image and instances.</p>"},{"location":"api/data/augmentation/#sleap_nn.data.augmentation.apply_geometric_augmentation","title":"<code>apply_geometric_augmentation(image, instances, rotation_min=-15.0, rotation_max=15.0, rotation_p=None, scale_min=0.9, scale_max=1.1, scale_p=None, translate_width=0.02, translate_height=0.02, translate_p=None, affine_p=0.0, erase_scale_min=0.0001, erase_scale_max=0.01, erase_ratio_min=1, erase_ratio_max=1, erase_p=0.0, mixup_lambda_min=0.01, mixup_lambda_max=0.05, mixup_p=0.0)</code>","text":"<p>Apply geometric augmentation on image and instances.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image. Shape: (n_samples, C, H, W)</p> required <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)</p> required <code>rotation_min</code> <code>Optional[float]</code> <p>Minimum rotation angle in degrees. Default: -15.0.</p> <code>-15.0</code> <code>rotation_max</code> <code>Optional[float]</code> <p>Maximum rotation angle in degrees. Default: 15.0.</p> <code>15.0</code> <code>rotation_p</code> <code>Optional[float]</code> <p>Probability of applying random rotation independently. If None, falls back to affine_p for bundled behavior. Default: None.</p> <code>None</code> <code>scale_min</code> <code>Optional[float]</code> <p>Minimum scaling factor for isotropic scaling. Default: 0.9.</p> <code>0.9</code> <code>scale_max</code> <code>Optional[float]</code> <p>Maximum scaling factor for isotropic scaling. Default: 1.1.</p> <code>1.1</code> <code>scale_p</code> <code>Optional[float]</code> <p>Probability of applying random scaling independently. If None, falls back to affine_p for bundled behavior. Default: None.</p> <code>None</code> <code>translate_width</code> <code>Optional[float]</code> <p>Maximum absolute fraction for horizontal translation. Default: 0.02.</p> <code>0.02</code> <code>translate_height</code> <code>Optional[float]</code> <p>Maximum absolute fraction for vertical translation. Default: 0.02.</p> <code>0.02</code> <code>translate_p</code> <code>Optional[float]</code> <p>Probability of applying random translation independently. If None, falls back to affine_p for bundled behavior. Default: None.</p> <code>None</code> <code>affine_p</code> <code>float</code> <p>Probability of applying random affine transformations (rotation, scale, translate bundled). Used when individual *_p params are None. Default: 0.0.</p> <code>0.0</code> <code>erase_scale_min</code> <code>Optional[float]</code> <p>Minimum value of range of proportion of erased area against input image. Default: 0.0001.</p> <code>0.0001</code> <code>erase_scale_max</code> <code>Optional[float]</code> <p>Maximum value of range of proportion of erased area against input image. Default: 0.01.</p> <code>0.01</code> <code>erase_ratio_min</code> <code>Optional[float]</code> <p>Minimum value of range of aspect ratio of erased area. Default: 1.</p> <code>1</code> <code>erase_ratio_max</code> <code>Optional[float]</code> <p>Maximum value of range of aspect ratio of erased area. Default: 1.</p> <code>1</code> <code>erase_p</code> <code>float</code> <p>Probability of applying random erase. Default: 0.0.</p> <code>0.0</code> <code>mixup_lambda_min</code> <code>Optional[float]</code> <p>Minimum mixup strength value. Default: 0.01.</p> <code>0.01</code> <code>mixup_lambda_max</code> <code>Optional[float]</code> <p>Maximum mixup strength value. Default: 0.05.</p> <code>0.05</code> <code>mixup_p</code> <code>float</code> <p>Probability of applying random mixup v2. Default: 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Returns tuple: (image, instances) with augmentation applied.</p> Source code in <code>sleap_nn/data/augmentation.py</code> <pre><code>def apply_geometric_augmentation(\n    image: torch.Tensor,\n    instances: torch.Tensor,\n    rotation_min: Optional[float] = -15.0,\n    rotation_max: Optional[float] = 15.0,\n    rotation_p: Optional[float] = None,\n    scale_min: Optional[float] = 0.9,\n    scale_max: Optional[float] = 1.1,\n    scale_p: Optional[float] = None,\n    translate_width: Optional[float] = 0.02,\n    translate_height: Optional[float] = 0.02,\n    translate_p: Optional[float] = None,\n    affine_p: float = 0.0,\n    erase_scale_min: Optional[float] = 0.0001,\n    erase_scale_max: Optional[float] = 0.01,\n    erase_ratio_min: Optional[float] = 1,\n    erase_ratio_max: Optional[float] = 1,\n    erase_p: float = 0.0,\n    mixup_lambda_min: Optional[float] = 0.01,\n    mixup_lambda_max: Optional[float] = 0.05,\n    mixup_p: float = 0.0,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply geometric augmentation on image and instances.\n\n    Args:\n        image: Input image. Shape: (n_samples, C, H, W)\n        instances: Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)\n        rotation_min: Minimum rotation angle in degrees. Default: -15.0.\n        rotation_max: Maximum rotation angle in degrees. Default: 15.0.\n        rotation_p: Probability of applying random rotation independently. If None,\n            falls back to affine_p for bundled behavior. Default: None.\n        scale_min: Minimum scaling factor for isotropic scaling. Default: 0.9.\n        scale_max: Maximum scaling factor for isotropic scaling. Default: 1.1.\n        scale_p: Probability of applying random scaling independently. If None,\n            falls back to affine_p for bundled behavior. Default: None.\n        translate_width: Maximum absolute fraction for horizontal translation. Default: 0.02.\n        translate_height: Maximum absolute fraction for vertical translation. Default: 0.02.\n        translate_p: Probability of applying random translation independently. If None,\n            falls back to affine_p for bundled behavior. Default: None.\n        affine_p: Probability of applying random affine transformations (rotation, scale,\n            translate bundled). Used when individual *_p params are None. Default: 0.0.\n        erase_scale_min: Minimum value of range of proportion of erased area against input image. Default: 0.0001.\n        erase_scale_max: Maximum value of range of proportion of erased area against input image. Default: 0.01.\n        erase_ratio_min: Minimum value of range of aspect ratio of erased area. Default: 1.\n        erase_ratio_max: Maximum value of range of aspect ratio of erased area. Default: 1.\n        erase_p: Probability of applying random erase. Default: 0.0.\n        mixup_lambda_min: Minimum mixup strength value. Default: 0.01.\n        mixup_lambda_max: Maximum mixup strength value. Default: 0.05.\n        mixup_p: Probability of applying random mixup v2. Default: 0.0.\n\n    Returns:\n        Returns tuple: (image, instances) with augmentation applied.\n    \"\"\"\n    return apply_geometric_augmentation_skia(\n        image=image,\n        instances=instances,\n        rotation_min=rotation_min,\n        rotation_max=rotation_max,\n        rotation_p=rotation_p,\n        scale_min=scale_min,\n        scale_max=scale_max,\n        scale_p=scale_p,\n        translate_width=translate_width,\n        translate_height=translate_height,\n        translate_p=translate_p,\n        affine_p=affine_p,\n        erase_scale_min=erase_scale_min,\n        erase_scale_max=erase_scale_max,\n        erase_ratio_min=erase_ratio_min,\n        erase_ratio_max=erase_ratio_max,\n        erase_p=erase_p,\n        mixup_lambda_min=mixup_lambda_min,\n        mixup_lambda_max=mixup_lambda_max,\n        mixup_p=mixup_p,\n    )\n</code></pre>"},{"location":"api/data/augmentation/#sleap_nn.data.augmentation.apply_intensity_augmentation","title":"<code>apply_intensity_augmentation(image, instances, uniform_noise_min=0.0, uniform_noise_max=0.04, uniform_noise_p=0.0, gaussian_noise_mean=0.02, gaussian_noise_std=0.004, gaussian_noise_p=0.0, contrast_min=0.5, contrast_max=2.0, contrast_p=0.0, brightness_min=1.0, brightness_max=1.0, brightness_p=0.0)</code>","text":"<p>Apply intensity augmentation on image and instances.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image. Shape: (n_samples, C, H, W)</p> required <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)</p> required <code>uniform_noise_min</code> <code>Optional[float]</code> <p>Minimum value for uniform noise (uniform_noise_min &gt;=0).</p> <code>0.0</code> <code>uniform_noise_max</code> <code>Optional[float]</code> <p>Maximum value for uniform noise (uniform_noise_max &lt;=1).</p> <code>0.04</code> <code>uniform_noise_p</code> <code>float</code> <p>Probability of applying random uniform noise.</p> <code>0.0</code> <code>gaussian_noise_mean</code> <code>Optional[float]</code> <p>The mean of the gaussian distribution.</p> <code>0.02</code> <code>gaussian_noise_std</code> <code>Optional[float]</code> <p>The standard deviation of the gaussian distribution.</p> <code>0.004</code> <code>gaussian_noise_p</code> <code>float</code> <p>Probability of applying random gaussian noise.</p> <code>0.0</code> <code>contrast_min</code> <code>Optional[float]</code> <p>Minimum contrast factor to apply. Default: 0.5.</p> <code>0.5</code> <code>contrast_max</code> <code>Optional[float]</code> <p>Maximum contrast factor to apply. Default: 2.0.</p> <code>2.0</code> <code>contrast_p</code> <code>float</code> <p>Probability of applying random contrast.</p> <code>0.0</code> <code>brightness_min</code> <code>Optional[float]</code> <p>Minimum brightness factor to apply. Default: 1.0.</p> <code>1.0</code> <code>brightness_max</code> <code>Optional[float]</code> <p>Maximum brightness factor to apply. Default: 1.0.</p> <code>1.0</code> <code>brightness_p</code> <code>float</code> <p>Probability of applying random brightness.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Returns tuple: (image, instances) with augmentation applied.</p> Source code in <code>sleap_nn/data/augmentation.py</code> <pre><code>def apply_intensity_augmentation(\n    image: torch.Tensor,\n    instances: torch.Tensor,\n    uniform_noise_min: Optional[float] = 0.0,\n    uniform_noise_max: Optional[float] = 0.04,\n    uniform_noise_p: float = 0.0,\n    gaussian_noise_mean: Optional[float] = 0.02,\n    gaussian_noise_std: Optional[float] = 0.004,\n    gaussian_noise_p: float = 0.0,\n    contrast_min: Optional[float] = 0.5,\n    contrast_max: Optional[float] = 2.0,\n    contrast_p: float = 0.0,\n    brightness_min: Optional[float] = 1.0,\n    brightness_max: Optional[float] = 1.0,\n    brightness_p: float = 0.0,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply intensity augmentation on image and instances.\n\n    Args:\n        image: Input image. Shape: (n_samples, C, H, W)\n        instances: Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2)\n        uniform_noise_min: Minimum value for uniform noise (uniform_noise_min &gt;=0).\n        uniform_noise_max: Maximum value for uniform noise (uniform_noise_max &lt;=1).\n        uniform_noise_p: Probability of applying random uniform noise.\n        gaussian_noise_mean: The mean of the gaussian distribution.\n        gaussian_noise_std: The standard deviation of the gaussian distribution.\n        gaussian_noise_p: Probability of applying random gaussian noise.\n        contrast_min: Minimum contrast factor to apply. Default: 0.5.\n        contrast_max: Maximum contrast factor to apply. Default: 2.0.\n        contrast_p: Probability of applying random contrast.\n        brightness_min: Minimum brightness factor to apply. Default: 1.0.\n        brightness_max: Maximum brightness factor to apply. Default: 1.0.\n        brightness_p: Probability of applying random brightness.\n\n    Returns:\n        Returns tuple: (image, instances) with augmentation applied.\n    \"\"\"\n    return apply_intensity_augmentation_skia(\n        image=image,\n        instances=instances,\n        uniform_noise_min=uniform_noise_min,\n        uniform_noise_max=uniform_noise_max,\n        uniform_noise_p=uniform_noise_p,\n        gaussian_noise_mean=gaussian_noise_mean,\n        gaussian_noise_std=gaussian_noise_std,\n        gaussian_noise_p=gaussian_noise_p,\n        contrast_min=contrast_min,\n        contrast_max=contrast_max,\n        contrast_p=contrast_p,\n        brightness_min=brightness_min,\n        brightness_max=brightness_max,\n        brightness_p=brightness_p,\n    )\n</code></pre>"},{"location":"api/data/confidence_maps/","title":"confidence_maps","text":""},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps","title":"<code>sleap_nn.data.confidence_maps</code>","text":"<p>Generate confidence maps.</p> <p>Functions:</p> Name Description <code>generate_confmaps</code> <p>Generate Confidence maps.</p> <code>generate_multiconfmaps</code> <p>Generate multi-instance confidence maps.</p> <code>make_confmaps</code> <p>Make confidence maps from a batch of points for multiple instances.</p> <code>make_multi_confmaps</code> <p>Make confidence maps for multiple instances through reduction.</p>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.generate_confmaps","title":"<code>generate_confmaps(instance, img_hw, sigma=1.5, output_stride=2)</code>","text":"<p>Generate Confidence maps.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or (n_samples, n_nodes, 2).</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <code>int</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps for the input keypoints.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def generate_confmaps(\n    instance: torch.Tensor,\n    img_hw: Tuple[int],\n    sigma: float = 1.5,\n    output_stride: int = 2,\n) -&gt; torch.Tensor:\n    \"\"\"Generate Confidence maps.\n\n    Args:\n        instance: Input keypoints. (n_samples, n_instances, n_nodes, 2) or\n            (n_samples, n_nodes, 2).\n        img_hw: Image size as tuple (height, width).\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n\n    Returns:\n        Confidence maps for the input keypoints.\n    \"\"\"\n    if instance.ndim != 3:\n        instance = instance.view(instance.shape[0], -1, 2)\n        # instances: (n_samples, n_nodes, 2)\n\n    height, width = img_hw\n\n    xv, yv = make_grid_vectors(height, width, output_stride)\n\n    confidence_maps = make_confmaps(\n        instance,\n        xv,\n        yv,\n        sigma * output_stride,\n    )  # (n_samples, n_nodes, height/ output_stride, width/ output_stride)\n\n    return confidence_maps\n</code></pre>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.generate_multiconfmaps","title":"<code>generate_multiconfmaps(instances, img_hw, num_instances, sigma=1.5, output_stride=2, is_centroids=False)</code>","text":"<p>Generate multi-instance confidence maps.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples, n_instances, n_nodes, 2) or for centroids - (n_samples, n_instances, 2)</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>num_instances</code> <code>int</code> <p>Original number of instances in the frame.</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <code>int</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <code>is_centroids</code> <code>bool</code> <p>True if confidence maps should be generates for centroids else False. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps for the input keypoints.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def generate_multiconfmaps(\n    instances: torch.Tensor,\n    img_hw: Tuple[int],\n    num_instances: int,\n    sigma: float = 1.5,\n    output_stride: int = 2,\n    is_centroids: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate multi-instance confidence maps.\n\n    Args:\n        instances: Input keypoints. (n_samples, n_instances, n_nodes, 2) or\n            for centroids - (n_samples, n_instances, 2)\n        img_hw: Image size as tuple (height, width).\n        num_instances: Original number of instances in the frame.\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n        is_centroids: True if confidence maps should be generates for centroids else False.\n            Default: False.\n\n    Returns:\n        Confidence maps for the input keypoints.\n    \"\"\"\n    if is_centroids:\n        points = instances[:, :num_instances, :].unsqueeze(dim=-2)\n        # (n_samples, n_instances, 1, 2)\n    else:\n        points = instances[\n            :, :num_instances, :, :\n        ]  # (n_samples, n_instances, n_nodes, 2)\n\n    height, width = img_hw\n\n    xv, yv = make_grid_vectors(height, width, output_stride)\n\n    confidence_maps = make_multi_confmaps(\n        points,\n        xv,\n        yv,\n        sigma * output_stride,\n    )  # (n_samples, n_nodes, height/ output_stride, width/ output_stride).\n    # If `is_centroids`, (n_samples, 1, height/ output_stride, width/ output_stride).\n\n    return confidence_maps\n</code></pre>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.make_confmaps","title":"<code>make_confmaps(points_batch, xv, yv, sigma)</code>","text":"<p>Make confidence maps from a batch of points for multiple instances.</p> <p>Parameters:</p> Name Type Description Default <code>points_batch</code> <code>Tensor</code> <p>A tensor of points of shape <code>(n_samples, n_nodes, 2)</code> and dtype <code>torch.float32</code> where the last axis corresponds to (x, y) pixel coordinates on the image for each instance. These can contain NaNs to indicate missing points.</p> required <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape <code>(grid_width,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape <code>(grid_height,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate confidence maps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps as a tensor of shape <code>(n_samples, n_nodes, grid_height, grid_width)</code> of dtype <code>torch.float32</code>.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def make_confmaps(\n    points_batch: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor, sigma: float\n) -&gt; torch.Tensor:\n    \"\"\"Make confidence maps from a batch of points for multiple instances.\n\n    Args:\n        points_batch: A tensor of points of shape `(n_samples, n_nodes, 2)` and dtype `torch.float32` where\n            the last axis corresponds to (x, y) pixel coordinates on the image for each instance.\n            These can contain NaNs to indicate missing points.\n        xv: Sampling grid vector for x-coordinates of shape `(grid_width,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape `(grid_height,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            confidence maps.\n\n    Returns:\n        Confidence maps as a tensor of shape `(n_samples, n_nodes, grid_height, grid_width)` of\n        dtype `torch.float32`.\n    \"\"\"\n    samples, n_nodes, _ = points_batch.shape\n\n    x = torch.reshape(points_batch[:, :, 0], (samples, n_nodes, 1, 1))\n    y = torch.reshape(points_batch[:, :, 1], (samples, n_nodes, 1, 1))\n\n    xv_reshaped = torch.reshape(xv, (1, 1, 1, -1))\n    yv_reshaped = torch.reshape(yv, (1, 1, -1, 1))\n\n    cm = torch.exp(-((xv_reshaped - x) ** 2 + (yv_reshaped - y) ** 2) / (2 * sigma**2))\n\n    # Replace NaNs with 0.\n    cm = torch.nan_to_num(cm)\n\n    return cm\n</code></pre>"},{"location":"api/data/confidence_maps/#sleap_nn.data.confidence_maps.make_multi_confmaps","title":"<code>make_multi_confmaps(points_batch, xv, yv, sigma)</code>","text":"<p>Make confidence maps for multiple instances through reduction.</p> <p>Parameters:</p> Name Type Description Default <code>points_batch</code> <code>Tensor</code> <p>A tensor of shape <code>(n_samples, n_instances, n_nodes, 2)</code> and dtype <code>tf.float32</code> containing instance points where the last axis corresponds to (x, y) pixel coordinates on the image. This must be rank-3 even if a single instance is present.</p> required <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape <code>(grid_width,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape <code>(grid_height,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>sleap.nn.data.utils.make_grid_vectors</code>.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate confidence maps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Confidence maps as a tensor of shape <code>(n_samples, n_nodes, grid_height, grid_width)</code> of dtype <code>torch.float32</code>.</p> <p>Each channel will contain the elementwise maximum of the confidence maps generated from all individual points for the associated node.</p> Source code in <code>sleap_nn/data/confidence_maps.py</code> <pre><code>def make_multi_confmaps(\n    points_batch: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor, sigma: float\n) -&gt; torch.Tensor:\n    \"\"\"Make confidence maps for multiple instances through reduction.\n\n    Args:\n        points_batch: A tensor of shape `(n_samples, n_instances, n_nodes, 2)`\n            and dtype `tf.float32` containing instance points where the last axis\n            corresponds to (x, y) pixel coordinates on the image. This must be rank-3\n            even if a single instance is present.\n        xv: Sampling grid vector for x-coordinates of shape `(grid_width,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape `(grid_height,)` and dtype\n            `torch.float32`. This can be generated by\n            `sleap.nn.data.utils.make_grid_vectors`.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            confidence maps.\n\n    Returns:\n        Confidence maps as a tensor of shape `(n_samples, n_nodes, grid_height, grid_width)` of\n        dtype `torch.float32`.\n\n        Each channel will contain the elementwise maximum of the confidence maps\n        generated from all individual points for the associated node.\n\n    \"\"\"\n    samples, n_inst, n_nodes, _ = points_batch.shape\n    w, h = xv.shape[0], yv.shape[0]\n    cms = torch.zeros((samples, n_nodes, h, w), dtype=torch.float32)\n    points = points_batch.reshape(samples * n_inst, n_nodes, 2)\n    for p in points:\n        cm_instance = make_confmaps(p.unsqueeze(dim=0), xv, yv, sigma)\n        cms = torch.maximum(cms, cm_instance)\n    return cms\n</code></pre>"},{"location":"api/data/custom_datasets/","title":"custom_datasets","text":""},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets","title":"<code>sleap_nn.data.custom_datasets</code>","text":"<p>Custom <code>torch.utils.data.Dataset</code>s for different model types.</p> <p>Classes:</p> Name Description <code>BaseDataset</code> <p>Base class for custom torch Datasets.</p> <code>BottomUpDataset</code> <p>Dataset class for bottom-up models.</p> <code>BottomUpMultiClassDataset</code> <p>Dataset class for bottom-up ID models.</p> <code>CenteredInstanceDataset</code> <p>Dataset class for instance-centered confidence map models.</p> <code>CentroidDataset</code> <p>Dataset class for centroid models.</p> <code>InfiniteDataLoader</code> <p>Dataloader that reuses workers for infinite iteration.</p> <code>ParallelCacheFiller</code> <p>Parallel implementation of image caching using thread-local video copies.</p> <code>SingleInstanceDataset</code> <p>Dataset class for single-instance models.</p> <code>TopDownCenteredInstanceMultiClassDataset</code> <p>Dataset class for instance-centered confidence map ID models.</p> <p>Functions:</p> Name Description <code>get_steps_per_epoch</code> <p>Compute the number of steps (iterations) per epoch for the given dataset.</p> <code>get_train_val_dataloaders</code> <p>Return the train and val dataloaders.</p> <code>get_train_val_datasets</code> <p>Return the train and val datasets.</p>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for custom torch Datasets.</p> <p>Attributes:</p> Name Type Description <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>parallel_caching</code> <p>If True, use parallel processing for caching (faster for large datasets). Default: True.</p> <code>cache_workers</code> <p>Number of worker threads for parallel caching. If 0, uses min(4, cpu_count). Default: 0.</p> <code>labels_list</code> <p>List of <code>sio.Labels</code> objects. Used to store the labels in the cache. (only used if <code>cache_img</code> is <code>None</code>)</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Returns the sample dict for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> <code>__iter__</code> <p>Returns an iterator.</p> <code>__len__</code> <p>Return the number of samples in the dataset.</p> <code>__next__</code> <p>Get the next sample from the dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class BaseDataset(Dataset):\n    \"\"\"Base class for custom torch Datasets.\n\n    Attributes:\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        parallel_caching: If True, use parallel processing for caching (faster for large datasets). Default: True.\n        cache_workers: Number of worker threads for parallel caching. If 0, uses min(4, cpu_count). Default: 0.\n        labels_list: List of `sio.Labels` objects. Used to store the labels in the cache. (only used if `cache_img` is `None`)\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        max_stride: int,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n        parallel_caching: bool = True,\n        cache_workers: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__()\n        self.user_instances_only = user_instances_only\n        self.ensure_rgb = ensure_rgb\n        self.ensure_grayscale = ensure_grayscale\n\n        # Handle intensity augmentation\n        if intensity_aug is not None:\n            if not isinstance(intensity_aug, DictConfig):\n                intensity_aug = get_aug_config(intensity_aug=intensity_aug)\n                config = OmegaConf.structured(intensity_aug)\n                OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n                intensity_aug = DictConfig(config.intensity)\n        self.intensity_aug = intensity_aug\n\n        # Handle geometric augmentation\n        if geometric_aug is not None:\n            if not isinstance(geometric_aug, DictConfig):\n                geometric_aug = get_aug_config(geometric_aug=geometric_aug)\n                config = OmegaConf.structured(geometric_aug)\n                OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n                geometric_aug = DictConfig(config.geometric)\n        self.geometric_aug = geometric_aug\n        self.curr_idx = 0\n        self.max_stride = max_stride\n        self.scale = scale\n        self.apply_aug = apply_aug\n        self.max_hw = max_hw\n        self.rank = rank\n        self.max_instances = 0\n        for x in labels:\n            max_instances = get_max_instances(x) if x else None\n\n            if max_instances &gt; self.max_instances:\n                self.max_instances = max_instances\n\n        self.cache_img = cache_img\n        self.cache_img_path = cache_img_path\n        self.use_existing_imgs = use_existing_imgs\n        self.parallel_caching = parallel_caching\n        self.cache_workers = cache_workers\n        if self.cache_img is not None and \"disk\" in self.cache_img:\n            if self.cache_img_path is None:\n                self.cache_img_path = \".\"\n            path = (\n                Path(self.cache_img_path)\n                if isinstance(self.cache_img_path, str)\n                else self.cache_img_path\n            )\n            if not path.is_dir():\n                path.mkdir(parents=True, exist_ok=True)\n\n        self.lf_idx_list = self._get_lf_idx_list(labels)\n\n        self.labels_list = None\n        # this is to ensure that the labels are not passed to the multiprocessing pool when caching is enabled\n        # (h5py objects can't be pickled error with num_workers &gt; 0) in mac and windows\n        if self.cache_img is None:\n            self.labels_list = labels\n\n        self.transform_to_pil = T.ToPILImage()\n        self.transform_pil_to_tensor = T.ToTensor()\n        self.cache = {}\n\n        if self.cache_img is not None:\n            if self.cache_img == \"memory\":\n                self._fill_cache(\n                    labels,\n                    parallel=self.parallel_caching,\n                    num_workers=self.cache_workers,\n                )\n            elif self.cache_img == \"disk\" and not self.use_existing_imgs:\n                if self.rank is None or self.rank == -1 or self.rank == 0:\n                    self._fill_cache(\n                        labels,\n                        parallel=self.parallel_caching,\n                        num_workers=self.cache_workers,\n                    )\n                # Synchronize all ranks after cache creation\n                if is_distributed_initialized():\n                    dist.barrier()\n\n    def _get_lf_idx_list(self, labels: List[sio.Labels]) -&gt; List[Tuple[int]]:\n        \"\"\"Return list of indices of labelled frames.\"\"\"\n        lf_idx_list = []\n        for labels_idx, label in enumerate(labels):\n            for lf_idx, lf in enumerate(label):\n                # Filter to user instances\n                if self.user_instances_only:\n                    if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                        lf.instances = lf.user_instances\n                    else:\n                        # Skip frames without user instances\n                        continue\n                is_empty = True\n                for _, inst in enumerate(lf.instances):\n                    if not inst.is_empty:  # filter all NaN instances.\n                        is_empty = False\n                if not is_empty:\n                    video_idx = labels[labels_idx].videos.index(lf.video)\n                    sample = {\n                        \"labels_idx\": labels_idx,\n                        \"lf_idx\": lf_idx,\n                        \"video_idx\": video_idx,\n                        \"frame_idx\": lf.frame_idx,\n                        \"instances\": (\n                            lf.instances if self.cache_img is not None else None\n                        ),\n                    }\n                    lf_idx_list.append(sample)\n                    # This is to ensure that the labels are not passed to the multiprocessing pool (h5py objects can't be pickled)\n        return lf_idx_list\n\n    def __next__(self):\n        \"\"\"Get the next sample from the dataset.\"\"\"\n        if self.curr_idx &gt;= len(self):\n            raise StopIteration\n\n        sample = self.__getitem__(self.curr_idx)\n        self.curr_idx += 1\n        return sample\n\n    def __iter__(self):\n        \"\"\"Returns an iterator.\"\"\"\n        return self\n\n    def _fill_cache(\n        self,\n        labels: List[sio.Labels],\n        parallel: bool = True,\n        num_workers: int = 0,\n    ):\n        \"\"\"Load all samples to cache.\n\n        Args:\n            labels: List of sio.Labels objects containing the data.\n            parallel: If True, use parallel processing for caching (faster for large\n                datasets). Default: True.\n            num_workers: Number of worker threads for parallel caching. If 0, uses\n                min(4, cpu_count). Default: 0.\n        \"\"\"\n        total_samples = len(self.lf_idx_list)\n        cache_type = \"disk\" if self.cache_img == \"disk\" else \"memory\"\n\n        # Check for NO_COLOR env var to disable progress bar\n        no_color = (\n            os.environ.get(\"NO_COLOR\") is not None\n            or os.environ.get(\"FORCE_COLOR\") == \"0\"\n        )\n        use_progress = not no_color\n\n        # Use parallel caching for larger datasets\n        use_parallel = parallel and total_samples &gt;= MIN_SAMPLES_FOR_PARALLEL_CACHING\n\n        logger.info(f\"Caching {total_samples} images to {cache_type}...\")\n\n        if use_parallel:\n            self._fill_cache_parallel(\n                labels, total_samples, cache_type, use_progress, num_workers\n            )\n        else:\n            self._fill_cache_sequential(labels, total_samples, cache_type, use_progress)\n\n        logger.info(f\"Caching complete.\")\n\n    def _fill_cache_sequential(\n        self,\n        labels: List[sio.Labels],\n        total_samples: int,\n        cache_type: str,\n        use_progress: bool,\n    ):\n        \"\"\"Sequential implementation of cache filling.\n\n        Args:\n            labels: List of sio.Labels objects.\n            total_samples: Total number of samples to cache.\n            cache_type: Either \"disk\" or \"memory\".\n            use_progress: Whether to show a progress bar.\n        \"\"\"\n\n        def process_samples(progress=None, task=None):\n            for sample in self.lf_idx_list:\n                labels_idx = sample[\"labels_idx\"]\n                lf_idx = sample[\"lf_idx\"]\n                img = labels[labels_idx][lf_idx].image\n                if img.shape[-1] == 1:\n                    img = np.squeeze(img)\n                if self.cache_img == \"disk\":\n                    f_name = f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    Image.fromarray(img).save(f_name, format=\"JPEG\")\n                if self.cache_img == \"memory\":\n                    self.cache[(labels_idx, lf_idx)] = img\n                if progress is not None:\n                    progress.update(task, advance=1)\n\n        if use_progress:\n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                TextColumn(\"{task.completed}/{task.total}\"),\n                TimeElapsedColumn(),\n                console=Console(force_terminal=True),\n                transient=True,\n            ) as progress:\n                task = progress.add_task(\n                    f\"Caching images to {cache_type}\", total=total_samples\n                )\n                process_samples(progress, task)\n        else:\n            process_samples()\n\n    def _fill_cache_parallel(\n        self,\n        labels: List[sio.Labels],\n        total_samples: int,\n        cache_type: str,\n        use_progress: bool,\n        num_workers: int = 0,\n    ):\n        \"\"\"Parallel implementation of cache filling using thread-local video copies.\n\n        Args:\n            labels: List of sio.Labels objects.\n            total_samples: Total number of samples to cache.\n            cache_type: Either \"disk\" or \"memory\".\n            use_progress: Whether to show a progress bar.\n            num_workers: Number of worker threads. If 0, uses min(4, cpu_count).\n        \"\"\"\n        # Determine number of workers\n        if num_workers &lt;= 0:\n            num_workers = min(4, os.cpu_count() or 1)\n\n        cache_path = Path(self.cache_img_path) if self.cache_img_path else None\n\n        filler = ParallelCacheFiller(\n            labels=labels,\n            lf_idx_list=self.lf_idx_list,\n            cache_type=cache_type,\n            cache_path=cache_path,\n            num_workers=num_workers,\n        )\n\n        if use_progress:\n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                TextColumn(\"{task.completed}/{task.total}\"),\n                TimeElapsedColumn(),\n                console=Console(force_terminal=True),\n                transient=True,\n            ) as progress:\n                task = progress.add_task(\n                    f\"Caching images to {cache_type} (parallel, {num_workers} workers)\",\n                    total=total_samples,\n                )\n\n                def progress_callback(completed):\n                    progress.update(task, completed=completed)\n\n                cache, errors = filler.fill_cache(progress_callback)\n        else:\n            logger.info(\n                f\"Caching {total_samples} images to {cache_type} \"\n                f\"(parallel, {num_workers} workers)...\"\n            )\n            cache, errors = filler.fill_cache()\n\n        # Update instance cache\n        if cache_type == \"memory\":\n            self.cache.update(cache)\n\n        # Log any errors\n        if errors:\n            logger.warning(\n                f\"Parallel caching completed with {len(errors)} errors. \"\n                f\"First error: {errors[0]}\"\n            )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples in the dataset.\"\"\"\n        return len(self.lf_idx_list)\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Returns the sample dict for given index.\"\"\"\n        message = \"Subclasses must implement __getitem__\"\n        logger.error(message)\n        raise NotImplementedError(message)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns the sample dict for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Returns the sample dict for given index.\"\"\"\n    message = \"Subclasses must implement __getitem__\"\n    logger.error(message)\n    raise NotImplementedError(message)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__init__","title":"<code>__init__(labels, max_stride, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None, parallel_caching=True, cache_workers=0)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    max_stride: int,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n    parallel_caching: bool = True,\n    cache_workers: int = 0,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__()\n    self.user_instances_only = user_instances_only\n    self.ensure_rgb = ensure_rgb\n    self.ensure_grayscale = ensure_grayscale\n\n    # Handle intensity augmentation\n    if intensity_aug is not None:\n        if not isinstance(intensity_aug, DictConfig):\n            intensity_aug = get_aug_config(intensity_aug=intensity_aug)\n            config = OmegaConf.structured(intensity_aug)\n            OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n            intensity_aug = DictConfig(config.intensity)\n    self.intensity_aug = intensity_aug\n\n    # Handle geometric augmentation\n    if geometric_aug is not None:\n        if not isinstance(geometric_aug, DictConfig):\n            geometric_aug = get_aug_config(geometric_aug=geometric_aug)\n            config = OmegaConf.structured(geometric_aug)\n            OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n            geometric_aug = DictConfig(config.geometric)\n    self.geometric_aug = geometric_aug\n    self.curr_idx = 0\n    self.max_stride = max_stride\n    self.scale = scale\n    self.apply_aug = apply_aug\n    self.max_hw = max_hw\n    self.rank = rank\n    self.max_instances = 0\n    for x in labels:\n        max_instances = get_max_instances(x) if x else None\n\n        if max_instances &gt; self.max_instances:\n            self.max_instances = max_instances\n\n    self.cache_img = cache_img\n    self.cache_img_path = cache_img_path\n    self.use_existing_imgs = use_existing_imgs\n    self.parallel_caching = parallel_caching\n    self.cache_workers = cache_workers\n    if self.cache_img is not None and \"disk\" in self.cache_img:\n        if self.cache_img_path is None:\n            self.cache_img_path = \".\"\n        path = (\n            Path(self.cache_img_path)\n            if isinstance(self.cache_img_path, str)\n            else self.cache_img_path\n        )\n        if not path.is_dir():\n            path.mkdir(parents=True, exist_ok=True)\n\n    self.lf_idx_list = self._get_lf_idx_list(labels)\n\n    self.labels_list = None\n    # this is to ensure that the labels are not passed to the multiprocessing pool when caching is enabled\n    # (h5py objects can't be pickled error with num_workers &gt; 0) in mac and windows\n    if self.cache_img is None:\n        self.labels_list = labels\n\n    self.transform_to_pil = T.ToPILImage()\n    self.transform_pil_to_tensor = T.ToTensor()\n    self.cache = {}\n\n    if self.cache_img is not None:\n        if self.cache_img == \"memory\":\n            self._fill_cache(\n                labels,\n                parallel=self.parallel_caching,\n                num_workers=self.cache_workers,\n            )\n        elif self.cache_img == \"disk\" and not self.use_existing_imgs:\n            if self.rank is None or self.rank == -1 or self.rank == 0:\n                self._fill_cache(\n                    labels,\n                    parallel=self.parallel_caching,\n                    num_workers=self.cache_workers,\n                )\n            # Synchronize all ranks after cache creation\n            if is_distributed_initialized():\n                dist.barrier()\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Returns an iterator.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Returns an iterator.\"\"\"\n    return self\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples in the dataset.\"\"\"\n    return len(self.lf_idx_list)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BaseDataset.__next__","title":"<code>__next__()</code>","text":"<p>Get the next sample from the dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __next__(self):\n    \"\"\"Get the next sample from the dataset.\"\"\"\n    if self.curr_idx &gt;= len(self):\n        raise StopIteration\n\n    sample = self.__getitem__(self.curr_idx)\n    self.curr_idx += 1\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpDataset","title":"<code>BottomUpDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for bottom-up models.</p> <p>Attributes:</p> Name Type Description <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section. (required keys: <code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ).</p> <code>pafs_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section (required keys: <code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ) for PAFs.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>labels_list</code> <p>List of <code>sio.Labels</code> objects. Used to store the labels in the cache. (only used if <code>cache_img</code> is <code>None</code>)</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image, confmaps and pafs for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class BottomUpDataset(BaseDataset):\n    \"\"\"Dataset class for bottom-up models.\n\n    Attributes:\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n            (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type ).\n        pafs_head_config: DictConfig object with all the keys in the `head_config` section\n            (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type )\n            for PAFs.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        labels_list: List of `sio.Labels` objects. Used to store the labels in the cache. (only used if `cache_img` is `None`)\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        pafs_head_config: DictConfig,\n        max_stride: int,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n        parallel_caching: bool = True,\n        cache_workers: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        self.confmap_head_config = confmap_head_config\n        self.pafs_head_config = pafs_head_config\n\n        self.edge_inds = labels[0].skeletons[0].edge_inds\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n        sample = self.lf_idx_list[index]\n        labels_idx = sample[\"labels_idx\"]\n        lf_idx = sample[\"lf_idx\"]\n        video_idx = sample[\"video_idx\"]\n        frame_idx = sample[\"frame_idx\"]\n\n        if self.cache_img is not None:\n            instances = sample[\"instances\"]\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n        else:\n            lf = self.labels_list[labels_idx][lf_idx]\n            instances = lf.instances\n            img = lf.image\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        # get dict\n        sample = process_lf(\n            instances_list=instances,\n            img=img,\n            frame_idx=frame_idx,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_multiconfmaps(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n            is_centroids=False,\n        )\n\n        # pafs\n        pafs = generate_pafs(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            sigma=self.pafs_head_config.sigma,\n            output_stride=self.pafs_head_config.output_stride,\n            edge_inds=torch.Tensor(self.edge_inds),\n            flatten_channels=True,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"part_affinity_fields\"] = pafs\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image, confmaps and pafs for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n    sample = self.lf_idx_list[index]\n    labels_idx = sample[\"labels_idx\"]\n    lf_idx = sample[\"lf_idx\"]\n    video_idx = sample[\"video_idx\"]\n    frame_idx = sample[\"frame_idx\"]\n\n    if self.cache_img is not None:\n        instances = sample[\"instances\"]\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n    else:\n        lf = self.labels_list[labels_idx][lf_idx]\n        instances = lf.instances\n        img = lf.image\n\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    # get dict\n    sample = process_lf(\n        instances_list=instances,\n        img=img,\n        frame_idx=frame_idx,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_multiconfmaps(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n        is_centroids=False,\n    )\n\n    # pafs\n    pafs = generate_pafs(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        sigma=self.pafs_head_config.sigma,\n        output_stride=self.pafs_head_config.output_stride,\n        edge_inds=torch.Tensor(self.edge_inds),\n        flatten_channels=True,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"part_affinity_fields\"] = pafs\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpDataset.__init__","title":"<code>__init__(labels, confmap_head_config, pafs_head_config, max_stride, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None, parallel_caching=True, cache_workers=0)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    pafs_head_config: DictConfig,\n    max_stride: int,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n    parallel_caching: bool = True,\n    cache_workers: int = 0,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n        parallel_caching=parallel_caching,\n        cache_workers=cache_workers,\n    )\n    self.confmap_head_config = confmap_head_config\n    self.pafs_head_config = pafs_head_config\n\n    self.edge_inds = labels[0].skeletons[0].edge_inds\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpMultiClassDataset","title":"<code>BottomUpMultiClassDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for bottom-up ID models.</p> <p>Attributes:</p> Name Type Description <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>class_map_threshold</code> <p>Minimum confidence map value below which map values will be replaced with zeros.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section. (required keys: <code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ).</p> <code>class_maps_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section (required keys: <code>sigma</code>, <code>output_stride</code> and <code>classes</code>) for class maps.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>labels_list</code> <p>List of <code>sio.Labels</code> objects. Used to store the labels in the cache. (only used if <code>cache_img</code> is <code>None</code>)</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image, confmaps and pafs for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class BottomUpMultiClassDataset(BaseDataset):\n    \"\"\"Dataset class for bottom-up ID models.\n\n    Attributes:\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        class_map_threshold: Minimum confidence map value below which map values will be\n            replaced with zeros.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n            (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type ).\n        class_maps_head_config: DictConfig object with all the keys in the `head_config` section\n            (required keys: `sigma`, `output_stride` and `classes`)\n            for class maps.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        labels_list: List of `sio.Labels` objects. Used to store the labels in the cache. (only used if `cache_img` is `None`)\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        class_maps_head_config: DictConfig,\n        max_stride: int,\n        class_map_threshold: float = 0.2,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n        parallel_caching: bool = True,\n        cache_workers: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        self.confmap_head_config = confmap_head_config\n        self.class_maps_head_config = class_maps_head_config\n\n        self.class_names = self.class_maps_head_config.classes\n        self.class_map_threshold = class_map_threshold\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n        sample = self.lf_idx_list[index]\n        labels_idx = sample[\"labels_idx\"]\n        lf_idx = sample[\"lf_idx\"]\n        video_idx = sample[\"video_idx\"]\n        frame_idx = sample[\"frame_idx\"]\n\n        if self.cache_img is not None:\n            instances = sample[\"instances\"]\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n        else:\n            lf = self.labels_list[labels_idx][lf_idx]\n            instances = lf.instances\n            img = lf.image\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        # get dict\n        sample = process_lf(\n            instances_list=instances,\n            img=img,\n            frame_idx=frame_idx,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        track_ids = torch.Tensor(\n            [\n                (\n                    self.class_names.index(instances[idx].track.name)\n                    if instances[idx].track is not None\n                    else -1\n                )\n                for idx in range(sample[\"num_instances\"])\n            ]\n        ).to(torch.int32)\n\n        sample[\"num_tracks\"] = torch.tensor(len(self.class_names), dtype=torch.int32)\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_multiconfmaps(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n            is_centroids=False,\n        )\n\n        # class maps\n        class_maps = generate_class_maps(\n            instances=sample[\"instances\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            class_inds=track_ids,\n            num_tracks=sample[\"num_tracks\"],\n            class_map_threshold=self.class_map_threshold,\n            sigma=self.class_maps_head_config.sigma,\n            output_stride=self.class_maps_head_config.output_stride,\n            is_centroids=False,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"class_maps\"] = class_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpMultiClassDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image, confmaps and pafs for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image, confmaps and pafs for given index.\"\"\"\n    sample = self.lf_idx_list[index]\n    labels_idx = sample[\"labels_idx\"]\n    lf_idx = sample[\"lf_idx\"]\n    video_idx = sample[\"video_idx\"]\n    frame_idx = sample[\"frame_idx\"]\n\n    if self.cache_img is not None:\n        instances = sample[\"instances\"]\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n    else:\n        lf = self.labels_list[labels_idx][lf_idx]\n        instances = lf.instances\n        img = lf.image\n\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    # get dict\n    sample = process_lf(\n        instances_list=instances,\n        img=img,\n        frame_idx=frame_idx,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    track_ids = torch.Tensor(\n        [\n            (\n                self.class_names.index(instances[idx].track.name)\n                if instances[idx].track is not None\n                else -1\n            )\n            for idx in range(sample[\"num_instances\"])\n        ]\n    ).to(torch.int32)\n\n    sample[\"num_tracks\"] = torch.tensor(len(self.class_names), dtype=torch.int32)\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_multiconfmaps(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n        is_centroids=False,\n    )\n\n    # class maps\n    class_maps = generate_class_maps(\n        instances=sample[\"instances\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        class_inds=track_ids,\n        num_tracks=sample[\"num_tracks\"],\n        class_map_threshold=self.class_map_threshold,\n        sigma=self.class_maps_head_config.sigma,\n        output_stride=self.class_maps_head_config.output_stride,\n        is_centroids=False,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"class_maps\"] = class_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.BottomUpMultiClassDataset.__init__","title":"<code>__init__(labels, confmap_head_config, class_maps_head_config, max_stride, class_map_threshold=0.2, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None, parallel_caching=True, cache_workers=0)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    class_maps_head_config: DictConfig,\n    max_stride: int,\n    class_map_threshold: float = 0.2,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n    parallel_caching: bool = True,\n    cache_workers: int = 0,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n        parallel_caching=parallel_caching,\n        cache_workers=cache_workers,\n    )\n    self.confmap_head_config = confmap_head_config\n    self.class_maps_head_config = class_maps_head_config\n\n    self.class_names = self.class_maps_head_config.classes\n    self.class_map_threshold = class_map_threshold\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset","title":"<code>CenteredInstanceDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for instance-centered confidence map models.</p> <p>Attributes:</p> Name Type Description <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>anchor_ind</code> <p>Index of the node to use as the anchor point, based on its index in the ordered list of skeleton nodes.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>crop_size</code> <p>Crop size of each instance for centered-instance model. If <code>scale</code> is provided, then the cropped image will be resized according to <code>scale</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section. (required keys: <code>sigma</code>, <code>output_stride</code>, <code>part_names</code> and <code>anchor_part</code> depending on the model type ).</p> <code>labels_list</code> <p>List of <code>sio.Labels</code> objects. Used to store the labels in the cache. (only used if <code>cache_img</code> is <code>None</code>)</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with cropped image and confmaps of instance for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> <code>__len__</code> <p>Return number of instances in the labels object.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class CenteredInstanceDataset(BaseDataset):\n    \"\"\"Dataset class for instance-centered confidence map models.\n\n    Attributes:\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        anchor_ind: Index of the node to use as the anchor point, based on its index in the\n            ordered list of skeleton nodes.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        crop_size: Crop size of each instance for centered-instance model. If `scale` is provided, then the cropped image will be resized according to `scale`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n            (required keys: `sigma`, `output_stride`, `part_names` and `anchor_part` depending on the model type ).\n        labels_list: List of `sio.Labels` objects. Used to store the labels in the cache. (only used if `cache_img` is `None`)\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        crop_size: int,\n        confmap_head_config: DictConfig,\n        max_stride: int,\n        anchor_ind: Optional[int] = None,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n        parallel_caching: bool = True,\n        cache_workers: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        self.labels = None\n        self.crop_size = crop_size\n        self.anchor_ind = anchor_ind\n        self.confmap_head_config = confmap_head_config\n        self.instance_idx_list = self._get_instance_idx_list(labels)\n        self.cache_lf = [None, None]\n\n    def _get_instance_idx_list(self, labels: List[sio.Labels]) -&gt; List[Tuple[int]]:\n        \"\"\"Return list of tuples with indices of labelled frames and instances.\"\"\"\n        instance_idx_list = []\n        for labels_idx, label in enumerate(labels):\n            for lf_idx, lf in enumerate(label):\n                # Filter to user instances\n                if self.user_instances_only:\n                    if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                        lf.instances = lf.user_instances\n                    else:\n                        # Skip frames without user instances\n                        continue\n                for inst_idx, inst in enumerate(lf.instances):\n                    if not inst.is_empty:  # filter all NaN instances.\n                        video_idx = labels[labels_idx].videos.index(lf.video)\n                        sample = {\n                            \"labels_idx\": labels_idx,\n                            \"lf_idx\": lf_idx,\n                            \"inst_idx\": inst_idx,\n                            \"video_idx\": video_idx,\n                            \"instances\": (\n                                lf.instances if self.cache_img is not None else None\n                            ),\n                            \"frame_idx\": lf.frame_idx,\n                        }\n                        instance_idx_list.append(sample)\n                        # This is to ensure that the labels are not passed to the multiprocessing pool (h5py objects can't be pickled)\n        return instance_idx_list\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of instances in the labels object.\"\"\"\n        return len(self.instance_idx_list)\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n        sample = self.instance_idx_list[index]\n        labels_idx = sample[\"labels_idx\"]\n        lf_idx = sample[\"lf_idx\"]\n        inst_idx = sample[\"inst_idx\"]\n        video_idx = sample[\"video_idx\"]\n        lf_frame_idx = sample[\"frame_idx\"]\n\n        if self.cache_img is not None:\n            instances_list = sample[\"instances\"]\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n        else:\n            lf = self.labels_list[labels_idx][lf_idx]\n            instances_list = lf.instances\n            img = lf.image\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n        instances = []\n        for inst in instances_list:\n            instances.append(\n                inst.numpy()\n            )  # no need to filter empty instances; handled while creating instance_idx_list\n        instances = np.stack(instances, axis=0)\n\n        # Add singleton time dimension for single frames.\n        image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n        instances = np.expand_dims(\n            instances, axis=0\n        )  # (n_samples=1, num_instances, num_nodes, 2)\n\n        instances = torch.from_numpy(instances.astype(\"float32\"))\n        image = torch.from_numpy(image.copy())\n\n        num_instances, _ = instances.shape[1:3]\n        orig_img_height, orig_img_width = image.shape[-2:]\n\n        instances = instances[:, inst_idx]\n\n        if self.ensure_rgb:\n            image = convert_to_rgb(image)\n        elif self.ensure_grayscale:\n            image = convert_to_grayscale(image)\n\n        # size matcher\n        image, eff_scale = apply_sizematcher(\n            image,\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        instances = instances * eff_scale\n\n        # get the centroids based on the anchor idx\n        centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n        instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n        crop_size = np.array([self.crop_size, self.crop_size]) * np.sqrt(\n            2\n        )  # crop extra for rotation augmentation\n        crop_size = crop_size.astype(np.int32).tolist()\n\n        sample = generate_crops(image, instance, centroid, crop_size)\n\n        sample[\"frame_idx\"] = torch.tensor(lf_frame_idx, dtype=torch.int32)\n        sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n        sample[\"num_instances\"] = num_instances\n        sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width]).unsqueeze(\n            0\n        )\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_intensity_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_geometric_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.geometric_aug,\n                )\n\n        # re-crop to original crop size\n        sample[\"instance_bbox\"] = torch.unsqueeze(\n            make_centered_bboxes(sample[\"centroid\"][0], self.crop_size, self.crop_size),\n            0,\n        )  # (n_samples=1, 4, 2)\n\n        sample[\"instance_image\"] = crop_and_resize(\n            sample[\"instance_image\"],\n            boxes=sample[\"instance_bbox\"],\n            size=(self.crop_size, self.crop_size),\n        )\n        point = sample[\"instance_bbox\"][0][0]\n        center_instance = sample[\"instance\"] - point\n        centered_centroid = sample[\"centroid\"] - point\n\n        sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n        sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n        # resize the cropped image\n        sample[\"instance_image\"], sample[\"instance\"] = apply_resizer(\n            sample[\"instance_image\"],\n            sample[\"instance\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"instance_image\"] = apply_pad_to_stride(\n            sample[\"instance_image\"], max_stride=self.max_stride\n        )\n\n        img_hw = sample[\"instance_image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_confmaps(\n            sample[\"instance\"],\n            img_hw=img_hw,\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with cropped image and confmaps of instance for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n    sample = self.instance_idx_list[index]\n    labels_idx = sample[\"labels_idx\"]\n    lf_idx = sample[\"lf_idx\"]\n    inst_idx = sample[\"inst_idx\"]\n    video_idx = sample[\"video_idx\"]\n    lf_frame_idx = sample[\"frame_idx\"]\n\n    if self.cache_img is not None:\n        instances_list = sample[\"instances\"]\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n    else:\n        lf = self.labels_list[labels_idx][lf_idx]\n        instances_list = lf.instances\n        img = lf.image\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n    instances = []\n    for inst in instances_list:\n        instances.append(\n            inst.numpy()\n        )  # no need to filter empty instances; handled while creating instance_idx_list\n    instances = np.stack(instances, axis=0)\n\n    # Add singleton time dimension for single frames.\n    image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n    instances = np.expand_dims(\n        instances, axis=0\n    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n    instances = torch.from_numpy(instances.astype(\"float32\"))\n    image = torch.from_numpy(image.copy())\n\n    num_instances, _ = instances.shape[1:3]\n    orig_img_height, orig_img_width = image.shape[-2:]\n\n    instances = instances[:, inst_idx]\n\n    if self.ensure_rgb:\n        image = convert_to_rgb(image)\n    elif self.ensure_grayscale:\n        image = convert_to_grayscale(image)\n\n    # size matcher\n    image, eff_scale = apply_sizematcher(\n        image,\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    instances = instances * eff_scale\n\n    # get the centroids based on the anchor idx\n    centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n    instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n    crop_size = np.array([self.crop_size, self.crop_size]) * np.sqrt(\n        2\n    )  # crop extra for rotation augmentation\n    crop_size = crop_size.astype(np.int32).tolist()\n\n    sample = generate_crops(image, instance, centroid, crop_size)\n\n    sample[\"frame_idx\"] = torch.tensor(lf_frame_idx, dtype=torch.int32)\n    sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n    sample[\"num_instances\"] = num_instances\n    sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width]).unsqueeze(\n        0\n    )\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_intensity_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_geometric_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.geometric_aug,\n            )\n\n    # re-crop to original crop size\n    sample[\"instance_bbox\"] = torch.unsqueeze(\n        make_centered_bboxes(sample[\"centroid\"][0], self.crop_size, self.crop_size),\n        0,\n    )  # (n_samples=1, 4, 2)\n\n    sample[\"instance_image\"] = crop_and_resize(\n        sample[\"instance_image\"],\n        boxes=sample[\"instance_bbox\"],\n        size=(self.crop_size, self.crop_size),\n    )\n    point = sample[\"instance_bbox\"][0][0]\n    center_instance = sample[\"instance\"] - point\n    centered_centroid = sample[\"centroid\"] - point\n\n    sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n    sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n    # resize the cropped image\n    sample[\"instance_image\"], sample[\"instance\"] = apply_resizer(\n        sample[\"instance_image\"],\n        sample[\"instance\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"instance_image\"] = apply_pad_to_stride(\n        sample[\"instance_image\"], max_stride=self.max_stride\n    )\n\n    img_hw = sample[\"instance_image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_confmaps(\n        sample[\"instance\"],\n        img_hw=img_hw,\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset.__init__","title":"<code>__init__(labels, crop_size, confmap_head_config, max_stride, anchor_ind=None, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None, parallel_caching=True, cache_workers=0)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    crop_size: int,\n    confmap_head_config: DictConfig,\n    max_stride: int,\n    anchor_ind: Optional[int] = None,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n    parallel_caching: bool = True,\n    cache_workers: int = 0,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n        parallel_caching=parallel_caching,\n        cache_workers=cache_workers,\n    )\n    self.labels = None\n    self.crop_size = crop_size\n    self.anchor_ind = anchor_ind\n    self.confmap_head_config = confmap_head_config\n    self.instance_idx_list = self._get_instance_idx_list(labels)\n    self.cache_lf = [None, None]\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CenteredInstanceDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return number of instances in the labels object.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of instances in the labels object.\"\"\"\n    return len(self.instance_idx_list)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CentroidDataset","title":"<code>CentroidDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for centroid models.</p> <p>Attributes:</p> Name Type Description <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>anchor_ind</code> <p>Index of the node to use as the anchor point, based on its index in the ordered list of skeleton nodes.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section.</p> <code>(required</code> <code>keys</code> <p><code>sigma</code>, <code>output_stride</code> and <code>anchor_part</code> depending on the model type ).</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>labels_list</code> <p>List of <code>sio.Labels</code> objects. Used to store the labels in the cache. (only used if <code>cache_img</code> is <code>None</code>)</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image and confmaps for centroids for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class CentroidDataset(BaseDataset):\n    \"\"\"Dataset class for centroid models.\n\n    Attributes:\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        anchor_ind: Index of the node to use as the anchor point, based on its index in the\n            ordered list of skeleton nodes.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n        (required keys: `sigma`, `output_stride` and `anchor_part` depending on the model type ).\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        labels_list: List of `sio.Labels` objects. Used to store the labels in the cache. (only used if `cache_img` is `None`)\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        max_stride: int,\n        anchor_ind: Optional[int] = None,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n        parallel_caching: bool = True,\n        cache_workers: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        self.anchor_ind = anchor_ind\n        self.confmap_head_config = confmap_head_config\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image and confmaps for centroids for given index.\"\"\"\n        sample = self.lf_idx_list[index]\n        labels_idx = sample[\"labels_idx\"]\n        lf_idx = sample[\"lf_idx\"]\n        video_idx = sample[\"video_idx\"]\n        lf_frame_idx = sample[\"frame_idx\"]\n\n        if self.cache_img is not None:\n            instances = sample[\"instances\"]\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n        else:\n            lf = self.labels_list[labels_idx][lf_idx]\n            instances = lf.instances\n            img = lf.image\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        # get dict\n        sample = process_lf(\n            instances_list=instances,\n            img=img,\n            frame_idx=lf_frame_idx,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # get the centroids based on the anchor idx\n        centroids = generate_centroids(sample[\"instances\"], anchor_ind=self.anchor_ind)\n\n        sample[\"centroids\"] = centroids\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"centroids\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"centroids\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"centroids\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"centroids\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_multiconfmaps(\n            sample[\"centroids\"],\n            img_hw=img_hw,\n            num_instances=sample[\"num_instances\"],\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n            is_centroids=True,\n        )\n\n        sample[\"centroids_confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CentroidDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image and confmaps for centroids for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image and confmaps for centroids for given index.\"\"\"\n    sample = self.lf_idx_list[index]\n    labels_idx = sample[\"labels_idx\"]\n    lf_idx = sample[\"lf_idx\"]\n    video_idx = sample[\"video_idx\"]\n    lf_frame_idx = sample[\"frame_idx\"]\n\n    if self.cache_img is not None:\n        instances = sample[\"instances\"]\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n    else:\n        lf = self.labels_list[labels_idx][lf_idx]\n        instances = lf.instances\n        img = lf.image\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    # get dict\n    sample = process_lf(\n        instances_list=instances,\n        img=img,\n        frame_idx=lf_frame_idx,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # get the centroids based on the anchor idx\n    centroids = generate_centroids(sample[\"instances\"], anchor_ind=self.anchor_ind)\n\n    sample[\"centroids\"] = centroids\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"centroids\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"centroids\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"centroids\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"centroids\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_multiconfmaps(\n        sample[\"centroids\"],\n        img_hw=img_hw,\n        num_instances=sample[\"num_instances\"],\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n        is_centroids=True,\n    )\n\n    sample[\"centroids_confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.CentroidDataset.__init__","title":"<code>__init__(labels, confmap_head_config, max_stride, anchor_ind=None, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None, parallel_caching=True, cache_workers=0)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    max_stride: int,\n    anchor_ind: Optional[int] = None,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n    parallel_caching: bool = True,\n    cache_workers: int = 0,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n        parallel_caching=parallel_caching,\n        cache_workers=cache_workers,\n    )\n    self.anchor_ind = anchor_ind\n    self.confmap_head_config = confmap_head_config\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader","title":"<code>InfiniteDataLoader</code>","text":"<p>               Bases: <code>DataLoader</code></p> <p>Dataloader that reuses workers for infinite iteration.</p> <p>This dataloader extends the PyTorch DataLoader to provide infinite recycling of workers, which improves efficiency for training loops that need to iterate through the dataset multiple times without recreating workers.</p> <p>Attributes:</p> Name Type Description <code>batch_sampler</code> <code>_RepeatSampler</code> <p>A sampler that repeats indefinitely.</p> <code>iterator</code> <code>Iterator</code> <p>The iterator from the parent DataLoader.</p> <code>len_dataloader</code> <code>Optional[int]</code> <p>Number of minibatches to be generated. If <code>None</code>, this is set to len(dataset)/batch_size.</p> <p>Methods:</p> Name Description <code>__len__</code> <p>Return the length of the batch sampler's sampler.</p> <code>__iter__</code> <p>Create a sampler that repeats indefinitely.</p> <code>__del__</code> <p>Ensure workers are properly terminated.</p> <code>reset</code> <p>Reset the iterator, useful when modifying dataset settings during training.</p> <p>Examples:</p> <p>Create an infinite dataloader for training</p> <pre><code>&gt;&gt;&gt; dataset = CenteredInstanceDataset(...)\n&gt;&gt;&gt; dataloader = InfiniteDataLoader(dataset, batch_size=16, shuffle=True)\n&gt;&gt;&gt; for batch in dataloader:  # Infinite iteration\n&gt;&gt;&gt;     train_step(batch)\n</code></pre> <p>Source: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/build.py</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class InfiniteDataLoader(DataLoader):\n    \"\"\"Dataloader that reuses workers for infinite iteration.\n\n    This dataloader extends the PyTorch DataLoader to provide infinite recycling of workers, which improves efficiency\n    for training loops that need to iterate through the dataset multiple times without recreating workers.\n\n    Attributes:\n        batch_sampler (_RepeatSampler): A sampler that repeats indefinitely.\n        iterator (Iterator): The iterator from the parent DataLoader.\n        len_dataloader (Optional[int]): Number of minibatches to be generated. If `None`, this is set to len(dataset)/batch_size.\n\n    Methods:\n        __len__: Return the length of the batch sampler's sampler.\n        __iter__: Create a sampler that repeats indefinitely.\n        __del__: Ensure workers are properly terminated.\n        reset: Reset the iterator, useful when modifying dataset settings during training.\n\n    Examples:\n        Create an infinite dataloader for training\n        &gt;&gt;&gt; dataset = CenteredInstanceDataset(...)\n        &gt;&gt;&gt; dataloader = InfiniteDataLoader(dataset, batch_size=16, shuffle=True)\n        &gt;&gt;&gt; for batch in dataloader:  # Infinite iteration\n        &gt;&gt;&gt;     train_step(batch)\n\n    Source: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/build.py\n    \"\"\"\n\n    def __init__(self, len_dataloader: Optional[int] = None, *args: Any, **kwargs: Any):\n        \"\"\"Initialize the InfiniteDataLoader with the same arguments as DataLoader.\"\"\"\n        super().__init__(*args, **kwargs)\n        object.__setattr__(self, \"batch_sampler\", _RepeatSampler(self.batch_sampler))\n        self.iterator = super().__iter__()\n        self.len_dataloader = len_dataloader\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the batch sampler's sampler.\"\"\"\n        # set the len to required number of steps per epoch as Lightning Trainer\n        # doesn't use the `__iter__` directly but instead uses the length to set\n        # the number of steps per epoch. If this is just set to len(sampler), then\n        # it only iterates through the samples in the dataset (and doesn't cycle through)\n        # if the required steps per epoch is more than batches in dataset.\n        return (\n            self.len_dataloader\n            if self.len_dataloader is not None\n            else len(self.batch_sampler.sampler)\n        )\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\n        while True:\n            yield next(self.iterator)\n\n    def __del__(self):\n        \"\"\"Ensure that workers are properly terminated when the dataloader is deleted.\"\"\"\n        try:\n            if not hasattr(self.iterator, \"_workers\"):\n                return\n            for w in self.iterator._workers:  # force terminate\n                if w.is_alive():\n                    w.terminate()\n            self.iterator._shutdown_workers()  # cleanup\n        except Exception:\n            pass\n\n    def reset(self):\n        \"\"\"Reset the iterator to allow modifications to the dataset during training.\"\"\"\n        self.iterator = self._get_iterator()\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__del__","title":"<code>__del__()</code>","text":"<p>Ensure that workers are properly terminated when the dataloader is deleted.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure that workers are properly terminated when the dataloader is deleted.\"\"\"\n    try:\n        if not hasattr(self.iterator, \"_workers\"):\n            return\n        for w in self.iterator._workers:  # force terminate\n            if w.is_alive():\n                w.terminate()\n        self.iterator._shutdown_workers()  # cleanup\n    except Exception:\n        pass\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__init__","title":"<code>__init__(len_dataloader=None, *args, **kwargs)</code>","text":"<p>Initialize the InfiniteDataLoader with the same arguments as DataLoader.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(self, len_dataloader: Optional[int] = None, *args: Any, **kwargs: Any):\n    \"\"\"Initialize the InfiniteDataLoader with the same arguments as DataLoader.\"\"\"\n    super().__init__(*args, **kwargs)\n    object.__setattr__(self, \"batch_sampler\", _RepeatSampler(self.batch_sampler))\n    self.iterator = super().__iter__()\n    self.len_dataloader = len_dataloader\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Create an iterator that yields indefinitely from the underlying iterator.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\n    while True:\n        yield next(self.iterator)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the batch sampler's sampler.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the batch sampler's sampler.\"\"\"\n    # set the len to required number of steps per epoch as Lightning Trainer\n    # doesn't use the `__iter__` directly but instead uses the length to set\n    # the number of steps per epoch. If this is just set to len(sampler), then\n    # it only iterates through the samples in the dataset (and doesn't cycle through)\n    # if the required steps per epoch is more than batches in dataset.\n    return (\n        self.len_dataloader\n        if self.len_dataloader is not None\n        else len(self.batch_sampler.sampler)\n    )\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.InfiniteDataLoader.reset","title":"<code>reset()</code>","text":"<p>Reset the iterator to allow modifications to the dataset during training.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the iterator to allow modifications to the dataset during training.\"\"\"\n    self.iterator = self._get_iterator()\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.ParallelCacheFiller","title":"<code>ParallelCacheFiller</code>","text":"<p>Parallel implementation of image caching using thread-local video copies.</p> <p>This class uses ThreadPoolExecutor to parallelize I/O-bound operations when caching images to disk or memory. Each worker thread gets its own copy of video objects to ensure thread safety.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>List of sio.Labels objects containing the data.</p> <code>lf_idx_list</code> <p>List of dictionaries with labeled frame indices.</p> <code>cache_type</code> <p>Either \"disk\" or \"memory\".</p> <code>cache_path</code> <p>Path to save cached images (for disk caching).</p> <code>num_workers</code> <p>Number of worker threads.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the parallel cache filler.</p> <code>fill_cache</code> <p>Fill the cache in parallel.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class ParallelCacheFiller:\n    \"\"\"Parallel implementation of image caching using thread-local video copies.\n\n    This class uses ThreadPoolExecutor to parallelize I/O-bound operations when\n    caching images to disk or memory. Each worker thread gets its own copy of\n    video objects to ensure thread safety.\n\n    Attributes:\n        labels: List of sio.Labels objects containing the data.\n        lf_idx_list: List of dictionaries with labeled frame indices.\n        cache_type: Either \"disk\" or \"memory\".\n        cache_path: Path to save cached images (for disk caching).\n        num_workers: Number of worker threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        lf_idx_list: List[Dict],\n        cache_type: str,\n        cache_path: Optional[Path] = None,\n        num_workers: int = 4,\n    ):\n        \"\"\"Initialize the parallel cache filler.\n\n        Args:\n            labels: List of sio.Labels objects.\n            lf_idx_list: List of sample dictionaries with frame indices.\n            cache_type: Either \"disk\" or \"memory\".\n            cache_path: Path for disk caching.\n            num_workers: Number of worker threads.\n        \"\"\"\n        self.labels = labels\n        self.lf_idx_list = lf_idx_list\n        self.cache_type = cache_type\n        self.cache_path = cache_path\n        self.num_workers = num_workers\n\n        self.cache: Dict = {}\n        self._cache_lock = threading.Lock()\n        self._local = threading.local()\n        self._video_info: Dict = {}\n\n        # Prepare video copies for thread-local access\n        self._prepare_video_copies()\n\n    def _prepare_video_copies(self):\n        \"\"\"Close original videos and prepare for thread-local copies.\"\"\"\n        for label in self.labels:\n            for video in label.videos:\n                vid_id = id(video)\n                if vid_id not in self._video_info:\n                    # Store original state\n                    original_open_backend = video.open_backend\n\n                    # Close the video backend\n                    video.close()\n                    video.open_backend = False\n\n                    self._video_info[vid_id] = {\n                        \"video\": video,\n                        \"original_open_backend\": original_open_backend,\n                    }\n\n    def _get_thread_local_video(self, video: sio.Video) -&gt; sio.Video:\n        \"\"\"Get or create a thread-local video copy.\n\n        Args:\n            video: The original video object.\n\n        Returns:\n            A thread-local copy of the video that is safe to use.\n        \"\"\"\n        vid_id = id(video)\n\n        if not hasattr(self._local, \"videos\"):\n            self._local.videos = {}\n\n        if vid_id not in self._local.videos:\n            # Create a thread-local copy\n            video_copy = deepcopy(video)\n            video_copy.open_backend = True\n            self._local.videos[vid_id] = video_copy\n\n        return self._local.videos[vid_id]\n\n    def _process_sample(\n        self, sample: Dict\n    ) -&gt; Tuple[int, int, Optional[np.ndarray], Optional[str]]:\n        \"\"\"Process a single sample (read image, optionally save/cache).\n\n        Args:\n            sample: Dictionary with labels_idx, lf_idx, etc.\n\n        Returns:\n            Tuple of (labels_idx, lf_idx, image_or_none, error_or_none).\n        \"\"\"\n        labels_idx = sample[\"labels_idx\"]\n        lf_idx = sample[\"lf_idx\"]\n\n        try:\n            # Get the labeled frame\n            lf = self.labels[labels_idx][lf_idx]\n\n            # Get thread-local video\n            video = self._get_thread_local_video(lf.video)\n\n            # Read the image\n            img = video[lf.frame_idx]\n\n            if img.shape[-1] == 1:\n                img = np.squeeze(img)\n\n            if self.cache_type == \"disk\":\n                f_name = self.cache_path / f\"sample_{labels_idx}_{lf_idx}.jpg\"\n                Image.fromarray(img).save(str(f_name), format=\"JPEG\")\n                return labels_idx, lf_idx, None, None\n            elif self.cache_type == \"memory\":\n                return labels_idx, lf_idx, img, None\n\n        except Exception as e:\n            return labels_idx, lf_idx, None, f\"{type(e).__name__}: {str(e)}\"\n\n    def fill_cache(\n        self, progress_callback=None\n    ) -&gt; Tuple[Dict, List[Tuple[int, int, str]]]:\n        \"\"\"Fill the cache in parallel.\n\n        Args:\n            progress_callback: Optional callback(completed_count) for progress updates.\n\n        Returns:\n            Tuple of (cache_dict, list_of_errors).\n        \"\"\"\n        errors = []\n        completed = 0\n\n        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n            futures = {\n                executor.submit(self._process_sample, sample): sample\n                for sample in self.lf_idx_list\n            }\n\n            for future in as_completed(futures):\n                labels_idx, lf_idx, img, error = future.result()\n\n                if error:\n                    errors.append((labels_idx, lf_idx, error))\n                elif self.cache_type == \"memory\" and img is not None:\n                    with self._cache_lock:\n                        self.cache[(labels_idx, lf_idx)] = img\n\n                completed += 1\n                if progress_callback:\n                    progress_callback(completed)\n\n        # Restore original video states\n        self._restore_videos()\n\n        return self.cache, errors\n\n    def _restore_videos(self):\n        \"\"\"Restore original video states after caching is complete.\"\"\"\n        for vid_info in self._video_info.values():\n            video = vid_info[\"video\"]\n            video.open_backend = vid_info[\"original_open_backend\"]\n            if video.open_backend:\n                try:\n                    video.open()\n                except Exception:\n                    pass\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.ParallelCacheFiller.__init__","title":"<code>__init__(labels, lf_idx_list, cache_type, cache_path=None, num_workers=4)</code>","text":"<p>Initialize the parallel cache filler.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>List[Labels]</code> <p>List of sio.Labels objects.</p> required <code>lf_idx_list</code> <code>List[Dict]</code> <p>List of sample dictionaries with frame indices.</p> required <code>cache_type</code> <code>str</code> <p>Either \"disk\" or \"memory\".</p> required <code>cache_path</code> <code>Optional[Path]</code> <p>Path for disk caching.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of worker threads.</p> <code>4</code> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    lf_idx_list: List[Dict],\n    cache_type: str,\n    cache_path: Optional[Path] = None,\n    num_workers: int = 4,\n):\n    \"\"\"Initialize the parallel cache filler.\n\n    Args:\n        labels: List of sio.Labels objects.\n        lf_idx_list: List of sample dictionaries with frame indices.\n        cache_type: Either \"disk\" or \"memory\".\n        cache_path: Path for disk caching.\n        num_workers: Number of worker threads.\n    \"\"\"\n    self.labels = labels\n    self.lf_idx_list = lf_idx_list\n    self.cache_type = cache_type\n    self.cache_path = cache_path\n    self.num_workers = num_workers\n\n    self.cache: Dict = {}\n    self._cache_lock = threading.Lock()\n    self._local = threading.local()\n    self._video_info: Dict = {}\n\n    # Prepare video copies for thread-local access\n    self._prepare_video_copies()\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.ParallelCacheFiller.fill_cache","title":"<code>fill_cache(progress_callback=None)</code>","text":"<p>Fill the cache in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>progress_callback</code> <p>Optional callback(completed_count) for progress updates.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Dict, List[Tuple[int, int, str]]]</code> <p>Tuple of (cache_dict, list_of_errors).</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def fill_cache(\n    self, progress_callback=None\n) -&gt; Tuple[Dict, List[Tuple[int, int, str]]]:\n    \"\"\"Fill the cache in parallel.\n\n    Args:\n        progress_callback: Optional callback(completed_count) for progress updates.\n\n    Returns:\n        Tuple of (cache_dict, list_of_errors).\n    \"\"\"\n    errors = []\n    completed = 0\n\n    with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n        futures = {\n            executor.submit(self._process_sample, sample): sample\n            for sample in self.lf_idx_list\n        }\n\n        for future in as_completed(futures):\n            labels_idx, lf_idx, img, error = future.result()\n\n            if error:\n                errors.append((labels_idx, lf_idx, error))\n            elif self.cache_type == \"memory\" and img is not None:\n                with self._cache_lock:\n                    self.cache[(labels_idx, lf_idx)] = img\n\n            completed += 1\n            if progress_callback:\n                progress_callback(completed)\n\n    # Restore original video states\n    self._restore_videos()\n\n    return self.cache, errors\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.SingleInstanceDataset","title":"<code>SingleInstanceDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Dataset class for single-instance models.</p> <p>Attributes:</p> Name Type Description <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section.</p> <code>(required</code> <code>keys</code> <p><code>sigma</code>, <code>output_stride</code> and <code>part_names</code> depending on the model type ).</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>labels_list</code> <p>List of <code>sio.Labels</code> objects. Used to store the labels in the cache. (only used if <code>cache_img</code> is <code>None</code>)</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with image and confmaps for instance for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class SingleInstanceDataset(BaseDataset):\n    \"\"\"Dataset class for single-instance models.\n\n    Attributes:\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n        (required keys: `sigma`, `output_stride` and `part_names` depending on the model type ).\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        labels_list: List of `sio.Labels` objects. Used to store the labels in the cache. (only used if `cache_img` is `None`)\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        confmap_head_config: DictConfig,\n        max_stride: int,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n        parallel_caching: bool = True,\n        cache_workers: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            max_stride=max_stride,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        self.confmap_head_config = confmap_head_config\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with image and confmaps for instance for given index.\"\"\"\n        sample = self.lf_idx_list[index]\n        labels_idx = sample[\"labels_idx\"]\n        lf_idx = sample[\"lf_idx\"]\n        video_idx = sample[\"video_idx\"]\n        lf_frame_idx = sample[\"frame_idx\"]\n\n        if self.cache_img is not None:\n            instances = sample[\"instances\"]\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n        else:\n            lf = self.labels_list[labels_idx][lf_idx]\n            instances = lf.instances\n            img = lf.image\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        # get dict\n        sample = process_lf(\n            instances_list=instances,\n            img=img,\n            frame_idx=lf_frame_idx,\n            video_idx=video_idx,\n            max_instances=self.max_instances,\n            user_instances_only=self.user_instances_only,\n        )\n\n        if self.ensure_rgb:\n            sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n        elif self.ensure_grayscale:\n            sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n        # size matcher\n        sample[\"image\"], eff_scale = apply_sizematcher(\n            sample[\"image\"],\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        sample[\"instances\"] = sample[\"instances\"] * eff_scale\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # resize image\n        sample[\"image\"], sample[\"instances\"] = apply_resizer(\n            sample[\"image\"],\n            sample[\"instances\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"image\"] = apply_pad_to_stride(\n            sample[\"image\"], max_stride=self.max_stride\n        )\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                    sample[\"image\"],\n                    sample[\"instances\"],\n                    **self.geometric_aug,\n                )\n\n        img_hw = sample[\"image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_confmaps(\n            sample[\"instances\"],\n            img_hw=img_hw,\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n        )\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.SingleInstanceDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with image and confmaps for instance for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with image and confmaps for instance for given index.\"\"\"\n    sample = self.lf_idx_list[index]\n    labels_idx = sample[\"labels_idx\"]\n    lf_idx = sample[\"lf_idx\"]\n    video_idx = sample[\"video_idx\"]\n    lf_frame_idx = sample[\"frame_idx\"]\n\n    if self.cache_img is not None:\n        instances = sample[\"instances\"]\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n    else:\n        lf = self.labels_list[labels_idx][lf_idx]\n        instances = lf.instances\n        img = lf.image\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    # get dict\n    sample = process_lf(\n        instances_list=instances,\n        img=img,\n        frame_idx=lf_frame_idx,\n        video_idx=video_idx,\n        max_instances=self.max_instances,\n        user_instances_only=self.user_instances_only,\n    )\n\n    if self.ensure_rgb:\n        sample[\"image\"] = convert_to_rgb(sample[\"image\"])\n    elif self.ensure_grayscale:\n        sample[\"image\"] = convert_to_grayscale(sample[\"image\"])\n\n    # size matcher\n    sample[\"image\"], eff_scale = apply_sizematcher(\n        sample[\"image\"],\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    sample[\"instances\"] = sample[\"instances\"] * eff_scale\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # resize image\n    sample[\"image\"], sample[\"instances\"] = apply_resizer(\n        sample[\"image\"],\n        sample[\"instances\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"image\"] = apply_pad_to_stride(\n        sample[\"image\"], max_stride=self.max_stride\n    )\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_intensity_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            sample[\"image\"], sample[\"instances\"] = apply_geometric_augmentation(\n                sample[\"image\"],\n                sample[\"instances\"],\n                **self.geometric_aug,\n            )\n\n    img_hw = sample[\"image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_confmaps(\n        sample[\"instances\"],\n        img_hw=img_hw,\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n    )\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.SingleInstanceDataset.__init__","title":"<code>__init__(labels, confmap_head_config, max_stride, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None, parallel_caching=True, cache_workers=0)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    confmap_head_config: DictConfig,\n    max_stride: int,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n    parallel_caching: bool = True,\n    cache_workers: int = 0,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        max_stride=max_stride,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n        parallel_caching=parallel_caching,\n        cache_workers=cache_workers,\n    )\n    self.confmap_head_config = confmap_head_config\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.TopDownCenteredInstanceMultiClassDataset","title":"<code>TopDownCenteredInstanceMultiClassDataset</code>","text":"<p>               Bases: <code>CenteredInstanceDataset</code></p> <p>Dataset class for instance-centered confidence map ID models.</p> <p>Attributes:</p> Name Type Description <code>max_stride</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> <code>anchor_ind</code> <p>Index of the node to use as the anchor point, based on its index in the ordered list of skeleton nodes.</p> <code>user_instances_only</code> <p><code>True</code> if only user labeled instances should be used for training. If <code>False</code>, both user labeled and predicted instances would be used.</p> <code>ensure_rgb</code> <p>(bool) True if the input image should have 3 channels (RGB image). If input has only one</p> <code>is</code> <code>replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default</code> <p><code>False</code>.</p> <code>ensure_grayscale</code> <p>(bool) True if the input image should only have a single channel. If input has three channels (RGB) and this</p> <code>image.</code> <code>If the source image has only one channel and this is set to False, then we retain the single channel input. Default</code> <p><code>False</code>.</p> <code>intensity_aug</code> <p>Intensity augmentation configuration. Can be: - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness'] - List of strings: Multiple intensity augmentations from the allowed values - Dictionary: Custom intensity configuration - None: No intensity augmentation applied</p> <code>geometric_aug</code> <p>Geometric augmentation configuration. Can be: - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup'] - List of strings: Multiple geometric augmentations from the allowed values - Dictionary: Custom geometric configuration - None: No geometric augmentation applied</p> <code>scale</code> <p>Factor to resize the image dimensions by, specified as a float. Default: 1.0.</p> <code>apply_aug</code> <p><code>True</code> if augmentations should be applied to the data pipeline, else <code>False</code>. Default: <code>False</code>.</p> <code>max_hw</code> <p>Maximum height and width of images across the labels file. If <code>max_height</code> and <code>max_width</code> in the config is None, then <code>max_hw</code> is used (computed with <code>sleap_nn.data.providers.get_max_height_width</code>). Else the values in the config are used.</p> <code>cache_img</code> <p>String to indicate which caching to use: <code>memory</code> or <code>disk</code>. If <code>None</code>, the images aren't cached and loaded from the <code>.slp</code> file on each access.</p> <code>cache_img_path</code> <p>Path to save the <code>.jpg</code> files. If <code>None</code>, current working dir is used.</p> <code>use_existing_imgs</code> <p>Use existing imgs/ chunks in the <code>cache_img_path</code>.</p> <code>crop_size</code> <p>Crop size of each instance for centered-instance model. If <code>scale</code> is provided, then the cropped image will be resized according to <code>scale</code>.</p> <code>rank</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>confmap_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section. (required keys: <code>sigma</code>, <code>output_stride</code>, <code>part_names</code> and <code>anchor_part</code> depending on the model type ).</p> <code>class_vectors_head_config</code> <p>DictConfig object with all the keys in the <code>head_config</code> section. (required keys: <code>classes</code>, <code>num_fc_layers</code>, <code>num_fc_units</code>, <code>output_stride</code>, <code>loss_weight</code>).</p> <code>labels_list</code> <p>List of <code>sio.Labels</code> objects. Used to store the labels in the cache. (only used if <code>cache_img</code> is <code>None</code>)</p> <p>Methods:</p> Name Description <code>__getitem__</code> <p>Return dict with cropped image and confmaps of instance for given index.</p> <code>__init__</code> <p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>class TopDownCenteredInstanceMultiClassDataset(CenteredInstanceDataset):\n    \"\"\"Dataset class for instance-centered confidence map ID models.\n\n    Attributes:\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n        anchor_ind: Index of the node to use as the anchor point, based on its index in the\n            ordered list of skeleton nodes.\n        user_instances_only: `True` if only user labeled instances should be used for training. If `False`,\n            both user labeled and predicted instances would be used.\n        ensure_rgb: (bool) True if the input image should have 3 channels (RGB image). If input has only one\n        channel when this is set to `True`, then the images from single-channel\n        is replicated along the channel axis. If the image has three channels and this is set to False, then we retain the three channels. Default: `False`.\n        ensure_grayscale: (bool) True if the input image should only have a single channel. If input has three channels (RGB) and this\n        is set to True, then we convert the image to grayscale (single-channel)\n        image. If the source image has only one channel and this is set to False, then we retain the single channel input. Default: `False`.\n        intensity_aug: Intensity augmentation configuration. Can be:\n            - String: One of ['uniform_noise', 'gaussian_noise', 'contrast', 'brightness']\n            - List of strings: Multiple intensity augmentations from the allowed values\n            - Dictionary: Custom intensity configuration\n            - None: No intensity augmentation applied\n        geometric_aug: Geometric augmentation configuration. Can be:\n            - String: One of ['rotation', 'scale', 'translate', 'erase_scale', 'mixup']\n            - List of strings: Multiple geometric augmentations from the allowed values\n            - Dictionary: Custom geometric configuration\n            - None: No geometric augmentation applied\n        scale: Factor to resize the image dimensions by, specified as a float. Default: 1.0.\n        apply_aug: `True` if augmentations should be applied to the data pipeline,\n            else `False`. Default: `False`.\n        max_hw: Maximum height and width of images across the labels file. If `max_height` and\n           `max_width` in the config is None, then `max_hw` is used (computed with\n            `sleap_nn.data.providers.get_max_height_width`). Else the values in the config\n            are used.\n        cache_img: String to indicate which caching to use: `memory` or `disk`. If `None`,\n            the images aren't cached and loaded from the `.slp` file on each access.\n        cache_img_path: Path to save the `.jpg` files. If `None`, current working dir is used.\n        use_existing_imgs: Use existing imgs/ chunks in the `cache_img_path`.\n        crop_size: Crop size of each instance for centered-instance model. If `scale` is provided, then the cropped image will be resized according to `scale`.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        confmap_head_config: DictConfig object with all the keys in the `head_config` section.\n            (required keys: `sigma`, `output_stride`, `part_names` and `anchor_part` depending on the model type ).\n        class_vectors_head_config: DictConfig object with all the keys in the `head_config` section.\n            (required keys: `classes`, `num_fc_layers`, `num_fc_units`, `output_stride`, `loss_weight`).\n        labels_list: List of `sio.Labels` objects. Used to store the labels in the cache. (only used if `cache_img` is `None`)\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: List[sio.Labels],\n        crop_size: int,\n        confmap_head_config: DictConfig,\n        class_vectors_head_config: DictConfig,\n        max_stride: int,\n        anchor_ind: Optional[int] = None,\n        user_instances_only: bool = True,\n        ensure_rgb: bool = False,\n        ensure_grayscale: bool = False,\n        intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n        scale: float = 1.0,\n        apply_aug: bool = False,\n        max_hw: Tuple[Optional[int]] = (None, None),\n        cache_img: Optional[str] = None,\n        cache_img_path: Optional[str] = None,\n        use_existing_imgs: bool = False,\n        rank: Optional[int] = None,\n        parallel_caching: bool = True,\n        cache_workers: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize class attributes.\"\"\"\n        super().__init__(\n            labels=labels,\n            crop_size=crop_size,\n            confmap_head_config=confmap_head_config,\n            max_stride=max_stride,\n            anchor_ind=anchor_ind,\n            user_instances_only=user_instances_only,\n            ensure_rgb=ensure_rgb,\n            ensure_grayscale=ensure_grayscale,\n            intensity_aug=intensity_aug,\n            geometric_aug=geometric_aug,\n            scale=scale,\n            apply_aug=apply_aug,\n            max_hw=max_hw,\n            cache_img=cache_img,\n            cache_img_path=cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        self.class_vectors_head_config = class_vectors_head_config\n        self.class_names = self.class_vectors_head_config.classes\n\n    def __getitem__(self, index) -&gt; Dict:\n        \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n        sample = self.instance_idx_list[index]\n        labels_idx = sample[\"labels_idx\"]\n        lf_idx = sample[\"lf_idx\"]\n        inst_idx = sample[\"inst_idx\"]\n        video_idx = sample[\"video_idx\"]\n        lf_frame_idx = sample[\"frame_idx\"]\n\n        if self.cache_img is not None:\n            instances_list = sample[\"instances\"]\n            if self.cache_img == \"disk\":\n                img = np.array(\n                    Image.open(\n                        f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                    )\n                )\n            elif self.cache_img == \"memory\":\n                img = self.cache[(labels_idx, lf_idx)].copy()\n        else:\n            lf = self.labels_list[labels_idx][lf_idx]\n            instances_list = lf.instances\n            img = lf.image\n\n        if img.ndim == 2:\n            img = np.expand_dims(img, axis=2)\n\n        image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n        instances = []\n        for inst in instances_list:\n            instances.append(\n                inst.numpy()\n            )  # no need to filter empty instance (handled while creating instance_idx)\n        instances = np.stack(instances, axis=0)\n\n        # Add singleton time dimension for single frames.\n        image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n        instances = np.expand_dims(\n            instances, axis=0\n        )  # (n_samples=1, num_instances, num_nodes, 2)\n\n        instances = torch.from_numpy(instances.astype(\"float32\"))\n        image = torch.from_numpy(image.copy())\n\n        num_instances, _ = instances.shape[1:3]\n        orig_img_height, orig_img_width = image.shape[-2:]\n\n        instances = instances[:, inst_idx]\n\n        if self.ensure_rgb:\n            image = convert_to_rgb(image)\n        elif self.ensure_grayscale:\n            image = convert_to_grayscale(image)\n\n        # size matcher\n        image, eff_scale = apply_sizematcher(\n            image,\n            max_height=self.max_hw[0],\n            max_width=self.max_hw[1],\n        )\n        instances = instances * eff_scale\n\n        # get class vectors\n        track_ids = torch.Tensor(\n            [\n                (\n                    self.class_names.index(instances_list[idx].track.name)\n                    if instances_list[idx].track is not None\n                    else -1\n                )\n                for idx in range(num_instances)\n            ]\n        ).to(torch.int32)\n        class_vectors = make_class_vectors(\n            class_inds=track_ids,\n            n_classes=torch.tensor(len(self.class_names), dtype=torch.int32),\n        )\n\n        # get the centroids based on the anchor idx\n        centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n        instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n        crop_size = np.array([self.crop_size, self.crop_size]) * np.sqrt(\n            2\n        )  # crop extra for rotation augmentation\n        crop_size = crop_size.astype(np.int32).tolist()\n\n        sample = generate_crops(image, instance, centroid, crop_size)\n\n        sample[\"frame_idx\"] = torch.tensor(lf_frame_idx, dtype=torch.int32)\n        sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n        sample[\"num_instances\"] = num_instances\n        sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width]).unsqueeze(\n            0\n        )\n        sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n        # apply augmentation\n        if self.apply_aug:\n            if self.intensity_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_intensity_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.intensity_aug,\n                )\n\n            if self.geometric_aug is not None:\n                (\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                ) = apply_geometric_augmentation(\n                    sample[\"instance_image\"],\n                    sample[\"instance\"],\n                    **self.geometric_aug,\n                )\n\n        # re-crop to original crop size\n        sample[\"instance_bbox\"] = torch.unsqueeze(\n            make_centered_bboxes(sample[\"centroid\"][0], self.crop_size, self.crop_size),\n            0,\n        )  # (n_samples=1, 4, 2)\n\n        sample[\"instance_image\"] = crop_and_resize(\n            sample[\"instance_image\"],\n            boxes=sample[\"instance_bbox\"],\n            size=(self.crop_size, self.crop_size),\n        )\n        point = sample[\"instance_bbox\"][0][0]\n        center_instance = sample[\"instance\"] - point\n        centered_centroid = sample[\"centroid\"] - point\n\n        sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n        sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n        # resize image\n        sample[\"instance_image\"], sample[\"instance\"] = apply_resizer(\n            sample[\"instance_image\"],\n            sample[\"instance\"],\n            scale=self.scale,\n        )\n\n        # Pad the image (if needed) according max stride\n        sample[\"instance_image\"] = apply_pad_to_stride(\n            sample[\"instance_image\"], max_stride=self.max_stride\n        )\n\n        img_hw = sample[\"instance_image\"].shape[-2:]\n\n        # Generate confidence maps\n        confidence_maps = generate_confmaps(\n            sample[\"instance\"],\n            img_hw=img_hw,\n            sigma=self.confmap_head_config.sigma,\n            output_stride=self.confmap_head_config.output_stride,\n        )\n\n        sample[\"class_vectors\"] = class_vectors[inst_idx].to(torch.float32)\n\n        sample[\"confidence_maps\"] = confidence_maps\n        sample[\"labels_idx\"] = labels_idx\n\n        return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.TopDownCenteredInstanceMultiClassDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return dict with cropped image and confmaps of instance for given index.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __getitem__(self, index) -&gt; Dict:\n    \"\"\"Return dict with cropped image and confmaps of instance for given index.\"\"\"\n    sample = self.instance_idx_list[index]\n    labels_idx = sample[\"labels_idx\"]\n    lf_idx = sample[\"lf_idx\"]\n    inst_idx = sample[\"inst_idx\"]\n    video_idx = sample[\"video_idx\"]\n    lf_frame_idx = sample[\"frame_idx\"]\n\n    if self.cache_img is not None:\n        instances_list = sample[\"instances\"]\n        if self.cache_img == \"disk\":\n            img = np.array(\n                Image.open(\n                    f\"{self.cache_img_path}/sample_{labels_idx}_{lf_idx}.jpg\"\n                )\n            )\n        elif self.cache_img == \"memory\":\n            img = self.cache[(labels_idx, lf_idx)].copy()\n    else:\n        lf = self.labels_list[labels_idx][lf_idx]\n        instances_list = lf.instances\n        img = lf.image\n\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n\n    image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n    instances = []\n    for inst in instances_list:\n        instances.append(\n            inst.numpy()\n        )  # no need to filter empty instance (handled while creating instance_idx)\n    instances = np.stack(instances, axis=0)\n\n    # Add singleton time dimension for single frames.\n    image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n    instances = np.expand_dims(\n        instances, axis=0\n    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n    instances = torch.from_numpy(instances.astype(\"float32\"))\n    image = torch.from_numpy(image.copy())\n\n    num_instances, _ = instances.shape[1:3]\n    orig_img_height, orig_img_width = image.shape[-2:]\n\n    instances = instances[:, inst_idx]\n\n    if self.ensure_rgb:\n        image = convert_to_rgb(image)\n    elif self.ensure_grayscale:\n        image = convert_to_grayscale(image)\n\n    # size matcher\n    image, eff_scale = apply_sizematcher(\n        image,\n        max_height=self.max_hw[0],\n        max_width=self.max_hw[1],\n    )\n    instances = instances * eff_scale\n\n    # get class vectors\n    track_ids = torch.Tensor(\n        [\n            (\n                self.class_names.index(instances_list[idx].track.name)\n                if instances_list[idx].track is not None\n                else -1\n            )\n            for idx in range(num_instances)\n        ]\n    ).to(torch.int32)\n    class_vectors = make_class_vectors(\n        class_inds=track_ids,\n        n_classes=torch.tensor(len(self.class_names), dtype=torch.int32),\n    )\n\n    # get the centroids based on the anchor idx\n    centroids = generate_centroids(instances, anchor_ind=self.anchor_ind)\n\n    instance, centroid = instances[0], centroids[0]  # (n_samples=1)\n\n    crop_size = np.array([self.crop_size, self.crop_size]) * np.sqrt(\n        2\n    )  # crop extra for rotation augmentation\n    crop_size = crop_size.astype(np.int32).tolist()\n\n    sample = generate_crops(image, instance, centroid, crop_size)\n\n    sample[\"frame_idx\"] = torch.tensor(lf_frame_idx, dtype=torch.int32)\n    sample[\"video_idx\"] = torch.tensor(video_idx, dtype=torch.int32)\n    sample[\"num_instances\"] = num_instances\n    sample[\"orig_size\"] = torch.Tensor([orig_img_height, orig_img_width]).unsqueeze(\n        0\n    )\n    sample[\"eff_scale\"] = torch.tensor(eff_scale, dtype=torch.float32)\n\n    # apply augmentation\n    if self.apply_aug:\n        if self.intensity_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_intensity_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.intensity_aug,\n            )\n\n        if self.geometric_aug is not None:\n            (\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n            ) = apply_geometric_augmentation(\n                sample[\"instance_image\"],\n                sample[\"instance\"],\n                **self.geometric_aug,\n            )\n\n    # re-crop to original crop size\n    sample[\"instance_bbox\"] = torch.unsqueeze(\n        make_centered_bboxes(sample[\"centroid\"][0], self.crop_size, self.crop_size),\n        0,\n    )  # (n_samples=1, 4, 2)\n\n    sample[\"instance_image\"] = crop_and_resize(\n        sample[\"instance_image\"],\n        boxes=sample[\"instance_bbox\"],\n        size=(self.crop_size, self.crop_size),\n    )\n    point = sample[\"instance_bbox\"][0][0]\n    center_instance = sample[\"instance\"] - point\n    centered_centroid = sample[\"centroid\"] - point\n\n    sample[\"instance\"] = center_instance  # (n_samples=1, n_nodes, 2)\n    sample[\"centroid\"] = centered_centroid  # (n_samples=1, 2)\n\n    # resize image\n    sample[\"instance_image\"], sample[\"instance\"] = apply_resizer(\n        sample[\"instance_image\"],\n        sample[\"instance\"],\n        scale=self.scale,\n    )\n\n    # Pad the image (if needed) according max stride\n    sample[\"instance_image\"] = apply_pad_to_stride(\n        sample[\"instance_image\"], max_stride=self.max_stride\n    )\n\n    img_hw = sample[\"instance_image\"].shape[-2:]\n\n    # Generate confidence maps\n    confidence_maps = generate_confmaps(\n        sample[\"instance\"],\n        img_hw=img_hw,\n        sigma=self.confmap_head_config.sigma,\n        output_stride=self.confmap_head_config.output_stride,\n    )\n\n    sample[\"class_vectors\"] = class_vectors[inst_idx].to(torch.float32)\n\n    sample[\"confidence_maps\"] = confidence_maps\n    sample[\"labels_idx\"] = labels_idx\n\n    return sample\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.TopDownCenteredInstanceMultiClassDataset.__init__","title":"<code>__init__(labels, crop_size, confmap_head_config, class_vectors_head_config, max_stride, anchor_ind=None, user_instances_only=True, ensure_rgb=False, ensure_grayscale=False, intensity_aug=None, geometric_aug=None, scale=1.0, apply_aug=False, max_hw=(None, None), cache_img=None, cache_img_path=None, use_existing_imgs=False, rank=None, parallel_caching=True, cache_workers=0)</code>","text":"<p>Initialize class attributes.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def __init__(\n    self,\n    labels: List[sio.Labels],\n    crop_size: int,\n    confmap_head_config: DictConfig,\n    class_vectors_head_config: DictConfig,\n    max_stride: int,\n    anchor_ind: Optional[int] = None,\n    user_instances_only: bool = True,\n    ensure_rgb: bool = False,\n    ensure_grayscale: bool = False,\n    intensity_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    geometric_aug: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n    scale: float = 1.0,\n    apply_aug: bool = False,\n    max_hw: Tuple[Optional[int]] = (None, None),\n    cache_img: Optional[str] = None,\n    cache_img_path: Optional[str] = None,\n    use_existing_imgs: bool = False,\n    rank: Optional[int] = None,\n    parallel_caching: bool = True,\n    cache_workers: int = 0,\n) -&gt; None:\n    \"\"\"Initialize class attributes.\"\"\"\n    super().__init__(\n        labels=labels,\n        crop_size=crop_size,\n        confmap_head_config=confmap_head_config,\n        max_stride=max_stride,\n        anchor_ind=anchor_ind,\n        user_instances_only=user_instances_only,\n        ensure_rgb=ensure_rgb,\n        ensure_grayscale=ensure_grayscale,\n        intensity_aug=intensity_aug,\n        geometric_aug=geometric_aug,\n        scale=scale,\n        apply_aug=apply_aug,\n        max_hw=max_hw,\n        cache_img=cache_img,\n        cache_img_path=cache_img_path,\n        use_existing_imgs=use_existing_imgs,\n        rank=rank,\n        parallel_caching=parallel_caching,\n        cache_workers=cache_workers,\n    )\n    self.class_vectors_head_config = class_vectors_head_config\n    self.class_names = self.class_vectors_head_config.classes\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.get_steps_per_epoch","title":"<code>get_steps_per_epoch(dataset, batch_size)</code>","text":"<p>Compute the number of steps (iterations) per epoch for the given dataset.</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def get_steps_per_epoch(dataset: BaseDataset, batch_size: int):\n    \"\"\"Compute the number of steps (iterations) per epoch for the given dataset.\"\"\"\n    return (len(dataset) // batch_size) + (1 if (len(dataset) % batch_size) else 0)\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.get_train_val_dataloaders","title":"<code>get_train_val_dataloaders(train_dataset, val_dataset, config, train_steps_per_epoch=None, val_steps_per_epoch=None, rank=None, trainer_devices=1)</code>","text":"<p>Return the train and val dataloaders.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>BaseDataset</code> <p>Train dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].</p> required <code>val_dataset</code> <code>BaseDataset</code> <p>Val dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].</p> required <code>config</code> <code>DictConfig</code> <p>Sleap-nn config.</p> required <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to train for in an epoch. If set to <code>None</code>, this is set to the number of batches in the training data. Note: In a multi-gpu training setup, the effective steps during training would be the <code>trainer_steps_per_epoch</code> / <code>trainer_devices</code>.</p> <code>None</code> <code>val_steps_per_epoch</code> <code>Optional[int]</code> <p>Number of minibatches (steps) to run validation for in an epoch. If set to <code>None</code>, this is set to the number of batches in the val data.</p> <code>None</code> <code>rank</code> <code>Optional[int]</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>None</code> <code>trainer_devices</code> <code>int</code> <p>Number of devices to use for training.</p> <code>1</code> <p>Returns:</p> Type Description <p>A tuple (train_dataloader, val_dataloader).</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def get_train_val_dataloaders(\n    train_dataset: BaseDataset,\n    val_dataset: BaseDataset,\n    config: DictConfig,\n    train_steps_per_epoch: Optional[int] = None,\n    val_steps_per_epoch: Optional[int] = None,\n    rank: Optional[int] = None,\n    trainer_devices: int = 1,\n):\n    \"\"\"Return the train and val dataloaders.\n\n    Args:\n        train_dataset: Train dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].\n        val_dataset: Val dataset-instance of one of the dataset classes [SingleInstanceDataset, CentroidDataset, CenteredInstanceDataset, BottomUpDataset, BottomUpMultiClassDataset, TopDownCenteredInstanceMultiClassDataset].\n        config: Sleap-nn config.\n        train_steps_per_epoch: Number of minibatches (steps) to train for in an epoch. If set to `None`, this is set to the number of batches in the training data. **Note**: In a multi-gpu training setup, the effective steps during training would be the `trainer_steps_per_epoch` / `trainer_devices`.\n        val_steps_per_epoch: Number of minibatches (steps) to run validation for in an epoch. If set to `None`, this is set to the number of batches in the val data.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n        trainer_devices: Number of devices to use for training.\n\n    Returns:\n        A tuple (train_dataloader, val_dataloader).\n    \"\"\"\n    pin_memory = (\n        config.trainer_config.train_data_loader.pin_memory\n        if \"pin_memory\" in config.trainer_config.train_data_loader\n        and config.trainer_config.train_data_loader.pin_memory is not None\n        else True\n    )\n\n    if train_steps_per_epoch is None:\n        train_steps_per_epoch = config.trainer_config.train_steps_per_epoch\n        if train_steps_per_epoch is None:\n            train_steps_per_epoch = get_steps_per_epoch(\n                dataset=train_dataset,\n                batch_size=config.trainer_config.train_data_loader.batch_size,\n            )\n\n    if val_steps_per_epoch is None:\n        val_steps_per_epoch = get_steps_per_epoch(\n            dataset=val_dataset,\n            batch_size=config.trainer_config.val_data_loader.batch_size,\n        )\n\n    train_sampler = (\n        DistributedSampler(\n            dataset=train_dataset,\n            shuffle=config.trainer_config.train_data_loader.shuffle,\n            rank=rank if rank is not None else 0,\n            num_replicas=trainer_devices,\n        )\n        if trainer_devices &gt; 1\n        else None\n    )\n\n    train_data_loader = InfiniteDataLoader(\n        dataset=train_dataset,\n        sampler=train_sampler,\n        len_dataloader=max(1, round(train_steps_per_epoch / trainer_devices)),\n        shuffle=(\n            config.trainer_config.train_data_loader.shuffle\n            if train_sampler is None\n            else None\n        ),\n        batch_size=config.trainer_config.train_data_loader.batch_size,\n        num_workers=config.trainer_config.train_data_loader.num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(\n            True if config.trainer_config.train_data_loader.num_workers &gt; 0 else None\n        ),\n        prefetch_factor=(\n            config.trainer_config.train_data_loader.batch_size\n            if config.trainer_config.train_data_loader.num_workers &gt; 0\n            else None\n        ),\n    )\n\n    val_sampler = (\n        DistributedSampler(\n            dataset=val_dataset,\n            shuffle=False,\n            rank=rank if rank is not None else 0,\n            num_replicas=trainer_devices,\n        )\n        if trainer_devices &gt; 1\n        else None\n    )\n    val_data_loader = InfiniteDataLoader(\n        dataset=val_dataset,\n        shuffle=False if val_sampler is None else None,\n        sampler=val_sampler,\n        len_dataloader=(\n            max(1, round(val_steps_per_epoch / trainer_devices))\n            if trainer_devices &gt; 1\n            else None\n        ),\n        batch_size=config.trainer_config.val_data_loader.batch_size,\n        num_workers=config.trainer_config.val_data_loader.num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(\n            True if config.trainer_config.val_data_loader.num_workers &gt; 0 else None\n        ),\n        prefetch_factor=(\n            config.trainer_config.val_data_loader.batch_size\n            if config.trainer_config.val_data_loader.num_workers &gt; 0\n            else None\n        ),\n    )\n\n    return train_data_loader, val_data_loader\n</code></pre>"},{"location":"api/data/custom_datasets/#sleap_nn.data.custom_datasets.get_train_val_datasets","title":"<code>get_train_val_datasets(train_labels, val_labels, config, rank=None)</code>","text":"<p>Return the train and val datasets.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels</code> <code>List[Labels]</code> <p>List of train labels.</p> required <code>val_labels</code> <code>List[Labels]</code> <p>List of val labels.</p> required <code>config</code> <code>DictConfig</code> <p>Sleap-nn config.</p> required <code>rank</code> <code>Optional[int]</code> <p>Indicates the rank of the process. Used during distributed training to ensure that image storage to disk occurs only once across all workers.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple (train_dataset, val_dataset).</p> Source code in <code>sleap_nn/data/custom_datasets.py</code> <pre><code>def get_train_val_datasets(\n    train_labels: List[sio.Labels],\n    val_labels: List[sio.Labels],\n    config: DictConfig,\n    rank: Optional[int] = None,\n):\n    \"\"\"Return the train and val datasets.\n\n    Args:\n        train_labels: List of train labels.\n        val_labels: List of val labels.\n        config: Sleap-nn config.\n        rank: Indicates the rank of the process. Used during distributed training to ensure that image storage to\n            disk occurs only once across all workers.\n\n    Returns:\n        A tuple (train_dataset, val_dataset).\n    \"\"\"\n    cache_imgs = (\n        config.data_config.data_pipeline_fw.split(\"_\")[-1]\n        if \"cache_img\" in config.data_config.data_pipeline_fw\n        else None\n    )\n    base_cache_img_path = config.data_config.cache_img_path\n    train_cache_img_path, val_cache_img_path = None, None\n\n    if cache_imgs == \"disk\":\n        train_cache_img_path = Path(base_cache_img_path) / \"train_imgs\"\n        val_cache_img_path = Path(base_cache_img_path) / \"val_imgs\"\n    use_existing_imgs = config.data_config.use_existing_imgs\n\n    # Parallel caching configuration\n    parallel_caching = getattr(config.data_config, \"parallel_caching\", True)\n    cache_workers = getattr(config.data_config, \"cache_workers\", 0)\n\n    model_type = get_model_type_from_cfg(config=config)\n    backbone_type = get_backbone_type_from_cfg(config=config)\n\n    if cache_imgs == \"disk\" and use_existing_imgs:\n        if not (\n            train_cache_img_path.exists()\n            and train_cache_img_path.is_dir()\n            and any(train_cache_img_path.glob(\"*.jpg\"))\n        ):\n            message = f\"There are no images in the path: {train_cache_img_path}\"\n            logger.error(message)\n            raise Exception(message)\n\n        if not (\n            val_cache_img_path.exists()\n            and val_cache_img_path.is_dir()\n            and any(val_cache_img_path.glob(\"*.jpg\"))\n        ):\n            message = f\"There are no images in the path: {val_cache_img_path}\"\n            logger.error(message)\n            raise Exception(message)\n\n    if model_type == \"bottomup\":\n        train_dataset = BottomUpDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.bottomup.confmaps,\n            pafs_head_config=config.model_config.head_configs.bottomup.pafs,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        val_dataset = BottomUpDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.bottomup.confmaps,\n            pafs_head_config=config.model_config.head_configs.bottomup.pafs,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n\n    elif model_type == \"multi_class_bottomup\":\n        train_dataset = BottomUpMultiClassDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_bottomup.confmaps,\n            class_maps_head_config=config.model_config.head_configs.multi_class_bottomup.class_maps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        val_dataset = BottomUpMultiClassDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_bottomup.confmaps,\n            class_maps_head_config=config.model_config.head_configs.multi_class_bottomup.class_maps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n\n    elif model_type == \"centered_instance\":\n        nodes = config.model_config.head_configs.centered_instance.confmaps.part_names\n        anchor_part = (\n            config.model_config.head_configs.centered_instance.confmaps.anchor_part\n        )\n        anchor_ind = nodes.index(anchor_part) if anchor_part is not None else None\n        train_dataset = CenteredInstanceDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.centered_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            crop_size=config.data_config.preprocessing.crop_size,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        val_dataset = CenteredInstanceDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.centered_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            crop_size=config.data_config.preprocessing.crop_size,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n\n    elif model_type == \"multi_class_topdown\":\n        nodes = config.model_config.head_configs.multi_class_topdown.confmaps.part_names\n        anchor_part = (\n            config.model_config.head_configs.multi_class_topdown.confmaps.anchor_part\n        )\n        anchor_ind = nodes.index(anchor_part) if anchor_part is not None else None\n        train_dataset = TopDownCenteredInstanceMultiClassDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_topdown.confmaps,\n            class_vectors_head_config=config.model_config.head_configs.multi_class_topdown.class_vectors,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            crop_size=config.data_config.preprocessing.crop_size,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        val_dataset = TopDownCenteredInstanceMultiClassDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.multi_class_topdown.confmaps,\n            class_vectors_head_config=config.model_config.head_configs.multi_class_topdown.class_vectors,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            crop_size=config.data_config.preprocessing.crop_size,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n\n    elif model_type == \"centroid\":\n        nodes = [x[\"name\"] for x in config.data_config.skeletons[0][\"nodes\"]]\n        anchor_part = config.model_config.head_configs.centroid.confmaps.anchor_part\n        anchor_ind = nodes.index(anchor_part) if anchor_part is not None else None\n        train_dataset = CentroidDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.centroid.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        val_dataset = CentroidDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.centroid.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            anchor_ind=anchor_ind,\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n\n    else:\n        train_dataset = SingleInstanceDataset(\n            labels=train_labels,\n            confmap_head_config=config.model_config.head_configs.single_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=(\n                config.data_config.augmentation_config.intensity\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            geometric_aug=(\n                config.data_config.augmentation_config.geometric\n                if config.data_config.augmentation_config is not None\n                else None\n            ),\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=config.data_config.use_augmentations_train,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=train_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n        val_dataset = SingleInstanceDataset(\n            labels=val_labels,\n            confmap_head_config=config.model_config.head_configs.single_instance.confmaps,\n            max_stride=config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n            user_instances_only=config.data_config.user_instances_only,\n            ensure_rgb=config.data_config.preprocessing.ensure_rgb,\n            ensure_grayscale=config.data_config.preprocessing.ensure_grayscale,\n            intensity_aug=None,\n            geometric_aug=None,\n            scale=config.data_config.preprocessing.scale,\n            apply_aug=False,\n            max_hw=(\n                config.data_config.preprocessing.max_height,\n                config.data_config.preprocessing.max_width,\n            ),\n            cache_img=cache_imgs,\n            cache_img_path=val_cache_img_path,\n            use_existing_imgs=use_existing_imgs,\n            rank=rank,\n            parallel_caching=parallel_caching,\n            cache_workers=cache_workers,\n        )\n\n    # If using caching, close the videos to prevent `h5py objects can't be pickled error` when num_workers &gt; 0.\n    if \"cache_img\" in config.data_config.data_pipeline_fw:\n        for train, val in zip(train_labels, val_labels):\n            for video in train.videos:\n                if video.is_open:\n                    video.close()\n            for video in val.videos:\n                if video.is_open:\n                    video.close()\n\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"api/data/edge_maps/","title":"edge_maps","text":""},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps","title":"<code>sleap_nn.data.edge_maps</code>","text":"<p>Transformers for generating edge confidence maps and part affinity fields.</p> <p>Functions:</p> Name Description <code>distance_to_edge</code> <p>Compute pairwise distance between points and undirected edges.</p> <code>generate_pafs</code> <p>Generate part-affinity fields.</p> <code>get_edge_points</code> <p>Return the points in each instance that form a directed graph.</p> <code>make_edge_maps</code> <p>Generate confidence maps for a set of undirected edges.</p> <code>make_multi_pafs</code> <p>Make multiple instance PAFs with addition reduction.</p> <code>make_pafs</code> <p>Generate part affinity fields for a set of directed edges.</p>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.distance_to_edge","title":"<code>distance_to_edge(points, edge_source, edge_destination)</code>","text":"<p>Compute pairwise distance between points and undirected edges.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (d_0, ..., d_n, 2) where the last axis corresponds to x- and y-coordinates. Distances will be broadcast across all point dimensions.</p> required <code>edge_source</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destination</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of dtype torch.float32 of shape (d_0, ..., d_n, n_edges) where the first axes correspond to the initial dimensions of <code>points</code>, and the last indicates the distance of each point to each edge.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def distance_to_edge(\n    points: torch.Tensor, edge_source: torch.Tensor, edge_destination: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute pairwise distance between points and undirected edges.\n\n    Args:\n        points: Tensor of dtype torch.float32 of shape (d_0, ..., d_n, 2) where the last\n            axis corresponds to x- and y-coordinates. Distances will be broadcast across\n            all point dimensions.\n        edge_source: Tensor of dtype torch.float32 of shape (n_edges, 2) where the last\n            axis corresponds to x- and y-coordinates of the source points of each edge.\n        edge_destination: Tensor of dtype torch.float32 of shape (n_edges, 2) where the\n            last axis corresponds to x- and y-coordinates of the source points of each\n            edge.\n\n    Returns:\n        A tensor of dtype torch.float32 of shape (d_0, ..., d_n, n_edges) where the first\n        axes correspond to the initial dimensions of `points`, and the last indicates\n        the distance of each point to each edge.\n    \"\"\"\n    # Ensure all points are at least rank 2.\n    points = expand_to_rank(points, 2)\n    edge_source = expand_to_rank(edge_source, 2)\n    edge_destination = expand_to_rank(edge_destination, 2)\n\n    # Compute number of point dimensions.\n    n_pt_dims = points.dim() - 1\n\n    # Direction vector.\n    direction_vector = edge_destination - edge_source  # (n_edges, 2)\n\n    # Edge length.\n    edge_length = torch.maximum(\n        direction_vector.square().sum(dim=1), torch.tensor(1.0)\n    )  # (n_edges,)\n\n    # Adjust query points relative to edge source point.\n    source_relative_points = torch.unsqueeze(points, dim=-2) - expand_to_rank(\n        edge_source, n_pt_dims + 2\n    )  # (..., n_edges, 2)\n\n    # Project points to edge line.\n    line_projections = torch.sum(\n        source_relative_points * expand_to_rank(direction_vector, n_pt_dims + 2), dim=3\n    ) / expand_to_rank(\n        edge_length, n_pt_dims + 1\n    )  # (..., n_edges)\n\n    # Crop to line segment.\n    line_projections = torch.clamp(line_projections, min=0, max=1)\n\n    # Compute distance from each point to the edge.\n    distances = torch.sum(\n        torch.square(\n            (\n                line_projections.unsqueeze(-1)\n                * expand_to_rank(direction_vector, n_pt_dims + 2)\n            )\n            - source_relative_points\n        ),\n        dim=-1,\n    )  # (..., n_edges)\n\n    return distances\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.generate_pafs","title":"<code>generate_pafs(instances, img_hw, sigma=1.5, output_stride=2, edge_inds=attrs.field(default=None, converter=(attrs.converters.optional(ensure_list))), flatten_channels=False)</code>","text":"<p>Generate part-affinity fields.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>Input instances. (n_samples, n_instances, n_nodes, 2)</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <code>edge_inds</code> <code>Optional[Tensor]</code> <p><code>torch.Tensor</code> to use for looking up the index of the edges.</p> <code>field(default=None, converter=optional(ensure_list))</code> <code>flatten_channels</code> <code>bool</code> <p>If False, the generated tensors are of shape [n_edges, 2, height, width]. If True, generated tensors are of shape [n_edges * 2, height, width] by flattening the last 2 axes.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The \"part_affinity_fields\" key will be a tensor of shape (n_edges, 2, grid_height, grid_width) containing the combined part affinity fields of all instances in the frame.</p> <p>If the <code>flatten_channels</code> attribute is set to True, the last 2 axes of the \"part_affinity_fields\" are flattened to produce a tensor of shape (n_edges * 2, grid_height, grid_width). This is a convenient form when training models as a rank-4 (batched) tensor will generally be expected.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def generate_pafs(\n    instances: torch.Tensor,\n    img_hw: Tuple[int],\n    sigma: float = 1.5,\n    output_stride=2,\n    edge_inds: Optional[torch.Tensor] = attrs.field(\n        default=None, converter=attrs.converters.optional(ensure_list)\n    ),\n    flatten_channels: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate part-affinity fields.\n\n    Args:\n        instances: Input instances. (n_samples, n_instances, n_nodes, 2)\n        img_hw: Image size as tuple (height, width).\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n        edge_inds: `torch.Tensor` to use for looking up the index of the\n            edges.\n        flatten_channels: If False, the generated tensors are of shape\n            [n_edges, 2, height, width]. If True, generated tensors are of shape\n            [n_edges * 2, height, width] by flattening the last 2 axes.\n\n    Returns:\n        The \"part_affinity_fields\" key will be a tensor of shape\n        (n_edges, 2, grid_height, grid_width) containing the combined part affinity\n        fields of all instances in the frame.\n\n        If the `flatten_channels` attribute is set to True, the last 2 axes of the\n        \"part_affinity_fields\" are flattened to produce a tensor of shape\n        (n_edges * 2, grid_height, grid_width). This is a convenient form when\n        training models as a rank-4 (batched) tensor will generally be expected.\n    \"\"\"\n    image_height, image_width = img_hw\n\n    # Generate sampling grid vectors.\n    xv, yv = make_grid_vectors(\n        image_height=image_height,\n        image_width=image_width,\n        output_stride=output_stride,\n    )\n    grid_height = len(yv)\n    grid_width = len(xv)\n    n_edges = len(edge_inds)\n\n    instances = instances[0]  # n_samples=1\n    in_img = (instances &gt; 0) &amp; (instances &lt; torch.stack([xv[-1], yv[-1]]).view(1, 1, 2))\n    in_img = in_img.all(dim=-1).any(dim=1)\n    assert len(in_img.shape) == 1\n    instances = instances[in_img]\n\n    edge_sources, edge_destinations = get_edge_points(instances, edge_inds)\n    assert len(edge_sources.shape) == 3\n    assert edge_sources.shape[1:] == (n_edges, 2)\n\n    assert len(edge_destinations.shape) == 3\n    assert edge_destinations.shape[1:] == (n_edges, 2)\n\n    pafs = make_multi_pafs(\n        xv=xv,\n        yv=yv,\n        edge_sources=edge_sources,\n        edge_destinations=edge_destinations,\n        sigma=sigma,\n    )\n    assert pafs.shape == (n_edges, 2, grid_height, grid_width)\n\n    if flatten_channels:\n        pafs = pafs.reshape(n_edges * 2, grid_height, grid_width)\n        assert pafs.shape == (n_edges * 2, grid_height, grid_width)\n\n    return pafs\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.get_edge_points","title":"<code>get_edge_points(instances, edge_inds)</code>","text":"<p>Return the points in each instance that form a directed graph.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>A tensor of shape (n_instances, n_nodes, 2) and dtype torch.float32 containing instance points where the last axis corresponds to (x, y) pixel coordinates on the image. This must be rank-3 even if a single instance is present.</p> required <code>edge_inds</code> <code>Tensor</code> <p>A tensor of shape (n_edges, 2) and dtype torch.int32 containing the node indices that define a directed graph, where the last axis corresponds to the source and destination node indices.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (edge_sources, edge_destinations) containing the edge and destination points respectively. Both will be tensors of shape (n_instances, n_edges, 2), where the last axis corresponds to (x, y) pixel coordinates on the image.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def get_edge_points(\n    instances: torch.Tensor, edge_inds: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Return the points in each instance that form a directed graph.\n\n    Args:\n        instances: A tensor of shape (n_instances, n_nodes, 2) and dtype torch.float32\n            containing instance points where the last axis corresponds to (x, y) pixel\n            coordinates on the image. This must be rank-3 even if a single instance is\n            present.\n        edge_inds: A tensor of shape (n_edges, 2) and dtype torch.int32 containing the node\n            indices that define a directed graph, where the last axis corresponds to the\n            source and destination node indices.\n\n    Returns:\n        Tuple of (edge_sources, edge_destinations) containing the edge and destination\n        points respectively. Both will be tensors of shape (n_instances, n_edges, 2),\n        where the last axis corresponds to (x, y) pixel coordinates on the image.\n    \"\"\"\n    source_inds = edge_inds[:, 0].to(torch.int32)\n    destination_inds = edge_inds[:, 1].to(torch.int32)\n\n    edge_sources = instances[:, source_inds]\n    edge_destinations = instances[:, destination_inds]\n    return edge_sources, edge_destinations\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.make_edge_maps","title":"<code>make_edge_maps(xv, yv, edge_source, edge_destination, sigma)</code>","text":"<p>Generate confidence maps for a set of undirected edges.</p> <p>Parameters:</p> Name Type Description Default <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape (grid_width,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape (grid_height,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>edge_source</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destination</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the destination points of each edge.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate confidence maps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A set of confidence maps corresponding to the probability of each point on a sampling grid being on each edge. These will be in a tensor of shape (grid_height, grid_width, n_edges) of dtype torch.float32.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def make_edge_maps(\n    xv: torch.Tensor,\n    yv: torch.Tensor,\n    edge_source: torch.Tensor,\n    edge_destination: torch.Tensor,\n    sigma: float,\n) -&gt; torch.Tensor:\n    \"\"\"Generate confidence maps for a set of undirected edges.\n\n    Args:\n        xv: Sampling grid vector for x-coordinates of shape (grid_width,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape (grid_height,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        edge_source: Tensor of dtype torch.float32 of shape (n_edges, 2) where the last\n            axis corresponds to x- and y-coordinates of the source points of each edge.\n        edge_destination: Tensor of dtype torch.float32 of shape (n_edges, 2) where the\n            last axis corresponds to x- and y-coordinates of the destination points of\n            each edge.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            confidence maps.\n\n    Returns:\n        A set of confidence maps corresponding to the probability of each point on a\n        sampling grid being on each edge. These will be in a tensor of shape\n        (grid_height, grid_width, n_edges) of dtype torch.float32.\n    \"\"\"\n    yy, xx = torch.meshgrid(yv, xv, indexing=\"ij\")\n    sampling_grid = torch.stack((xx, yy), dim=-1)  # (height, width, 2)\n\n    distances = distance_to_edge(\n        sampling_grid, edge_source=edge_source, edge_destination=edge_destination\n    )\n    edge_maps = gaussian_pdf(distances, sigma=sigma)\n    return edge_maps\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.make_multi_pafs","title":"<code>make_multi_pafs(xv, yv, edge_sources, edge_destinations, sigma)</code>","text":"<p>Make multiple instance PAFs with addition reduction.</p> <p>Parameters:</p> Name Type Description Default <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape (grid_width,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape (grid_height,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>edge_sources</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destinations</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2) where the last axis corresponds to x- and y-coordinates of the destination points of each edge.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate the edge maps for masking the PAFs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A set of part affinity fields generated for each instance. These will be in a tensor of shape (n_edges, 2, grid_height, grid_width). If multiple instance PAFs are defined on the same pixel, they will be summed.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def make_multi_pafs(\n    xv: torch.Tensor,\n    yv: torch.Tensor,\n    edge_sources: torch.Tensor,\n    edge_destinations: torch.Tensor,\n    sigma: float,\n) -&gt; torch.Tensor:\n    \"\"\"Make multiple instance PAFs with addition reduction.\n\n    Args:\n        xv: Sampling grid vector for x-coordinates of shape (grid_width,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape (grid_height,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        edge_sources: Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2)\n            where the last axis corresponds to x- and y-coordinates of the source points\n            of each edge.\n        edge_destinations: Tensor of dtype torch.float32 of shape (n_instances, n_edges, 2)\n            where the last axis corresponds to x- and y-coordinates of the destination\n            points of each edge.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            the edge maps for masking the PAFs.\n\n    Returns:\n        A set of part affinity fields generated for each instance. These will be in a\n        tensor of shape (n_edges, 2, grid_height, grid_width). If multiple instance\n        PAFs are defined on the same pixel, they will be summed.\n    \"\"\"\n    grid_height = yv.shape[0]\n    grid_width = xv.shape[0]\n    n_edges = edge_sources.shape[1]\n    n_instances = edge_sources.shape[0]\n\n    pafs = torch.zeros((n_edges, 2, grid_height, grid_width), dtype=torch.float32)\n\n    for i in range(n_instances):\n        edge_source = edge_sources[i, :]\n        edge_destination = edge_destinations[i, :]\n\n        paf = make_pafs(\n            xv=xv,\n            yv=yv,\n            edge_source=edge_source,\n            edge_destination=edge_destination,\n            sigma=sigma,\n        )\n\n        paf[torch.isnan(paf)] = 0.0\n\n        pafs += paf\n\n    return pafs\n</code></pre>"},{"location":"api/data/edge_maps/#sleap_nn.data.edge_maps.make_pafs","title":"<code>make_pafs(xv, yv, edge_source, edge_destination, sigma)</code>","text":"<p>Generate part affinity fields for a set of directed edges.</p> <p>Parameters:</p> Name Type Description Default <code>xv</code> <code>Tensor</code> <p>Sampling grid vector for x-coordinates of shape (grid_width,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>yv</code> <code>Tensor</code> <p>Sampling grid vector for y-coordinates of shape (grid_height,) and dtype torch.float32. This can be generated by <code>sleap_nn.data.utils.make_grid_vectors</code>.</p> required <code>edge_source</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the source points of each edge.</p> required <code>edge_destination</code> <code>Tensor</code> <p>Tensor of dtype torch.float32 of shape (n_edges, 2) where the last axis corresponds to x- and y-coordinates of the destination points of each edge.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the 2D Gaussian distribution sampled to generate the edge maps for masking the PAFs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A set of part affinity fields corresponding to the unit vector pointing along the direction of each edge weighted by the probability of each point on a sampling grid being on each edge. These will be in a tensor of shape (n_edges, 2, grid_height, grid_width) of dtype torch.float32. The last axis corresponds to the x- and y-coordinates of the unit vectors.</p> Source code in <code>sleap_nn/data/edge_maps.py</code> <pre><code>def make_pafs(\n    xv: torch.Tensor,\n    yv: torch.Tensor,\n    edge_source: torch.Tensor,\n    edge_destination: torch.Tensor,\n    sigma: float,\n) -&gt; torch.Tensor:\n    \"\"\"Generate part affinity fields for a set of directed edges.\n\n    Args:\n        xv: Sampling grid vector for x-coordinates of shape (grid_width,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        yv: Sampling grid vector for y-coordinates of shape (grid_height,) and dtype\n            torch.float32. This can be generated by\n            `sleap_nn.data.utils.make_grid_vectors`.\n        edge_source: Tensor of dtype torch.float32 of shape (n_edges, 2) where the last\n            axis corresponds to x- and y-coordinates of the source points of each edge.\n        edge_destination: Tensor of dtype torch.float32 of shape (n_edges, 2) where the\n            last axis corresponds to x- and y-coordinates of the destination points of\n            each edge.\n        sigma: Standard deviation of the 2D Gaussian distribution sampled to generate\n            the edge maps for masking the PAFs.\n\n    Returns:\n        A set of part affinity fields corresponding to the unit vector pointing along\n        the direction of each edge weighted by the probability of each point on a\n        sampling grid being on each edge. These will be in a tensor of shape\n        (n_edges, 2, grid_height, grid_width) of dtype torch.float32. The last axis\n        corresponds to the x- and y-coordinates of the unit vectors.\n    \"\"\"\n    unit_vectors = edge_destination - edge_source\n    unit_vectors = unit_vectors / torch.norm(unit_vectors, dim=-1, keepdim=True)\n    edge_confidence_map = make_edge_maps(\n        xv=xv,\n        yv=yv,\n        edge_source=edge_source,\n        edge_destination=edge_destination,\n        sigma=sigma,\n    )\n    pafs = torch.unsqueeze(edge_confidence_map, dim=-1) * expand_to_rank(\n        unit_vectors, 4\n    )\n    pafs = pafs.permute(2, 3, 0, 1)\n    return pafs\n</code></pre>"},{"location":"api/data/identity/","title":"identity","text":""},{"location":"api/data/identity/#sleap_nn.data.identity","title":"<code>sleap_nn.data.identity</code>","text":"<p>Utilities for generating data for track identity models.</p> <p>Functions:</p> Name Description <code>generate_class_maps</code> <p>Generate class maps from track indices.</p> <code>make_class_maps</code> <p>Generate identity class maps using instance-wise confidence maps.</p> <code>make_class_vectors</code> <p>Make a binary class vectors from class indices.</p>"},{"location":"api/data/identity/#sleap_nn.data.identity.generate_class_maps","title":"<code>generate_class_maps(instances, img_hw, num_instances, class_inds, num_tracks, class_map_threshold=0.2, sigma=1.5, output_stride=2, is_centroids=False)</code>","text":"<p>Generate class maps from track indices.</p> <p>Parameters:</p> Name Type Description Default <code>instances</code> <code>Tensor</code> <p>Input keypoints. (n_samples=1, n_instances, n_nodes, 2) or for centroids - (n_samples=1, n_instances, 2)</p> required <code>img_hw</code> <code>Tuple[int]</code> <p>Image size as tuple (height, width).</p> required <code>num_instances</code> <code>int</code> <p>Original number of instances in the frame.</p> required <code>class_inds</code> <code>Tensor</code> <p>Class indices as <code>torch.int32</code> tensor of shape <code>(n_instances)</code>.</p> required <code>num_tracks</code> <code>int</code> <p>Total number of tracks in the dataset.</p> required <code>class_map_threshold</code> <code>float</code> <p>Minimum confidence map value below which map values will be replaced with zeros.</p> <code>0.2</code> <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian distribution that is used to generate confidence maps. Default: 1.5.</p> <code>1.5</code> <code>output_stride</code> <code>int</code> <p>The relative stride to use when generating confidence maps. A larger stride will generate smaller confidence maps. Default: 2.</p> <code>2</code> <code>is_centroids</code> <code>bool</code> <p>True if confidence maps should be generates for centroids else False. Default: False.</p> <code>False</code> Source code in <code>sleap_nn/data/identity.py</code> <pre><code>def generate_class_maps(\n    instances: torch.Tensor,\n    img_hw: Tuple[int],\n    num_instances: int,\n    class_inds: torch.Tensor,\n    num_tracks: int,\n    class_map_threshold: float = 0.2,\n    sigma: float = 1.5,\n    output_stride: int = 2,\n    is_centroids: bool = False,\n):\n    \"\"\"Generate class maps from track indices.\n\n    Args:\n        instances: Input keypoints. (n_samples=1, n_instances, n_nodes, 2) or\n            for centroids - (n_samples=1, n_instances, 2)\n        img_hw: Image size as tuple (height, width).\n        num_instances: Original number of instances in the frame.\n        class_inds: Class indices as `torch.int32` tensor of shape `(n_instances)`.\n        num_tracks: Total number of tracks in the dataset.\n        class_map_threshold: Minimum confidence map value below which map values will be\n            replaced with zeros.\n        sigma: The standard deviation of the Gaussian distribution that is used to\n            generate confidence maps. Default: 1.5.\n        output_stride: The relative stride to use when generating confidence maps.\n            A larger stride will generate smaller confidence maps. Default: 2.\n        is_centroids: True if confidence maps should be generates for centroids else False.\n            Default: False.\n\n    \"\"\"\n    height, width = img_hw\n    xv, yv = make_grid_vectors(height, width, output_stride)\n\n    if is_centroids:\n        points = instances[:, :num_instances, :].unsqueeze(dim=-3)\n        # (n_samples=1, 1, n_instances, 2)\n    else:\n        points = instances[:, :num_instances, :, :].permute(\n            0, 2, 1, 3\n        )  # (n_samples=1, n_nodes, n_instances, 2)\n\n    # Generate confidene maps for masking.\n    cms = make_multi_confmaps(\n        points, xv, yv, sigma * output_stride\n    )  # (n_samples=1, n_instances, height/ output_stride, width/ output_stride).\n\n    class_maps = make_class_maps(\n        cms,\n        class_inds=class_inds,\n        n_classes=num_tracks,\n        threshold=class_map_threshold,\n    )  # (n_samples=1, n_classes, height/ output_stride, width/ output_stride)\n    return class_maps\n</code></pre>"},{"location":"api/data/identity/#sleap_nn.data.identity.make_class_maps","title":"<code>make_class_maps(confmaps, class_inds, n_classes, threshold=0.2)</code>","text":"<p>Generate identity class maps using instance-wise confidence maps.</p> <p>This is useful for making class maps defined on local neighborhoods around the peaks.</p> <p>Parameters:</p> Name Type Description Default <code>confmaps</code> <code>Tensor</code> <p>Confidence maps for the same points as the offset maps as a <code>torch.Tensor</code> of shape <code>(n_samples=1, n_instances, grid_height, grid_width)</code>. This can be generated by <code>sleap_nn.data.confidence_maps.make_confmaps</code>.</p> required <code>class_inds</code> <code>Tensor</code> <p>Class indices as <code>torch.int32</code> tensor of shape <code>(n_instances)</code>.</p> required <code>n_classes</code> <code>int</code> <p>Integer number of maximum classes.</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence map value below which map values will be replaced with zeros.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The class maps with shape <code>(n_samples=1, n_classes, grid_height, grid_width)</code> and dtype <code>torch.float32</code> where each channel will be a binary mask with 1 where the instance confidence maps were higher than the threshold.</p> Notes <p>Pixels that have confidence map values from more than one animal will have the class vectors weighed by the relative contribution of each instance.</p> Source code in <code>sleap_nn/data/identity.py</code> <pre><code>def make_class_maps(\n    confmaps: torch.Tensor,\n    class_inds: torch.Tensor,\n    n_classes: int,\n    threshold: float = 0.2,\n) -&gt; torch.Tensor:\n    \"\"\"Generate identity class maps using instance-wise confidence maps.\n\n    This is useful for making class maps defined on local neighborhoods around the\n    peaks.\n\n    Args:\n        confmaps: Confidence maps for the same points as the offset maps as a\n            `torch.Tensor` of shape `(n_samples=1, n_instances, grid_height, grid_width)`. This can be generated by\n            `sleap_nn.data.confidence_maps.make_confmaps`.\n        class_inds: Class indices as `torch.int32` tensor of shape `(n_instances)`.\n        n_classes: Integer number of maximum classes.\n        threshold: Minimum confidence map value below which map values will be replaced\n            with zeros.\n\n    Returns:\n        The class maps with shape `(n_samples=1, n_classes, grid_height, grid_width)` and dtype\n        `torch.float32` where each channel will be a binary mask with 1 where the instance\n        confidence maps were higher than the threshold.\n\n    Notes:\n        Pixels that have confidence map values from more than one animal will have the\n        class vectors weighed by the relative contribution of each instance.\n\n    \"\"\"\n    n_instances = confmaps.shape[-3]\n    class_vectors = make_class_vectors(class_inds, n_classes)\n    class_vectors = torch.reshape(\n        class_vectors.to(torch.float32),\n        [n_classes, n_instances, 1, 1],\n    )\n\n    # Normalize instance mask\n    mask = confmaps / torch.sum(confmaps, dim=-3, keepdim=True)\n    mask = torch.where(\n        confmaps &gt; threshold,\n        mask,\n        torch.tensor(0.0, dtype=mask.dtype, device=mask.device),\n    )  # (1, num_instances, H, W)\n\n    # Apply mask to vectors and reduce over instances\n    class_maps = torch.max(mask * class_vectors, dim=-3).values\n\n    return class_maps.unsqueeze(0)  # (n_samples=1, n_classes, H, W)\n</code></pre>"},{"location":"api/data/identity/#sleap_nn.data.identity.make_class_vectors","title":"<code>make_class_vectors(class_inds, n_classes)</code>","text":"<p>Make a binary class vectors from class indices.</p> <p>Parameters:</p> Name Type Description Default <code>class_inds</code> <code>Tensor</code> <p>Class indices as <code>torch.Tensor</code> of dtype <code>torch.int32</code> and shape <code>(n_instances,)</code>. Indices of <code>-1</code> will be interpreted as having no class.</p> required <code>n_classes</code> <code>int</code> <p>Integer number of maximum classes.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with binary class vectors of shape <code>(n_instances, n_classes)</code> of dtype <code>torch.int32</code>. Instances with no class will have all zeros in their row.</p> <p>Notes: A class index can be used to represent a track index.</p> Source code in <code>sleap_nn/data/identity.py</code> <pre><code>def make_class_vectors(class_inds: torch.Tensor, n_classes: int) -&gt; torch.Tensor:\n    \"\"\"Make a binary class vectors from class indices.\n\n    Args:\n        class_inds: Class indices as `torch.Tensor` of dtype `torch.int32` and shape\n            `(n_instances,)`. Indices of `-1` will be interpreted as having no class.\n        n_classes: Integer number of maximum classes.\n\n    Returns:\n        A tensor with binary class vectors of shape `(n_instances, n_classes)` of dtype\n        `torch.int32`. Instances with no class will have all zeros in their row.\n\n    Notes: A class index can be used to represent a track index.\n    \"\"\"\n    # Create mask of valid IDs\n    mask = class_inds &gt;= 0\n    class_inds_masked = class_inds.clone()\n    class_inds_masked[~mask] = 0\n\n    one_hot = F.one_hot(class_inds_masked.long(), num_classes=n_classes)\n    one_hot[~mask] = 0  # zero out invalids\n    return one_hot.to(torch.int32)\n</code></pre>"},{"location":"api/data/instance_centroids/","title":"instance_centroids","text":""},{"location":"api/data/instance_centroids/#sleap_nn.data.instance_centroids","title":"<code>sleap_nn.data.instance_centroids</code>","text":"<p>Handle calculation of instance centroids.</p> <p>Functions:</p> Name Description <code>find_points_bbox_midpoint</code> <p>Find the midpoint of the bounding box of a set of points.</p> <code>generate_centroids</code> <p>Return centroids, falling back to bounding box midpoints.</p>"},{"location":"api/data/instance_centroids/#sleap_nn.data.instance_centroids.find_points_bbox_midpoint","title":"<code>find_points_bbox_midpoint(points)</code>","text":"<p>Find the midpoint of the bounding box of a set of points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>A torch.Tensor of dtype torch.float32 and of shape (..., n_points, 2), i.e., rank &gt;= 2.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The midpoints between the bounds of each set of points. The output will be of shape (..., 2), reducing the rank of the input by 1. NaNs will be ignored in the calculation.</p> Notes <p>The midpoint is calculated as:     xy_mid = xy_min + ((xy_max - xy_min) / 2)            = ((2 * xy_min) / 2) + ((xy_max - xy_min) / 2)            = (2 * xy_min + xy_max - xy_min) / 2            = (xy_min + xy_max) / 2</p> Source code in <code>sleap_nn/data/instance_centroids.py</code> <pre><code>def find_points_bbox_midpoint(points: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Find the midpoint of the bounding box of a set of points.\n\n    Args:\n        points: A torch.Tensor of dtype torch.float32 and of shape (..., n_points, 2),\n            i.e., rank &gt;= 2.\n\n    Returns:\n        The midpoints between the bounds of each set of points. The output will be of\n        shape (..., 2), reducing the rank of the input by 1. NaNs will be ignored in the\n        calculation.\n\n    Notes:\n        The midpoint is calculated as:\n            xy_mid = xy_min + ((xy_max - xy_min) / 2)\n                   = ((2 * xy_min) / 2) + ((xy_max - xy_min) / 2)\n                   = (2 * xy_min + xy_max - xy_min) / 2\n                   = (xy_min + xy_max) / 2\n    \"\"\"\n    pts_min = torch.min(\n        torch.where(torch.isnan(points), torch.inf, points), dim=-2\n    ).values\n    pts_max = torch.max(\n        torch.where(torch.isnan(points), -torch.inf, points), dim=-2\n    ).values\n\n    return (pts_max + pts_min) * 0.5\n</code></pre>"},{"location":"api/data/instance_centroids/#sleap_nn.data.instance_centroids.generate_centroids","title":"<code>generate_centroids(points, anchor_ind=None)</code>","text":"<p>Return centroids, falling back to bounding box midpoints.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>A torch.Tensor of dtype torch.float32 and of shape (..., n_nodes, 2), i.e., rank &gt;= 2.</p> required <code>anchor_ind</code> <code>Optional[int]</code> <p>The index of the node to use as the anchor for the centroid. If not provided or if not present in the instance, the midpoint of the bounding box is used instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The centroids of the instances. The output will be of shape (..., 2), reducing the rank of the input by 1. NaNs will be ignored in the calculation.</p> Source code in <code>sleap_nn/data/instance_centroids.py</code> <pre><code>def generate_centroids(\n    points: torch.Tensor, anchor_ind: Optional[int] = None\n) -&gt; torch.Tensor:\n    \"\"\"Return centroids, falling back to bounding box midpoints.\n\n    Args:\n        points: A torch.Tensor of dtype torch.float32 and of shape (..., n_nodes, 2),\n            i.e., rank &gt;= 2.\n        anchor_ind: The index of the node to use as the anchor for the centroid. If not\n            provided or if not present in the instance, the midpoint of the bounding box\n            is used instead.\n\n    Returns:\n        The centroids of the instances. The output will be of shape (..., 2), reducing\n        the rank of the input by 1. NaNs will be ignored in the calculation.\n    \"\"\"\n    if anchor_ind is not None:\n        centroids = points[..., anchor_ind, :].clone()\n    else:\n        centroids = torch.full_like(points[..., 0, :], torch.nan).clone()\n\n    missing_anchors = torch.isnan(centroids).any(dim=-1)\n    if missing_anchors.any():\n        centroids[missing_anchors] = find_points_bbox_midpoint(points[missing_anchors])\n\n    return centroids  # (..., n_instances, 2)\n</code></pre>"},{"location":"api/data/instance_cropping/","title":"instance_cropping","text":""},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping","title":"<code>sleap_nn.data.instance_cropping</code>","text":"<p>Handle cropping of instances.</p> <p>Functions:</p> Name Description <code>compute_augmentation_padding</code> <p>Compute padding needed to accommodate augmentation transforms.</p> <code>find_instance_crop_size</code> <p>Compute the size of the largest instance bounding box from labels.</p> <code>find_max_instance_bbox_size</code> <p>Find the maximum bounding box dimension across all instances in labels.</p> <code>generate_crops</code> <p>Generate cropped image for the given centroid.</p> <code>make_centered_bboxes</code> <p>Create centered bounding boxes around centroid.</p>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.compute_augmentation_padding","title":"<code>compute_augmentation_padding(bbox_size, rotation_max=0.0, scale_max=1.0)</code>","text":"<p>Compute padding needed to accommodate augmentation transforms.</p> <p>When rotation and scaling augmentations are applied, the bounding box of an instance can expand beyond its original size. This function calculates the padding needed to ensure the full instance remains visible after augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_size</code> <code>float</code> <p>The size of the instance bounding box (max of width/height).</p> required <code>rotation_max</code> <code>float</code> <p>Maximum absolute rotation angle in degrees. For symmetric rotation ranges like [-180, 180], pass 180.</p> <code>0.0</code> <code>scale_max</code> <code>float</code> <p>Maximum scaling factor. For scale range [0.9, 1.1], pass 1.1.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>int</code> <p>Padding in pixels to add around the bounding box (total, not per side).</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def compute_augmentation_padding(\n    bbox_size: float,\n    rotation_max: float = 0.0,\n    scale_max: float = 1.0,\n) -&gt; int:\n    \"\"\"Compute padding needed to accommodate augmentation transforms.\n\n    When rotation and scaling augmentations are applied, the bounding box of an\n    instance can expand beyond its original size. This function calculates the\n    padding needed to ensure the full instance remains visible after augmentation.\n\n    Args:\n        bbox_size: The size of the instance bounding box (max of width/height).\n        rotation_max: Maximum absolute rotation angle in degrees. For symmetric\n            rotation ranges like [-180, 180], pass 180.\n        scale_max: Maximum scaling factor. For scale range [0.9, 1.1], pass 1.1.\n\n    Returns:\n        Padding in pixels to add around the bounding box (total, not per side).\n    \"\"\"\n    if rotation_max == 0.0 and scale_max &lt;= 1.0:\n        return 0\n\n    # For a square bbox rotated by angle \u03b8, the new bbox has side length:\n    # L' = L * (|cos(\u03b8)| + |sin(\u03b8)|)\n    # Maximum expansion occurs at 45\u00b0: L' = L * sqrt(2)\n    # For arbitrary angle: we use the worst case within the rotation range\n    rotation_rad = math.radians(min(abs(rotation_max), 90))\n    rotation_factor = abs(math.cos(rotation_rad)) + abs(math.sin(rotation_rad))\n\n    # For angles &gt; 45\u00b0, the factor increases, max at 45\u00b0 = sqrt(2)\n    # But for angles approaching 90\u00b0, it goes back to 1\n    # Worst case in any range including 45\u00b0 is sqrt(2)\n    if abs(rotation_max) &gt;= 45:\n        rotation_factor = math.sqrt(2)\n\n    # Combined expansion factor\n    expansion_factor = rotation_factor * max(scale_max, 1.0)\n\n    # Total padding needed (both sides)\n    expanded_size = bbox_size * expansion_factor\n    padding = expanded_size - bbox_size\n\n    return int(math.ceil(padding))\n</code></pre>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.find_instance_crop_size","title":"<code>find_instance_crop_size(labels, padding=0, maximum_stride=2, min_crop_size=None)</code>","text":"<p>Compute the size of the largest instance bounding box from labels.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>A <code>sio.Labels</code> containing user-labeled instances.</p> required <code>padding</code> <code>int</code> <p>Integer number of pixels to add to the bounds as margin padding.</p> <code>0</code> <code>maximum_stride</code> <code>int</code> <p>Ensure that the returned crop size is divisible by this value. Useful for ensuring that the crop size will not be truncated in a given architecture.</p> <code>2</code> <code>min_crop_size</code> <code>Optional[int]</code> <p>The crop size set by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>An integer crop size denoting the length of the side of the bounding boxes that will contain the instances when cropped. The returned crop size will be larger or equal to the input <code>min_crop_size</code>.</p> <p>This accounts for stride and padding when ensuring divisibility.</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def find_instance_crop_size(\n    labels: sio.Labels,\n    padding: int = 0,\n    maximum_stride: int = 2,\n    min_crop_size: Optional[int] = None,\n) -&gt; int:\n    \"\"\"Compute the size of the largest instance bounding box from labels.\n\n    Args:\n        labels: A `sio.Labels` containing user-labeled instances.\n        padding: Integer number of pixels to add to the bounds as margin padding.\n        maximum_stride: Ensure that the returned crop size is divisible by this value.\n            Useful for ensuring that the crop size will not be truncated in a given\n            architecture.\n        min_crop_size: The crop size set by the user.\n\n    Returns:\n        An integer crop size denoting the length of the side of the bounding boxes that\n        will contain the instances when cropped. The returned crop size will be larger\n        or equal to the input `min_crop_size`.\n\n        This accounts for stride and padding when ensuring divisibility.\n    \"\"\"\n    # Check if user-specified crop size is divisible by max stride\n    min_crop_size = 0 if min_crop_size is None else min_crop_size\n    if (min_crop_size &gt; 0) and (min_crop_size % maximum_stride == 0):\n        return min_crop_size\n\n    # Calculate crop size\n    min_crop_size_no_pad = min_crop_size - padding\n    max_length = 0.0\n    for lf in labels:\n        for inst in lf.instances:\n            if not inst.is_empty:  # only if at least one point is not nan\n                pts = inst.numpy()\n                diff_x = np.nanmax(pts[:, 0]) - np.nanmin(pts[:, 0])\n                diff_x = 0 if np.isnan(diff_x) else diff_x\n                max_length = np.maximum(max_length, diff_x)\n                diff_y = np.nanmax(pts[:, 1]) - np.nanmin(pts[:, 1])\n                diff_y = 0 if np.isnan(diff_y) else diff_y\n                max_length = np.maximum(max_length, diff_y)\n                max_length = np.maximum(max_length, min_crop_size_no_pad)\n\n    max_length += float(padding)\n    crop_size = math.ceil(max_length / float(maximum_stride)) * maximum_stride\n\n    return int(crop_size)\n</code></pre>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.find_max_instance_bbox_size","title":"<code>find_max_instance_bbox_size(labels)</code>","text":"<p>Find the maximum bounding box dimension across all instances in labels.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>A <code>sio.Labels</code> containing user-labeled instances.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The maximum bounding box dimension (max of width or height) across all instances.</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def find_max_instance_bbox_size(labels: sio.Labels) -&gt; float:\n    \"\"\"Find the maximum bounding box dimension across all instances in labels.\n\n    Args:\n        labels: A `sio.Labels` containing user-labeled instances.\n\n    Returns:\n        The maximum bounding box dimension (max of width or height) across all instances.\n    \"\"\"\n    max_length = 0.0\n    for lf in labels:\n        for inst in lf.instances:\n            if not inst.is_empty:\n                pts = inst.numpy()\n                diff_x = np.nanmax(pts[:, 0]) - np.nanmin(pts[:, 0])\n                diff_x = 0 if np.isnan(diff_x) else diff_x\n                max_length = np.maximum(max_length, diff_x)\n                diff_y = np.nanmax(pts[:, 1]) - np.nanmin(pts[:, 1])\n                diff_y = 0 if np.isnan(diff_y) else diff_y\n                max_length = np.maximum(max_length, diff_y)\n    return float(max_length)\n</code></pre>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.generate_crops","title":"<code>generate_crops(image, instance, centroid, crop_size)</code>","text":"<p>Generate cropped image for the given centroid.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input source image. (n_samples, C, H, W)</p> required <code>instance</code> <code>Tensor</code> <p>Keypoints for the instance to be cropped. (n_nodes, 2)</p> required <code>centroid</code> <code>Tensor</code> <p>Centroid of the instance to be cropped. (2)</p> required <code>crop_size</code> <code>Tuple[int]</code> <p>(height, width) of the crop to be generated.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary with cropped images, bounding box for the cropped instance, keypoints and centroids adjusted to the crop.</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def generate_crops(\n    image: torch.Tensor,\n    instance: torch.Tensor,\n    centroid: torch.Tensor,\n    crop_size: Tuple[int],\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Generate cropped image for the given centroid.\n\n    Args:\n        image: Input source image. (n_samples, C, H, W)\n        instance: Keypoints for the instance to be cropped. (n_nodes, 2)\n        centroid: Centroid of the instance to be cropped. (2)\n        crop_size: (height, width) of the crop to be generated.\n\n    Returns:\n        A dictionary with cropped images, bounding box for the cropped instance, keypoints and\n        centroids adjusted to the crop.\n    \"\"\"\n    box_size = crop_size\n\n    # Generate bounding boxes from centroid.\n    instance_bbox = torch.unsqueeze(\n        make_centered_bboxes(centroid, box_size[0], box_size[1]), 0\n    )  # (n_samples=1, 4, 2)\n\n    # Generate cropped image of shape (n_samples, C, crop_H, crop_W)\n    instance_image = crop_and_resize(\n        image,\n        boxes=instance_bbox,\n        size=box_size,\n    )\n\n    # Access top left point (x,y) of bounding box and subtract this offset from\n    # position of nodes.\n    point = instance_bbox[0][0]\n    center_instance = (instance - point).unsqueeze(0)  # (n_samples=1, n_nodes, 2)\n    centered_centroid = (centroid - point).unsqueeze(0)  # (n_samples=1, 2)\n\n    cropped_sample = {\n        \"instance_image\": instance_image,\n        \"instance_bbox\": instance_bbox,\n        \"instance\": center_instance,\n        \"centroid\": centered_centroid,\n    }\n\n    return cropped_sample\n</code></pre>"},{"location":"api/data/instance_cropping/#sleap_nn.data.instance_cropping.make_centered_bboxes","title":"<code>make_centered_bboxes(centroids, box_height, box_width)</code>","text":"<p>Create centered bounding boxes around centroid.</p> <p>To be used with <code>kornia.geometry.transform.crop_and_resize</code>in the following (clockwise) order: top-left, top-right, bottom-right and bottom-left.</p> <p>Parameters:</p> Name Type Description Default <code>centroids</code> <code>Tensor</code> <p>A tensor of centroids with shape (n_centroids, 2), where n_centroids is the number of centroids, and the last dimension represents x and y coordinates.</p> required <code>box_height</code> <code>int</code> <p>The desired height of the bounding boxes.</p> required <code>box_width</code> <code>int</code> <p>The desired width of the bounding boxes.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing bounding box coordinates for each centroid.     The output tensor has shape (n_centroids, 4, 2), where n_centroids is the number     of centroids, and the second dimension represents the four corner points of     the bounding boxes, each with x and y coordinates. The order of the corners     follows a clockwise arrangement: top-left, top-right, bottom-right, and     bottom-left.</p> Source code in <code>sleap_nn/data/instance_cropping.py</code> <pre><code>def make_centered_bboxes(\n    centroids: torch.Tensor, box_height: int, box_width: int\n) -&gt; torch.Tensor:\n    \"\"\"Create centered bounding boxes around centroid.\n\n    To be used with `kornia.geometry.transform.crop_and_resize`in the following\n    (clockwise) order: top-left, top-right, bottom-right and bottom-left.\n\n    Args:\n        centroids: A tensor of centroids with shape (n_centroids, 2), where n_centroids is the\n            number of centroids, and the last dimension represents x and y coordinates.\n        box_height: The desired height of the bounding boxes.\n        box_width: The desired width of the bounding boxes.\n\n    Returns:\n        torch.Tensor: A tensor containing bounding box coordinates for each centroid.\n            The output tensor has shape (n_centroids, 4, 2), where n_centroids is the number\n            of centroids, and the second dimension represents the four corner points of\n            the bounding boxes, each with x and y coordinates. The order of the corners\n            follows a clockwise arrangement: top-left, top-right, bottom-right, and\n            bottom-left.\n    \"\"\"\n    half_h = box_height / 2\n    half_w = box_width / 2\n\n    # Get x and y values from the centroids tensor.\n    x = centroids[..., 0]\n    y = centroids[..., 1]\n\n    # Calculate the corner points.\n    top_left = torch.stack([x - half_w, y - half_h], dim=-1)\n    top_right = torch.stack([x + half_w, y - half_h], dim=-1)\n    bottom_left = torch.stack([x - half_w, y + half_h], dim=-1)\n    bottom_right = torch.stack([x + half_w, y + half_h], dim=-1)\n\n    # Get bounding box.\n    corners = torch.stack([top_left, top_right, bottom_right, bottom_left], dim=-2)\n\n    offset = torch.tensor([[+0.5, +0.5], [-0.5, +0.5], [-0.5, -0.5], [+0.5, -0.5]]).to(\n        corners.device\n    )\n\n    return corners + offset\n</code></pre>"},{"location":"api/data/normalization/","title":"normalization","text":""},{"location":"api/data/normalization/#sleap_nn.data.normalization","title":"<code>sleap_nn.data.normalization</code>","text":"<p>This module implements data pipeline blocks for normalization operations.</p> <p>Functions:</p> Name Description <code>apply_normalization</code> <p>Normalize image tensor from uint8 [0, 255] to float32 [0, 1].</p> <code>convert_to_grayscale</code> <p>Convert given image to Grayscale image (single-channel).</p> <code>convert_to_rgb</code> <p>Convert given image to RGB image (three-channel image).</p> <code>normalize_on_gpu</code> <p>Normalize image tensor on GPU after transfer.</p>"},{"location":"api/data/normalization/#sleap_nn.data.normalization.apply_normalization","title":"<code>apply_normalization(image)</code>","text":"<p>Normalize image tensor from uint8 [0, 255] to float32 [0, 1].</p> <p>This function is used during training data preprocessing where augmentation operations (kornia) require float32 input.</p> <p>For inference, normalization is deferred to GPU via <code>normalize_on_gpu()</code> in the model's forward() method to reduce PCIe bandwidth.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Tensor image (typically uint8 with values in [0, 255]).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Float32 tensor normalized to [0, 1] range.</p> Source code in <code>sleap_nn/data/normalization.py</code> <pre><code>def apply_normalization(image: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Normalize image tensor from uint8 [0, 255] to float32 [0, 1].\n\n    This function is used during training data preprocessing where augmentation\n    operations (kornia) require float32 input.\n\n    For inference, normalization is deferred to GPU via `normalize_on_gpu()` in the\n    model's forward() method to reduce PCIe bandwidth.\n\n    Args:\n        image: Tensor image (typically uint8 with values in [0, 255]).\n\n    Returns:\n        Float32 tensor normalized to [0, 1] range.\n    \"\"\"\n    if not torch.is_floating_point(image):\n        image = image.to(torch.float32) / 255.0\n    return image\n</code></pre>"},{"location":"api/data/normalization/#sleap_nn.data.normalization.convert_to_grayscale","title":"<code>convert_to_grayscale(image)</code>","text":"<p>Convert given image to Grayscale image (single-channel).</p> <p>This functions converts the input image to grayscale only if the given image is not a single-channeled image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Tensor image of shape (..., 3, H, W)</p> required <p>Returns:</p> Type Description <p>Tensor image of shape (..., 1, H, W).</p> Source code in <code>sleap_nn/data/normalization.py</code> <pre><code>def convert_to_grayscale(image: torch.Tensor):\n    \"\"\"Convert given image to Grayscale image (single-channel).\n\n    This functions converts the input image to grayscale only if the given image is not\n    a single-channeled image.\n\n    Args:\n        image: Tensor image of shape (..., 3, H, W)\n\n    Returns:\n        Tensor image of shape (..., 1, H, W).\n    \"\"\"\n    if image.shape[-3] != 1:\n        image = F.rgb_to_grayscale(image, num_output_channels=1)\n    return image\n</code></pre>"},{"location":"api/data/normalization/#sleap_nn.data.normalization.convert_to_rgb","title":"<code>convert_to_rgb(image)</code>","text":"<p>Convert given image to RGB image (three-channel image).</p> <p>This functions converts the input image to RGB only if the given image is not a RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Tensor image of shape (..., 1, H, W)</p> required <p>Returns:</p> Type Description <p>Tensor image of shape (..., 3, H, W).</p> Source code in <code>sleap_nn/data/normalization.py</code> <pre><code>def convert_to_rgb(image: torch.Tensor):\n    \"\"\"Convert given image to RGB image (three-channel image).\n\n    This functions converts the input image to RGB only if the given image is not\n    a RGB image.\n\n    Args:\n        image: Tensor image of shape (..., 1, H, W)\n\n    Returns:\n        Tensor image of shape (..., 3, H, W).\n    \"\"\"\n    if image.shape[-3] != 3:\n        image = image.repeat(1, 3, 1, 1)\n    return image\n</code></pre>"},{"location":"api/data/normalization/#sleap_nn.data.normalization.normalize_on_gpu","title":"<code>normalize_on_gpu(image)</code>","text":"<p>Normalize image tensor on GPU after transfer.</p> <p>This function is called in the model's forward() method after the image has been transferred to GPU. It converts uint8 images to float32 and normalizes to [0, 1].</p> <p>By performing normalization on GPU after transfer, we reduce PCIe bandwidth by 4x (transferring 1 byte/pixel as uint8 instead of 4 bytes/pixel as float32). This provides up to 17x speedup for the transfer+normalization stage.</p> <p>This function handles two cases: 1. uint8 tensor with values in [0, 255] -&gt; convert to float32 and divide by 255 2. float32 tensor with values in [0, 255] (e.g., from preprocessing that cast to    float32 without normalizing) -&gt; divide by 255</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Tensor image that may be uint8 or float32 with values in [0, 255] range.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Float32 tensor normalized to [0, 1] range.</p> Source code in <code>sleap_nn/data/normalization.py</code> <pre><code>def normalize_on_gpu(image: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Normalize image tensor on GPU after transfer.\n\n    This function is called in the model's forward() method after the image has been\n    transferred to GPU. It converts uint8 images to float32 and normalizes to [0, 1].\n\n    By performing normalization on GPU after transfer, we reduce PCIe bandwidth by 4x\n    (transferring 1 byte/pixel as uint8 instead of 4 bytes/pixel as float32). This\n    provides up to 17x speedup for the transfer+normalization stage.\n\n    This function handles two cases:\n    1. uint8 tensor with values in [0, 255] -&gt; convert to float32 and divide by 255\n    2. float32 tensor with values in [0, 255] (e.g., from preprocessing that cast to\n       float32 without normalizing) -&gt; divide by 255\n\n    Args:\n        image: Tensor image that may be uint8 or float32 with values in [0, 255] range.\n\n    Returns:\n        Float32 tensor normalized to [0, 1] range.\n    \"\"\"\n    if not torch.is_floating_point(image):\n        # uint8 -&gt; float32 normalized\n        image = image.float() / 255.0\n    elif image.max() &gt; 1.0:\n        # float32 but not normalized (values &gt; 1 indicate [0, 255] range)\n        image = image / 255.0\n    return image\n</code></pre>"},{"location":"api/data/providers/","title":"providers","text":""},{"location":"api/data/providers/#sleap_nn.data.providers","title":"<code>sleap_nn.data.providers</code>","text":"<p>This module implements pipeline blocks for reading input data such as labels.</p> <p>Classes:</p> Name Description <code>LabelsReader</code> <p>Thread module for reading images from sleap-io Labels object.</p> <code>VideoReader</code> <p>Thread module for reading frames from sleap-io Video object.</p> <p>Functions:</p> Name Description <code>get_max_height_width</code> <p>Return <code>(height, width)</code> that is the maximum of all videos.</p> <code>get_max_instances</code> <p>Get the maximum number of instances in a single LabeledFrame.</p> <code>process_lf</code> <p>Get sample dict from <code>sio.LabeledFrame</code>.</p>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader","title":"<code>LabelsReader</code>","text":"<p>               Bases: <code>Thread</code></p> <p>Thread module for reading images from sleap-io Labels object.</p> <p>This module will load the images from <code>.slp</code> files and pushes them as Tensors into a buffer queue as a dictionary with (image, frame index, video index, (height, width)) which are then batched and consumed during the inference process.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <p>sleap_io.Labels object that contains LabeledFrames that will be     accessed through a torchdata DataPipe.</p> <code>frame_buffer</code> <p>Frame buffer queue.</p> <code>instances_key</code> <p>If <code>True</code>, then instances are appended to the output dictionary.</p> <code>only_labeled_frames</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>only_suggested_frames</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attribute of the class.</p> <code>from_filename</code> <p>Create LabelsReader from a .slp filename.</p> <code>run</code> <p>Adds frames to the buffer queue.</p> <code>total_len</code> <p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>class LabelsReader(Thread):\n    \"\"\"Thread module for reading images from sleap-io Labels object.\n\n    This module will load the images from `.slp` files and pushes them as Tensors into a\n    buffer queue as a dictionary with (image, frame index, video index, (height, width))\n    which are then batched and consumed during the inference process.\n\n    Attributes:\n        labels: sleap_io.Labels object that contains LabeledFrames that will be\n                accessed through a torchdata DataPipe.\n        frame_buffer: Frame buffer queue.\n        instances_key: If `True`, then instances are appended to the output dictionary.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n    \"\"\"\n\n    def __init__(\n        self,\n        labels: sio.Labels,\n        frame_buffer: Queue,\n        instances_key: bool = False,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n    ):\n        \"\"\"Initialize attribute of the class.\"\"\"\n        super().__init__()\n        self.labels = labels\n        self.frame_buffer = frame_buffer\n        self.instances_key = instances_key\n        self.max_instances = get_max_instances(self.labels)\n\n        self._daemonic = True  # needs to be set to True for graceful stop; all threads should be killed when main thread is killed\n\n        self.only_labeled_frames = only_labeled_frames\n        self.only_suggested_frames = only_suggested_frames\n        self.exclude_user_labeled = exclude_user_labeled\n        self.only_predicted_frames = only_predicted_frames\n\n        # Filter to only user labeled instances\n        if self.only_labeled_frames:\n            self.filtered_lfs = []\n            for lf in self.labels:\n                if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                    lf.instances = lf.user_instances\n                    self.filtered_lfs.append(lf)\n\n        # Filter to only unlabeled suggested instances\n        elif self.only_suggested_frames:\n            self.filtered_lfs = []\n            for suggestion in self.labels.suggestions:\n                lf = self.labels.find(suggestion.video, suggestion.frame_idx)\n                if len(lf) == 0 or not lf[0].has_user_instances:\n                    new_lf = sio.LabeledFrame(\n                        video=suggestion.video, frame_idx=suggestion.frame_idx\n                    )\n                    self.filtered_lfs.append(new_lf)\n\n        # Filter out user labeled frames\n        elif self.exclude_user_labeled:\n            self.filtered_lfs = []\n            for lf in self.labels:\n                if not lf.has_user_instances:\n                    self.filtered_lfs.append(lf)\n\n        # Filter to only predicted frames\n        elif self.only_predicted_frames:\n            self.filtered_lfs = []\n            for lf in self.labels:\n                if lf.has_predicted_instances:\n                    self.filtered_lfs.append(lf)\n\n        else:\n            self.filtered_lfs = [lf for lf in self.labels]\n\n        # Close the backend\n        self.local_video_copy = []\n        for video in self.labels.videos:\n            video.close()\n            self.backend_status = video.open_backend\n            video.open_backend = False\n\n            # make a thread-local copy\n            self.local_video_copy.append(deepcopy(video))\n\n            # Set it to open the backend on first read\n            self.local_video_copy[-1].open_backend = True\n\n    def total_len(self):\n        \"\"\"Returns the total number of frames in the video.\"\"\"\n        return len(self.filtered_lfs)\n\n    @property\n    def max_height_and_width(self) -&gt; Tuple[int, int]:\n        \"\"\"Return `(height, width)` of frames in the video.\"\"\"\n        return max(video.shape[1] for video in self.labels.videos), max(\n            video.shape[2] for video in self.labels.videos\n        )\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename: str,\n        queue_maxsize: int,\n        instances_key: bool = False,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n    ):\n        \"\"\"Create LabelsReader from a .slp filename.\"\"\"\n        labels = sio.load_slp(filename)\n        frame_buffer = Queue(maxsize=queue_maxsize)\n        return cls(\n            labels,\n            frame_buffer,\n            instances_key,\n            only_labeled_frames,\n            only_suggested_frames,\n            exclude_user_labeled,\n            only_predicted_frames,\n        )\n\n    def run(self):\n        \"\"\"Adds frames to the buffer queue.\"\"\"\n        try:\n            for lf in self.filtered_lfs:\n                video_idx = self.labels.videos.index(lf.video)\n                img = self.local_video_copy[video_idx][lf.frame_idx]\n                img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n                img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n                sample = {\n                    \"image\": torch.from_numpy(img.copy()),\n                    \"frame_idx\": torch.tensor(lf.frame_idx, dtype=torch.int32),\n                    \"video_idx\": torch.tensor(video_idx, dtype=torch.int32),\n                    \"orig_size\": torch.Tensor(img.shape[-2:]).unsqueeze(0),\n                }\n\n                if self.instances_key:\n                    instances = []\n                    for inst in lf:\n                        if not inst.is_empty:\n                            instances.append(inst.numpy())\n                    if len(instances) == 0:\n                        continue\n                    instances = np.stack(instances, axis=0)\n\n                    # Add singleton time dimension for single frames.\n                    instances = np.expand_dims(\n                        instances, axis=0\n                    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n                    instances = torch.from_numpy(instances.astype(\"float32\"))\n\n                    num_instances, nodes = instances.shape[1:3]\n\n                    # append with nans for broadcasting\n                    if self.max_instances != 1:\n                        nans = torch.full(\n                            (1, np.abs(self.max_instances - num_instances), nodes, 2),\n                            torch.nan,\n                        )\n                        instances = torch.cat(\n                            [instances, nans], dim=1\n                        )  # (n_samples, max_instances, num_nodes, 2)\n\n                    sample[\"instances\"] = instances\n\n                self.frame_buffer.put(sample)\n\n        except Exception as e:\n            logger.error(\n                f\"Error when reading labelled frame. Stopping labels reader.\\n{e}\"\n            )\n\n        finally:\n            self.frame_buffer.put(\n                {\n                    \"image\": None,\n                    \"frame_idx\": None,\n                    \"video_idx\": None,\n                    \"orig_size\": None,\n                }\n            )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.max_height_and_width","title":"<code>max_height_and_width</code>  <code>property</code>","text":"<p>Return <code>(height, width)</code> of frames in the video.</p>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.__init__","title":"<code>__init__(labels, frame_buffer, instances_key=False, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False)</code>","text":"<p>Initialize attribute of the class.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def __init__(\n    self,\n    labels: sio.Labels,\n    frame_buffer: Queue,\n    instances_key: bool = False,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n):\n    \"\"\"Initialize attribute of the class.\"\"\"\n    super().__init__()\n    self.labels = labels\n    self.frame_buffer = frame_buffer\n    self.instances_key = instances_key\n    self.max_instances = get_max_instances(self.labels)\n\n    self._daemonic = True  # needs to be set to True for graceful stop; all threads should be killed when main thread is killed\n\n    self.only_labeled_frames = only_labeled_frames\n    self.only_suggested_frames = only_suggested_frames\n    self.exclude_user_labeled = exclude_user_labeled\n    self.only_predicted_frames = only_predicted_frames\n\n    # Filter to only user labeled instances\n    if self.only_labeled_frames:\n        self.filtered_lfs = []\n        for lf in self.labels:\n            if lf.user_instances is not None and len(lf.user_instances) &gt; 0:\n                lf.instances = lf.user_instances\n                self.filtered_lfs.append(lf)\n\n    # Filter to only unlabeled suggested instances\n    elif self.only_suggested_frames:\n        self.filtered_lfs = []\n        for suggestion in self.labels.suggestions:\n            lf = self.labels.find(suggestion.video, suggestion.frame_idx)\n            if len(lf) == 0 or not lf[0].has_user_instances:\n                new_lf = sio.LabeledFrame(\n                    video=suggestion.video, frame_idx=suggestion.frame_idx\n                )\n                self.filtered_lfs.append(new_lf)\n\n    # Filter out user labeled frames\n    elif self.exclude_user_labeled:\n        self.filtered_lfs = []\n        for lf in self.labels:\n            if not lf.has_user_instances:\n                self.filtered_lfs.append(lf)\n\n    # Filter to only predicted frames\n    elif self.only_predicted_frames:\n        self.filtered_lfs = []\n        for lf in self.labels:\n            if lf.has_predicted_instances:\n                self.filtered_lfs.append(lf)\n\n    else:\n        self.filtered_lfs = [lf for lf in self.labels]\n\n    # Close the backend\n    self.local_video_copy = []\n    for video in self.labels.videos:\n        video.close()\n        self.backend_status = video.open_backend\n        video.open_backend = False\n\n        # make a thread-local copy\n        self.local_video_copy.append(deepcopy(video))\n\n        # Set it to open the backend on first read\n        self.local_video_copy[-1].open_backend = True\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.from_filename","title":"<code>from_filename(filename, queue_maxsize, instances_key=False, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False)</code>  <code>classmethod</code>","text":"<p>Create LabelsReader from a .slp filename.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>@classmethod\ndef from_filename(\n    cls,\n    filename: str,\n    queue_maxsize: int,\n    instances_key: bool = False,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n):\n    \"\"\"Create LabelsReader from a .slp filename.\"\"\"\n    labels = sio.load_slp(filename)\n    frame_buffer = Queue(maxsize=queue_maxsize)\n    return cls(\n        labels,\n        frame_buffer,\n        instances_key,\n        only_labeled_frames,\n        only_suggested_frames,\n        exclude_user_labeled,\n        only_predicted_frames,\n    )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.run","title":"<code>run()</code>","text":"<p>Adds frames to the buffer queue.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def run(self):\n    \"\"\"Adds frames to the buffer queue.\"\"\"\n    try:\n        for lf in self.filtered_lfs:\n            video_idx = self.labels.videos.index(lf.video)\n            img = self.local_video_copy[video_idx][lf.frame_idx]\n            img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n            img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n            sample = {\n                \"image\": torch.from_numpy(img.copy()),\n                \"frame_idx\": torch.tensor(lf.frame_idx, dtype=torch.int32),\n                \"video_idx\": torch.tensor(video_idx, dtype=torch.int32),\n                \"orig_size\": torch.Tensor(img.shape[-2:]).unsqueeze(0),\n            }\n\n            if self.instances_key:\n                instances = []\n                for inst in lf:\n                    if not inst.is_empty:\n                        instances.append(inst.numpy())\n                if len(instances) == 0:\n                    continue\n                instances = np.stack(instances, axis=0)\n\n                # Add singleton time dimension for single frames.\n                instances = np.expand_dims(\n                    instances, axis=0\n                )  # (n_samples=1, num_instances, num_nodes, 2)\n\n                instances = torch.from_numpy(instances.astype(\"float32\"))\n\n                num_instances, nodes = instances.shape[1:3]\n\n                # append with nans for broadcasting\n                if self.max_instances != 1:\n                    nans = torch.full(\n                        (1, np.abs(self.max_instances - num_instances), nodes, 2),\n                        torch.nan,\n                    )\n                    instances = torch.cat(\n                        [instances, nans], dim=1\n                    )  # (n_samples, max_instances, num_nodes, 2)\n\n                sample[\"instances\"] = instances\n\n            self.frame_buffer.put(sample)\n\n    except Exception as e:\n        logger.error(\n            f\"Error when reading labelled frame. Stopping labels reader.\\n{e}\"\n        )\n\n    finally:\n        self.frame_buffer.put(\n            {\n                \"image\": None,\n                \"frame_idx\": None,\n                \"video_idx\": None,\n                \"orig_size\": None,\n            }\n        )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.LabelsReader.total_len","title":"<code>total_len()</code>","text":"<p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def total_len(self):\n    \"\"\"Returns the total number of frames in the video.\"\"\"\n    return len(self.filtered_lfs)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader","title":"<code>VideoReader</code>","text":"<p>               Bases: <code>Thread</code></p> <p>Thread module for reading frames from sleap-io Video object.</p> <p>This module will load the frames from video and pushes them as Tensors into a buffer queue as a dictionary with (image, frame index, video index, (height, width)) which are then batched and consumed during the inference process.</p> <p>Attributes:</p> Name Type Description <code>video</code> <p>sleap_io.Video object that contains images that will be     accessed through a torchdata DataPipe.</p> <code>frame_buffer</code> <p>Frame buffer queue.</p> <code>frames</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attribute of the class.</p> <code>from_filename</code> <p>Create VideoReader from a .slp filename.</p> <code>from_video</code> <p>Create VideoReader from a video object.</p> <code>run</code> <p>Adds frames to the buffer queue.</p> <code>total_len</code> <p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>class VideoReader(Thread):\n    \"\"\"Thread module for reading frames from sleap-io Video object.\n\n    This module will load the frames from video and pushes them as Tensors into a buffer\n    queue as a dictionary with (image, frame index, video index, (height, width))\n    which are then batched and consumed during the inference process.\n\n    Attributes:\n        video: sleap_io.Video object that contains images that will be\n                accessed through a torchdata DataPipe.\n        frame_buffer: Frame buffer queue.\n        frames: List of frames indices. If `None`, all frames in the video are used.\n    \"\"\"\n\n    def __init__(\n        self,\n        video: sio.Video,\n        frame_buffer: Queue,\n        frames: Optional[list] = None,\n    ):\n        \"\"\"Initialize attribute of the class.\"\"\"\n        super().__init__()\n        self.video = video\n        self.frame_buffer = frame_buffer\n        self.frames = frames\n        self._daemonic = True  # needs to be set to True for graceful stop; all threads should be killed when main thread is killed\n        if self.frames is None:\n            self.frames = [x for x in range(0, len(self.video))]\n\n        # Close the backend\n        self.video.close()\n        self.backend_status = self.video.open_backend\n        self.video.open_backend = False\n\n        # Make a thread-local copy\n        self.local_video_copy = deepcopy(self.video)\n\n        # Set it to open the backend on first read\n        self.local_video_copy.open_backend = True\n\n    def total_len(self):\n        \"\"\"Returns the total number of frames in the video.\"\"\"\n        return len(self.frames)\n\n    @property\n    def max_height_and_width(self) -&gt; Tuple[int, int]:\n        \"\"\"Return `(height, width)` of frames in the video.\"\"\"\n        return self.video.shape[1], self.video.shape[2]\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename: str,\n        queue_maxsize: int,\n        frames: Optional[list] = None,\n        dataset: Optional[str] = None,\n        input_format: str = \"channels_last\",\n    ):\n        \"\"\"Create VideoReader from a .slp filename.\"\"\"\n        video = sio.load_video(filename, dataset=dataset, input_format=input_format)\n        frame_buffer = Queue(maxsize=queue_maxsize)\n        return cls(video, frame_buffer, frames)\n\n    @classmethod\n    def from_video(\n        cls,\n        video: sio.Video,\n        queue_maxsize: int,\n        frames: Optional[list] = None,\n    ):\n        \"\"\"Create VideoReader from a video object.\"\"\"\n        frame_buffer = Queue(maxsize=queue_maxsize)\n        return cls(video, frame_buffer, frames)\n\n    def run(self):\n        \"\"\"Adds frames to the buffer queue.\"\"\"\n        try:\n            for idx in self.frames:\n                img = self.local_video_copy[idx]\n                img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n                img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n                self.frame_buffer.put(\n                    {\n                        \"image\": torch.from_numpy(img.copy()),\n                        \"frame_idx\": torch.tensor(idx, dtype=torch.int32),\n                        \"video_idx\": torch.tensor(0, dtype=torch.int32),\n                        \"orig_size\": torch.Tensor(img.shape[-2:]).unsqueeze(0),\n                    }\n                )\n\n        except Exception as e:\n            logger.error(f\"Error when reading video frame. Stopping video reader.\\n{e}\")\n\n        finally:\n            self.frame_buffer.put(\n                {\n                    \"image\": None,\n                    \"frame_idx\": None,\n                    \"video_idx\": None,\n                    \"orig_size\": None,\n                }\n            )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.max_height_and_width","title":"<code>max_height_and_width</code>  <code>property</code>","text":"<p>Return <code>(height, width)</code> of frames in the video.</p>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.__init__","title":"<code>__init__(video, frame_buffer, frames=None)</code>","text":"<p>Initialize attribute of the class.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def __init__(\n    self,\n    video: sio.Video,\n    frame_buffer: Queue,\n    frames: Optional[list] = None,\n):\n    \"\"\"Initialize attribute of the class.\"\"\"\n    super().__init__()\n    self.video = video\n    self.frame_buffer = frame_buffer\n    self.frames = frames\n    self._daemonic = True  # needs to be set to True for graceful stop; all threads should be killed when main thread is killed\n    if self.frames is None:\n        self.frames = [x for x in range(0, len(self.video))]\n\n    # Close the backend\n    self.video.close()\n    self.backend_status = self.video.open_backend\n    self.video.open_backend = False\n\n    # Make a thread-local copy\n    self.local_video_copy = deepcopy(self.video)\n\n    # Set it to open the backend on first read\n    self.local_video_copy.open_backend = True\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.from_filename","title":"<code>from_filename(filename, queue_maxsize, frames=None, dataset=None, input_format='channels_last')</code>  <code>classmethod</code>","text":"<p>Create VideoReader from a .slp filename.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>@classmethod\ndef from_filename(\n    cls,\n    filename: str,\n    queue_maxsize: int,\n    frames: Optional[list] = None,\n    dataset: Optional[str] = None,\n    input_format: str = \"channels_last\",\n):\n    \"\"\"Create VideoReader from a .slp filename.\"\"\"\n    video = sio.load_video(filename, dataset=dataset, input_format=input_format)\n    frame_buffer = Queue(maxsize=queue_maxsize)\n    return cls(video, frame_buffer, frames)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.from_video","title":"<code>from_video(video, queue_maxsize, frames=None)</code>  <code>classmethod</code>","text":"<p>Create VideoReader from a video object.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>@classmethod\ndef from_video(\n    cls,\n    video: sio.Video,\n    queue_maxsize: int,\n    frames: Optional[list] = None,\n):\n    \"\"\"Create VideoReader from a video object.\"\"\"\n    frame_buffer = Queue(maxsize=queue_maxsize)\n    return cls(video, frame_buffer, frames)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.run","title":"<code>run()</code>","text":"<p>Adds frames to the buffer queue.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def run(self):\n    \"\"\"Adds frames to the buffer queue.\"\"\"\n    try:\n        for idx in self.frames:\n            img = self.local_video_copy[idx]\n            img = np.transpose(img, (2, 0, 1))  # convert H,W,C to C,H,W\n            img = np.expand_dims(img, axis=0)  # (1, C, H, W)\n\n            self.frame_buffer.put(\n                {\n                    \"image\": torch.from_numpy(img.copy()),\n                    \"frame_idx\": torch.tensor(idx, dtype=torch.int32),\n                    \"video_idx\": torch.tensor(0, dtype=torch.int32),\n                    \"orig_size\": torch.Tensor(img.shape[-2:]).unsqueeze(0),\n                }\n            )\n\n    except Exception as e:\n        logger.error(f\"Error when reading video frame. Stopping video reader.\\n{e}\")\n\n    finally:\n        self.frame_buffer.put(\n            {\n                \"image\": None,\n                \"frame_idx\": None,\n                \"video_idx\": None,\n                \"orig_size\": None,\n            }\n        )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.VideoReader.total_len","title":"<code>total_len()</code>","text":"<p>Returns the total number of frames in the video.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def total_len(self):\n    \"\"\"Returns the total number of frames in the video.\"\"\"\n    return len(self.frames)\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.get_max_height_width","title":"<code>get_max_height_width(labels)</code>","text":"<p>Return <code>(height, width)</code> that is the maximum of all videos.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def get_max_height_width(labels: sio.Labels) -&gt; Tuple[int, int]:\n    \"\"\"Return `(height, width)` that is the maximum of all videos.\"\"\"\n    return int(max(video.shape[1] for video in labels.videos)), int(\n        max(video.shape[2] for video in labels.videos)\n    )\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.get_max_instances","title":"<code>get_max_instances(labels)</code>","text":"<p>Get the maximum number of instances in a single LabeledFrame.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>sleap_io.Labels object that contains LabeledFrames.</p> required <p>Returns:</p> Type Description <p>Maximum number of instances that could occur in a single LabeledFrame.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def get_max_instances(labels: sio.Labels):\n    \"\"\"Get the maximum number of instances in a single LabeledFrame.\n\n    Args:\n        labels: sleap_io.Labels object that contains LabeledFrames.\n\n    Returns:\n        Maximum number of instances that could occur in a single LabeledFrame.\n    \"\"\"\n    max_instances = -1\n    for lf in labels:\n        num_inst = len(lf.instances)\n        if num_inst &gt; max_instances:\n            max_instances = num_inst\n    return max_instances\n</code></pre>"},{"location":"api/data/providers/#sleap_nn.data.providers.process_lf","title":"<code>process_lf(instances_list, img, frame_idx, video_idx, max_instances, user_instances_only=True)</code>","text":"<p>Get sample dict from <code>sio.LabeledFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instances_list</code> <code>List[Instance]</code> <p>List of <code>sio.Instance</code> objects.</p> required <code>img</code> <code>ndarray</code> <p>Input image.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index of the given lf.</p> required <code>video_idx</code> <code>int</code> <p>Video index of the given lf.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances that could occur in a single LabeledFrame.</p> required <code>user_instances_only</code> <code>bool</code> <p>True if filter labels only to user instances else False. Default: True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with image, instancs, frame index, video index, original image size and number of instances.</p> Source code in <code>sleap_nn/data/providers.py</code> <pre><code>def process_lf(\n    instances_list: List[sio.Instance],\n    img: np.ndarray,\n    frame_idx: int,\n    video_idx: int,\n    max_instances: int,\n    user_instances_only: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"Get sample dict from `sio.LabeledFrame`.\n\n    Args:\n        instances_list: List of `sio.Instance` objects.\n        img: Input image.\n        frame_idx: Frame index of the given lf.\n        video_idx: Video index of the given lf.\n        max_instances: Maximum number of instances that could occur in a single LabeledFrame.\n        user_instances_only: True if filter labels only to user instances else False.\n            Default: True.\n\n    Returns:\n        Dict with image, instancs, frame index, video index, original image size and\n        number of instances.\n\n    \"\"\"\n    # Filter to user instances\n    if user_instances_only:\n        user_instances = [inst for inst in instances_list if type(inst) is sio.Instance]\n        if len(user_instances) &gt; 0:\n            instances_list = user_instances\n\n    image = np.transpose(img, (2, 0, 1))  # HWC -&gt; CHW\n\n    instances = []\n    for inst in instances_list:\n        if not inst.is_empty:\n            instances.append(inst.numpy())\n    if len(instances) == 0:\n        return None\n    instances = np.stack(instances, axis=0)\n\n    # Add singleton time dimension for single frames.\n    image = np.expand_dims(image, axis=0)  # (n_samples=1, C, H, W)\n    instances = np.expand_dims(\n        instances, axis=0\n    )  # (n_samples=1, num_instances, num_nodes, 2)\n\n    instances = torch.from_numpy(instances.astype(\"float32\"))\n\n    num_instances, nodes = instances.shape[1:3]\n    img_height, img_width = image.shape[-2:]\n\n    # append with nans for broadcasting\n    if max_instances != 1:\n        nans = torch.full(\n            (1, np.abs(max_instances - num_instances), nodes, 2), torch.nan\n        )\n        instances = torch.cat(\n            [instances, nans], dim=1\n        )  # (n_samples, max_instances, num_nodes, 2)\n\n    ex = {\n        \"image\": torch.from_numpy(image.copy()),\n        \"instances\": instances,\n        \"video_idx\": torch.tensor(video_idx, dtype=torch.int32),\n        \"frame_idx\": torch.tensor(frame_idx, dtype=torch.int32),\n        \"orig_size\": torch.Tensor([img_height, img_width]).unsqueeze(0),\n        \"num_instances\": num_instances,\n    }\n\n    return ex\n</code></pre>"},{"location":"api/data/resizing/","title":"resizing","text":""},{"location":"api/data/resizing/#sleap_nn.data.resizing","title":"<code>sleap_nn.data.resizing</code>","text":"<p>This module implements image resizing and padding.</p> <p>Functions:</p> Name Description <code>apply_pad_to_stride</code> <p>Pad an image to meet a max stride constraint.</p> <code>apply_resizer</code> <p>Rescale image and keypoints by a scale factor.</p> <code>apply_sizematcher</code> <p>Apply scaling and padding to image to (max_height, max_width) shape.</p> <code>find_padding_for_stride</code> <p>Compute padding required to ensure image is divisible by a stride.</p> <code>resize_image</code> <p>Rescale an image by a scale factor.</p>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.apply_pad_to_stride","title":"<code>apply_pad_to_stride(image, max_stride)</code>","text":"<p>Pad an image to meet a max stride constraint.</p> <p>This is useful for ensuring there is no size mismatch between an image and the output tensors after multiple downsampling and upsampling steps.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Single image tensor of shape (..., channels, height, width).</p> required <code>max_stride</code> <code>int</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by. This is the ratio between the length of the image and the length of the smallest tensor it is converted to. This is typically <code>2 ** n_down_blocks</code>, where <code>n_down_blocks</code> is the number of 2-strided reduction layers in the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The input image with 0-padding applied to the bottom and/or right such that the new shape's height and width are both divisible by <code>max_stride</code>.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def apply_pad_to_stride(image: torch.Tensor, max_stride: int) -&gt; torch.Tensor:\n    \"\"\"Pad an image to meet a max stride constraint.\n\n    This is useful for ensuring there is no size mismatch between an image and the\n    output tensors after multiple downsampling and upsampling steps.\n\n    Args:\n        image: Single image tensor of shape (..., channels, height, width).\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by. This is the ratio between the length of the image and the\n            length of the smallest tensor it is converted to. This is typically\n            `2 ** n_down_blocks`, where `n_down_blocks` is the number of 2-strided\n            reduction layers in the model.\n\n    Returns:\n        The input image with 0-padding applied to the bottom and/or right such that the\n        new shape's height and width are both divisible by `max_stride`.\n    \"\"\"\n    if max_stride &gt; 1:\n        image_height, image_width = image.shape[-2:]\n        pad_height, pad_width = find_padding_for_stride(\n            image_height=image_height,\n            image_width=image_width,\n            max_stride=max_stride,\n        )\n\n        if pad_height &gt; 0 or pad_width &gt; 0:\n            image = F.pad(\n                image,\n                (0, pad_width, 0, pad_height),\n                mode=\"constant\",\n            )\n    return image\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.apply_resizer","title":"<code>apply_resizer(image, instances, scale=1.0)</code>","text":"<p>Rescale image and keypoints by a scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Image tensor of shape (..., channels, height, width)</p> required <code>instances</code> <code>Tensor</code> <p>Keypoints tensor.</p> required <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float scalar. Default: 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>Tuple with resized image and corresponding keypoints.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def apply_resizer(image: torch.Tensor, instances: torch.Tensor, scale: float = 1.0):\n    \"\"\"Rescale image and keypoints by a scale factor.\n\n    Args:\n        image: Image tensor of shape (..., channels, height, width)\n        instances: Keypoints tensor.\n        scale: Factor to resize the image dimensions by, specified as a float\n            scalar. Default: 1.0.\n\n    Returns:\n        Tuple with resized image and corresponding keypoints.\n    \"\"\"\n    if scale != 1.0:\n        image = resize_image(image, scale)\n        instances = instances * scale\n    return image, instances\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.apply_sizematcher","title":"<code>apply_sizematcher(image, max_height=None, max_width=None)</code>","text":"<p>Apply scaling and padding to image to (max_height, max_width) shape.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def apply_sizematcher(\n    image: torch.Tensor,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n):\n    \"\"\"Apply scaling and padding to image to (max_height, max_width) shape.\"\"\"\n    img_height, img_width = image.shape[-2:]\n    # pad images to max_height and max_width\n    if max_height is None:\n        max_height = img_height\n    if max_width is None:\n        max_width = img_width\n    if img_height != max_height or img_width != max_width:\n        hratio = max_height / img_height\n        wratio = max_width / img_width\n\n        if hratio &gt; wratio:\n            eff_scale_ratio = wratio\n            target_h = int(round(img_height * wratio))\n            target_w = int(round(img_width * wratio))\n        else:\n            eff_scale_ratio = hratio\n            target_w = int(round(img_width * hratio))\n            target_h = int(round(img_height * hratio))\n\n        image = tvf.resize(image, size=(target_h, target_w))\n\n        pad_height = max_height - target_h\n        pad_width = max_width - target_w\n\n        image = F.pad(\n            image,\n            (0, pad_width, 0, pad_height),\n            mode=\"constant\",\n        )\n\n        return image, eff_scale_ratio\n    else:\n        return image, 1.0\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.find_padding_for_stride","title":"<code>find_padding_for_stride(image_height, image_width, max_stride)</code>","text":"<p>Compute padding required to ensure image is divisible by a stride.</p> <p>This function is useful for determining how to pad images such that they will not have issues with divisibility after repeated pooling steps.</p> <p>Parameters:</p> Name Type Description Default <code>image_height</code> <code>int</code> <p>Scalar integer specifying the image height (rows).</p> required <code>image_width</code> <code>int</code> <p>Scalar integer specifying the image height (columns).</p> required <code>max_stride</code> <code>int</code> <p>Scalar integer specifying the maximum stride that the image must be divisible by.</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>A tuple of (pad_height, pad_width), integers with the number of pixels that the image would need to be padded by to meet the divisibility requirement.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def find_padding_for_stride(\n    image_height: int, image_width: int, max_stride: int\n) -&gt; Tuple[int, int]:\n    \"\"\"Compute padding required to ensure image is divisible by a stride.\n\n    This function is useful for determining how to pad images such that they will not\n    have issues with divisibility after repeated pooling steps.\n\n    Args:\n        image_height: Scalar integer specifying the image height (rows).\n        image_width: Scalar integer specifying the image height (columns).\n        max_stride: Scalar integer specifying the maximum stride that the image must be\n            divisible by.\n\n    Returns:\n        A tuple of (pad_height, pad_width), integers with the number of pixels that the\n        image would need to be padded by to meet the divisibility requirement.\n    \"\"\"\n    # The outer-most modulo handles edge case when image_height % max_stride == 0\n    pad_height = (max_stride - (image_height % max_stride)) % max_stride\n    pad_width = (max_stride - (image_width % max_stride)) % max_stride\n    return pad_height, pad_width\n</code></pre>"},{"location":"api/data/resizing/#sleap_nn.data.resizing.resize_image","title":"<code>resize_image(image, scale)</code>","text":"<p>Rescale an image by a scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Single image tensor of shape (..., channels, height, width).</p> required <code>scale</code> <code>float</code> <p>Factor to resize the image dimensions by, specified as a float scalar.</p> required <p>Returns:</p> Type Description <p>The resized image tensor of the same dtype but scaled height and width.</p> Source code in <code>sleap_nn/data/resizing.py</code> <pre><code>def resize_image(image: torch.Tensor, scale: float):\n    \"\"\"Rescale an image by a scale factor.\n\n    Args:\n        image: Single image tensor of shape (..., channels, height, width).\n        scale: Factor to resize the image dimensions by, specified as a float\n            scalar.\n\n    Returns:\n        The resized image tensor of the same dtype but scaled height and width.\n    \"\"\"\n    img_height, img_width = image.shape[-2:]\n    new_size = [int(img_height * scale), int(img_width * scale)]\n    image = tvf.resize(image, size=new_size)\n    return image\n</code></pre>"},{"location":"api/data/skia_augmentation/","title":"skia_augmentation","text":""},{"location":"api/data/skia_augmentation/#sleap_nn.data.skia_augmentation","title":"<code>sleap_nn.data.skia_augmentation</code>","text":"<p>Skia-based augmentation functions that operate on uint8 tensors.</p> <p>This module provides augmentation functions using skia-python that: 1. Match the exact API of sleap_nn.data.augmentation 2. Operate on uint8 tensors throughout (avoiding float32 conversions) 3. Provide ~1.5x faster augmentation compared to Kornia</p> Usage <p>from sleap_nn.data.skia_augmentation import (     apply_intensity_augmentation_skia,     apply_geometric_augmentation_skia, )</p> <p>Functions:</p> Name Description <code>apply_geometric_augmentation_skia</code> <p>Apply geometric augmentations using Skia.</p> <code>apply_intensity_augmentation_skia</code> <p>Apply intensity augmentations on uint8 image tensor.</p> <code>crop_and_resize_skia</code> <p>Crop and resize image regions using Skia.</p>"},{"location":"api/data/skia_augmentation/#sleap_nn.data.skia_augmentation--apply-augmentations-uint8-in-uint8-out","title":"Apply augmentations (uint8 in, uint8 out)","text":"<p>image, instances = apply_intensity_augmentation_skia(image, instances, **config) image, instances = apply_geometric_augmentation_skia(image, instances, **config)</p>"},{"location":"api/data/skia_augmentation/#sleap_nn.data.skia_augmentation.apply_geometric_augmentation_skia","title":"<code>apply_geometric_augmentation_skia(image, instances, rotation_min=-15.0, rotation_max=15.0, rotation_p=None, scale_min=0.9, scale_max=1.1, scale_p=None, translate_width=0.02, translate_height=0.02, translate_p=None, affine_p=0.0, erase_scale_min=0.0001, erase_scale_max=0.01, erase_ratio_min=1.0, erase_ratio_max=1.0, erase_p=0.0, mixup_lambda_min=0.01, mixup_lambda_max=0.05, mixup_p=0.0)</code>","text":"<p>Apply geometric augmentations using Skia.</p> <p>Matches API of sleap_nn.data.augmentation.apply_geometric_augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (1, C, H, W) with dtype uint8 or float32.</p> required <code>instances</code> <code>Tensor</code> <p>Keypoints tensor of shape (1, n_instances, n_nodes, 2) or (1, n_nodes, 2).</p> required <code>rotation_min</code> <code>float</code> <p>Minimum rotation angle in degrees.</p> <code>-15.0</code> <code>rotation_max</code> <code>float</code> <p>Maximum rotation angle in degrees.</p> <code>15.0</code> <code>rotation_p</code> <code>Optional[float]</code> <p>Probability of rotation (independent). None = use affine_p.</p> <code>None</code> <code>scale_min</code> <code>float</code> <p>Minimum scale factor.</p> <code>0.9</code> <code>scale_max</code> <code>float</code> <p>Maximum scale factor.</p> <code>1.1</code> <code>scale_p</code> <code>Optional[float]</code> <p>Probability of scaling (independent). None = use affine_p.</p> <code>None</code> <code>translate_width</code> <code>float</code> <p>Max horizontal translation as fraction of width.</p> <code>0.02</code> <code>translate_height</code> <code>float</code> <p>Max vertical translation as fraction of height.</p> <code>0.02</code> <code>translate_p</code> <code>Optional[float]</code> <p>Probability of translation (independent). None = use affine_p.</p> <code>None</code> <code>affine_p</code> <code>float</code> <p>Probability of bundled affine transform.</p> <code>0.0</code> <code>erase_scale_min</code> <code>float</code> <p>Min proportion of image to erase.</p> <code>0.0001</code> <code>erase_scale_max</code> <code>float</code> <p>Max proportion of image to erase.</p> <code>0.01</code> <code>erase_ratio_min</code> <code>float</code> <p>Min aspect ratio of erased area.</p> <code>1.0</code> <code>erase_ratio_max</code> <code>float</code> <p>Max aspect ratio of erased area.</p> <code>1.0</code> <code>erase_p</code> <code>float</code> <p>Probability of random erasing.</p> <code>0.0</code> <code>mixup_lambda_min</code> <code>float</code> <p>Min mixup strength (not implemented).</p> <code>0.01</code> <code>mixup_lambda_max</code> <code>float</code> <p>Max mixup strength (not implemented).</p> <code>0.05</code> <code>mixup_p</code> <code>float</code> <p>Probability of mixup (not implemented).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (augmented_image, augmented_instances). Image dtype matches input.</p> Source code in <code>sleap_nn/data/skia_augmentation.py</code> <pre><code>def apply_geometric_augmentation_skia(\n    image: torch.Tensor,\n    instances: torch.Tensor,\n    rotation_min: float = -15.0,\n    rotation_max: float = 15.0,\n    rotation_p: Optional[float] = None,\n    scale_min: float = 0.9,\n    scale_max: float = 1.1,\n    scale_p: Optional[float] = None,\n    translate_width: float = 0.02,\n    translate_height: float = 0.02,\n    translate_p: Optional[float] = None,\n    affine_p: float = 0.0,\n    erase_scale_min: float = 0.0001,\n    erase_scale_max: float = 0.01,\n    erase_ratio_min: float = 1.0,\n    erase_ratio_max: float = 1.0,\n    erase_p: float = 0.0,\n    mixup_lambda_min: float = 0.01,\n    mixup_lambda_max: float = 0.05,\n    mixup_p: float = 0.0,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply geometric augmentations using Skia.\n\n    Matches API of sleap_nn.data.augmentation.apply_geometric_augmentation.\n\n    Args:\n        image: Input tensor of shape (1, C, H, W) with dtype uint8 or float32.\n        instances: Keypoints tensor of shape (1, n_instances, n_nodes, 2) or (1, n_nodes, 2).\n        rotation_min: Minimum rotation angle in degrees.\n        rotation_max: Maximum rotation angle in degrees.\n        rotation_p: Probability of rotation (independent). None = use affine_p.\n        scale_min: Minimum scale factor.\n        scale_max: Maximum scale factor.\n        scale_p: Probability of scaling (independent). None = use affine_p.\n        translate_width: Max horizontal translation as fraction of width.\n        translate_height: Max vertical translation as fraction of height.\n        translate_p: Probability of translation (independent). None = use affine_p.\n        affine_p: Probability of bundled affine transform.\n        erase_scale_min: Min proportion of image to erase.\n        erase_scale_max: Max proportion of image to erase.\n        erase_ratio_min: Min aspect ratio of erased area.\n        erase_ratio_max: Max aspect ratio of erased area.\n        erase_p: Probability of random erasing.\n        mixup_lambda_min: Min mixup strength (not implemented).\n        mixup_lambda_max: Max mixup strength (not implemented).\n        mixup_p: Probability of mixup (not implemented).\n\n    Returns:\n        Tuple of (augmented_image, augmented_instances). Image dtype matches input.\n    \"\"\"\n    # Convert to numpy for Skia processing\n    is_float = image.dtype == torch.float32\n    if is_float:\n        img_np = (image[0].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n    else:\n        img_np = image[0].permute(1, 2, 0).numpy().copy()\n\n    h, w = img_np.shape[:2]\n    cx, cy = w / 2, h / 2\n\n    # Build transformation matrix\n    matrix = skia.Matrix()\n    has_transform = False\n\n    use_independent = (\n        rotation_p is not None or scale_p is not None or translate_p is not None\n    )\n\n    if use_independent:\n        if (\n            rotation_p is not None\n            and rotation_p &gt; 0\n            and np.random.random() &lt; rotation_p\n        ):\n            angle = np.random.uniform(rotation_min, rotation_max)\n            rot_matrix = skia.Matrix()\n            rot_matrix.setRotate(angle, cx, cy)\n            matrix = matrix.preConcat(rot_matrix)\n            has_transform = True\n\n        if scale_p is not None and scale_p &gt; 0 and np.random.random() &lt; scale_p:\n            scale = np.random.uniform(scale_min, scale_max)\n            scale_matrix = skia.Matrix()\n            scale_matrix.setScale(scale, scale, cx, cy)\n            matrix = matrix.preConcat(scale_matrix)\n            has_transform = True\n\n        if (\n            translate_p is not None\n            and translate_p &gt; 0\n            and np.random.random() &lt; translate_p\n        ):\n            tx = np.random.uniform(-translate_width, translate_width) * w\n            ty = np.random.uniform(-translate_height, translate_height) * h\n            trans_matrix = skia.Matrix()\n            trans_matrix.setTranslate(tx, ty)\n            matrix = matrix.preConcat(trans_matrix)\n            has_transform = True\n\n    elif affine_p &gt; 0 and np.random.random() &lt; affine_p:\n        angle = np.random.uniform(rotation_min, rotation_max)\n        scale = np.random.uniform(scale_min, scale_max)\n        tx = np.random.uniform(-translate_width, translate_width) * w\n        ty = np.random.uniform(-translate_height, translate_height) * h\n\n        matrix.setRotate(angle, cx, cy)\n        matrix.preScale(scale, scale, cx, cy)\n        matrix.preTranslate(tx, ty)\n        has_transform = True\n\n    # Apply geometric transform\n    if has_transform:\n        img_np = _transform_image_skia(img_np, matrix)\n        instances = _transform_keypoints_tensor(instances, matrix)\n\n    # Apply random erasing\n    if erase_p &gt; 0 and np.random.random() &lt; erase_p:\n        img_np = _apply_random_erase(\n            img_np, erase_scale_min, erase_scale_max, erase_ratio_min, erase_ratio_max\n        )\n\n    # Convert back to tensor\n    result_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0)\n    if is_float:\n        result_tensor = result_tensor.float() / 255.0\n\n    return result_tensor, instances\n</code></pre>"},{"location":"api/data/skia_augmentation/#sleap_nn.data.skia_augmentation.apply_intensity_augmentation_skia","title":"<code>apply_intensity_augmentation_skia(image, instances, uniform_noise_min=0.0, uniform_noise_max=0.04, uniform_noise_p=0.0, gaussian_noise_mean=0.02, gaussian_noise_std=0.004, gaussian_noise_p=0.0, contrast_min=0.5, contrast_max=2.0, contrast_p=0.0, brightness_min=1.0, brightness_max=1.0, brightness_p=0.0)</code>","text":"<p>Apply intensity augmentations on uint8 image tensor.</p> <p>Matches API of sleap_nn.data.augmentation.apply_intensity_augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (1, C, H, W) with dtype uint8 or float32.</p> required <code>instances</code> <code>Tensor</code> <p>Keypoints tensor (not modified, just passed through).</p> required <code>uniform_noise_min</code> <code>float</code> <p>Minimum uniform noise (0-1 scale, maps to 0-255).</p> <code>0.0</code> <code>uniform_noise_max</code> <code>float</code> <p>Maximum uniform noise (0-1 scale).</p> <code>0.04</code> <code>uniform_noise_p</code> <code>float</code> <p>Probability of uniform noise.</p> <code>0.0</code> <code>gaussian_noise_mean</code> <code>float</code> <p>Gaussian noise mean (0-1 scale).</p> <code>0.02</code> <code>gaussian_noise_std</code> <code>float</code> <p>Gaussian noise std (0-1 scale).</p> <code>0.004</code> <code>gaussian_noise_p</code> <code>float</code> <p>Probability of Gaussian noise.</p> <code>0.0</code> <code>contrast_min</code> <code>float</code> <p>Minimum contrast factor.</p> <code>0.5</code> <code>contrast_max</code> <code>float</code> <p>Maximum contrast factor.</p> <code>2.0</code> <code>contrast_p</code> <code>float</code> <p>Probability of contrast adjustment.</p> <code>0.0</code> <code>brightness_min</code> <code>float</code> <p>Minimum brightness factor.</p> <code>1.0</code> <code>brightness_max</code> <code>float</code> <p>Maximum brightness factor.</p> <code>1.0</code> <code>brightness_p</code> <code>float</code> <p>Probability of brightness adjustment.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (augmented_image, instances). Image dtype matches input.</p> Source code in <code>sleap_nn/data/skia_augmentation.py</code> <pre><code>def apply_intensity_augmentation_skia(\n    image: torch.Tensor,\n    instances: torch.Tensor,\n    uniform_noise_min: float = 0.0,\n    uniform_noise_max: float = 0.04,\n    uniform_noise_p: float = 0.0,\n    gaussian_noise_mean: float = 0.02,\n    gaussian_noise_std: float = 0.004,\n    gaussian_noise_p: float = 0.0,\n    contrast_min: float = 0.5,\n    contrast_max: float = 2.0,\n    contrast_p: float = 0.0,\n    brightness_min: float = 1.0,\n    brightness_max: float = 1.0,\n    brightness_p: float = 0.0,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply intensity augmentations on uint8 image tensor.\n\n    Matches API of sleap_nn.data.augmentation.apply_intensity_augmentation.\n\n    Args:\n        image: Input tensor of shape (1, C, H, W) with dtype uint8 or float32.\n        instances: Keypoints tensor (not modified, just passed through).\n        uniform_noise_min: Minimum uniform noise (0-1 scale, maps to 0-255).\n        uniform_noise_max: Maximum uniform noise (0-1 scale).\n        uniform_noise_p: Probability of uniform noise.\n        gaussian_noise_mean: Gaussian noise mean (0-1 scale).\n        gaussian_noise_std: Gaussian noise std (0-1 scale).\n        gaussian_noise_p: Probability of Gaussian noise.\n        contrast_min: Minimum contrast factor.\n        contrast_max: Maximum contrast factor.\n        contrast_p: Probability of contrast adjustment.\n        brightness_min: Minimum brightness factor.\n        brightness_max: Maximum brightness factor.\n        brightness_p: Probability of brightness adjustment.\n\n    Returns:\n        Tuple of (augmented_image, instances). Image dtype matches input.\n    \"\"\"\n    # Convert to numpy for Skia processing\n    is_float = image.dtype == torch.float32\n    if is_float:\n        img_np = (image[0].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n    else:\n        img_np = image[0].permute(1, 2, 0).numpy()\n\n    result = img_np.copy()\n\n    # Apply uniform noise (in uint8 space)\n    if uniform_noise_p &gt; 0 and np.random.random() &lt; uniform_noise_p:\n        noise = np.random.randint(\n            int(uniform_noise_min * 255),\n            int(uniform_noise_max * 255) + 1,\n            img_np.shape,\n            dtype=np.int16,\n        )\n        result = np.clip(result.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n\n    # Apply Gaussian noise (in uint8 space)\n    if gaussian_noise_p &gt; 0 and np.random.random() &lt; gaussian_noise_p:\n        noise = np.random.normal(\n            gaussian_noise_mean * 255, gaussian_noise_std * 255, img_np.shape\n        ).astype(np.int16)\n        result = np.clip(result.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n\n    # Apply contrast using lookup table (pure uint8)\n    if contrast_p &gt; 0 and np.random.random() &lt; contrast_p:\n        factor = np.random.uniform(contrast_min, contrast_max)\n        lut = np.arange(256, dtype=np.float32)\n        lut = np.clip((lut - 127.5) * factor + 127.5, 0, 255).astype(np.uint8)\n        result = lut[result]\n\n    # Apply brightness using lookup table (pure uint8)\n    if brightness_p &gt; 0 and np.random.random() &lt; brightness_p:\n        factor = np.random.uniform(brightness_min, brightness_max)\n        lut = np.arange(256, dtype=np.float32)\n        lut = np.clip(lut * factor, 0, 255).astype(np.uint8)\n        result = lut[result]\n\n    # Convert back to tensor\n    result_tensor = torch.from_numpy(result).permute(2, 0, 1).unsqueeze(0)\n    if is_float:\n        result_tensor = result_tensor.float() / 255.0\n\n    return result_tensor, instances\n</code></pre>"},{"location":"api/data/skia_augmentation/#sleap_nn.data.skia_augmentation.crop_and_resize_skia","title":"<code>crop_and_resize_skia(image, boxes, size)</code>","text":"<p>Crop and resize image regions using Skia.</p> <p>Replacement for kornia.geometry.transform.crop_and_resize.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (1, C, H, W).</p> required <code>boxes</code> <code>Tensor</code> <p>Bounding boxes tensor of shape (1, 4, 2) with corners: [top-left, top-right, bottom-right, bottom-left].</p> required <code>size</code> <code>Tuple[int, int]</code> <p>Output size (height, width).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Cropped and resized tensor of shape (1, C, out_h, out_w).</p> Source code in <code>sleap_nn/data/skia_augmentation.py</code> <pre><code>def crop_and_resize_skia(\n    image: torch.Tensor,\n    boxes: torch.Tensor,\n    size: Tuple[int, int],\n) -&gt; torch.Tensor:\n    \"\"\"Crop and resize image regions using Skia.\n\n    Replacement for kornia.geometry.transform.crop_and_resize.\n\n    Args:\n        image: Input tensor of shape (1, C, H, W).\n        boxes: Bounding boxes tensor of shape (1, 4, 2) with corners:\n            [top-left, top-right, bottom-right, bottom-left].\n        size: Output size (height, width).\n\n    Returns:\n        Cropped and resized tensor of shape (1, C, out_h, out_w).\n    \"\"\"\n    is_float = image.dtype == torch.float32\n    if is_float:\n        img_np = (image[0].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n    else:\n        img_np = image[0].permute(1, 2, 0).numpy()\n\n    h, w = img_np.shape[:2]\n    out_h, out_w = size\n    channels = img_np.shape[2] if img_np.ndim == 3 else 1\n\n    # Get box coordinates (top-left and bottom-right)\n    box = boxes[0].numpy()  # (4, 2)\n    x1, y1 = box[0]  # top-left\n    x2, y2 = box[2]  # bottom-right\n\n    crop_w = x2 - x1\n    crop_h = y2 - y1\n\n    # Create transformation matrix\n    matrix = skia.Matrix()\n    scale_x = out_w / crop_w\n    scale_y = out_h / crop_h\n    matrix.setScale(scale_x, scale_y)\n    matrix.preTranslate(-x1, -y1)\n\n    # Skia needs RGBA\n    if channels == 1:\n        image_rgba = np.stack(\n            [img_np.squeeze()] * 3 + [np.full((h, w), 255, dtype=np.uint8)], axis=-1\n        )\n    elif channels == 3:\n        alpha = np.full((h, w, 1), 255, dtype=np.uint8)\n        image_rgba = np.concatenate([img_np, alpha], axis=-1)\n    else:\n        raise ValueError(f\"Unsupported channels: {channels}\")\n\n    image_rgba = np.ascontiguousarray(image_rgba, dtype=np.uint8)\n    skia_image = skia.Image.fromarray(\n        image_rgba, colorType=skia.ColorType.kRGBA_8888_ColorType\n    )\n\n    surface = skia.Surface(out_w, out_h)\n    canvas = surface.getCanvas()\n    canvas.clear(skia.Color4f(0, 0, 0, 1))\n    canvas.setMatrix(matrix)\n\n    paint = skia.Paint()\n    paint.setAntiAlias(True)\n    sampling = skia.SamplingOptions(skia.FilterMode.kLinear)\n    canvas.drawImage(skia_image, 0, 0, sampling, paint)\n\n    result = surface.makeImageSnapshot().toarray()\n\n    if channels == 1:\n        result = result[:, :, 0:1]\n    else:\n        result = result[:, :, :channels]\n\n    result_tensor = torch.from_numpy(result).permute(2, 0, 1).unsqueeze(0)\n    if is_float:\n        result_tensor = result_tensor.float() / 255.0\n\n    return result_tensor\n</code></pre>"},{"location":"api/data/utils/","title":"utils","text":""},{"location":"api/data/utils/#sleap_nn.data.utils","title":"<code>sleap_nn.data.utils</code>","text":"<p>Miscellaneous utility functions for data processing.</p> <p>Functions:</p> Name Description <code>check_cache_memory</code> <p>Check memory requirements for in-memory caching dataset pipeline.</p> <code>check_memory</code> <p>Return memory required for caching the image samples from a single labels object.</p> <code>ensure_list</code> <p>Convert the input into a list if it is not already.</p> <code>estimate_cache_memory</code> <p>Estimate memory requirements for in-memory caching dataset pipeline.</p> <code>expand_to_rank</code> <p>Expand a tensor to a target rank by adding singleton dimensions in PyTorch.</p> <code>gaussian_pdf</code> <p>Compute the PDF of an unnormalized 0-centered Gaussian distribution.</p> <code>make_grid_vectors</code> <p>Make sampling grid vectors from image dimensions.</p>"},{"location":"api/data/utils/#sleap_nn.data.utils.check_cache_memory","title":"<code>check_cache_memory(train_labels, val_labels, memory_buffer=0.2, num_workers=0)</code>","text":"<p>Check memory requirements for in-memory caching dataset pipeline.</p> <p>This function determines if the system has sufficient memory for in-memory image caching, accounting for DataLoader worker processes.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels</code> <code>List[Labels]</code> <p>List of <code>sleap_io.Labels</code> objects for training data.</p> required <code>val_labels</code> <code>List[Labels]</code> <p>List of <code>sleap_io.Labels</code> objects for validation data.</p> required <code>memory_buffer</code> <code>float</code> <p>Fraction of memory to reserve as buffer. Default: 0.2 (20%).</p> <code>0.2</code> <code>num_workers</code> <code>int</code> <p>Number of DataLoader worker processes. When &gt; 0, additional memory overhead is estimated for worker process duplication.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the total memory required for caching is within available system     memory, False otherwise.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def check_cache_memory(\n    train_labels: List[sio.Labels],\n    val_labels: List[sio.Labels],\n    memory_buffer: float = 0.2,\n    num_workers: int = 0,\n) -&gt; bool:\n    \"\"\"Check memory requirements for in-memory caching dataset pipeline.\n\n    This function determines if the system has sufficient memory for in-memory\n    image caching, accounting for DataLoader worker processes.\n\n    Args:\n        train_labels: List of `sleap_io.Labels` objects for training data.\n        val_labels: List of `sleap_io.Labels` objects for validation data.\n        memory_buffer: Fraction of memory to reserve as buffer. Default: 0.2 (20%).\n        num_workers: Number of DataLoader worker processes. When &gt; 0, additional memory\n            overhead is estimated for worker process duplication.\n\n    Returns:\n        bool: True if the total memory required for caching is within available system\n            memory, False otherwise.\n    \"\"\"\n    estimate = estimate_cache_memory(\n        train_labels=train_labels,\n        val_labels=val_labels,\n        num_workers=num_workers,\n        memory_buffer=memory_buffer,\n    )\n\n    if not estimate[\"sufficient\"]:\n        total_gb = estimate[\"total_bytes\"] / (1024**3)\n        available_gb = estimate[\"available_bytes\"] / (1024**3)\n        raw_gb = estimate[\"raw_cache_bytes\"] / (1024**3)\n        logger.info(\n            f\"Memory check failed: need ~{total_gb:.2f} GB \"\n            f\"(raw cache: {raw_gb:.2f} GB, {estimate['num_samples']} samples), \"\n            f\"available: {available_gb:.2f} GB\"\n        )\n\n    return estimate[\"sufficient\"]\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.check_memory","title":"<code>check_memory(labels)</code>","text":"<p>Return memory required for caching the image samples from a single labels object.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>A <code>sleap_io.Labels</code> object containing the labels for a single dataset.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Memory in bytes required to cache the image samples from the labels object.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def check_memory(\n    labels: sio.Labels,\n) -&gt; int:\n    \"\"\"Return memory required for caching the image samples from a single labels object.\n\n    Args:\n        labels: A `sleap_io.Labels` object containing the labels for a single dataset.\n\n    Returns:\n        Memory in bytes required to cache the image samples from the labels object.\n    \"\"\"\n    imgs_bytes = []\n    for label in labels:\n        if label.image is not None:\n            img = label.image\n            img_bytes = img.nbytes\n            imgs_bytes.append(img_bytes)\n        else:\n            raise ValueError(\n                \"Labels object contains a label with no image data, which is required for training.\"\n            )\n    img_mem = sum(imgs_bytes)\n    return img_mem\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.ensure_list","title":"<code>ensure_list(x)</code>","text":"<p>Convert the input into a list if it is not already.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def ensure_list(x: Any) -&gt; List[Any]:\n    \"\"\"Convert the input into a list if it is not already.\"\"\"\n    if not isinstance(x, list):\n        return [x]\n    return x\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.estimate_cache_memory","title":"<code>estimate_cache_memory(train_labels, val_labels, num_workers=0, memory_buffer=0.2)</code>","text":"<p>Estimate memory requirements for in-memory caching dataset pipeline.</p> <p>This function calculates the total memory needed for caching images, accounting for: - Raw image data size - Python object overhead (dictionary keys, numpy array wrappers) - DataLoader worker memory overhead (Copy-on-Write duplication on Unix systems) - General memory buffer for training overhead</p> <p>When using DataLoader with num_workers &gt; 0, worker processes are spawned via fork() on Unix systems. While Copy-on-Write (CoW) initially shares memory, Python's reference counting can trigger memory page duplication when workers access cached data.</p> <p>Parameters:</p> Name Type Description Default <code>train_labels</code> <code>List[Labels]</code> <p>List of <code>sleap_io.Labels</code> objects for training data.</p> required <code>val_labels</code> <code>List[Labels]</code> <p>List of <code>sleap_io.Labels</code> objects for validation data.</p> required <code>num_workers</code> <code>int</code> <p>Number of DataLoader worker processes. When &gt; 0, additional memory overhead is estimated for worker process duplication.</p> <code>0</code> <code>memory_buffer</code> <code>float</code> <p>Fraction of memory to reserve as buffer for training overhead (model weights, activations, gradients, etc.). Default: 0.2 (20%).</p> <code>0.2</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Memory estimation breakdown with keys:     - 'raw_cache_bytes': Raw image data size in bytes     - 'python_overhead_bytes': Estimated Python object overhead     - 'worker_overhead_bytes': Estimated memory for DataLoader workers     - 'buffer_bytes': Memory buffer for training overhead     - 'total_bytes': Total estimated memory requirement     - 'available_bytes': Available system memory     - 'sufficient': True if total &lt;= available, False otherwise</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def estimate_cache_memory(\n    train_labels: List[sio.Labels],\n    val_labels: List[sio.Labels],\n    num_workers: int = 0,\n    memory_buffer: float = 0.2,\n) -&gt; dict:\n    \"\"\"Estimate memory requirements for in-memory caching dataset pipeline.\n\n    This function calculates the total memory needed for caching images, accounting for:\n    - Raw image data size\n    - Python object overhead (dictionary keys, numpy array wrappers)\n    - DataLoader worker memory overhead (Copy-on-Write duplication on Unix systems)\n    - General memory buffer for training overhead\n\n    When using DataLoader with num_workers &gt; 0, worker processes are spawned via fork()\n    on Unix systems. While Copy-on-Write (CoW) initially shares memory, Python's reference\n    counting can trigger memory page duplication when workers access cached data.\n\n    Args:\n        train_labels: List of `sleap_io.Labels` objects for training data.\n        val_labels: List of `sleap_io.Labels` objects for validation data.\n        num_workers: Number of DataLoader worker processes. When &gt; 0, additional memory\n            overhead is estimated for worker process duplication.\n        memory_buffer: Fraction of memory to reserve as buffer for training overhead\n            (model weights, activations, gradients, etc.). Default: 0.2 (20%).\n\n    Returns:\n        dict: Memory estimation breakdown with keys:\n            - 'raw_cache_bytes': Raw image data size in bytes\n            - 'python_overhead_bytes': Estimated Python object overhead\n            - 'worker_overhead_bytes': Estimated memory for DataLoader workers\n            - 'buffer_bytes': Memory buffer for training overhead\n            - 'total_bytes': Total estimated memory requirement\n            - 'available_bytes': Available system memory\n            - 'sufficient': True if total &lt;= available, False otherwise\n    \"\"\"\n    # Calculate raw image cache size\n    train_cache_bytes = 0\n    val_cache_bytes = 0\n    num_train_samples = 0\n    num_val_samples = 0\n\n    for train, val in zip(train_labels, val_labels):\n        train_cache_bytes += check_memory(train)\n        val_cache_bytes += check_memory(val)\n        num_train_samples += len(train)\n        num_val_samples += len(val)\n\n    raw_cache_bytes = train_cache_bytes + val_cache_bytes\n    total_samples = num_train_samples + num_val_samples\n\n    # Python object overhead: dict keys, numpy array wrappers, tuple keys\n    # Estimate ~200 bytes per sample for Python object overhead\n    python_overhead_per_sample = 200\n    python_overhead_bytes = total_samples * python_overhead_per_sample\n\n    # Worker memory overhead\n    # When num_workers &gt; 0, workers are forked or spawned depending on platform.\n    # Default start methods (Python 3.8+):\n    #   - Linux: fork (Copy-on-Write, partial memory duplication)\n    #   - macOS: spawn (full dataset copy to each worker, changed in Python 3.8)\n    #   - Windows: spawn (full dataset copy to each worker)\n    worker_overhead_bytes = 0\n    if num_workers &gt; 0:\n        if sys.platform == \"linux\":\n            # Linux uses fork() with Copy-on-Write by default\n            # Estimate 25% duplication per worker due to Python refcounting\n            # triggering CoW page copies\n            worker_overhead_bytes = int(raw_cache_bytes * 0.25 * num_workers)\n            if num_workers &gt;= 4:\n                logger.info(\n                    f\"Using in-memory caching with {num_workers} DataLoader workers. \"\n                    f\"Estimated additional memory for workers: \"\n                    f\"{worker_overhead_bytes / (1024**3):.2f} GB\"\n                )\n        else:\n            # macOS (darwin) and Windows use spawn - dataset is copied to each worker\n            # Since Python 3.8, macOS defaults to spawn due to fork safety issues\n            # With caching enabled, we avoid pickling labels_list, but the cache\n            # dict is still part of the dataset and gets copied to each worker\n            worker_overhead_bytes = int(raw_cache_bytes * 0.5 * num_workers)\n            platform_name = \"macOS\" if sys.platform == \"darwin\" else \"Windows\"\n            logger.warning(\n                f\"Using in-memory caching with {num_workers} DataLoader workers on {platform_name}. \"\n                f\"Memory usage may be significantly higher than estimated (~{worker_overhead_bytes / (1024**3):.1f} GB extra) \"\n                f\"due to spawn-based multiprocessing. \"\n                f\"Consider using disk caching or num_workers=0 for large datasets.\"\n            )\n\n    # Memory buffer for training overhead (model, gradients, activations)\n    subtotal = raw_cache_bytes + python_overhead_bytes + worker_overhead_bytes\n    buffer_bytes = int(subtotal * memory_buffer)\n\n    total_bytes = subtotal + buffer_bytes\n    available_bytes = psutil.virtual_memory().available\n\n    return {\n        \"raw_cache_bytes\": raw_cache_bytes,\n        \"python_overhead_bytes\": python_overhead_bytes,\n        \"worker_overhead_bytes\": worker_overhead_bytes,\n        \"buffer_bytes\": buffer_bytes,\n        \"total_bytes\": total_bytes,\n        \"available_bytes\": available_bytes,\n        \"sufficient\": total_bytes &lt;= available_bytes,\n        \"num_samples\": total_samples,\n    }\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.expand_to_rank","title":"<code>expand_to_rank(x, target_rank, prepend=True)</code>","text":"<p>Expand a tensor to a target rank by adding singleton dimensions in PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Any <code>torch.Tensor</code> with rank &lt;= <code>target_rank</code>. If the rank is higher than <code>target_rank</code>, the tensor will be returned with the same shape.</p> required <code>target_rank</code> <code>int</code> <p>Rank to expand the input to.</p> required <code>prepend</code> <code>bool</code> <p>If True, singleton dimensions are added before the first axis of the data. If False, singleton dimensions are added after the last axis.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The expanded tensor of the same dtype as the input, but with rank <code>target_rank</code>. The output has the same exact data as the input tensor and will be identical if they are both flattened.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def expand_to_rank(\n    x: torch.Tensor, target_rank: int, prepend: bool = True\n) -&gt; torch.Tensor:\n    \"\"\"Expand a tensor to a target rank by adding singleton dimensions in PyTorch.\n\n    Args:\n        x: Any `torch.Tensor` with rank &lt;= `target_rank`. If the rank is higher than\n            `target_rank`, the tensor will be returned with the same shape.\n        target_rank: Rank to expand the input to.\n        prepend: If True, singleton dimensions are added before the first axis of the\n            data. If False, singleton dimensions are added after the last axis.\n\n    Returns:\n        The expanded tensor of the same dtype as the input, but with rank `target_rank`.\n        The output has the same exact data as the input tensor and will be identical if\n        they are both flattened.\n    \"\"\"\n    n_singleton_dims = max(target_rank - x.dim(), 0)\n    singleton_dims = [1] * n_singleton_dims\n    if prepend:\n        new_shape = singleton_dims + list(x.shape)\n    else:\n        new_shape = list(x.shape) + singleton_dims\n    return x.reshape(new_shape)\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.gaussian_pdf","title":"<code>gaussian_pdf(x, sigma)</code>","text":"<p>Compute the PDF of an unnormalized 0-centered Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor of dtype torch.float32 with values to compute the PDF for.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of the same shape as <code>x</code>, but with values of a PDF of an unnormalized Gaussian distribution. Values of 0 have an unnormalized PDF value of 1.0.</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def gaussian_pdf(x: torch.Tensor, sigma: float) -&gt; torch.Tensor:\n    \"\"\"Compute the PDF of an unnormalized 0-centered Gaussian distribution.\n\n    Args:\n        x: A tensor of dtype torch.float32 with values to compute the PDF for.\n        sigma: Standard deviation of the Gaussian distribution.\n\n    Returns:\n        A tensor of the same shape as `x`, but with values of a PDF of an unnormalized\n        Gaussian distribution. Values of 0 have an unnormalized PDF value of 1.0.\n    \"\"\"\n    return torch.exp(-(x**2) / (2 * sigma**2))\n</code></pre>"},{"location":"api/data/utils/#sleap_nn.data.utils.make_grid_vectors","title":"<code>make_grid_vectors(image_height, image_width, output_stride=1)</code>","text":"<p>Make sampling grid vectors from image dimensions.</p> <p>This is a useful function for creating the x- and y-vectors that define a sampling grid over an image space. These vectors can be used to generate a full meshgrid or for equivalent broadcasting operations.</p> <p>Parameters:</p> Name Type Description Default <code>image_height</code> <code>int</code> <p>Height of the image grid that will be sampled, specified as a scalar integer.</p> required <code>image_width</code> <code>int</code> <p>width of the image grid that will be sampled, specified as a scalar integer.</p> required <code>output_stride</code> <code>int</code> <p>Sampling step size, specified as a scalar integer. This can be used to specify a sampling grid that has a smaller shape than the image grid but with values span the same range. This can be thought of as the reciprocal of the output scale, i.e., it will induce subsampling when set to values greater than 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of grid vectors (xv, yv). These are tensors of dtype tf.float32 with shapes (grid_width,) and (grid_height,) respectively.</p> <p>The grid dimensions are calculated as:     grid_width = image_width // output_stride     grid_height = image_height // output_stride</p> Source code in <code>sleap_nn/data/utils.py</code> <pre><code>def make_grid_vectors(\n    image_height: int, image_width: int, output_stride: int = 1\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Make sampling grid vectors from image dimensions.\n\n    This is a useful function for creating the x- and y-vectors that define a sampling\n    grid over an image space. These vectors can be used to generate a full meshgrid or\n    for equivalent broadcasting operations.\n\n    Args:\n        image_height: Height of the image grid that will be sampled, specified as a\n            scalar integer.\n        image_width: width of the image grid that will be sampled, specified as a\n            scalar integer.\n        output_stride: Sampling step size, specified as a scalar integer. This can be\n            used to specify a sampling grid that has a smaller shape than the image\n            grid but with values span the same range. This can be thought of as the\n            reciprocal of the output scale, i.e., it will induce subsampling when set to\n            values greater than 1.\n\n    Returns:\n        Tuple of grid vectors (xv, yv). These are tensors of dtype tf.float32 with\n        shapes (grid_width,) and (grid_height,) respectively.\n\n        The grid dimensions are calculated as:\n            grid_width = image_width // output_stride\n            grid_height = image_height // output_stride\n    \"\"\"\n    xv = torch.arange(0, image_width, step=output_stride, dtype=torch.float32)\n    yv = torch.arange(0, image_height, step=output_stride, dtype=torch.float32)\n    return xv, yv\n</code></pre>"},{"location":"api/export/","title":"export","text":""},{"location":"api/export/#sleap_nn.export","title":"<code>sleap_nn.export</code>","text":"<p>Export utilities for sleap-nn.</p> <p>Modules:</p> Name Description <code>cli</code> <p>CLI entry points for export workflows.</p> <code>exporters</code> <p>Exporters for serialized model formats.</p> <code>metadata</code> <p>Metadata helpers for exported models.</p> <code>predictors</code> <p>Predictors for exported models.</p> <code>utils</code> <p>Utilities for export workflows.</p> <code>wrappers</code> <p>ONNX/TensorRT export wrappers.</p> <p>Classes:</p> Name Description <code>ExportMetadata</code> <p>Metadata embedded or saved alongside exported models.</p> <code>ONNXPredictor</code> <p>ONNX Runtime inference with provider selection.</p> <code>TensorRTPredictor</code> <p>TensorRT inference for exported models.</p> <p>Functions:</p> Name Description <code>build_bottomup_candidate_template</code> <p>Build candidate template matching ONNX wrapper's line_scores ordering.</p> <code>export_model</code> <p>Export a model to the requested format.</p> <code>export_to_onnx</code> <p>Export a PyTorch model to ONNX.</p> <code>export_to_tensorrt</code> <p>Export a PyTorch model to TensorRT format.</p> <code>load_exported_model</code> <p>Load an exported model and return a predictor instance.</p>"},{"location":"api/export/#sleap_nn.export.ExportMetadata","title":"<code>ExportMetadata</code>  <code>dataclass</code>","text":"<p>Metadata embedded or saved alongside exported models.</p> <p>Methods:</p> Name Description <code>default_timestamp</code> <p>Return an ISO timestamp for export.</p> <code>from_dict</code> <p>Load from dict.</p> <code>load</code> <p>Load from JSON file.</p> <code>save</code> <p>Save to JSON file.</p> <code>to_dict</code> <p>Convert to JSON-serializable dict.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@dataclass\nclass ExportMetadata:\n    \"\"\"Metadata embedded or saved alongside exported models.\"\"\"\n\n    # Version info\n    sleap_nn_version: str\n    export_timestamp: str\n    export_format: str  # \"onnx\" or \"tensorrt\"\n\n    # Model info\n    model_type: str  # \"centroid\", \"centered_instance\", \"topdown\", \"bottomup\"\n    model_name: str\n    checkpoint_path: str\n\n    # Architecture\n    backbone: str\n    n_nodes: int\n    n_edges: int\n    node_names: List[str]\n    edge_inds: List[Tuple[int, int]]\n\n    # Input/output spec\n    input_scale: float\n    input_channels: int\n    output_stride: int\n    crop_size: Optional[Tuple[int, int]] = None\n\n    # Export parameters\n    max_instances: Optional[int] = None\n    max_peaks_per_node: Optional[int] = None\n    max_batch_size: int = 1\n    precision: str = \"fp32\"\n\n    # Preprocessing - input is uint8 [0,255], normalized internally to float32 [0,1]\n    input_dtype: str = \"uint8\"\n    normalization: str = \"0_to_1\"\n\n    # Multiclass model fields (optional)\n    n_classes: Optional[int] = None\n    class_names: Optional[List[str]] = None\n\n    # Training config reference\n    training_config_embedded: bool = False\n    training_config_hash: str = \"\"\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to JSON-serializable dict.\"\"\"\n        data = asdict(self)\n        data[\"edge_inds\"] = [list(pair) for pair in self.edge_inds]\n        if self.crop_size is not None:\n            data[\"crop_size\"] = list(self.crop_size)\n        return data\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"ExportMetadata\":\n        \"\"\"Load from dict.\"\"\"\n        edge_inds = [tuple(pair) for pair in data.get(\"edge_inds\", [])]\n        crop_size = data.get(\"crop_size\")\n        if crop_size is not None:\n            crop_size = tuple(crop_size)\n        return cls(\n            sleap_nn_version=data.get(\"sleap_nn_version\", \"\"),\n            export_timestamp=data.get(\"export_timestamp\", \"\"),\n            export_format=data.get(\"export_format\", \"\"),\n            model_type=data.get(\"model_type\", \"\"),\n            model_name=data.get(\"model_name\", \"\"),\n            checkpoint_path=data.get(\"checkpoint_path\", \"\"),\n            backbone=data.get(\"backbone\", \"\"),\n            n_nodes=int(data.get(\"n_nodes\", 0)),\n            n_edges=int(data.get(\"n_edges\", 0)),\n            node_names=list(data.get(\"node_names\", [])),\n            edge_inds=edge_inds,\n            input_scale=float(data.get(\"input_scale\", 1.0)),\n            input_channels=int(data.get(\"input_channels\", 1)),\n            output_stride=int(data.get(\"output_stride\", 1)),\n            crop_size=crop_size,\n            max_instances=data.get(\"max_instances\"),\n            max_peaks_per_node=data.get(\"max_peaks_per_node\"),\n            max_batch_size=int(data.get(\"max_batch_size\", 1)),\n            precision=data.get(\"precision\", \"fp32\"),\n            input_dtype=data.get(\"input_dtype\", \"uint8\"),\n            normalization=data.get(\"normalization\", \"0_to_1\"),\n            n_classes=data.get(\"n_classes\"),\n            class_names=data.get(\"class_names\"),\n            training_config_embedded=bool(data.get(\"training_config_embedded\", False)),\n            training_config_hash=data.get(\"training_config_hash\", \"\"),\n        )\n\n    def save(self, path: str | Path) -&gt; None:\n        \"\"\"Save to JSON file.\"\"\"\n        path = Path(path)\n        path.write_text(json.dumps(self.to_dict(), indent=2, sort_keys=True))\n\n    @classmethod\n    def load(cls, path: str | Path) -&gt; \"ExportMetadata\":\n        \"\"\"Load from JSON file.\"\"\"\n        path = Path(path)\n        data = json.loads(path.read_text())\n        return cls.from_dict(data)\n\n    @classmethod\n    def default_timestamp(cls) -&gt; str:\n        \"\"\"Return an ISO timestamp for export.\"\"\"\n        return datetime.now().isoformat()\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ExportMetadata.default_timestamp","title":"<code>default_timestamp()</code>  <code>classmethod</code>","text":"<p>Return an ISO timestamp for export.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@classmethod\ndef default_timestamp(cls) -&gt; str:\n    \"\"\"Return an ISO timestamp for export.\"\"\"\n    return datetime.now().isoformat()\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ExportMetadata.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Load from dict.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"ExportMetadata\":\n    \"\"\"Load from dict.\"\"\"\n    edge_inds = [tuple(pair) for pair in data.get(\"edge_inds\", [])]\n    crop_size = data.get(\"crop_size\")\n    if crop_size is not None:\n        crop_size = tuple(crop_size)\n    return cls(\n        sleap_nn_version=data.get(\"sleap_nn_version\", \"\"),\n        export_timestamp=data.get(\"export_timestamp\", \"\"),\n        export_format=data.get(\"export_format\", \"\"),\n        model_type=data.get(\"model_type\", \"\"),\n        model_name=data.get(\"model_name\", \"\"),\n        checkpoint_path=data.get(\"checkpoint_path\", \"\"),\n        backbone=data.get(\"backbone\", \"\"),\n        n_nodes=int(data.get(\"n_nodes\", 0)),\n        n_edges=int(data.get(\"n_edges\", 0)),\n        node_names=list(data.get(\"node_names\", [])),\n        edge_inds=edge_inds,\n        input_scale=float(data.get(\"input_scale\", 1.0)),\n        input_channels=int(data.get(\"input_channels\", 1)),\n        output_stride=int(data.get(\"output_stride\", 1)),\n        crop_size=crop_size,\n        max_instances=data.get(\"max_instances\"),\n        max_peaks_per_node=data.get(\"max_peaks_per_node\"),\n        max_batch_size=int(data.get(\"max_batch_size\", 1)),\n        precision=data.get(\"precision\", \"fp32\"),\n        input_dtype=data.get(\"input_dtype\", \"uint8\"),\n        normalization=data.get(\"normalization\", \"0_to_1\"),\n        n_classes=data.get(\"n_classes\"),\n        class_names=data.get(\"class_names\"),\n        training_config_embedded=bool(data.get(\"training_config_embedded\", False)),\n        training_config_hash=data.get(\"training_config_hash\", \"\"),\n    )\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ExportMetadata.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load from JSON file.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@classmethod\ndef load(cls, path: str | Path) -&gt; \"ExportMetadata\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(path)\n    data = json.loads(path.read_text())\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ExportMetadata.save","title":"<code>save(path)</code>","text":"<p>Save to JSON file.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(path)\n    path.write_text(json.dumps(self.to_dict(), indent=2, sort_keys=True))\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ExportMetadata.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to JSON-serializable dict.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to JSON-serializable dict.\"\"\"\n    data = asdict(self)\n    data[\"edge_inds\"] = [list(pair) for pair in self.edge_inds]\n    if self.crop_size is not None:\n        data[\"crop_size\"] = list(self.crop_size)\n    return data\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ONNXPredictor","title":"<code>ONNXPredictor</code>","text":"<p>               Bases: <code>ExportPredictor</code></p> <p>ONNX Runtime inference with provider selection.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize ONNX predictor with execution providers.</p> <code>benchmark</code> <p>Benchmark inference latency and throughput.</p> <code>predict</code> <p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>class ONNXPredictor(ExportPredictor):\n    \"\"\"ONNX Runtime inference with provider selection.\"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        device: str = \"auto\",\n        providers: Optional[Iterable[str]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize ONNX predictor with execution providers.\n\n        Args:\n            model_path: Path to the ONNX model file.\n            device: Device for inference (\"auto\", \"cpu\", or \"cuda\").\n            providers: ONNX Runtime execution providers. Auto-selected if None.\n        \"\"\"\n        try:\n            import onnxruntime as ort\n        except ImportError as exc:\n            raise ImportError(\n                \"onnxruntime is required for ONNXPredictor. Install with \"\n                \"`pip install onnxruntime` or `onnxruntime-gpu`.\"\n            ) from exc\n\n        # Preload CUDA/cuDNN libraries from pip-installed nvidia packages\n        # This is required for onnxruntime-gpu to find the CUDA libraries\n        if hasattr(ort, \"preload_dlls\"):\n            ort.preload_dlls()\n\n        self.ort = ort\n        if providers is None:\n            providers = _select_providers(device, ort.get_available_providers())\n\n        self.session = ort.InferenceSession(model_path, providers=list(providers))\n        input_info = self.session.get_inputs()[0]\n        self.input_name = input_info.name\n        self.input_type = input_info.type\n        self.input_dtype = _onnx_type_to_numpy(self.input_type)\n        self.output_names = [out.name for out in self.session.get_outputs()]\n\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run inference on a batch of images.\"\"\"\n        image = _as_numpy(image, expected_dtype=self.input_dtype)\n        outputs = self.session.run(None, {self.input_name: image})\n        return dict(zip(self.output_names, outputs))\n\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference latency and throughput.\"\"\"\n        image = _as_numpy(image, expected_dtype=self.input_dtype)\n        for _ in range(n_warmup):\n            self.session.run(None, {self.input_name: image})\n\n        times = []\n        for _ in range(n_runs):\n            start = time.perf_counter()\n            self.session.run(None, {self.input_name: image})\n            times.append(time.perf_counter() - start)\n\n        times_ms = np.array(times) * 1000.0\n        mean_ms = float(times_ms.mean())\n        p50_ms = float(np.percentile(times_ms, 50))\n        p95_ms = float(np.percentile(times_ms, 95))\n        fps = float(1000.0 / mean_ms) if mean_ms &gt; 0 else 0.0\n\n        return {\n            \"latency_ms_mean\": mean_ms,\n            \"latency_ms_p50\": p50_ms,\n            \"latency_ms_p95\": p95_ms,\n            \"fps\": fps,\n        }\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ONNXPredictor.__init__","title":"<code>__init__(model_path, device='auto', providers=None)</code>","text":"<p>Initialize ONNX predictor with execution providers.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the ONNX model file.</p> required <code>device</code> <code>str</code> <p>Device for inference (\"auto\", \"cpu\", or \"cuda\").</p> <code>'auto'</code> <code>providers</code> <code>Optional[Iterable[str]]</code> <p>ONNX Runtime execution providers. Auto-selected if None.</p> <code>None</code> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def __init__(\n    self,\n    model_path: str,\n    device: str = \"auto\",\n    providers: Optional[Iterable[str]] = None,\n) -&gt; None:\n    \"\"\"Initialize ONNX predictor with execution providers.\n\n    Args:\n        model_path: Path to the ONNX model file.\n        device: Device for inference (\"auto\", \"cpu\", or \"cuda\").\n        providers: ONNX Runtime execution providers. Auto-selected if None.\n    \"\"\"\n    try:\n        import onnxruntime as ort\n    except ImportError as exc:\n        raise ImportError(\n            \"onnxruntime is required for ONNXPredictor. Install with \"\n            \"`pip install onnxruntime` or `onnxruntime-gpu`.\"\n        ) from exc\n\n    # Preload CUDA/cuDNN libraries from pip-installed nvidia packages\n    # This is required for onnxruntime-gpu to find the CUDA libraries\n    if hasattr(ort, \"preload_dlls\"):\n        ort.preload_dlls()\n\n    self.ort = ort\n    if providers is None:\n        providers = _select_providers(device, ort.get_available_providers())\n\n    self.session = ort.InferenceSession(model_path, providers=list(providers))\n    input_info = self.session.get_inputs()[0]\n    self.input_name = input_info.name\n    self.input_type = input_info.type\n    self.input_dtype = _onnx_type_to_numpy(self.input_type)\n    self.output_names = [out.name for out in self.session.get_outputs()]\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ONNXPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>","text":"<p>Benchmark inference latency and throughput.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference latency and throughput.\"\"\"\n    image = _as_numpy(image, expected_dtype=self.input_dtype)\n    for _ in range(n_warmup):\n        self.session.run(None, {self.input_name: image})\n\n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        self.session.run(None, {self.input_name: image})\n        times.append(time.perf_counter() - start)\n\n    times_ms = np.array(times) * 1000.0\n    mean_ms = float(times_ms.mean())\n    p50_ms = float(np.percentile(times_ms, 50))\n    p95_ms = float(np.percentile(times_ms, 95))\n    fps = float(1000.0 / mean_ms) if mean_ms &gt; 0 else 0.0\n\n    return {\n        \"latency_ms_mean\": mean_ms,\n        \"latency_ms_p50\": p50_ms,\n        \"latency_ms_p95\": p95_ms,\n        \"fps\": fps,\n    }\n</code></pre>"},{"location":"api/export/#sleap_nn.export.ONNXPredictor.predict","title":"<code>predict(image)</code>","text":"<p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run inference on a batch of images.\"\"\"\n    image = _as_numpy(image, expected_dtype=self.input_dtype)\n    outputs = self.session.run(None, {self.input_name: image})\n    return dict(zip(self.output_names, outputs))\n</code></pre>"},{"location":"api/export/#sleap_nn.export.TensorRTPredictor","title":"<code>TensorRTPredictor</code>","text":"<p>               Bases: <code>ExportPredictor</code></p> <p>TensorRT inference for exported models.</p> <p>This predictor loads a native TensorRT engine file (.trt) and provides inference capabilities using CUDA for high-throughput predictions.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the TensorRT engine file (.trt).</p> required <code>device</code> <code>str</code> <p>Device to run inference on (only \"cuda\" supported for TRT).</p> <code>'cuda'</code> Example <p>predictor = TensorRTPredictor(\"model.trt\") outputs = predictor.predict(images)  # uint8 [B, C, H, W]</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize TensorRT predictor with a serialized engine.</p> <code>benchmark</code> <p>Benchmark inference performance.</p> <code>predict</code> <p>Run TensorRT inference.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>class TensorRTPredictor(ExportPredictor):\n    \"\"\"TensorRT inference for exported models.\n\n    This predictor loads a native TensorRT engine file (.trt) and provides\n    inference capabilities using CUDA for high-throughput predictions.\n\n    Args:\n        engine_path: Path to the TensorRT engine file (.trt).\n        device: Device to run inference on (only \"cuda\" supported for TRT).\n\n    Example:\n        &gt;&gt;&gt; predictor = TensorRTPredictor(\"model.trt\")\n        &gt;&gt;&gt; outputs = predictor.predict(images)  # uint8 [B, C, H, W]\n    \"\"\"\n\n    def __init__(\n        self,\n        engine_path: str | Path,\n        device: str = \"cuda\",\n    ) -&gt; None:\n        \"\"\"Initialize TensorRT predictor with a serialized engine.\n\n        Args:\n            engine_path: Path to the TensorRT engine file.\n            device: Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.\n        \"\"\"\n        import tensorrt as trt\n\n        if device not in (\"cuda\", \"auto\"):\n            raise ValueError(f\"TensorRT only supports CUDA devices, got: {device}\")\n\n        self.engine_path = Path(engine_path)\n        if not self.engine_path.exists():\n            raise FileNotFoundError(f\"TensorRT engine not found: {engine_path}\")\n\n        self.logger = trt.Logger(trt.Logger.WARNING)\n\n        # Load engine\n        with open(self.engine_path, \"rb\") as f:\n            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n        if self.engine is None:\n            raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n        # Create execution context\n        self.context = self.engine.create_execution_context()\n\n        # Get input/output info\n        self.input_names: List[str] = []\n        self.output_names: List[str] = []\n        self.input_shapes: Dict[str, tuple] = {}\n        self.output_shapes: Dict[str, tuple] = {}\n\n        for i in range(self.engine.num_io_tensors):\n            name = self.engine.get_tensor_name(i)\n            shape = tuple(self.engine.get_tensor_shape(name))\n            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                self.input_names.append(name)\n                self.input_shapes[name] = shape\n            else:\n                self.output_names.append(name)\n                self.output_shapes[name] = shape\n\n        self.device = torch.device(\"cuda\")\n\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run TensorRT inference.\n\n        Args:\n            image: Input image(s) as numpy array [B, C, H, W] with uint8 dtype.\n\n        Returns:\n            Dict mapping output names to numpy arrays.\n        \"\"\"\n        import tensorrt as trt\n\n        # Convert input to torch tensor on GPU\n        input_tensor = torch.from_numpy(image).to(self.device)\n\n        # Check if engine expects uint8 or float32 and convert if needed\n        input_name = self.input_names[0]\n        expected_dtype = self.engine.get_tensor_dtype(input_name)\n        if expected_dtype == trt.DataType.UINT8:\n            # Engine expects uint8 - keep as uint8\n            if input_tensor.dtype != torch.uint8:\n                input_tensor = input_tensor.to(torch.uint8)\n        else:\n            # Engine expects float - convert uint8 to float32\n            if input_tensor.dtype == torch.uint8:\n                input_tensor = input_tensor.to(torch.float32)\n\n        # Ensure contiguous memory\n        input_tensor = input_tensor.contiguous()\n\n        # Set input shape for dynamic dimensions\n        input_name = self.input_names[0]\n        self.context.set_input_shape(input_name, tuple(input_tensor.shape))\n\n        # Allocate output tensors\n        outputs: Dict[str, torch.Tensor] = {}\n        bindings: Dict[str, int] = {}\n\n        # Set input binding\n        bindings[input_name] = input_tensor.data_ptr()\n\n        # Allocate outputs\n        for name in self.output_names:\n            shape = self.context.get_tensor_shape(name)\n            dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n            outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=self.device)\n            bindings[name] = outputs[name].data_ptr()\n\n        # Set tensor addresses\n        for name, ptr in bindings.items():\n            self.context.set_tensor_address(name, ptr)\n\n        # Run inference\n        stream = torch.cuda.current_stream().cuda_stream\n        success = self.context.execute_async_v3(stream)\n        torch.cuda.current_stream().synchronize()\n\n        if not success:\n            raise RuntimeError(\"TensorRT inference failed\")\n\n        # Convert outputs to numpy\n        return {name: tensor.cpu().numpy() for name, tensor in outputs.items()}\n\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference performance.\n\n        Args:\n            image: Input image(s) as numpy array [B, C, H, W].\n            n_warmup: Number of warmup runs (not timed).\n            n_runs: Number of timed runs.\n\n        Returns:\n            Dict with timing statistics:\n                - mean_ms: Mean inference time in milliseconds\n                - std_ms: Standard deviation of inference time\n                - min_ms: Minimum inference time\n                - max_ms: Maximum inference time\n                - fps: Frames per second (based on mean time and batch size)\n        \"\"\"\n        batch_size = image.shape[0]\n\n        # Warmup\n        for _ in range(n_warmup):\n            _ = self.predict(image)\n\n        # Timed runs\n        times = []\n        for _ in range(n_runs):\n            start = time.perf_counter()\n            _ = self.predict(image)\n            times.append((time.perf_counter() - start) * 1000)\n\n        times_arr = np.array(times)\n        mean_ms = float(np.mean(times_arr))\n\n        return {\n            \"mean_ms\": mean_ms,\n            \"std_ms\": float(np.std(times_arr)),\n            \"min_ms\": float(np.min(times_arr)),\n            \"max_ms\": float(np.max(times_arr)),\n            \"fps\": (batch_size * 1000) / mean_ms if mean_ms &gt; 0 else 0.0,\n        }\n\n    @staticmethod\n    def _trt_dtype_to_torch(trt_dtype):\n        \"\"\"Convert TensorRT dtype to PyTorch dtype.\"\"\"\n        import tensorrt as trt\n\n        mapping = {\n            trt.DataType.FLOAT: torch.float32,\n            trt.DataType.HALF: torch.float16,\n            trt.DataType.INT32: torch.int32,\n            trt.DataType.INT8: torch.int8,\n            trt.DataType.BOOL: torch.bool,\n        }\n        return mapping.get(trt_dtype, torch.float32)\n</code></pre>"},{"location":"api/export/#sleap_nn.export.TensorRTPredictor.__init__","title":"<code>__init__(engine_path, device='cuda')</code>","text":"<p>Initialize TensorRT predictor with a serialized engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the TensorRT engine file.</p> required <code>device</code> <code>str</code> <p>Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.</p> <code>'cuda'</code> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def __init__(\n    self,\n    engine_path: str | Path,\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize TensorRT predictor with a serialized engine.\n\n    Args:\n        engine_path: Path to the TensorRT engine file.\n        device: Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.\n    \"\"\"\n    import tensorrt as trt\n\n    if device not in (\"cuda\", \"auto\"):\n        raise ValueError(f\"TensorRT only supports CUDA devices, got: {device}\")\n\n    self.engine_path = Path(engine_path)\n    if not self.engine_path.exists():\n        raise FileNotFoundError(f\"TensorRT engine not found: {engine_path}\")\n\n    self.logger = trt.Logger(trt.Logger.WARNING)\n\n    # Load engine\n    with open(self.engine_path, \"rb\") as f:\n        self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n    if self.engine is None:\n        raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n    # Create execution context\n    self.context = self.engine.create_execution_context()\n\n    # Get input/output info\n    self.input_names: List[str] = []\n    self.output_names: List[str] = []\n    self.input_shapes: Dict[str, tuple] = {}\n    self.output_shapes: Dict[str, tuple] = {}\n\n    for i in range(self.engine.num_io_tensors):\n        name = self.engine.get_tensor_name(i)\n        shape = tuple(self.engine.get_tensor_shape(name))\n        if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n            self.input_names.append(name)\n            self.input_shapes[name] = shape\n        else:\n            self.output_names.append(name)\n            self.output_shapes[name] = shape\n\n    self.device = torch.device(\"cuda\")\n</code></pre>"},{"location":"api/export/#sleap_nn.export.TensorRTPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>","text":"<p>Benchmark inference performance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image(s) as numpy array [B, C, H, W].</p> required <code>n_warmup</code> <code>int</code> <p>Number of warmup runs (not timed).</p> <code>50</code> <code>n_runs</code> <code>int</code> <p>Number of timed runs.</p> <code>200</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict with timing statistics:     - mean_ms: Mean inference time in milliseconds     - std_ms: Standard deviation of inference time     - min_ms: Minimum inference time     - max_ms: Maximum inference time     - fps: Frames per second (based on mean time and batch size)</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference performance.\n\n    Args:\n        image: Input image(s) as numpy array [B, C, H, W].\n        n_warmup: Number of warmup runs (not timed).\n        n_runs: Number of timed runs.\n\n    Returns:\n        Dict with timing statistics:\n            - mean_ms: Mean inference time in milliseconds\n            - std_ms: Standard deviation of inference time\n            - min_ms: Minimum inference time\n            - max_ms: Maximum inference time\n            - fps: Frames per second (based on mean time and batch size)\n    \"\"\"\n    batch_size = image.shape[0]\n\n    # Warmup\n    for _ in range(n_warmup):\n        _ = self.predict(image)\n\n    # Timed runs\n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        _ = self.predict(image)\n        times.append((time.perf_counter() - start) * 1000)\n\n    times_arr = np.array(times)\n    mean_ms = float(np.mean(times_arr))\n\n    return {\n        \"mean_ms\": mean_ms,\n        \"std_ms\": float(np.std(times_arr)),\n        \"min_ms\": float(np.min(times_arr)),\n        \"max_ms\": float(np.max(times_arr)),\n        \"fps\": (batch_size * 1000) / mean_ms if mean_ms &gt; 0 else 0.0,\n    }\n</code></pre>"},{"location":"api/export/#sleap_nn.export.TensorRTPredictor.predict","title":"<code>predict(image)</code>","text":"<p>Run TensorRT inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image(s) as numpy array [B, C, H, W] with uint8 dtype.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict mapping output names to numpy arrays.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run TensorRT inference.\n\n    Args:\n        image: Input image(s) as numpy array [B, C, H, W] with uint8 dtype.\n\n    Returns:\n        Dict mapping output names to numpy arrays.\n    \"\"\"\n    import tensorrt as trt\n\n    # Convert input to torch tensor on GPU\n    input_tensor = torch.from_numpy(image).to(self.device)\n\n    # Check if engine expects uint8 or float32 and convert if needed\n    input_name = self.input_names[0]\n    expected_dtype = self.engine.get_tensor_dtype(input_name)\n    if expected_dtype == trt.DataType.UINT8:\n        # Engine expects uint8 - keep as uint8\n        if input_tensor.dtype != torch.uint8:\n            input_tensor = input_tensor.to(torch.uint8)\n    else:\n        # Engine expects float - convert uint8 to float32\n        if input_tensor.dtype == torch.uint8:\n            input_tensor = input_tensor.to(torch.float32)\n\n    # Ensure contiguous memory\n    input_tensor = input_tensor.contiguous()\n\n    # Set input shape for dynamic dimensions\n    input_name = self.input_names[0]\n    self.context.set_input_shape(input_name, tuple(input_tensor.shape))\n\n    # Allocate output tensors\n    outputs: Dict[str, torch.Tensor] = {}\n    bindings: Dict[str, int] = {}\n\n    # Set input binding\n    bindings[input_name] = input_tensor.data_ptr()\n\n    # Allocate outputs\n    for name in self.output_names:\n        shape = self.context.get_tensor_shape(name)\n        dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n        outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=self.device)\n        bindings[name] = outputs[name].data_ptr()\n\n    # Set tensor addresses\n    for name, ptr in bindings.items():\n        self.context.set_tensor_address(name, ptr)\n\n    # Run inference\n    stream = torch.cuda.current_stream().cuda_stream\n    success = self.context.execute_async_v3(stream)\n    torch.cuda.current_stream().synchronize()\n\n    if not success:\n        raise RuntimeError(\"TensorRT inference failed\")\n\n    # Convert outputs to numpy\n    return {name: tensor.cpu().numpy() for name, tensor in outputs.items()}\n</code></pre>"},{"location":"api/export/#sleap_nn.export.build_bottomup_candidate_template","title":"<code>build_bottomup_candidate_template(n_nodes, max_peaks_per_node, edge_inds)</code>","text":"<p>Build candidate template matching ONNX wrapper's line_scores ordering.</p> <p>The ONNX BottomUpONNXWrapper produces line_scores with shape (n_edges, k*k) where for each edge connecting (src_node, dst_node), position i*k + j corresponds to: - src peak flat index: src_node * k + i - dst peak flat index: dst_node * k + j</p> <p>This function builds edge_inds and edge_peak_inds tensors that match this exact ordering, so that line_scores_flat[idx] corresponds to edge_peak_inds[idx].</p> <p>Parameters:</p> Name Type Description Default <code>n_nodes</code> <code>int</code> <p>Number of nodes in the skeleton.</p> required <code>max_peaks_per_node</code> <code>int</code> <p>Maximum peaks per node (k) used during export.</p> required <code>edge_inds</code> <code>List[Tuple[int, int]]</code> <p>List of (src_node, dst_node) tuples defining skeleton edges.</p> required <p>Returns:</p> Type Description <code>Tuple['torch.Tensor', 'torch.Tensor', 'torch.Tensor']</code> <p>Tuple of (peak_channel_inds, edge_inds_tensor, edge_peak_inds_tensor): - peak_channel_inds: (n_nodes * k,) tensor mapping flat peak index to node - edge_inds_tensor: (n_edges * k * k,) tensor of edge indices for each candidate - edge_peak_inds_tensor: (n_edges * k * k, 2) tensor of (src, dst) peak indices</p> Example <p>from sleap_nn.export.utils import build_bottomup_candidate_template peak_ch, edge_inds, edge_peaks = build_bottomup_candidate_template( ...     n_nodes=15, max_peaks_per_node=20, edge_inds=[(1, 2), (1, 5)] ... )</p> Note <p>This function is necessary because <code>get_connection_candidates()</code> in <code>sleap_nn.inference.paf_grouping</code> uses unstable argsort, which shuffles peak indices within each node and breaks alignment with ONNX output ordering.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def build_bottomup_candidate_template(\n    n_nodes: int, max_peaks_per_node: int, edge_inds: List[Tuple[int, int]]\n) -&gt; Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n    \"\"\"Build candidate template matching ONNX wrapper's line_scores ordering.\n\n    The ONNX BottomUpONNXWrapper produces line_scores with shape (n_edges, k*k) where\n    for each edge connecting (src_node, dst_node), position i*k + j corresponds to:\n    - src peak flat index: src_node * k + i\n    - dst peak flat index: dst_node * k + j\n\n    This function builds edge_inds and edge_peak_inds tensors that match this exact\n    ordering, so that line_scores_flat[idx] corresponds to edge_peak_inds[idx].\n\n    Args:\n        n_nodes: Number of nodes in the skeleton.\n        max_peaks_per_node: Maximum peaks per node (k) used during export.\n        edge_inds: List of (src_node, dst_node) tuples defining skeleton edges.\n\n    Returns:\n        Tuple of (peak_channel_inds, edge_inds_tensor, edge_peak_inds_tensor):\n        - peak_channel_inds: (n_nodes * k,) tensor mapping flat peak index to node\n        - edge_inds_tensor: (n_edges * k * k,) tensor of edge indices for each candidate\n        - edge_peak_inds_tensor: (n_edges * k * k, 2) tensor of (src, dst) peak indices\n\n    Example:\n        &gt;&gt;&gt; from sleap_nn.export.utils import build_bottomup_candidate_template\n        &gt;&gt;&gt; peak_ch, edge_inds, edge_peaks = build_bottomup_candidate_template(\n        ...     n_nodes=15, max_peaks_per_node=20, edge_inds=[(1, 2), (1, 5)]\n        ... )\n        &gt;&gt;&gt; # Use with ONNX output:\n        &gt;&gt;&gt; line_scores_flat = line_scores.reshape(-1)\n        &gt;&gt;&gt; valid_scores = line_scores_flat[valid_mask]\n        &gt;&gt;&gt; valid_edge_peaks = edge_peaks[valid_mask]\n\n    Note:\n        This function is necessary because `get_connection_candidates()` in\n        `sleap_nn.inference.paf_grouping` uses unstable argsort, which shuffles\n        peak indices within each node and breaks alignment with ONNX output ordering.\n    \"\"\"\n    import torch\n\n    k = max_peaks_per_node\n    n_edges = len(edge_inds)\n\n    # peak_channel_inds: [0,0,...0, 1,1,...1, ...] (k times each)\n    peak_channel_inds = torch.arange(n_nodes, dtype=torch.int32).repeat_interleave(k)\n\n    edge_inds_list = []\n    edge_peak_inds_list = []\n\n    for edge_idx, (src_node, dst_node) in enumerate(edge_inds):\n        # Build k*k candidate pairs in row-major order (i*k + j)\n        # src indices: [src_node*k + 0, src_node*k + 0, ..., src_node*k + 1, ...]\n        # dst indices: [dst_node*k + 0, dst_node*k + 1, ..., dst_node*k + 0, ...]\n        src_base = src_node * k\n        dst_base = dst_node * k\n\n        src_indices = torch.arange(k, dtype=torch.int32).repeat_interleave(k) + src_base\n        dst_indices = torch.arange(k, dtype=torch.int32).repeat(k) + dst_base\n\n        edge_inds_list.append(torch.full((k * k,), edge_idx, dtype=torch.int32))\n        edge_peak_inds_list.append(torch.stack([src_indices, dst_indices], dim=1))\n\n    if edge_inds_list:\n        edge_inds_tensor = torch.cat(edge_inds_list)\n        edge_peak_inds_tensor = torch.cat(edge_peak_inds_list)\n    else:\n        edge_inds_tensor = torch.empty((0,), dtype=torch.int32)\n        edge_peak_inds_tensor = torch.empty((0, 2), dtype=torch.int32)\n\n    return peak_channel_inds, edge_inds_tensor, edge_peak_inds_tensor\n</code></pre>"},{"location":"api/export/#sleap_nn.export.build_bottomup_candidate_template--use-with-onnx-output","title":"Use with ONNX output:","text":"<p>line_scores_flat = line_scores.reshape(-1) valid_scores = line_scores_flat[valid_mask] valid_edge_peaks = edge_peaks[valid_mask]</p>"},{"location":"api/export/#sleap_nn.export.export_model","title":"<code>export_model(model, save_path, fmt='onnx', input_shape=(1, 1, 512, 512), opset_version=17, output_names=None, verify=True, **kwargs)</code>","text":"<p>Export a model to the requested format.</p> Source code in <code>sleap_nn/export/exporters/__init__.py</code> <pre><code>def export_model(\n    model: torch.nn.Module,\n    save_path: str | Path,\n    fmt: str = \"onnx\",\n    input_shape: Iterable[int] = (1, 1, 512, 512),\n    opset_version: int = 17,\n    output_names: Optional[list] = None,\n    verify: bool = True,\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Export a model to the requested format.\"\"\"\n    fmt = fmt.lower()\n    if fmt == \"onnx\":\n        return export_to_onnx(\n            model,\n            save_path,\n            input_shape=input_shape,\n            opset_version=opset_version,\n            output_names=output_names,\n            verify=verify,\n        )\n    if fmt == \"tensorrt\":\n        return export_to_tensorrt(model, save_path, input_shape=input_shape, **kwargs)\n    if fmt == \"both\":\n        export_to_onnx(\n            model,\n            save_path,\n            input_shape=input_shape,\n            opset_version=opset_version,\n            output_names=output_names,\n            verify=verify,\n        )\n        return export_to_tensorrt(model, save_path, input_shape=input_shape, **kwargs)\n\n    raise ValueError(f\"Unknown export format: {fmt}\")\n</code></pre>"},{"location":"api/export/#sleap_nn.export.export_to_onnx","title":"<code>export_to_onnx(model, save_path, input_shape=(1, 1, 512, 512), input_dtype=torch.uint8, opset_version=17, dynamic_axes=None, input_names=None, output_names=None, do_constant_folding=True, verify=True)</code>","text":"<p>Export a PyTorch model to ONNX.</p> Source code in <code>sleap_nn/export/exporters/onnx_exporter.py</code> <pre><code>def export_to_onnx(\n    model: torch.nn.Module,\n    save_path: str | Path,\n    input_shape: Iterable[int] = (1, 1, 512, 512),\n    input_dtype: torch.dtype = torch.uint8,\n    opset_version: int = 17,\n    dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None,\n    input_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    do_constant_folding: bool = True,\n    verify: bool = True,\n) -&gt; Path:\n    \"\"\"Export a PyTorch model to ONNX.\"\"\"\n    save_path = Path(save_path)\n    model.eval()\n\n    if input_names is None:\n        input_names = [\"image\"]\n    if dynamic_axes is None:\n        dynamic_axes = {\"image\": {0: \"batch\", 2: \"height\", 3: \"width\"}}\n\n    device = None\n    try:\n        device = next(model.parameters()).device\n    except StopIteration:\n        device = torch.device(\"cpu\")\n\n    if input_dtype.is_floating_point:\n        dummy_input = torch.randn(*input_shape, device=device, dtype=input_dtype)\n    else:\n        dummy_input = torch.randint(\n            0, 256, input_shape, device=device, dtype=input_dtype\n        )\n\n    if output_names is None:\n        with torch.no_grad():\n            test_out = model(dummy_input)\n        output_names = _infer_output_names(test_out)\n\n    torch.onnx.export(\n        model,\n        dummy_input,\n        save_path.as_posix(),\n        opset_version=opset_version,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        do_constant_folding=do_constant_folding,\n        dynamo=False,\n    )\n\n    if verify:\n        _verify_onnx(save_path)\n\n    return save_path\n</code></pre>"},{"location":"api/export/#sleap_nn.export.export_to_tensorrt","title":"<code>export_to_tensorrt(model, save_path, input_shape=(1, 1, 512, 512), input_dtype=torch.uint8, precision='fp16', min_shape=None, opt_shape=None, max_shape=None, workspace_size=2 &lt;&lt; 30, method='onnx', verbose=True)</code>","text":"<p>Export a PyTorch model to TensorRT format.</p> <p>This function supports multiple compilation methods: - \"onnx\": Exports to ONNX first, then compiles with TensorRT (most reliable) - \"jit\": Uses torch.jit.trace + torch_tensorrt.compile (alternative)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to export (typically an ONNX wrapper).</p> required <code>save_path</code> <code>str | Path</code> <p>Path to save the TensorRT engine (.trt file).</p> required <code>input_shape</code> <code>Tuple[int, int, int, int]</code> <p>(B, C, H, W) optimal input tensor shape.</p> <code>(1, 1, 512, 512)</code> <code>input_dtype</code> <code>dtype</code> <p>Input tensor dtype (torch.uint8 or torch.float32).</p> <code>uint8</code> <code>precision</code> <code>str</code> <p>Model precision - \"fp32\" or \"fp16\".</p> <code>'fp16'</code> <code>min_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Minimum input shape for dynamic shapes (default: batch=1, H/W halved).</p> <code>None</code> <code>opt_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Optimal input shape (default: same as input_shape).</p> <code>None</code> <code>max_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Maximum input shape (default: batch=16, H/W doubled).</p> <code>None</code> <code>workspace_size</code> <code>int</code> <p>TensorRT workspace size in bytes (default 2GB).</p> <code>2 &lt;&lt; 30</code> <code>method</code> <code>str</code> <p>Compilation method - \"onnx\" or \"jit\".</p> <code>'onnx'</code> <code>verbose</code> <code>bool</code> <p>Print export info.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the exported TensorRT engine.</p> Note <p>TensorRT models are NOT cross-platform. The exported model will only work on the same GPU architecture and TensorRT version used for export.</p> Source code in <code>sleap_nn/export/exporters/tensorrt_exporter.py</code> <pre><code>def export_to_tensorrt(\n    model: nn.Module,\n    save_path: str | Path,\n    input_shape: Tuple[int, int, int, int] = (1, 1, 512, 512),\n    input_dtype: torch.dtype = torch.uint8,\n    precision: str = \"fp16\",\n    min_shape: Optional[Tuple[int, int, int, int]] = None,\n    opt_shape: Optional[Tuple[int, int, int, int]] = None,\n    max_shape: Optional[Tuple[int, int, int, int]] = None,\n    workspace_size: int = 2 &lt;&lt; 30,  # 2GB default\n    method: str = \"onnx\",\n    verbose: bool = True,\n) -&gt; Path:\n    \"\"\"Export a PyTorch model to TensorRT format.\n\n    This function supports multiple compilation methods:\n    - \"onnx\": Exports to ONNX first, then compiles with TensorRT (most reliable)\n    - \"jit\": Uses torch.jit.trace + torch_tensorrt.compile (alternative)\n\n    Args:\n        model: The PyTorch model to export (typically an ONNX wrapper).\n        save_path: Path to save the TensorRT engine (.trt file).\n        input_shape: (B, C, H, W) optimal input tensor shape.\n        input_dtype: Input tensor dtype (torch.uint8 or torch.float32).\n        precision: Model precision - \"fp32\" or \"fp16\".\n        min_shape: Minimum input shape for dynamic shapes (default: batch=1, H/W halved).\n        opt_shape: Optimal input shape (default: same as input_shape).\n        max_shape: Maximum input shape (default: batch=16, H/W doubled).\n        workspace_size: TensorRT workspace size in bytes (default 2GB).\n        method: Compilation method - \"onnx\" or \"jit\".\n        verbose: Print export info.\n\n    Returns:\n        Path to the exported TensorRT engine.\n\n    Note:\n        TensorRT models are NOT cross-platform. The exported model will only\n        work on the same GPU architecture and TensorRT version used for export.\n    \"\"\"\n    import tensorrt as trt\n\n    model.eval()\n    device = next(model.parameters()).device\n\n    save_path = Path(save_path)\n    if not save_path.suffix:\n        save_path = save_path.with_suffix(\".trt\")\n\n    B, C, H, W = input_shape\n\n    if min_shape is None:\n        min_shape = (1, C, H // 2, W // 2)\n    if opt_shape is None:\n        opt_shape = input_shape\n    if max_shape is None:\n        max_shape = (min(16, B * 4), C, H * 2, W * 2)\n\n    if verbose:\n        print(f\"Exporting model to TensorRT...\")\n        print(f\"  Input shape: {input_shape}\")\n        print(f\"  Min/Opt/Max: {min_shape} / {opt_shape} / {max_shape}\")\n        print(f\"  Precision: {precision}\")\n        print(f\"  Workspace: {workspace_size / 1e9:.1f} GB\")\n        print(f\"  Method: {method}\")\n\n    if method == \"onnx\":\n        return _export_tensorrt_onnx(\n            model,\n            save_path,\n            input_shape,\n            input_dtype,\n            min_shape,\n            opt_shape,\n            max_shape,\n            precision,\n            workspace_size,\n            verbose,\n        )\n    elif method == \"jit\":\n        return _export_tensorrt_jit(\n            model,\n            save_path,\n            input_shape,\n            input_dtype,\n            min_shape,\n            opt_shape,\n            max_shape,\n            precision,\n            workspace_size,\n            verbose,\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}. Use 'onnx' or 'jit'.\")\n</code></pre>"},{"location":"api/export/#sleap_nn.export.load_exported_model","title":"<code>load_exported_model(model_path, runtime='auto', device='auto', **kwargs)</code>","text":"<p>Load an exported model and return a predictor instance.</p> Source code in <code>sleap_nn/export/predictors/__init__.py</code> <pre><code>def load_exported_model(\n    model_path: str | Path,\n    runtime: str = \"auto\",\n    device: str = \"auto\",\n    **kwargs,\n) -&gt; ExportPredictor:\n    \"\"\"Load an exported model and return a predictor instance.\"\"\"\n    if runtime == \"auto\":\n        runtime = detect_runtime(model_path)\n\n    if device == \"auto\":\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    if runtime == \"onnx\":\n        return ONNXPredictor(str(model_path), device=device, **kwargs)\n    if runtime == \"tensorrt\":\n        return TensorRTPredictor(str(model_path), device=device, **kwargs)\n\n    raise ValueError(f\"Unknown runtime: {runtime}\")\n</code></pre>"},{"location":"api/export/cli/","title":"cli","text":""},{"location":"api/export/cli/#sleap_nn.export.cli","title":"<code>sleap_nn.export.cli</code>","text":"<p>CLI entry points for export workflows.</p> <p>Functions:</p> Name Description <code>export</code> <p>Export trained models to ONNX/TensorRT formats.</p> <code>predict</code> <p>Run inference on exported models and save predictions to SLP.</p>"},{"location":"api/export/cli/#sleap_nn.export.cli.export","title":"<code>export(model_paths, output, fmt, opset_version, max_instances, max_batch_size, input_scale, input_height, input_width, crop_size, max_peaks_per_node, n_line_points, max_edge_length_ratio, dist_penalty_weight, device, precision, verify)</code>","text":"<p>Export trained models to ONNX/TensorRT formats.</p> Source code in <code>sleap_nn/export/cli.py</code> <pre><code>@click.command()\n@click.argument(\n    \"model_paths\",\n    nargs=-1,\n    type=click.Path(exists=True, file_okay=False, path_type=Path),\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    type=click.Path(file_okay=False, path_type=Path),\n    default=None,\n    help=\"Output directory for exported model files.\",\n)\n@click.option(\n    \"--format\",\n    \"-f\",\n    \"fmt\",\n    type=click.Choice([\"onnx\", \"tensorrt\", \"both\"], case_sensitive=False),\n    default=\"onnx\",\n    show_default=True,\n)\n@click.option(\"--opset-version\", type=int, default=17, show_default=True)\n@click.option(\"--max-instances\", type=int, default=20, show_default=True)\n@click.option(\"--max-batch-size\", type=int, default=8, show_default=True)\n@click.option(\"--input-scale\", type=float, default=None)\n@click.option(\"--input-height\", type=int, default=None)\n@click.option(\"--input-width\", type=int, default=None)\n@click.option(\"--crop-size\", type=int, default=None)\n@click.option(\"--max-peaks-per-node\", type=int, default=20, show_default=True)\n@click.option(\"--n-line-points\", type=int, default=10, show_default=True)\n@click.option(\"--max-edge-length-ratio\", type=float, default=0.25, show_default=True)\n@click.option(\"--dist-penalty-weight\", type=float, default=1.0, show_default=True)\n@click.option(\"--device\", type=str, default=\"cpu\", show_default=True)\n@click.option(\n    \"--precision\",\n    type=click.Choice([\"fp32\", \"fp16\"], case_sensitive=False),\n    default=\"fp16\",\n    show_default=True,\n    help=\"TensorRT precision mode.\",\n)\n@click.option(\"--verify/--no-verify\", default=True, show_default=True)\ndef export(\n    model_paths: tuple[Path, ...],\n    output: Optional[Path],\n    fmt: str,\n    opset_version: int,\n    max_instances: int,\n    max_batch_size: int,\n    input_scale: Optional[float],\n    input_height: Optional[int],\n    input_width: Optional[int],\n    crop_size: Optional[int],\n    max_peaks_per_node: int,\n    n_line_points: int,\n    max_edge_length_ratio: float,\n    dist_penalty_weight: float,\n    device: str,\n    precision: str,\n    verify: bool,\n) -&gt; None:\n    \"\"\"Export trained models to ONNX/TensorRT formats.\"\"\"\n    fmt = fmt.lower()\n\n    if not model_paths:\n        raise click.ClickException(\"Provide at least one model path to export.\")\n\n    model_paths = list(model_paths)\n    cfgs = [load_training_config(path) for path in model_paths]\n    model_types = [resolve_model_type(cfg) for cfg in cfgs]\n    backbone_types = [resolve_backbone_type(cfg) for cfg in cfgs]\n\n    if len(model_paths) == 1:\n        model_path = model_paths[0]\n        cfg = cfgs[0]\n        model_type = model_types[0]\n        backbone_type = backbone_types[0]\n\n        if model_type not in (\n            \"centroid\",\n            \"centered_instance\",\n            \"bottomup\",\n            \"single_instance\",\n            \"multi_class_topdown\",\n            \"multi_class_bottomup\",\n        ):\n            raise click.ClickException(\n                f\"Model type '{model_type}' is not supported for export yet.\"\n            )\n\n        ckpt_path = model_path / \"best.ckpt\"\n        if not ckpt_path.exists():\n            raise click.ClickException(f\"Checkpoint not found: {ckpt_path}\")\n\n        lightning_model = _load_lightning_model(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            cfg=cfg,\n            ckpt_path=ckpt_path,\n            device=device,\n        )\n\n        torch_model = lightning_model.model\n        torch_model.eval()\n        torch_model.to(device)\n\n        export_dir = output or (model_path / \"exported\")\n        export_dir.mkdir(parents=True, exist_ok=True)\n\n        resolved_scale = (\n            input_scale if input_scale is not None else resolve_input_scale(cfg)\n        )\n        output_stride = resolve_output_stride(cfg, model_type)\n        resolved_crop_size = (\n            (crop_size, crop_size) if crop_size is not None else resolve_crop_size(cfg)\n        )\n        metadata_max_instances = None\n        metadata_max_peaks = None\n        metadata_n_classes = None\n        metadata_class_names = None\n\n        if model_type == \"centroid\":\n            wrapper = CentroidONNXWrapper(\n                torch_model,\n                max_instances=max_instances,\n                output_stride=output_stride,\n                input_scale=resolved_scale,\n            )\n            output_names = [\"centroids\", \"centroid_vals\", \"instance_valid\"]\n            metadata_max_instances = max_instances\n            node_names = resolve_node_names(cfg, model_type)\n            edge_inds = resolve_edge_inds(cfg, node_names)\n        elif model_type == \"centered_instance\":\n            wrapper = CenteredInstanceONNXWrapper(\n                torch_model,\n                output_stride=output_stride,\n                input_scale=resolved_scale,\n            )\n            output_names = [\"peaks\", \"peak_vals\"]\n            node_names = resolve_node_names(cfg, model_type)\n            edge_inds = resolve_edge_inds(cfg, node_names)\n        elif model_type == \"bottomup\":\n            node_names = resolve_node_names(cfg, model_type)\n            edge_inds = resolve_edge_inds(cfg, node_names)\n            pafs_output_stride = resolve_pafs_output_stride(cfg)\n            wrapper = BottomUpONNXWrapper(\n                torch_model,\n                skeleton_edges=edge_inds,\n                n_nodes=len(node_names),\n                max_peaks_per_node=max_peaks_per_node,\n                n_line_points=n_line_points,\n                cms_output_stride=output_stride,\n                pafs_output_stride=pafs_output_stride,\n                max_edge_length_ratio=max_edge_length_ratio,\n                dist_penalty_weight=dist_penalty_weight,\n                input_scale=resolved_scale,\n            )\n            output_names = [\n                \"peaks\",\n                \"peak_vals\",\n                \"peak_mask\",\n                \"line_scores\",\n                \"candidate_mask\",\n            ]\n            metadata_max_peaks = max_peaks_per_node\n        elif model_type == \"single_instance\":\n            wrapper = SingleInstanceONNXWrapper(\n                torch_model,\n                output_stride=output_stride,\n                input_scale=resolved_scale,\n            )\n            output_names = [\"peaks\", \"peak_vals\"]\n            node_names = resolve_node_names(cfg, model_type)\n            edge_inds = resolve_edge_inds(cfg, node_names)\n        elif model_type == \"multi_class_topdown\":\n            n_classes = resolve_n_classes(cfg, model_type)\n            class_names = resolve_class_names(cfg, model_type)\n            wrapper = TopDownMultiClassONNXWrapper(\n                torch_model,\n                output_stride=output_stride,\n                input_scale=resolved_scale,\n                n_classes=n_classes,\n            )\n            output_names = [\"peaks\", \"peak_vals\", \"class_logits\"]\n            node_names = resolve_node_names(cfg, model_type)\n            edge_inds = resolve_edge_inds(cfg, node_names)\n            metadata_n_classes = n_classes\n            metadata_class_names = class_names\n        elif model_type == \"multi_class_bottomup\":\n            node_names = resolve_node_names(cfg, model_type)\n            edge_inds = resolve_edge_inds(cfg, node_names)\n            n_classes = resolve_n_classes(cfg, model_type)\n            class_names = resolve_class_names(cfg, model_type)\n            class_maps_output_stride = resolve_class_maps_output_stride(cfg)\n            wrapper = BottomUpMultiClassONNXWrapper(\n                torch_model,\n                n_nodes=len(node_names),\n                n_classes=n_classes,\n                max_peaks_per_node=max_peaks_per_node,\n                cms_output_stride=output_stride,\n                class_maps_output_stride=class_maps_output_stride,\n                input_scale=resolved_scale,\n            )\n            output_names = [\"peaks\", \"peak_vals\", \"peak_mask\", \"class_probs\"]\n            metadata_max_peaks = max_peaks_per_node\n            metadata_n_classes = n_classes\n            metadata_class_names = class_names\n        else:\n            raise click.ClickException(\n                f\"Model type '{model_type}' is not supported for export yet.\"\n            )\n\n        wrapper.eval()\n        wrapper.to(device)\n\n        input_shape = resolve_input_shape(\n            cfg, input_height=input_height, input_width=input_width\n        )\n        model_out_path = export_dir / \"model.onnx\"\n\n        export_to_onnx(\n            wrapper,\n            model_out_path,\n            input_shape=input_shape,\n            input_dtype=torch.uint8,\n            opset_version=opset_version,\n            output_names=output_names,\n            verify=verify,\n        )\n\n        training_config_path = _copy_training_config(model_path, export_dir, None)\n        if training_config_path is not None:\n            training_config_hash = hash_file(training_config_path)\n            training_config_text = training_config_path.read_text()\n        else:\n            training_config_hash = \"\"\n            training_config_text = None\n\n        metadata = build_base_metadata(\n            export_format=\"onnx\",\n            model_type=model_type,\n            model_name=model_path.name,\n            checkpoint_path=str(ckpt_path),\n            backbone=backbone_type,\n            n_nodes=len(node_names),\n            n_edges=len(edge_inds),\n            node_names=node_names,\n            edge_inds=edge_inds,\n            input_scale=resolved_scale,\n            input_channels=resolve_input_channels(cfg),\n            output_stride=output_stride,\n            crop_size=resolved_crop_size,\n            max_instances=metadata_max_instances,\n            max_peaks_per_node=metadata_max_peaks,\n            max_batch_size=max_batch_size,\n            precision=\"fp32\",\n            training_config_hash=training_config_hash,\n            training_config_embedded=training_config_text is not None,\n            input_dtype=\"uint8\",\n            normalization=\"0_to_1\",\n            n_classes=metadata_n_classes,\n            class_names=metadata_class_names,\n        )\n\n        metadata.save(export_dir / \"export_metadata.json\")\n\n        if training_config_text is not None:\n            try:\n                embed_metadata_in_onnx(model_out_path, metadata, training_config_text)\n            except ImportError:\n                pass\n\n        # Export to TensorRT if requested\n        if fmt in (\"tensorrt\", \"both\"):\n            trt_out_path = export_dir / \"model.trt\"\n            B, C, H, W = input_shape\n\n            # For centered_instance and single_instance models, use crop size\n            # for TensorRT shape profiles since inference uses cropped inputs\n            if model_type in (\"centered_instance\", \"single_instance\"):\n                if resolved_crop_size is not None:\n                    crop_h, crop_w = resolved_crop_size\n                    trt_input_shape = (1, C, crop_h, crop_w)\n                    # Use crop size for min/opt, allow flexibility for max\n                    trt_min_shape = (1, C, crop_h, crop_w)\n                    trt_opt_shape = (1, C, crop_h, crop_w)\n                    trt_max_shape = (max_batch_size, C, crop_h * 2, crop_w * 2)\n                else:\n                    trt_input_shape = input_shape\n                    trt_min_shape = None\n                    trt_opt_shape = None\n                    trt_max_shape = (max_batch_size, C, H * 2, W * 2)\n            else:\n                trt_input_shape = input_shape\n                trt_min_shape = None\n                trt_opt_shape = None\n                trt_max_shape = (max_batch_size, C, H * 2, W * 2)\n\n            export_to_tensorrt(\n                wrapper,\n                trt_out_path,\n                input_shape=trt_input_shape,\n                input_dtype=torch.uint8,\n                precision=precision,\n                min_shape=trt_min_shape,\n                opt_shape=trt_opt_shape,\n                max_shape=trt_max_shape,\n                verbose=True,\n            )\n            # Update metadata for TensorRT\n            trt_metadata = build_base_metadata(\n                export_format=\"tensorrt\",\n                model_type=model_type,\n                model_name=model_path.name,\n                checkpoint_path=str(ckpt_path),\n                backbone=backbone_type,\n                n_nodes=len(node_names),\n                n_edges=len(edge_inds),\n                node_names=node_names,\n                edge_inds=edge_inds,\n                input_scale=resolved_scale,\n                input_channels=resolve_input_channels(cfg),\n                output_stride=output_stride,\n                crop_size=resolved_crop_size,\n                max_instances=metadata_max_instances,\n                max_peaks_per_node=metadata_max_peaks,\n                max_batch_size=max_batch_size,\n                precision=precision,\n                training_config_hash=training_config_hash,\n                training_config_embedded=training_config_text is not None,\n                input_dtype=\"uint8\",\n                normalization=\"0_to_1\",\n                n_classes=metadata_n_classes,\n                class_names=metadata_class_names,\n            )\n            trt_metadata.save(export_dir / \"model.trt.metadata.json\")\n        return\n\n    if len(model_paths) == 2 and set(model_types) == {\n        \"centroid\",\n        \"centered_instance\",\n    }:\n        centroid_idx = model_types.index(\"centroid\")\n        instance_idx = model_types.index(\"centered_instance\")\n\n        centroid_path = model_paths[centroid_idx]\n        instance_path = model_paths[instance_idx]\n        centroid_cfg = cfgs[centroid_idx]\n        instance_cfg = cfgs[instance_idx]\n        centroid_backbone = backbone_types[centroid_idx]\n        instance_backbone = backbone_types[instance_idx]\n\n        centroid_ckpt = centroid_path / \"best.ckpt\"\n        instance_ckpt = instance_path / \"best.ckpt\"\n        if not centroid_ckpt.exists():\n            raise click.ClickException(f\"Checkpoint not found: {centroid_ckpt}\")\n        if not instance_ckpt.exists():\n            raise click.ClickException(f\"Checkpoint not found: {instance_ckpt}\")\n\n        centroid_model = _load_lightning_model(\n            model_type=\"centroid\",\n            backbone_type=centroid_backbone,\n            cfg=centroid_cfg,\n            ckpt_path=centroid_ckpt,\n            device=device,\n        ).model\n        instance_model = _load_lightning_model(\n            model_type=\"centered_instance\",\n            backbone_type=instance_backbone,\n            cfg=instance_cfg,\n            ckpt_path=instance_ckpt,\n            device=device,\n        ).model\n\n        centroid_model.eval()\n        instance_model.eval()\n        centroid_model.to(device)\n        instance_model.to(device)\n\n        export_dir = output or (centroid_path / \"exported_topdown\")\n        export_dir.mkdir(parents=True, exist_ok=True)\n\n        centroid_scale = (\n            input_scale\n            if input_scale is not None\n            else resolve_input_scale(centroid_cfg)\n        )\n        instance_scale = (\n            input_scale\n            if input_scale is not None\n            else resolve_input_scale(instance_cfg)\n        )\n        centroid_stride = resolve_output_stride(centroid_cfg, \"centroid\")\n        instance_stride = resolve_output_stride(instance_cfg, \"centered_instance\")\n\n        resolved_crop = resolve_crop_size(instance_cfg)\n        if crop_size is not None:\n            resolved_crop = (crop_size, crop_size)\n        if resolved_crop is None:\n            raise click.ClickException(\n                \"Top-down export requires crop_size. Provide --crop-size or ensure \"\n                \"data_config.preprocessing.crop_size is set.\"\n            )\n\n        node_names = resolve_node_names(instance_cfg, \"centered_instance\")\n        edge_inds = resolve_edge_inds(instance_cfg, node_names)\n\n        wrapper = TopDownONNXWrapper(\n            centroid_model=centroid_model,\n            instance_model=instance_model,\n            max_instances=max_instances,\n            crop_size=resolved_crop,\n            centroid_output_stride=centroid_stride,\n            instance_output_stride=instance_stride,\n            centroid_input_scale=centroid_scale,\n            instance_input_scale=instance_scale,\n            n_nodes=len(node_names),\n        )\n        wrapper.eval()\n        wrapper.to(device)\n\n        input_shape = resolve_input_shape(\n            centroid_cfg, input_height=input_height, input_width=input_width\n        )\n        model_out_path = export_dir / \"model.onnx\"\n\n        export_to_onnx(\n            wrapper,\n            model_out_path,\n            input_shape=input_shape,\n            input_dtype=torch.uint8,\n            opset_version=opset_version,\n            output_names=[\n                \"centroids\",\n                \"centroid_vals\",\n                \"peaks\",\n                \"peak_vals\",\n                \"instance_valid\",\n            ],\n            verify=verify,\n        )\n\n        centroid_cfg_path = _copy_training_config(centroid_path, export_dir, \"centroid\")\n        instance_cfg_path = _copy_training_config(\n            instance_path, export_dir, \"centered_instance\"\n        )\n        config_payload = {}\n        config_hashes = []\n        if centroid_cfg_path is not None:\n            config_payload[\"centroid\"] = centroid_cfg_path.read_text()\n            config_hashes.append(f\"centroid:{hash_file(centroid_cfg_path)}\")\n        if instance_cfg_path is not None:\n            config_payload[\"centered_instance\"] = instance_cfg_path.read_text()\n            config_hashes.append(f\"centered_instance:{hash_file(instance_cfg_path)}\")\n\n        training_config_hash = \";\".join(config_hashes) if config_hashes else \"\"\n        training_config_text = json.dumps(config_payload) if config_payload else None\n\n        metadata = build_base_metadata(\n            export_format=\"onnx\",\n            model_type=\"topdown\",\n            model_name=f\"{centroid_path.name}+{instance_path.name}\",\n            checkpoint_path=(\n                f\"centroid:{centroid_ckpt};centered_instance:{instance_ckpt}\"\n            ),\n            backbone=(\n                f\"centroid:{centroid_backbone};centered_instance:{instance_backbone}\"\n            ),\n            n_nodes=len(node_names),\n            n_edges=len(edge_inds),\n            node_names=node_names,\n            edge_inds=edge_inds,\n            input_scale=centroid_scale,\n            input_channels=resolve_input_channels(centroid_cfg),\n            output_stride=instance_stride,\n            crop_size=resolved_crop,\n            max_instances=max_instances,\n            max_batch_size=max_batch_size,\n            precision=\"fp32\",\n            training_config_hash=training_config_hash,\n            training_config_embedded=training_config_text is not None,\n            input_dtype=\"uint8\",\n            normalization=\"0_to_1\",\n        )\n\n        metadata.save(export_dir / \"export_metadata.json\")\n\n        if training_config_text is not None:\n            try:\n                embed_metadata_in_onnx(model_out_path, metadata, training_config_text)\n            except ImportError:\n                pass\n\n        # Export to TensorRT if requested\n        if fmt in (\"tensorrt\", \"both\"):\n            trt_out_path = export_dir / \"model.trt\"\n            B, C, H, W = input_shape\n            export_to_tensorrt(\n                wrapper,\n                trt_out_path,\n                input_shape=input_shape,\n                input_dtype=torch.uint8,\n                precision=precision,\n                max_shape=(max_batch_size, C, H * 2, W * 2),\n                verbose=True,\n            )\n            # Update metadata for TensorRT\n            trt_metadata = build_base_metadata(\n                export_format=\"tensorrt\",\n                model_type=\"topdown\",\n                model_name=f\"{centroid_path.name}+{instance_path.name}\",\n                checkpoint_path=(\n                    f\"centroid:{centroid_ckpt};centered_instance:{instance_ckpt}\"\n                ),\n                backbone=(\n                    f\"centroid:{centroid_backbone};centered_instance:{instance_backbone}\"\n                ),\n                n_nodes=len(node_names),\n                n_edges=len(edge_inds),\n                node_names=node_names,\n                edge_inds=edge_inds,\n                input_scale=centroid_scale,\n                input_channels=resolve_input_channels(centroid_cfg),\n                output_stride=instance_stride,\n                crop_size=resolved_crop,\n                max_instances=max_instances,\n                max_batch_size=max_batch_size,\n                precision=precision,\n                training_config_hash=training_config_hash,\n                training_config_embedded=training_config_text is not None,\n                input_dtype=\"uint8\",\n                normalization=\"0_to_1\",\n            )\n            trt_metadata.save(export_dir / \"model.trt.metadata.json\")\n        return\n\n    # Combined multiclass top-down export (centroid + multi_class_topdown)\n    if len(model_paths) == 2 and set(model_types) == {\n        \"centroid\",\n        \"multi_class_topdown\",\n    }:\n        centroid_idx = model_types.index(\"centroid\")\n        instance_idx = model_types.index(\"multi_class_topdown\")\n\n        centroid_path = model_paths[centroid_idx]\n        instance_path = model_paths[instance_idx]\n        centroid_cfg = cfgs[centroid_idx]\n        instance_cfg = cfgs[instance_idx]\n        centroid_backbone = backbone_types[centroid_idx]\n        instance_backbone = backbone_types[instance_idx]\n\n        centroid_ckpt = centroid_path / \"best.ckpt\"\n        instance_ckpt = instance_path / \"best.ckpt\"\n        if not centroid_ckpt.exists():\n            raise click.ClickException(f\"Checkpoint not found: {centroid_ckpt}\")\n        if not instance_ckpt.exists():\n            raise click.ClickException(f\"Checkpoint not found: {instance_ckpt}\")\n\n        centroid_model = _load_lightning_model(\n            model_type=\"centroid\",\n            backbone_type=centroid_backbone,\n            cfg=centroid_cfg,\n            ckpt_path=centroid_ckpt,\n            device=device,\n        ).model\n        instance_model = _load_lightning_model(\n            model_type=\"multi_class_topdown\",\n            backbone_type=instance_backbone,\n            cfg=instance_cfg,\n            ckpt_path=instance_ckpt,\n            device=device,\n        ).model\n\n        centroid_model.eval()\n        instance_model.eval()\n        centroid_model.to(device)\n        instance_model.to(device)\n\n        export_dir = output or (centroid_path / \"exported_multi_class_topdown\")\n        export_dir.mkdir(parents=True, exist_ok=True)\n\n        centroid_scale = (\n            input_scale\n            if input_scale is not None\n            else resolve_input_scale(centroid_cfg)\n        )\n        instance_scale = (\n            input_scale\n            if input_scale is not None\n            else resolve_input_scale(instance_cfg)\n        )\n        centroid_stride = resolve_output_stride(centroid_cfg, \"centroid\")\n        instance_stride = resolve_output_stride(instance_cfg, \"multi_class_topdown\")\n\n        resolved_crop = resolve_crop_size(instance_cfg)\n        if crop_size is not None:\n            resolved_crop = (crop_size, crop_size)\n        if resolved_crop is None:\n            raise click.ClickException(\n                \"Multiclass top-down export requires crop_size. Provide --crop-size or \"\n                \"ensure data_config.preprocessing.crop_size is set.\"\n            )\n\n        node_names = resolve_node_names(instance_cfg, \"multi_class_topdown\")\n        edge_inds = resolve_edge_inds(instance_cfg, node_names)\n        n_classes = resolve_n_classes(instance_cfg, \"multi_class_topdown\")\n        class_names = resolve_class_names(instance_cfg, \"multi_class_topdown\")\n\n        wrapper = TopDownMultiClassCombinedONNXWrapper(\n            centroid_model=centroid_model,\n            instance_model=instance_model,\n            max_instances=max_instances,\n            crop_size=resolved_crop,\n            centroid_output_stride=centroid_stride,\n            instance_output_stride=instance_stride,\n            centroid_input_scale=centroid_scale,\n            instance_input_scale=instance_scale,\n            n_nodes=len(node_names),\n            n_classes=n_classes,\n        )\n        wrapper.eval()\n        wrapper.to(device)\n\n        input_shape = resolve_input_shape(\n            centroid_cfg, input_height=input_height, input_width=input_width\n        )\n        model_out_path = export_dir / \"model.onnx\"\n\n        export_to_onnx(\n            wrapper,\n            model_out_path,\n            input_shape=input_shape,\n            input_dtype=torch.uint8,\n            opset_version=opset_version,\n            output_names=[\n                \"centroids\",\n                \"centroid_vals\",\n                \"peaks\",\n                \"peak_vals\",\n                \"class_logits\",\n                \"instance_valid\",\n            ],\n            verify=verify,\n        )\n\n        centroid_cfg_path = _copy_training_config(centroid_path, export_dir, \"centroid\")\n        instance_cfg_path = _copy_training_config(\n            instance_path, export_dir, \"multi_class_topdown\"\n        )\n        config_payload = {}\n        config_hashes = []\n        if centroid_cfg_path is not None:\n            config_payload[\"centroid\"] = centroid_cfg_path.read_text()\n            config_hashes.append(f\"centroid:{hash_file(centroid_cfg_path)}\")\n        if instance_cfg_path is not None:\n            config_payload[\"multi_class_topdown\"] = instance_cfg_path.read_text()\n            config_hashes.append(f\"multi_class_topdown:{hash_file(instance_cfg_path)}\")\n\n        training_config_hash = \";\".join(config_hashes) if config_hashes else \"\"\n        training_config_text = json.dumps(config_payload) if config_payload else None\n\n        metadata = build_base_metadata(\n            export_format=\"onnx\",\n            model_type=\"multi_class_topdown_combined\",\n            model_name=f\"{centroid_path.name}+{instance_path.name}\",\n            checkpoint_path=(\n                f\"centroid:{centroid_ckpt};multi_class_topdown:{instance_ckpt}\"\n            ),\n            backbone=(\n                f\"centroid:{centroid_backbone};multi_class_topdown:{instance_backbone}\"\n            ),\n            n_nodes=len(node_names),\n            n_edges=len(edge_inds),\n            node_names=node_names,\n            edge_inds=edge_inds,\n            input_scale=centroid_scale,\n            input_channels=resolve_input_channels(centroid_cfg),\n            output_stride=instance_stride,\n            crop_size=resolved_crop,\n            max_instances=max_instances,\n            max_batch_size=max_batch_size,\n            training_config_hash=training_config_hash,\n            training_config_embedded=training_config_text is not None,\n            input_dtype=\"uint8\",\n            normalization=\"0_to_1\",\n            n_classes=n_classes,\n            class_names=class_names,\n        )\n        metadata.save(export_dir / \"export_metadata.json\")\n        click.echo(f\"ONNX model exported to: {model_out_path}\")\n        click.echo(f\"Metadata saved to: {export_dir / 'export_metadata.json'}\")\n\n        # TensorRT export for combined multiclass top-down\n        if fmt in (\"tensorrt\", \"both\"):\n            trt_out_path = export_dir / \"model.trt\"\n            B, C, H, W = input_shape\n            export_to_tensorrt(\n                wrapper,\n                trt_out_path,\n                input_shape=input_shape,\n                input_dtype=torch.uint8,\n                precision=precision,\n                max_shape=(max_batch_size, C, H * 2, W * 2),\n                verbose=True,\n            )\n            trt_metadata = build_base_metadata(\n                export_format=\"tensorrt\",\n                model_type=\"multi_class_topdown_combined\",\n                model_name=f\"{centroid_path.name}+{instance_path.name}\",\n                checkpoint_path=(\n                    f\"centroid:{centroid_ckpt};multi_class_topdown:{instance_ckpt}\"\n                ),\n                backbone=(\n                    f\"centroid:{centroid_backbone};multi_class_topdown:{instance_backbone}\"\n                ),\n                n_nodes=len(node_names),\n                n_edges=len(edge_inds),\n                node_names=node_names,\n                edge_inds=edge_inds,\n                input_scale=centroid_scale,\n                input_channels=resolve_input_channels(centroid_cfg),\n                output_stride=instance_stride,\n                crop_size=resolved_crop,\n                max_instances=max_instances,\n                max_batch_size=max_batch_size,\n                precision=precision,\n                training_config_hash=training_config_hash,\n                training_config_embedded=training_config_text is not None,\n                input_dtype=\"uint8\",\n                normalization=\"0_to_1\",\n                n_classes=n_classes,\n                class_names=class_names,\n            )\n            trt_metadata.save(export_dir / \"model.trt.metadata.json\")\n        return\n\n    raise click.ClickException(\n        \"Provide one model path for centroid/centered-instance/bottom-up export, \"\n        \"or two paths (centroid + centered_instance or centroid + multi_class_topdown) \"\n        \"for combined top-down export.\"\n    )\n</code></pre>"},{"location":"api/export/cli/#sleap_nn.export.cli.predict","title":"<code>predict(export_dir, video_path, output, runtime, device, batch_size, n_frames, max_edge_length_ratio, dist_penalty_weight, n_points, min_instance_peaks, min_line_scores, peak_conf_threshold, max_instances)</code>","text":"<p>Run inference on exported models and save predictions to SLP.</p> <p>EXPORT_DIR is the directory containing the exported model (model.onnx or model.trt) along with export_metadata.json and training_config.yaml.</p> <p>VIDEO_PATH is the path to the video file to process.</p> Source code in <code>sleap_nn/export/cli.py</code> <pre><code>@click.command()\n@click.argument(\n    \"export_dir\",\n    type=click.Path(exists=True, file_okay=False, path_type=Path),\n)\n@click.argument(\n    \"video_path\",\n    type=click.Path(exists=True, dir_okay=False, path_type=Path),\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    type=click.Path(dir_okay=False, path_type=Path),\n    default=None,\n    help=\"Output SLP file path. Default: video_name.predictions.slp\",\n)\n@click.option(\n    \"--runtime\",\n    type=click.Choice([\"auto\", \"onnx\", \"tensorrt\"], case_sensitive=False),\n    default=\"auto\",\n    show_default=True,\n    help=\"Runtime to use for inference.\",\n)\n@click.option(\"--device\", type=str, default=\"auto\", show_default=True)\n@click.option(\"--batch-size\", type=int, default=4, show_default=True)\n@click.option(\"--n-frames\", type=int, default=None, help=\"Limit to first N frames.\")\n@click.option(\n    \"--max-edge-length-ratio\",\n    type=float,\n    default=0.25,\n    show_default=True,\n    help=\"Bottom-up: max edge length as ratio of PAF dimensions.\",\n)\n@click.option(\n    \"--dist-penalty-weight\",\n    type=float,\n    default=1.0,\n    show_default=True,\n    help=\"Bottom-up: weight for distance penalty in PAF scoring.\",\n)\n@click.option(\n    \"--n-points\",\n    type=int,\n    default=10,\n    show_default=True,\n    help=\"Bottom-up: number of points to sample along PAF.\",\n)\n@click.option(\n    \"--min-instance-peaks\",\n    type=float,\n    default=0,\n    show_default=True,\n    help=\"Bottom-up: minimum peaks required per instance.\",\n)\n@click.option(\n    \"--min-line-scores\",\n    type=float,\n    default=-0.5,\n    show_default=True,\n    help=\"Bottom-up: minimum line score threshold.\",\n)\n@click.option(\n    \"--peak-conf-threshold\",\n    type=float,\n    default=0.1,\n    show_default=True,\n    help=\"Bottom-up: peak confidence threshold for filtering candidates.\",\n)\n@click.option(\n    \"--max-instances\",\n    type=int,\n    default=None,\n    help=\"Maximum instances to output per frame.\",\n)\ndef predict(\n    export_dir: Path,\n    video_path: Path,\n    output: Optional[Path],\n    runtime: str,\n    device: str,\n    batch_size: int,\n    n_frames: Optional[int],\n    max_edge_length_ratio: float,\n    dist_penalty_weight: float,\n    n_points: int,\n    min_instance_peaks: float,\n    min_line_scores: float,\n    peak_conf_threshold: float,\n    max_instances: Optional[int],\n) -&gt; None:\n    \"\"\"Run inference on exported models and save predictions to SLP.\n\n    EXPORT_DIR is the directory containing the exported model (model.onnx or model.trt)\n    along with export_metadata.json and training_config.yaml.\n\n    VIDEO_PATH is the path to the video file to process.\n    \"\"\"\n    import time\n    from datetime import datetime\n\n    import numpy as np\n    import sleap_io as sio\n\n    from sleap_nn.export.metadata import ExportMetadata\n    from sleap_nn.export.predictors import load_exported_model\n    from sleap_nn.export.utils import build_bottomup_candidate_template\n    from sleap_nn.inference.paf_grouping import PAFScorer\n    from sleap_nn.inference.utils import get_skeleton_from_config\n\n    # Load metadata\n    metadata_path = export_dir / \"export_metadata.json\"\n    if not metadata_path.exists():\n        raise click.ClickException(f\"Metadata not found: {metadata_path}\")\n    metadata = ExportMetadata.load(metadata_path)\n\n    # Find model file\n    onnx_path = export_dir / \"model.onnx\"\n    trt_path = export_dir / \"model.trt\"\n\n    if runtime == \"auto\":\n        if trt_path.exists():\n            model_path = trt_path\n            runtime = \"tensorrt\"\n        elif onnx_path.exists():\n            model_path = onnx_path\n            runtime = \"onnx\"\n        else:\n            raise click.ClickException(\n                f\"No model found in {export_dir}. Expected model.onnx or model.trt.\"\n            )\n    elif runtime == \"onnx\":\n        if not onnx_path.exists():\n            raise click.ClickException(f\"ONNX model not found: {onnx_path}\")\n        model_path = onnx_path\n    elif runtime == \"tensorrt\":\n        if not trt_path.exists():\n            raise click.ClickException(f\"TensorRT model not found: {trt_path}\")\n        model_path = trt_path\n    else:\n        raise click.ClickException(f\"Unknown runtime: {runtime}\")\n\n    # Load training config for skeleton\n    cfg_path = _find_training_config_for_predict(export_dir, metadata.model_type)\n    if cfg_path.suffix in {\".yaml\", \".yml\"}:\n        cfg = OmegaConf.load(cfg_path.as_posix())\n    else:\n        from sleap_nn.config.training_job_config import TrainingJobConfig\n\n        cfg = TrainingJobConfig.load_sleap_config(cfg_path.as_posix())\n    skeletons = get_skeleton_from_config(cfg.data_config.skeletons)\n    skeleton = skeletons[0]\n\n    # Load video\n    video = sio.Video.from_filename(video_path.as_posix())\n    total_frames = len(video) if n_frames is None else min(n_frames, len(video))\n    frame_indices = list(range(total_frames))\n\n    click.echo(f\"Loading model from: {model_path}\")\n    click.echo(f\"  Model type: {metadata.model_type}\")\n    click.echo(f\"  Runtime: {runtime}\")\n    click.echo(f\"  Device: {device}\")\n\n    predictor = load_exported_model(\n        model_path.as_posix(), runtime=runtime, device=device\n    )\n\n    click.echo(f\"Processing video: {video_path}\")\n    click.echo(f\"  Total frames: {total_frames}\")\n    click.echo(f\"  Batch size: {batch_size}\")\n\n    # Set up centroid anchor node if needed\n    anchor_node_idx = None\n    if metadata.model_type == \"centroid\":\n        anchor_part = cfg.model_config.head_configs.centroid.confmaps.anchor_part\n        node_names = [n.name for n in skeleton.nodes]\n        if anchor_part in node_names:\n            anchor_node_idx = node_names.index(anchor_part)\n        else:\n            raise click.ClickException(\n                f\"Anchor part '{anchor_part}' not found in skeleton nodes: {node_names}\"\n            )\n\n    # Set up bottom-up post-processing if needed\n    paf_scorer = None\n    candidate_template = None\n    if metadata.model_type == \"bottomup\":\n        paf_scorer = PAFScorer.from_config(\n            cfg.model_config.head_configs.bottomup,\n            max_edge_length_ratio=max_edge_length_ratio,\n            dist_penalty_weight=dist_penalty_weight,\n            n_points=n_points,\n            min_instance_peaks=min_instance_peaks,\n            min_line_scores=min_line_scores,\n        )\n        max_peaks = metadata.max_peaks_per_node\n        if max_peaks is None:\n            raise click.ClickException(\n                \"Bottom-up export metadata missing max_peaks_per_node.\"\n            )\n        edge_inds_tuples = [(int(e[0]), int(e[1])) for e in paf_scorer.edge_inds]\n        peak_channel_inds, edge_inds_tensor, edge_peak_inds = (\n            build_bottomup_candidate_template(\n                n_nodes=metadata.n_nodes,\n                max_peaks_per_node=max_peaks,\n                edge_inds=edge_inds_tuples,\n            )\n        )\n        candidate_template = {\n            \"peak_channel_inds\": peak_channel_inds,\n            \"edge_inds\": edge_inds_tensor,\n            \"edge_peak_inds\": edge_peak_inds,\n        }\n\n    labeled_frames = []\n    total_start = time.perf_counter()\n    infer_time = 0.0\n    post_time = 0.0\n\n    for start in range(0, len(frame_indices), batch_size):\n        batch_indices = frame_indices[start : start + batch_size]\n        batch = _load_video_batch(video, batch_indices)\n\n        infer_start = time.perf_counter()\n        outputs = predictor.predict(batch)\n        infer_time += time.perf_counter() - infer_start\n\n        post_start = time.perf_counter()\n        if metadata.model_type == \"topdown\":\n            labeled_frames.extend(\n                _predict_topdown_frames(\n                    outputs,\n                    batch_indices,\n                    video,\n                    skeleton,\n                    max_instances=max_instances,\n                )\n            )\n        elif metadata.model_type == \"bottomup\":\n            labeled_frames.extend(\n                _predict_bottomup_frames(\n                    outputs,\n                    batch_indices,\n                    video,\n                    skeleton,\n                    paf_scorer,\n                    candidate_template,\n                    input_scale=metadata.input_scale,\n                    peak_conf_threshold=peak_conf_threshold,\n                    max_instances=max_instances,\n                )\n            )\n        elif metadata.model_type == \"single_instance\":\n            labeled_frames.extend(\n                _predict_single_instance_frames(\n                    outputs,\n                    batch_indices,\n                    video,\n                    skeleton,\n                )\n            )\n        elif metadata.model_type == \"centroid\":\n            labeled_frames.extend(\n                _predict_centroid_frames(\n                    outputs,\n                    batch_indices,\n                    video,\n                    skeleton,\n                    anchor_node_idx=anchor_node_idx,\n                    max_instances=max_instances,\n                )\n            )\n        elif metadata.model_type == \"multi_class_bottomup\":\n            labeled_frames.extend(\n                _predict_multiclass_bottomup_frames(\n                    outputs,\n                    batch_indices,\n                    video,\n                    skeleton,\n                    class_names=metadata.class_names or [],\n                    input_scale=metadata.input_scale,\n                    peak_conf_threshold=peak_conf_threshold,\n                    max_instances=max_instances,\n                )\n            )\n        elif metadata.model_type == \"multi_class_topdown_combined\":\n            labeled_frames.extend(\n                _predict_multiclass_topdown_combined_frames(\n                    outputs,\n                    batch_indices,\n                    video,\n                    skeleton,\n                    class_names=metadata.class_names or [],\n                    max_instances=max_instances,\n                )\n            )\n        else:\n            raise click.ClickException(\n                f\"Unsupported model_type for predict: {metadata.model_type}\"\n            )\n        post_time += time.perf_counter() - post_start\n\n        # Progress update\n        processed = min(start + batch_size, len(frame_indices))\n        click.echo(\n            f\"\\r  Processed {processed}/{len(frame_indices)} frames...\",\n            nl=False,\n        )\n\n    click.echo()  # Newline after progress\n\n    total_time = time.perf_counter() - total_start\n    fps = len(frame_indices) / total_time if total_time &gt; 0 else 0\n\n    # Save predictions\n    output_path = output or video_path.with_suffix(\".predictions.slp\")\n    labels = sio.Labels(\n        videos=[video],\n        skeletons=[skeleton],\n        labeled_frames=labeled_frames,\n    )\n    labels.provenance = {\n        \"sleap_nn_version\": metadata.sleap_nn_version,\n        \"export_format\": runtime,\n        \"model_type\": metadata.model_type,\n        \"inference_timestamp\": datetime.now().isoformat(),\n    }\n    sio.save_file(labels, output_path.as_posix())\n\n    click.echo(f\"\\nInference complete:\")\n    click.echo(f\"  Total time: {total_time:.2f}s\")\n    click.echo(f\"  Inference time: {infer_time:.2f}s\")\n    click.echo(f\"  Post-processing time: {post_time:.2f}s\")\n    click.echo(f\"  FPS: {fps:.2f}\")\n    click.echo(f\"  Frames with predictions: {len(labeled_frames)}\")\n    click.echo(f\"  Output saved to: {output_path}\")\n</code></pre>"},{"location":"api/export/metadata/","title":"metadata","text":""},{"location":"api/export/metadata/#sleap_nn.export.metadata","title":"<code>sleap_nn.export.metadata</code>","text":"<p>Metadata helpers for exported models.</p> <p>Classes:</p> Name Description <code>ExportMetadata</code> <p>Metadata embedded or saved alongside exported models.</p> <p>Functions:</p> Name Description <code>build_base_metadata</code> <p>Create an ExportMetadata instance with standard defaults.</p> <code>embed_metadata_in_onnx</code> <p>Embed metadata into an ONNX model file.</p> <code>extract_metadata_from_onnx</code> <p>Extract metadata from an ONNX model file.</p> <code>hash_file</code> <p>Compute SHA256 hash for a file.</p>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.ExportMetadata","title":"<code>ExportMetadata</code>  <code>dataclass</code>","text":"<p>Metadata embedded or saved alongside exported models.</p> <p>Methods:</p> Name Description <code>default_timestamp</code> <p>Return an ISO timestamp for export.</p> <code>from_dict</code> <p>Load from dict.</p> <code>load</code> <p>Load from JSON file.</p> <code>save</code> <p>Save to JSON file.</p> <code>to_dict</code> <p>Convert to JSON-serializable dict.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@dataclass\nclass ExportMetadata:\n    \"\"\"Metadata embedded or saved alongside exported models.\"\"\"\n\n    # Version info\n    sleap_nn_version: str\n    export_timestamp: str\n    export_format: str  # \"onnx\" or \"tensorrt\"\n\n    # Model info\n    model_type: str  # \"centroid\", \"centered_instance\", \"topdown\", \"bottomup\"\n    model_name: str\n    checkpoint_path: str\n\n    # Architecture\n    backbone: str\n    n_nodes: int\n    n_edges: int\n    node_names: List[str]\n    edge_inds: List[Tuple[int, int]]\n\n    # Input/output spec\n    input_scale: float\n    input_channels: int\n    output_stride: int\n    crop_size: Optional[Tuple[int, int]] = None\n\n    # Export parameters\n    max_instances: Optional[int] = None\n    max_peaks_per_node: Optional[int] = None\n    max_batch_size: int = 1\n    precision: str = \"fp32\"\n\n    # Preprocessing - input is uint8 [0,255], normalized internally to float32 [0,1]\n    input_dtype: str = \"uint8\"\n    normalization: str = \"0_to_1\"\n\n    # Multiclass model fields (optional)\n    n_classes: Optional[int] = None\n    class_names: Optional[List[str]] = None\n\n    # Training config reference\n    training_config_embedded: bool = False\n    training_config_hash: str = \"\"\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to JSON-serializable dict.\"\"\"\n        data = asdict(self)\n        data[\"edge_inds\"] = [list(pair) for pair in self.edge_inds]\n        if self.crop_size is not None:\n            data[\"crop_size\"] = list(self.crop_size)\n        return data\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"ExportMetadata\":\n        \"\"\"Load from dict.\"\"\"\n        edge_inds = [tuple(pair) for pair in data.get(\"edge_inds\", [])]\n        crop_size = data.get(\"crop_size\")\n        if crop_size is not None:\n            crop_size = tuple(crop_size)\n        return cls(\n            sleap_nn_version=data.get(\"sleap_nn_version\", \"\"),\n            export_timestamp=data.get(\"export_timestamp\", \"\"),\n            export_format=data.get(\"export_format\", \"\"),\n            model_type=data.get(\"model_type\", \"\"),\n            model_name=data.get(\"model_name\", \"\"),\n            checkpoint_path=data.get(\"checkpoint_path\", \"\"),\n            backbone=data.get(\"backbone\", \"\"),\n            n_nodes=int(data.get(\"n_nodes\", 0)),\n            n_edges=int(data.get(\"n_edges\", 0)),\n            node_names=list(data.get(\"node_names\", [])),\n            edge_inds=edge_inds,\n            input_scale=float(data.get(\"input_scale\", 1.0)),\n            input_channels=int(data.get(\"input_channels\", 1)),\n            output_stride=int(data.get(\"output_stride\", 1)),\n            crop_size=crop_size,\n            max_instances=data.get(\"max_instances\"),\n            max_peaks_per_node=data.get(\"max_peaks_per_node\"),\n            max_batch_size=int(data.get(\"max_batch_size\", 1)),\n            precision=data.get(\"precision\", \"fp32\"),\n            input_dtype=data.get(\"input_dtype\", \"uint8\"),\n            normalization=data.get(\"normalization\", \"0_to_1\"),\n            n_classes=data.get(\"n_classes\"),\n            class_names=data.get(\"class_names\"),\n            training_config_embedded=bool(data.get(\"training_config_embedded\", False)),\n            training_config_hash=data.get(\"training_config_hash\", \"\"),\n        )\n\n    def save(self, path: str | Path) -&gt; None:\n        \"\"\"Save to JSON file.\"\"\"\n        path = Path(path)\n        path.write_text(json.dumps(self.to_dict(), indent=2, sort_keys=True))\n\n    @classmethod\n    def load(cls, path: str | Path) -&gt; \"ExportMetadata\":\n        \"\"\"Load from JSON file.\"\"\"\n        path = Path(path)\n        data = json.loads(path.read_text())\n        return cls.from_dict(data)\n\n    @classmethod\n    def default_timestamp(cls) -&gt; str:\n        \"\"\"Return an ISO timestamp for export.\"\"\"\n        return datetime.now().isoformat()\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.ExportMetadata.default_timestamp","title":"<code>default_timestamp()</code>  <code>classmethod</code>","text":"<p>Return an ISO timestamp for export.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@classmethod\ndef default_timestamp(cls) -&gt; str:\n    \"\"\"Return an ISO timestamp for export.\"\"\"\n    return datetime.now().isoformat()\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.ExportMetadata.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Load from dict.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"ExportMetadata\":\n    \"\"\"Load from dict.\"\"\"\n    edge_inds = [tuple(pair) for pair in data.get(\"edge_inds\", [])]\n    crop_size = data.get(\"crop_size\")\n    if crop_size is not None:\n        crop_size = tuple(crop_size)\n    return cls(\n        sleap_nn_version=data.get(\"sleap_nn_version\", \"\"),\n        export_timestamp=data.get(\"export_timestamp\", \"\"),\n        export_format=data.get(\"export_format\", \"\"),\n        model_type=data.get(\"model_type\", \"\"),\n        model_name=data.get(\"model_name\", \"\"),\n        checkpoint_path=data.get(\"checkpoint_path\", \"\"),\n        backbone=data.get(\"backbone\", \"\"),\n        n_nodes=int(data.get(\"n_nodes\", 0)),\n        n_edges=int(data.get(\"n_edges\", 0)),\n        node_names=list(data.get(\"node_names\", [])),\n        edge_inds=edge_inds,\n        input_scale=float(data.get(\"input_scale\", 1.0)),\n        input_channels=int(data.get(\"input_channels\", 1)),\n        output_stride=int(data.get(\"output_stride\", 1)),\n        crop_size=crop_size,\n        max_instances=data.get(\"max_instances\"),\n        max_peaks_per_node=data.get(\"max_peaks_per_node\"),\n        max_batch_size=int(data.get(\"max_batch_size\", 1)),\n        precision=data.get(\"precision\", \"fp32\"),\n        input_dtype=data.get(\"input_dtype\", \"uint8\"),\n        normalization=data.get(\"normalization\", \"0_to_1\"),\n        n_classes=data.get(\"n_classes\"),\n        class_names=data.get(\"class_names\"),\n        training_config_embedded=bool(data.get(\"training_config_embedded\", False)),\n        training_config_hash=data.get(\"training_config_hash\", \"\"),\n    )\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.ExportMetadata.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load from JSON file.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>@classmethod\ndef load(cls, path: str | Path) -&gt; \"ExportMetadata\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(path)\n    data = json.loads(path.read_text())\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.ExportMetadata.save","title":"<code>save(path)</code>","text":"<p>Save to JSON file.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(path)\n    path.write_text(json.dumps(self.to_dict(), indent=2, sort_keys=True))\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.ExportMetadata.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to JSON-serializable dict.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to JSON-serializable dict.\"\"\"\n    data = asdict(self)\n    data[\"edge_inds\"] = [list(pair) for pair in self.edge_inds]\n    if self.crop_size is not None:\n        data[\"crop_size\"] = list(self.crop_size)\n    return data\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.build_base_metadata","title":"<code>build_base_metadata(*, export_format, model_type, model_name, checkpoint_path, backbone, n_nodes, n_edges, node_names, edge_inds, input_scale, input_channels, output_stride, crop_size=None, max_instances=None, max_peaks_per_node=None, max_batch_size=1, precision='fp32', training_config_hash='', training_config_embedded=False, input_dtype='uint8', normalization='0_to_1', n_classes=None, class_names=None)</code>","text":"<p>Create an ExportMetadata instance with standard defaults.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def build_base_metadata(\n    *,\n    export_format: str,\n    model_type: str,\n    model_name: str,\n    checkpoint_path: str,\n    backbone: str,\n    n_nodes: int,\n    n_edges: int,\n    node_names: List[str],\n    edge_inds: List[Tuple[int, int]],\n    input_scale: float,\n    input_channels: int,\n    output_stride: int,\n    crop_size: Optional[Tuple[int, int]] = None,\n    max_instances: Optional[int] = None,\n    max_peaks_per_node: Optional[int] = None,\n    max_batch_size: int = 1,\n    precision: str = \"fp32\",\n    training_config_hash: str = \"\",\n    training_config_embedded: bool = False,\n    input_dtype: str = \"uint8\",\n    normalization: str = \"0_to_1\",\n    n_classes: Optional[int] = None,\n    class_names: Optional[List[str]] = None,\n) -&gt; ExportMetadata:\n    \"\"\"Create an ExportMetadata instance with standard defaults.\"\"\"\n    return ExportMetadata(\n        sleap_nn_version=__version__,\n        export_timestamp=ExportMetadata.default_timestamp(),\n        export_format=export_format,\n        model_type=model_type,\n        model_name=model_name,\n        checkpoint_path=checkpoint_path,\n        backbone=backbone,\n        n_nodes=n_nodes,\n        n_edges=n_edges,\n        node_names=node_names,\n        edge_inds=edge_inds,\n        input_scale=input_scale,\n        input_channels=input_channels,\n        output_stride=output_stride,\n        crop_size=crop_size,\n        max_instances=max_instances,\n        max_peaks_per_node=max_peaks_per_node,\n        max_batch_size=max_batch_size,\n        precision=precision,\n        input_dtype=input_dtype,\n        normalization=normalization,\n        n_classes=n_classes,\n        class_names=class_names,\n        training_config_embedded=training_config_embedded,\n        training_config_hash=training_config_hash,\n    )\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.embed_metadata_in_onnx","title":"<code>embed_metadata_in_onnx(model_path, metadata, training_config_text=None)</code>","text":"<p>Embed metadata into an ONNX model file.</p> <p>Raises ImportError if onnx is unavailable.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def embed_metadata_in_onnx(\n    model_path: str | Path,\n    metadata: ExportMetadata,\n    training_config_text: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Embed metadata into an ONNX model file.\n\n    Raises ImportError if onnx is unavailable.\n    \"\"\"\n    import onnx  # local import to keep dependency optional\n\n    model_path = Path(model_path)\n    model = onnx.load(model_path.as_posix())\n    model.metadata_props.append(\n        onnx.StringStringEntryProto(\n            key=\"sleap_nn_metadata\", value=json.dumps(metadata.to_dict())\n        )\n    )\n    if training_config_text:\n        model.metadata_props.append(\n            onnx.StringStringEntryProto(\n                key=\"training_config\", value=training_config_text\n            )\n        )\n    onnx.save(model, model_path.as_posix())\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.extract_metadata_from_onnx","title":"<code>extract_metadata_from_onnx(model_path)</code>","text":"<p>Extract metadata from an ONNX model file.</p> <p>Raises ValueError if metadata is missing.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def extract_metadata_from_onnx(model_path: str | Path) -&gt; ExportMetadata:\n    \"\"\"Extract metadata from an ONNX model file.\n\n    Raises ValueError if metadata is missing.\n    \"\"\"\n    import onnx  # local import to keep dependency optional\n\n    model = onnx.load(Path(model_path).as_posix())\n    for prop in model.metadata_props:\n        if prop.key == \"sleap_nn_metadata\":\n            return ExportMetadata.from_dict(json.loads(prop.value))\n    raise ValueError(\"No sleap_nn metadata found in ONNX model\")\n</code></pre>"},{"location":"api/export/metadata/#sleap_nn.export.metadata.hash_file","title":"<code>hash_file(path)</code>","text":"<p>Compute SHA256 hash for a file.</p> Source code in <code>sleap_nn/export/metadata.py</code> <pre><code>def hash_file(path: str | Path) -&gt; str:\n    \"\"\"Compute SHA256 hash for a file.\"\"\"\n    path = Path(path)\n    hasher = hashlib.sha256()\n    with path.open(\"rb\") as handle:\n        for chunk in iter(lambda: handle.read(8192), b\"\"):\n            hasher.update(chunk)\n    return hasher.hexdigest()\n</code></pre>"},{"location":"api/export/utils/","title":"utils","text":""},{"location":"api/export/utils/#sleap_nn.export.utils","title":"<code>sleap_nn.export.utils</code>","text":"<p>Utilities for export workflows.</p> <p>Functions:</p> Name Description <code>build_bottomup_candidate_template</code> <p>Build candidate template matching ONNX wrapper's line_scores ordering.</p> <code>load_training_config</code> <p>Load training configuration from a model directory.</p> <code>resolve_backbone_type</code> <p>Return backbone type from config.</p> <code>resolve_class_maps_output_stride</code> <p>Resolve class maps output stride for multiclass bottom-up models.</p> <code>resolve_class_names</code> <p>Resolve class names for multiclass models.</p> <code>resolve_crop_size</code> <p>Resolve crop size from preprocessing config.</p> <code>resolve_edge_inds</code> <p>Resolve edge indices for metadata.</p> <code>resolve_input_channels</code> <p>Resolve input channels from backbone config.</p> <code>resolve_input_scale</code> <p>Resolve preprocessing scale from config.</p> <code>resolve_input_shape</code> <p>Resolve a dummy input shape for export.</p> <code>resolve_model_type</code> <p>Return model type from config.</p> <code>resolve_n_classes</code> <p>Resolve number of classes for multiclass models.</p> <code>resolve_node_names</code> <p>Resolve node names for metadata.</p> <code>resolve_output_stride</code> <p>Resolve output stride from head config.</p> <code>resolve_pafs_output_stride</code> <p>Resolve PAFs output stride for bottom-up models.</p>"},{"location":"api/export/utils/#sleap_nn.export.utils.build_bottomup_candidate_template","title":"<code>build_bottomup_candidate_template(n_nodes, max_peaks_per_node, edge_inds)</code>","text":"<p>Build candidate template matching ONNX wrapper's line_scores ordering.</p> <p>The ONNX BottomUpONNXWrapper produces line_scores with shape (n_edges, k*k) where for each edge connecting (src_node, dst_node), position i*k + j corresponds to: - src peak flat index: src_node * k + i - dst peak flat index: dst_node * k + j</p> <p>This function builds edge_inds and edge_peak_inds tensors that match this exact ordering, so that line_scores_flat[idx] corresponds to edge_peak_inds[idx].</p> <p>Parameters:</p> Name Type Description Default <code>n_nodes</code> <code>int</code> <p>Number of nodes in the skeleton.</p> required <code>max_peaks_per_node</code> <code>int</code> <p>Maximum peaks per node (k) used during export.</p> required <code>edge_inds</code> <code>List[Tuple[int, int]]</code> <p>List of (src_node, dst_node) tuples defining skeleton edges.</p> required <p>Returns:</p> Type Description <code>Tuple['torch.Tensor', 'torch.Tensor', 'torch.Tensor']</code> <p>Tuple of (peak_channel_inds, edge_inds_tensor, edge_peak_inds_tensor): - peak_channel_inds: (n_nodes * k,) tensor mapping flat peak index to node - edge_inds_tensor: (n_edges * k * k,) tensor of edge indices for each candidate - edge_peak_inds_tensor: (n_edges * k * k, 2) tensor of (src, dst) peak indices</p> Example <p>from sleap_nn.export.utils import build_bottomup_candidate_template peak_ch, edge_inds, edge_peaks = build_bottomup_candidate_template( ...     n_nodes=15, max_peaks_per_node=20, edge_inds=[(1, 2), (1, 5)] ... )</p> Note <p>This function is necessary because <code>get_connection_candidates()</code> in <code>sleap_nn.inference.paf_grouping</code> uses unstable argsort, which shuffles peak indices within each node and breaks alignment with ONNX output ordering.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def build_bottomup_candidate_template(\n    n_nodes: int, max_peaks_per_node: int, edge_inds: List[Tuple[int, int]]\n) -&gt; Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n    \"\"\"Build candidate template matching ONNX wrapper's line_scores ordering.\n\n    The ONNX BottomUpONNXWrapper produces line_scores with shape (n_edges, k*k) where\n    for each edge connecting (src_node, dst_node), position i*k + j corresponds to:\n    - src peak flat index: src_node * k + i\n    - dst peak flat index: dst_node * k + j\n\n    This function builds edge_inds and edge_peak_inds tensors that match this exact\n    ordering, so that line_scores_flat[idx] corresponds to edge_peak_inds[idx].\n\n    Args:\n        n_nodes: Number of nodes in the skeleton.\n        max_peaks_per_node: Maximum peaks per node (k) used during export.\n        edge_inds: List of (src_node, dst_node) tuples defining skeleton edges.\n\n    Returns:\n        Tuple of (peak_channel_inds, edge_inds_tensor, edge_peak_inds_tensor):\n        - peak_channel_inds: (n_nodes * k,) tensor mapping flat peak index to node\n        - edge_inds_tensor: (n_edges * k * k,) tensor of edge indices for each candidate\n        - edge_peak_inds_tensor: (n_edges * k * k, 2) tensor of (src, dst) peak indices\n\n    Example:\n        &gt;&gt;&gt; from sleap_nn.export.utils import build_bottomup_candidate_template\n        &gt;&gt;&gt; peak_ch, edge_inds, edge_peaks = build_bottomup_candidate_template(\n        ...     n_nodes=15, max_peaks_per_node=20, edge_inds=[(1, 2), (1, 5)]\n        ... )\n        &gt;&gt;&gt; # Use with ONNX output:\n        &gt;&gt;&gt; line_scores_flat = line_scores.reshape(-1)\n        &gt;&gt;&gt; valid_scores = line_scores_flat[valid_mask]\n        &gt;&gt;&gt; valid_edge_peaks = edge_peaks[valid_mask]\n\n    Note:\n        This function is necessary because `get_connection_candidates()` in\n        `sleap_nn.inference.paf_grouping` uses unstable argsort, which shuffles\n        peak indices within each node and breaks alignment with ONNX output ordering.\n    \"\"\"\n    import torch\n\n    k = max_peaks_per_node\n    n_edges = len(edge_inds)\n\n    # peak_channel_inds: [0,0,...0, 1,1,...1, ...] (k times each)\n    peak_channel_inds = torch.arange(n_nodes, dtype=torch.int32).repeat_interleave(k)\n\n    edge_inds_list = []\n    edge_peak_inds_list = []\n\n    for edge_idx, (src_node, dst_node) in enumerate(edge_inds):\n        # Build k*k candidate pairs in row-major order (i*k + j)\n        # src indices: [src_node*k + 0, src_node*k + 0, ..., src_node*k + 1, ...]\n        # dst indices: [dst_node*k + 0, dst_node*k + 1, ..., dst_node*k + 0, ...]\n        src_base = src_node * k\n        dst_base = dst_node * k\n\n        src_indices = torch.arange(k, dtype=torch.int32).repeat_interleave(k) + src_base\n        dst_indices = torch.arange(k, dtype=torch.int32).repeat(k) + dst_base\n\n        edge_inds_list.append(torch.full((k * k,), edge_idx, dtype=torch.int32))\n        edge_peak_inds_list.append(torch.stack([src_indices, dst_indices], dim=1))\n\n    if edge_inds_list:\n        edge_inds_tensor = torch.cat(edge_inds_list)\n        edge_peak_inds_tensor = torch.cat(edge_peak_inds_list)\n    else:\n        edge_inds_tensor = torch.empty((0,), dtype=torch.int32)\n        edge_peak_inds_tensor = torch.empty((0, 2), dtype=torch.int32)\n\n    return peak_channel_inds, edge_inds_tensor, edge_peak_inds_tensor\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.build_bottomup_candidate_template--use-with-onnx-output","title":"Use with ONNX output:","text":"<p>line_scores_flat = line_scores.reshape(-1) valid_scores = line_scores_flat[valid_mask] valid_edge_peaks = edge_peaks[valid_mask]</p>"},{"location":"api/export/utils/#sleap_nn.export.utils.load_training_config","title":"<code>load_training_config(model_dir)</code>","text":"<p>Load training configuration from a model directory.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def load_training_config(model_dir: str | Path) -&gt; DictConfig:\n    \"\"\"Load training configuration from a model directory.\"\"\"\n    model_dir = Path(model_dir)\n    yaml_path = model_dir / \"training_config.yaml\"\n    json_path = model_dir / \"training_config.json\"\n\n    if yaml_path.exists():\n        return OmegaConf.load(yaml_path.as_posix())\n    if json_path.exists():\n        return TrainingJobConfig.load_sleap_config(json_path.as_posix())\n\n    raise FileNotFoundError(\n        f\"No training_config.yaml or training_config.json found in {model_dir}\"\n    )\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_backbone_type","title":"<code>resolve_backbone_type(cfg)</code>","text":"<p>Return backbone type from config.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_backbone_type(cfg: DictConfig) -&gt; str:\n    \"\"\"Return backbone type from config.\"\"\"\n    return get_backbone_type_from_cfg(cfg)\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_class_maps_output_stride","title":"<code>resolve_class_maps_output_stride(cfg)</code>","text":"<p>Resolve class maps output stride for multiclass bottom-up models.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_class_maps_output_stride(cfg: DictConfig) -&gt; int:\n    \"\"\"Resolve class maps output stride for multiclass bottom-up models.\"\"\"\n    mc_bottomup_cfg = getattr(\n        cfg.model_config.head_configs, \"multi_class_bottomup\", None\n    )\n    if mc_bottomup_cfg is not None and mc_bottomup_cfg.class_maps is not None:\n        return int(mc_bottomup_cfg.class_maps.output_stride)\n    return 8\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_class_names","title":"<code>resolve_class_names(cfg, model_type)</code>","text":"<p>Resolve class names for multiclass models.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_class_names(cfg: DictConfig, model_type: str) -&gt; List[str]:\n    \"\"\"Resolve class names for multiclass models.\"\"\"\n    head_cfg = cfg.model_config.head_configs.get(model_type)\n    if head_cfg is None:\n        return []\n\n    # Top-down multiclass: class_vectors.classes\n    if hasattr(head_cfg, \"class_vectors\") and head_cfg.class_vectors is not None:\n        classes = getattr(head_cfg.class_vectors, \"classes\", None)\n        if classes:\n            return list(classes)\n\n    # Bottom-up multiclass: class_maps.classes\n    if hasattr(head_cfg, \"class_maps\") and head_cfg.class_maps is not None:\n        classes = getattr(head_cfg.class_maps, \"classes\", None)\n        if classes:\n            return list(classes)\n\n    return []\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_crop_size","title":"<code>resolve_crop_size(cfg)</code>","text":"<p>Resolve crop size from preprocessing config.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_crop_size(cfg: DictConfig) -&gt; Optional[Tuple[int, int]]:\n    \"\"\"Resolve crop size from preprocessing config.\"\"\"\n    crop_size = cfg.data_config.preprocessing.crop_size\n    if crop_size is None:\n        return None\n    # Check for list/tuple or OmegaConf ListConfig\n    if isinstance(crop_size, (list, tuple)) or (\n        hasattr(crop_size, \"__iter__\")\n        and hasattr(crop_size, \"__len__\")\n        and not isinstance(crop_size, (str, int))\n    ):\n        if len(crop_size) == 2:\n            return int(crop_size[0]), int(crop_size[1])\n        if len(crop_size) == 1:\n            return int(crop_size[0]), int(crop_size[0])\n    return int(crop_size), int(crop_size)\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_edge_inds","title":"<code>resolve_edge_inds(cfg, node_names)</code>","text":"<p>Resolve edge indices for metadata.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_edge_inds(cfg: DictConfig, node_names: List[str]) -&gt; List[Tuple[int, int]]:\n    \"\"\"Resolve edge indices for metadata.\"\"\"\n    edges = _edge_inds_from_skeletons(cfg.data_config.skeletons)\n    if edges:\n        return _normalize_edges(edges, node_names)\n\n    bottomup_cfg = getattr(cfg.model_config.head_configs, \"bottomup\", None)\n    if bottomup_cfg is not None and bottomup_cfg.pafs is not None:\n        edges = bottomup_cfg.pafs.edges\n        if edges:\n            return _normalize_edges(edges, node_names)\n\n    return []\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_input_channels","title":"<code>resolve_input_channels(cfg)</code>","text":"<p>Resolve input channels from backbone config.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_input_channels(cfg: DictConfig) -&gt; int:\n    \"\"\"Resolve input channels from backbone config.\"\"\"\n    backbone_type = get_backbone_type_from_cfg(cfg)\n    return int(cfg.model_config.backbone_config[backbone_type].in_channels)\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_input_scale","title":"<code>resolve_input_scale(cfg)</code>","text":"<p>Resolve preprocessing scale from config.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_input_scale(cfg: DictConfig) -&gt; float:\n    \"\"\"Resolve preprocessing scale from config.\"\"\"\n    scale = cfg.data_config.preprocessing.scale\n    # Check for list/tuple or OmegaConf ListConfig\n    if isinstance(scale, (list, tuple)) or (\n        hasattr(scale, \"__iter__\")\n        and hasattr(scale, \"__len__\")\n        and not isinstance(scale, str)\n    ):\n        return float(scale[0]) if len(scale) &gt; 0 else 1.0\n    return float(scale)\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_input_shape","title":"<code>resolve_input_shape(cfg, input_height=None, input_width=None)</code>","text":"<p>Resolve a dummy input shape for export.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_input_shape(\n    cfg: DictConfig,\n    input_height: Optional[int] = None,\n    input_width: Optional[int] = None,\n) -&gt; Tuple[int, int, int, int]:\n    \"\"\"Resolve a dummy input shape for export.\"\"\"\n    channels = resolve_input_channels(cfg)\n    height = input_height or cfg.data_config.preprocessing.max_height or 512\n    width = input_width or cfg.data_config.preprocessing.max_width or 512\n    return 1, channels, int(height), int(width)\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_model_type","title":"<code>resolve_model_type(cfg)</code>","text":"<p>Return model type from config.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_model_type(cfg: DictConfig) -&gt; str:\n    \"\"\"Return model type from config.\"\"\"\n    return get_model_type_from_cfg(cfg)\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_n_classes","title":"<code>resolve_n_classes(cfg, model_type)</code>","text":"<p>Resolve number of classes for multiclass models.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_n_classes(cfg: DictConfig, model_type: str) -&gt; int:\n    \"\"\"Resolve number of classes for multiclass models.\"\"\"\n    class_names = resolve_class_names(cfg, model_type)\n    return len(class_names) if class_names else 0\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_node_names","title":"<code>resolve_node_names(cfg, model_type)</code>","text":"<p>Resolve node names for metadata.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_node_names(cfg: DictConfig, model_type: str) -&gt; List[str]:\n    \"\"\"Resolve node names for metadata.\"\"\"\n    skeleton_nodes = _node_names_from_skeletons(cfg.data_config.skeletons)\n    if skeleton_nodes:\n        return skeleton_nodes\n\n    head_cfg = cfg.model_config.head_configs.get(model_type)\n    if head_cfg is None:\n        return []\n\n    if hasattr(head_cfg, \"confmaps\") and head_cfg.confmaps is not None:\n        part_names = getattr(head_cfg.confmaps, \"part_names\", None)\n        if part_names:\n            return list(part_names)\n\n    if model_type == \"centroid\":\n        anchor = getattr(head_cfg.confmaps, \"anchor_part\", None) if head_cfg else None\n        return [anchor] if anchor else [\"centroid\"]\n\n    return []\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_output_stride","title":"<code>resolve_output_stride(cfg, model_type)</code>","text":"<p>Resolve output stride from head config.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_output_stride(cfg: DictConfig, model_type: str) -&gt; int:\n    \"\"\"Resolve output stride from head config.\"\"\"\n    head_cfg = cfg.model_config.head_configs[model_type]\n    if head_cfg is None:\n        return 1\n    if hasattr(head_cfg, \"confmaps\") and head_cfg.confmaps is not None:\n        return int(head_cfg.confmaps.output_stride)\n    if hasattr(head_cfg, \"pafs\") and head_cfg.pafs is not None:\n        return int(head_cfg.pafs.output_stride)\n    return 1\n</code></pre>"},{"location":"api/export/utils/#sleap_nn.export.utils.resolve_pafs_output_stride","title":"<code>resolve_pafs_output_stride(cfg)</code>","text":"<p>Resolve PAFs output stride for bottom-up models.</p> Source code in <code>sleap_nn/export/utils.py</code> <pre><code>def resolve_pafs_output_stride(cfg: DictConfig) -&gt; int:\n    \"\"\"Resolve PAFs output stride for bottom-up models.\"\"\"\n    bottomup_cfg = getattr(cfg.model_config.head_configs, \"bottomup\", None)\n    if bottomup_cfg is not None and bottomup_cfg.pafs is not None:\n        return int(bottomup_cfg.pafs.output_stride)\n    return 1\n</code></pre>"},{"location":"api/export/exporters/","title":"exporters","text":""},{"location":"api/export/exporters/#sleap_nn.export.exporters","title":"<code>sleap_nn.export.exporters</code>","text":"<p>Exporters for serialized model formats.</p> <p>Modules:</p> Name Description <code>onnx_exporter</code> <p>ONNX export utilities.</p> <code>tensorrt_exporter</code> <p>TensorRT export utilities.</p> <p>Functions:</p> Name Description <code>export_model</code> <p>Export a model to the requested format.</p> <code>export_to_onnx</code> <p>Export a PyTorch model to ONNX.</p> <code>export_to_tensorrt</code> <p>Export a PyTorch model to TensorRT format.</p>"},{"location":"api/export/exporters/#sleap_nn.export.exporters.export_model","title":"<code>export_model(model, save_path, fmt='onnx', input_shape=(1, 1, 512, 512), opset_version=17, output_names=None, verify=True, **kwargs)</code>","text":"<p>Export a model to the requested format.</p> Source code in <code>sleap_nn/export/exporters/__init__.py</code> <pre><code>def export_model(\n    model: torch.nn.Module,\n    save_path: str | Path,\n    fmt: str = \"onnx\",\n    input_shape: Iterable[int] = (1, 1, 512, 512),\n    opset_version: int = 17,\n    output_names: Optional[list] = None,\n    verify: bool = True,\n    **kwargs,\n) -&gt; Path:\n    \"\"\"Export a model to the requested format.\"\"\"\n    fmt = fmt.lower()\n    if fmt == \"onnx\":\n        return export_to_onnx(\n            model,\n            save_path,\n            input_shape=input_shape,\n            opset_version=opset_version,\n            output_names=output_names,\n            verify=verify,\n        )\n    if fmt == \"tensorrt\":\n        return export_to_tensorrt(model, save_path, input_shape=input_shape, **kwargs)\n    if fmt == \"both\":\n        export_to_onnx(\n            model,\n            save_path,\n            input_shape=input_shape,\n            opset_version=opset_version,\n            output_names=output_names,\n            verify=verify,\n        )\n        return export_to_tensorrt(model, save_path, input_shape=input_shape, **kwargs)\n\n    raise ValueError(f\"Unknown export format: {fmt}\")\n</code></pre>"},{"location":"api/export/exporters/#sleap_nn.export.exporters.export_to_onnx","title":"<code>export_to_onnx(model, save_path, input_shape=(1, 1, 512, 512), input_dtype=torch.uint8, opset_version=17, dynamic_axes=None, input_names=None, output_names=None, do_constant_folding=True, verify=True)</code>","text":"<p>Export a PyTorch model to ONNX.</p> Source code in <code>sleap_nn/export/exporters/onnx_exporter.py</code> <pre><code>def export_to_onnx(\n    model: torch.nn.Module,\n    save_path: str | Path,\n    input_shape: Iterable[int] = (1, 1, 512, 512),\n    input_dtype: torch.dtype = torch.uint8,\n    opset_version: int = 17,\n    dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None,\n    input_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    do_constant_folding: bool = True,\n    verify: bool = True,\n) -&gt; Path:\n    \"\"\"Export a PyTorch model to ONNX.\"\"\"\n    save_path = Path(save_path)\n    model.eval()\n\n    if input_names is None:\n        input_names = [\"image\"]\n    if dynamic_axes is None:\n        dynamic_axes = {\"image\": {0: \"batch\", 2: \"height\", 3: \"width\"}}\n\n    device = None\n    try:\n        device = next(model.parameters()).device\n    except StopIteration:\n        device = torch.device(\"cpu\")\n\n    if input_dtype.is_floating_point:\n        dummy_input = torch.randn(*input_shape, device=device, dtype=input_dtype)\n    else:\n        dummy_input = torch.randint(\n            0, 256, input_shape, device=device, dtype=input_dtype\n        )\n\n    if output_names is None:\n        with torch.no_grad():\n            test_out = model(dummy_input)\n        output_names = _infer_output_names(test_out)\n\n    torch.onnx.export(\n        model,\n        dummy_input,\n        save_path.as_posix(),\n        opset_version=opset_version,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        do_constant_folding=do_constant_folding,\n        dynamo=False,\n    )\n\n    if verify:\n        _verify_onnx(save_path)\n\n    return save_path\n</code></pre>"},{"location":"api/export/exporters/#sleap_nn.export.exporters.export_to_tensorrt","title":"<code>export_to_tensorrt(model, save_path, input_shape=(1, 1, 512, 512), input_dtype=torch.uint8, precision='fp16', min_shape=None, opt_shape=None, max_shape=None, workspace_size=2 &lt;&lt; 30, method='onnx', verbose=True)</code>","text":"<p>Export a PyTorch model to TensorRT format.</p> <p>This function supports multiple compilation methods: - \"onnx\": Exports to ONNX first, then compiles with TensorRT (most reliable) - \"jit\": Uses torch.jit.trace + torch_tensorrt.compile (alternative)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to export (typically an ONNX wrapper).</p> required <code>save_path</code> <code>str | Path</code> <p>Path to save the TensorRT engine (.trt file).</p> required <code>input_shape</code> <code>Tuple[int, int, int, int]</code> <p>(B, C, H, W) optimal input tensor shape.</p> <code>(1, 1, 512, 512)</code> <code>input_dtype</code> <code>dtype</code> <p>Input tensor dtype (torch.uint8 or torch.float32).</p> <code>uint8</code> <code>precision</code> <code>str</code> <p>Model precision - \"fp32\" or \"fp16\".</p> <code>'fp16'</code> <code>min_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Minimum input shape for dynamic shapes (default: batch=1, H/W halved).</p> <code>None</code> <code>opt_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Optimal input shape (default: same as input_shape).</p> <code>None</code> <code>max_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Maximum input shape (default: batch=16, H/W doubled).</p> <code>None</code> <code>workspace_size</code> <code>int</code> <p>TensorRT workspace size in bytes (default 2GB).</p> <code>2 &lt;&lt; 30</code> <code>method</code> <code>str</code> <p>Compilation method - \"onnx\" or \"jit\".</p> <code>'onnx'</code> <code>verbose</code> <code>bool</code> <p>Print export info.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the exported TensorRT engine.</p> Note <p>TensorRT models are NOT cross-platform. The exported model will only work on the same GPU architecture and TensorRT version used for export.</p> Source code in <code>sleap_nn/export/exporters/tensorrt_exporter.py</code> <pre><code>def export_to_tensorrt(\n    model: nn.Module,\n    save_path: str | Path,\n    input_shape: Tuple[int, int, int, int] = (1, 1, 512, 512),\n    input_dtype: torch.dtype = torch.uint8,\n    precision: str = \"fp16\",\n    min_shape: Optional[Tuple[int, int, int, int]] = None,\n    opt_shape: Optional[Tuple[int, int, int, int]] = None,\n    max_shape: Optional[Tuple[int, int, int, int]] = None,\n    workspace_size: int = 2 &lt;&lt; 30,  # 2GB default\n    method: str = \"onnx\",\n    verbose: bool = True,\n) -&gt; Path:\n    \"\"\"Export a PyTorch model to TensorRT format.\n\n    This function supports multiple compilation methods:\n    - \"onnx\": Exports to ONNX first, then compiles with TensorRT (most reliable)\n    - \"jit\": Uses torch.jit.trace + torch_tensorrt.compile (alternative)\n\n    Args:\n        model: The PyTorch model to export (typically an ONNX wrapper).\n        save_path: Path to save the TensorRT engine (.trt file).\n        input_shape: (B, C, H, W) optimal input tensor shape.\n        input_dtype: Input tensor dtype (torch.uint8 or torch.float32).\n        precision: Model precision - \"fp32\" or \"fp16\".\n        min_shape: Minimum input shape for dynamic shapes (default: batch=1, H/W halved).\n        opt_shape: Optimal input shape (default: same as input_shape).\n        max_shape: Maximum input shape (default: batch=16, H/W doubled).\n        workspace_size: TensorRT workspace size in bytes (default 2GB).\n        method: Compilation method - \"onnx\" or \"jit\".\n        verbose: Print export info.\n\n    Returns:\n        Path to the exported TensorRT engine.\n\n    Note:\n        TensorRT models are NOT cross-platform. The exported model will only\n        work on the same GPU architecture and TensorRT version used for export.\n    \"\"\"\n    import tensorrt as trt\n\n    model.eval()\n    device = next(model.parameters()).device\n\n    save_path = Path(save_path)\n    if not save_path.suffix:\n        save_path = save_path.with_suffix(\".trt\")\n\n    B, C, H, W = input_shape\n\n    if min_shape is None:\n        min_shape = (1, C, H // 2, W // 2)\n    if opt_shape is None:\n        opt_shape = input_shape\n    if max_shape is None:\n        max_shape = (min(16, B * 4), C, H * 2, W * 2)\n\n    if verbose:\n        print(f\"Exporting model to TensorRT...\")\n        print(f\"  Input shape: {input_shape}\")\n        print(f\"  Min/Opt/Max: {min_shape} / {opt_shape} / {max_shape}\")\n        print(f\"  Precision: {precision}\")\n        print(f\"  Workspace: {workspace_size / 1e9:.1f} GB\")\n        print(f\"  Method: {method}\")\n\n    if method == \"onnx\":\n        return _export_tensorrt_onnx(\n            model,\n            save_path,\n            input_shape,\n            input_dtype,\n            min_shape,\n            opt_shape,\n            max_shape,\n            precision,\n            workspace_size,\n            verbose,\n        )\n    elif method == \"jit\":\n        return _export_tensorrt_jit(\n            model,\n            save_path,\n            input_shape,\n            input_dtype,\n            min_shape,\n            opt_shape,\n            max_shape,\n            precision,\n            workspace_size,\n            verbose,\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}. Use 'onnx' or 'jit'.\")\n</code></pre>"},{"location":"api/export/exporters/onnx_exporter/","title":"onnx_exporter","text":""},{"location":"api/export/exporters/onnx_exporter/#sleap_nn.export.exporters.onnx_exporter","title":"<code>sleap_nn.export.exporters.onnx_exporter</code>","text":"<p>ONNX export utilities.</p> <p>Functions:</p> Name Description <code>export_to_onnx</code> <p>Export a PyTorch model to ONNX.</p>"},{"location":"api/export/exporters/onnx_exporter/#sleap_nn.export.exporters.onnx_exporter.export_to_onnx","title":"<code>export_to_onnx(model, save_path, input_shape=(1, 1, 512, 512), input_dtype=torch.uint8, opset_version=17, dynamic_axes=None, input_names=None, output_names=None, do_constant_folding=True, verify=True)</code>","text":"<p>Export a PyTorch model to ONNX.</p> Source code in <code>sleap_nn/export/exporters/onnx_exporter.py</code> <pre><code>def export_to_onnx(\n    model: torch.nn.Module,\n    save_path: str | Path,\n    input_shape: Iterable[int] = (1, 1, 512, 512),\n    input_dtype: torch.dtype = torch.uint8,\n    opset_version: int = 17,\n    dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None,\n    input_names: Optional[List[str]] = None,\n    output_names: Optional[List[str]] = None,\n    do_constant_folding: bool = True,\n    verify: bool = True,\n) -&gt; Path:\n    \"\"\"Export a PyTorch model to ONNX.\"\"\"\n    save_path = Path(save_path)\n    model.eval()\n\n    if input_names is None:\n        input_names = [\"image\"]\n    if dynamic_axes is None:\n        dynamic_axes = {\"image\": {0: \"batch\", 2: \"height\", 3: \"width\"}}\n\n    device = None\n    try:\n        device = next(model.parameters()).device\n    except StopIteration:\n        device = torch.device(\"cpu\")\n\n    if input_dtype.is_floating_point:\n        dummy_input = torch.randn(*input_shape, device=device, dtype=input_dtype)\n    else:\n        dummy_input = torch.randint(\n            0, 256, input_shape, device=device, dtype=input_dtype\n        )\n\n    if output_names is None:\n        with torch.no_grad():\n            test_out = model(dummy_input)\n        output_names = _infer_output_names(test_out)\n\n    torch.onnx.export(\n        model,\n        dummy_input,\n        save_path.as_posix(),\n        opset_version=opset_version,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        do_constant_folding=do_constant_folding,\n        dynamo=False,\n    )\n\n    if verify:\n        _verify_onnx(save_path)\n\n    return save_path\n</code></pre>"},{"location":"api/export/exporters/tensorrt_exporter/","title":"tensorrt_exporter","text":""},{"location":"api/export/exporters/tensorrt_exporter/#sleap_nn.export.exporters.tensorrt_exporter","title":"<code>sleap_nn.export.exporters.tensorrt_exporter</code>","text":"<p>TensorRT export utilities.</p> <p>Functions:</p> Name Description <code>export_to_tensorrt</code> <p>Export a PyTorch model to TensorRT format.</p>"},{"location":"api/export/exporters/tensorrt_exporter/#sleap_nn.export.exporters.tensorrt_exporter.export_to_tensorrt","title":"<code>export_to_tensorrt(model, save_path, input_shape=(1, 1, 512, 512), input_dtype=torch.uint8, precision='fp16', min_shape=None, opt_shape=None, max_shape=None, workspace_size=2 &lt;&lt; 30, method='onnx', verbose=True)</code>","text":"<p>Export a PyTorch model to TensorRT format.</p> <p>This function supports multiple compilation methods: - \"onnx\": Exports to ONNX first, then compiles with TensorRT (most reliable) - \"jit\": Uses torch.jit.trace + torch_tensorrt.compile (alternative)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to export (typically an ONNX wrapper).</p> required <code>save_path</code> <code>str | Path</code> <p>Path to save the TensorRT engine (.trt file).</p> required <code>input_shape</code> <code>Tuple[int, int, int, int]</code> <p>(B, C, H, W) optimal input tensor shape.</p> <code>(1, 1, 512, 512)</code> <code>input_dtype</code> <code>dtype</code> <p>Input tensor dtype (torch.uint8 or torch.float32).</p> <code>uint8</code> <code>precision</code> <code>str</code> <p>Model precision - \"fp32\" or \"fp16\".</p> <code>'fp16'</code> <code>min_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Minimum input shape for dynamic shapes (default: batch=1, H/W halved).</p> <code>None</code> <code>opt_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Optimal input shape (default: same as input_shape).</p> <code>None</code> <code>max_shape</code> <code>Optional[Tuple[int, int, int, int]]</code> <p>Maximum input shape (default: batch=16, H/W doubled).</p> <code>None</code> <code>workspace_size</code> <code>int</code> <p>TensorRT workspace size in bytes (default 2GB).</p> <code>2 &lt;&lt; 30</code> <code>method</code> <code>str</code> <p>Compilation method - \"onnx\" or \"jit\".</p> <code>'onnx'</code> <code>verbose</code> <code>bool</code> <p>Print export info.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the exported TensorRT engine.</p> Note <p>TensorRT models are NOT cross-platform. The exported model will only work on the same GPU architecture and TensorRT version used for export.</p> Source code in <code>sleap_nn/export/exporters/tensorrt_exporter.py</code> <pre><code>def export_to_tensorrt(\n    model: nn.Module,\n    save_path: str | Path,\n    input_shape: Tuple[int, int, int, int] = (1, 1, 512, 512),\n    input_dtype: torch.dtype = torch.uint8,\n    precision: str = \"fp16\",\n    min_shape: Optional[Tuple[int, int, int, int]] = None,\n    opt_shape: Optional[Tuple[int, int, int, int]] = None,\n    max_shape: Optional[Tuple[int, int, int, int]] = None,\n    workspace_size: int = 2 &lt;&lt; 30,  # 2GB default\n    method: str = \"onnx\",\n    verbose: bool = True,\n) -&gt; Path:\n    \"\"\"Export a PyTorch model to TensorRT format.\n\n    This function supports multiple compilation methods:\n    - \"onnx\": Exports to ONNX first, then compiles with TensorRT (most reliable)\n    - \"jit\": Uses torch.jit.trace + torch_tensorrt.compile (alternative)\n\n    Args:\n        model: The PyTorch model to export (typically an ONNX wrapper).\n        save_path: Path to save the TensorRT engine (.trt file).\n        input_shape: (B, C, H, W) optimal input tensor shape.\n        input_dtype: Input tensor dtype (torch.uint8 or torch.float32).\n        precision: Model precision - \"fp32\" or \"fp16\".\n        min_shape: Minimum input shape for dynamic shapes (default: batch=1, H/W halved).\n        opt_shape: Optimal input shape (default: same as input_shape).\n        max_shape: Maximum input shape (default: batch=16, H/W doubled).\n        workspace_size: TensorRT workspace size in bytes (default 2GB).\n        method: Compilation method - \"onnx\" or \"jit\".\n        verbose: Print export info.\n\n    Returns:\n        Path to the exported TensorRT engine.\n\n    Note:\n        TensorRT models are NOT cross-platform. The exported model will only\n        work on the same GPU architecture and TensorRT version used for export.\n    \"\"\"\n    import tensorrt as trt\n\n    model.eval()\n    device = next(model.parameters()).device\n\n    save_path = Path(save_path)\n    if not save_path.suffix:\n        save_path = save_path.with_suffix(\".trt\")\n\n    B, C, H, W = input_shape\n\n    if min_shape is None:\n        min_shape = (1, C, H // 2, W // 2)\n    if opt_shape is None:\n        opt_shape = input_shape\n    if max_shape is None:\n        max_shape = (min(16, B * 4), C, H * 2, W * 2)\n\n    if verbose:\n        print(f\"Exporting model to TensorRT...\")\n        print(f\"  Input shape: {input_shape}\")\n        print(f\"  Min/Opt/Max: {min_shape} / {opt_shape} / {max_shape}\")\n        print(f\"  Precision: {precision}\")\n        print(f\"  Workspace: {workspace_size / 1e9:.1f} GB\")\n        print(f\"  Method: {method}\")\n\n    if method == \"onnx\":\n        return _export_tensorrt_onnx(\n            model,\n            save_path,\n            input_shape,\n            input_dtype,\n            min_shape,\n            opt_shape,\n            max_shape,\n            precision,\n            workspace_size,\n            verbose,\n        )\n    elif method == \"jit\":\n        return _export_tensorrt_jit(\n            model,\n            save_path,\n            input_shape,\n            input_dtype,\n            min_shape,\n            opt_shape,\n            max_shape,\n            precision,\n            workspace_size,\n            verbose,\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}. Use 'onnx' or 'jit'.\")\n</code></pre>"},{"location":"api/export/predictors/","title":"predictors","text":""},{"location":"api/export/predictors/#sleap_nn.export.predictors","title":"<code>sleap_nn.export.predictors</code>","text":"<p>Predictors for exported models.</p> <p>Modules:</p> Name Description <code>base</code> <p>Predictor base class for exported models.</p> <code>onnx</code> <p>ONNX Runtime predictor.</p> <code>tensorrt</code> <p>TensorRT predictor for exported models.</p> <p>Classes:</p> Name Description <code>ExportPredictor</code> <p>Base interface for exported model inference.</p> <code>ONNXPredictor</code> <p>ONNX Runtime inference with provider selection.</p> <code>TensorRTPredictor</code> <p>TensorRT inference for exported models.</p> <p>Functions:</p> Name Description <code>detect_runtime</code> <p>Auto-detect runtime from file extension or folder contents.</p> <code>load_exported_model</code> <p>Load an exported model and return a predictor instance.</p>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.ExportPredictor","title":"<code>ExportPredictor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base interface for exported model inference.</p> <p>Methods:</p> Name Description <code>benchmark</code> <p>Benchmark inference latency and throughput.</p> <code>predict</code> <p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/base.py</code> <pre><code>class ExportPredictor(ABC):\n    \"\"\"Base interface for exported model inference.\"\"\"\n\n    @abstractmethod\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run inference on a batch of images.\"\"\"\n\n    @abstractmethod\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference latency and throughput.\"\"\"\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.ExportPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>  <code>abstractmethod</code>","text":"<p>Benchmark inference latency and throughput.</p> Source code in <code>sleap_nn/export/predictors/base.py</code> <pre><code>@abstractmethod\ndef benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference latency and throughput.\"\"\"\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.ExportPredictor.predict","title":"<code>predict(image)</code>  <code>abstractmethod</code>","text":"<p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/base.py</code> <pre><code>@abstractmethod\ndef predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run inference on a batch of images.\"\"\"\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.ONNXPredictor","title":"<code>ONNXPredictor</code>","text":"<p>               Bases: <code>ExportPredictor</code></p> <p>ONNX Runtime inference with provider selection.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize ONNX predictor with execution providers.</p> <code>benchmark</code> <p>Benchmark inference latency and throughput.</p> <code>predict</code> <p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>class ONNXPredictor(ExportPredictor):\n    \"\"\"ONNX Runtime inference with provider selection.\"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        device: str = \"auto\",\n        providers: Optional[Iterable[str]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize ONNX predictor with execution providers.\n\n        Args:\n            model_path: Path to the ONNX model file.\n            device: Device for inference (\"auto\", \"cpu\", or \"cuda\").\n            providers: ONNX Runtime execution providers. Auto-selected if None.\n        \"\"\"\n        try:\n            import onnxruntime as ort\n        except ImportError as exc:\n            raise ImportError(\n                \"onnxruntime is required for ONNXPredictor. Install with \"\n                \"`pip install onnxruntime` or `onnxruntime-gpu`.\"\n            ) from exc\n\n        # Preload CUDA/cuDNN libraries from pip-installed nvidia packages\n        # This is required for onnxruntime-gpu to find the CUDA libraries\n        if hasattr(ort, \"preload_dlls\"):\n            ort.preload_dlls()\n\n        self.ort = ort\n        if providers is None:\n            providers = _select_providers(device, ort.get_available_providers())\n\n        self.session = ort.InferenceSession(model_path, providers=list(providers))\n        input_info = self.session.get_inputs()[0]\n        self.input_name = input_info.name\n        self.input_type = input_info.type\n        self.input_dtype = _onnx_type_to_numpy(self.input_type)\n        self.output_names = [out.name for out in self.session.get_outputs()]\n\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run inference on a batch of images.\"\"\"\n        image = _as_numpy(image, expected_dtype=self.input_dtype)\n        outputs = self.session.run(None, {self.input_name: image})\n        return dict(zip(self.output_names, outputs))\n\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference latency and throughput.\"\"\"\n        image = _as_numpy(image, expected_dtype=self.input_dtype)\n        for _ in range(n_warmup):\n            self.session.run(None, {self.input_name: image})\n\n        times = []\n        for _ in range(n_runs):\n            start = time.perf_counter()\n            self.session.run(None, {self.input_name: image})\n            times.append(time.perf_counter() - start)\n\n        times_ms = np.array(times) * 1000.0\n        mean_ms = float(times_ms.mean())\n        p50_ms = float(np.percentile(times_ms, 50))\n        p95_ms = float(np.percentile(times_ms, 95))\n        fps = float(1000.0 / mean_ms) if mean_ms &gt; 0 else 0.0\n\n        return {\n            \"latency_ms_mean\": mean_ms,\n            \"latency_ms_p50\": p50_ms,\n            \"latency_ms_p95\": p95_ms,\n            \"fps\": fps,\n        }\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.ONNXPredictor.__init__","title":"<code>__init__(model_path, device='auto', providers=None)</code>","text":"<p>Initialize ONNX predictor with execution providers.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the ONNX model file.</p> required <code>device</code> <code>str</code> <p>Device for inference (\"auto\", \"cpu\", or \"cuda\").</p> <code>'auto'</code> <code>providers</code> <code>Optional[Iterable[str]]</code> <p>ONNX Runtime execution providers. Auto-selected if None.</p> <code>None</code> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def __init__(\n    self,\n    model_path: str,\n    device: str = \"auto\",\n    providers: Optional[Iterable[str]] = None,\n) -&gt; None:\n    \"\"\"Initialize ONNX predictor with execution providers.\n\n    Args:\n        model_path: Path to the ONNX model file.\n        device: Device for inference (\"auto\", \"cpu\", or \"cuda\").\n        providers: ONNX Runtime execution providers. Auto-selected if None.\n    \"\"\"\n    try:\n        import onnxruntime as ort\n    except ImportError as exc:\n        raise ImportError(\n            \"onnxruntime is required for ONNXPredictor. Install with \"\n            \"`pip install onnxruntime` or `onnxruntime-gpu`.\"\n        ) from exc\n\n    # Preload CUDA/cuDNN libraries from pip-installed nvidia packages\n    # This is required for onnxruntime-gpu to find the CUDA libraries\n    if hasattr(ort, \"preload_dlls\"):\n        ort.preload_dlls()\n\n    self.ort = ort\n    if providers is None:\n        providers = _select_providers(device, ort.get_available_providers())\n\n    self.session = ort.InferenceSession(model_path, providers=list(providers))\n    input_info = self.session.get_inputs()[0]\n    self.input_name = input_info.name\n    self.input_type = input_info.type\n    self.input_dtype = _onnx_type_to_numpy(self.input_type)\n    self.output_names = [out.name for out in self.session.get_outputs()]\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.ONNXPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>","text":"<p>Benchmark inference latency and throughput.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference latency and throughput.\"\"\"\n    image = _as_numpy(image, expected_dtype=self.input_dtype)\n    for _ in range(n_warmup):\n        self.session.run(None, {self.input_name: image})\n\n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        self.session.run(None, {self.input_name: image})\n        times.append(time.perf_counter() - start)\n\n    times_ms = np.array(times) * 1000.0\n    mean_ms = float(times_ms.mean())\n    p50_ms = float(np.percentile(times_ms, 50))\n    p95_ms = float(np.percentile(times_ms, 95))\n    fps = float(1000.0 / mean_ms) if mean_ms &gt; 0 else 0.0\n\n    return {\n        \"latency_ms_mean\": mean_ms,\n        \"latency_ms_p50\": p50_ms,\n        \"latency_ms_p95\": p95_ms,\n        \"fps\": fps,\n    }\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.ONNXPredictor.predict","title":"<code>predict(image)</code>","text":"<p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run inference on a batch of images.\"\"\"\n    image = _as_numpy(image, expected_dtype=self.input_dtype)\n    outputs = self.session.run(None, {self.input_name: image})\n    return dict(zip(self.output_names, outputs))\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.TensorRTPredictor","title":"<code>TensorRTPredictor</code>","text":"<p>               Bases: <code>ExportPredictor</code></p> <p>TensorRT inference for exported models.</p> <p>This predictor loads a native TensorRT engine file (.trt) and provides inference capabilities using CUDA for high-throughput predictions.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the TensorRT engine file (.trt).</p> required <code>device</code> <code>str</code> <p>Device to run inference on (only \"cuda\" supported for TRT).</p> <code>'cuda'</code> Example <p>predictor = TensorRTPredictor(\"model.trt\") outputs = predictor.predict(images)  # uint8 [B, C, H, W]</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize TensorRT predictor with a serialized engine.</p> <code>benchmark</code> <p>Benchmark inference performance.</p> <code>predict</code> <p>Run TensorRT inference.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>class TensorRTPredictor(ExportPredictor):\n    \"\"\"TensorRT inference for exported models.\n\n    This predictor loads a native TensorRT engine file (.trt) and provides\n    inference capabilities using CUDA for high-throughput predictions.\n\n    Args:\n        engine_path: Path to the TensorRT engine file (.trt).\n        device: Device to run inference on (only \"cuda\" supported for TRT).\n\n    Example:\n        &gt;&gt;&gt; predictor = TensorRTPredictor(\"model.trt\")\n        &gt;&gt;&gt; outputs = predictor.predict(images)  # uint8 [B, C, H, W]\n    \"\"\"\n\n    def __init__(\n        self,\n        engine_path: str | Path,\n        device: str = \"cuda\",\n    ) -&gt; None:\n        \"\"\"Initialize TensorRT predictor with a serialized engine.\n\n        Args:\n            engine_path: Path to the TensorRT engine file.\n            device: Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.\n        \"\"\"\n        import tensorrt as trt\n\n        if device not in (\"cuda\", \"auto\"):\n            raise ValueError(f\"TensorRT only supports CUDA devices, got: {device}\")\n\n        self.engine_path = Path(engine_path)\n        if not self.engine_path.exists():\n            raise FileNotFoundError(f\"TensorRT engine not found: {engine_path}\")\n\n        self.logger = trt.Logger(trt.Logger.WARNING)\n\n        # Load engine\n        with open(self.engine_path, \"rb\") as f:\n            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n        if self.engine is None:\n            raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n        # Create execution context\n        self.context = self.engine.create_execution_context()\n\n        # Get input/output info\n        self.input_names: List[str] = []\n        self.output_names: List[str] = []\n        self.input_shapes: Dict[str, tuple] = {}\n        self.output_shapes: Dict[str, tuple] = {}\n\n        for i in range(self.engine.num_io_tensors):\n            name = self.engine.get_tensor_name(i)\n            shape = tuple(self.engine.get_tensor_shape(name))\n            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                self.input_names.append(name)\n                self.input_shapes[name] = shape\n            else:\n                self.output_names.append(name)\n                self.output_shapes[name] = shape\n\n        self.device = torch.device(\"cuda\")\n\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run TensorRT inference.\n\n        Args:\n            image: Input image(s) as numpy array [B, C, H, W] with uint8 dtype.\n\n        Returns:\n            Dict mapping output names to numpy arrays.\n        \"\"\"\n        import tensorrt as trt\n\n        # Convert input to torch tensor on GPU\n        input_tensor = torch.from_numpy(image).to(self.device)\n\n        # Check if engine expects uint8 or float32 and convert if needed\n        input_name = self.input_names[0]\n        expected_dtype = self.engine.get_tensor_dtype(input_name)\n        if expected_dtype == trt.DataType.UINT8:\n            # Engine expects uint8 - keep as uint8\n            if input_tensor.dtype != torch.uint8:\n                input_tensor = input_tensor.to(torch.uint8)\n        else:\n            # Engine expects float - convert uint8 to float32\n            if input_tensor.dtype == torch.uint8:\n                input_tensor = input_tensor.to(torch.float32)\n\n        # Ensure contiguous memory\n        input_tensor = input_tensor.contiguous()\n\n        # Set input shape for dynamic dimensions\n        input_name = self.input_names[0]\n        self.context.set_input_shape(input_name, tuple(input_tensor.shape))\n\n        # Allocate output tensors\n        outputs: Dict[str, torch.Tensor] = {}\n        bindings: Dict[str, int] = {}\n\n        # Set input binding\n        bindings[input_name] = input_tensor.data_ptr()\n\n        # Allocate outputs\n        for name in self.output_names:\n            shape = self.context.get_tensor_shape(name)\n            dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n            outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=self.device)\n            bindings[name] = outputs[name].data_ptr()\n\n        # Set tensor addresses\n        for name, ptr in bindings.items():\n            self.context.set_tensor_address(name, ptr)\n\n        # Run inference\n        stream = torch.cuda.current_stream().cuda_stream\n        success = self.context.execute_async_v3(stream)\n        torch.cuda.current_stream().synchronize()\n\n        if not success:\n            raise RuntimeError(\"TensorRT inference failed\")\n\n        # Convert outputs to numpy\n        return {name: tensor.cpu().numpy() for name, tensor in outputs.items()}\n\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference performance.\n\n        Args:\n            image: Input image(s) as numpy array [B, C, H, W].\n            n_warmup: Number of warmup runs (not timed).\n            n_runs: Number of timed runs.\n\n        Returns:\n            Dict with timing statistics:\n                - mean_ms: Mean inference time in milliseconds\n                - std_ms: Standard deviation of inference time\n                - min_ms: Minimum inference time\n                - max_ms: Maximum inference time\n                - fps: Frames per second (based on mean time and batch size)\n        \"\"\"\n        batch_size = image.shape[0]\n\n        # Warmup\n        for _ in range(n_warmup):\n            _ = self.predict(image)\n\n        # Timed runs\n        times = []\n        for _ in range(n_runs):\n            start = time.perf_counter()\n            _ = self.predict(image)\n            times.append((time.perf_counter() - start) * 1000)\n\n        times_arr = np.array(times)\n        mean_ms = float(np.mean(times_arr))\n\n        return {\n            \"mean_ms\": mean_ms,\n            \"std_ms\": float(np.std(times_arr)),\n            \"min_ms\": float(np.min(times_arr)),\n            \"max_ms\": float(np.max(times_arr)),\n            \"fps\": (batch_size * 1000) / mean_ms if mean_ms &gt; 0 else 0.0,\n        }\n\n    @staticmethod\n    def _trt_dtype_to_torch(trt_dtype):\n        \"\"\"Convert TensorRT dtype to PyTorch dtype.\"\"\"\n        import tensorrt as trt\n\n        mapping = {\n            trt.DataType.FLOAT: torch.float32,\n            trt.DataType.HALF: torch.float16,\n            trt.DataType.INT32: torch.int32,\n            trt.DataType.INT8: torch.int8,\n            trt.DataType.BOOL: torch.bool,\n        }\n        return mapping.get(trt_dtype, torch.float32)\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.TensorRTPredictor.__init__","title":"<code>__init__(engine_path, device='cuda')</code>","text":"<p>Initialize TensorRT predictor with a serialized engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the TensorRT engine file.</p> required <code>device</code> <code>str</code> <p>Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.</p> <code>'cuda'</code> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def __init__(\n    self,\n    engine_path: str | Path,\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize TensorRT predictor with a serialized engine.\n\n    Args:\n        engine_path: Path to the TensorRT engine file.\n        device: Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.\n    \"\"\"\n    import tensorrt as trt\n\n    if device not in (\"cuda\", \"auto\"):\n        raise ValueError(f\"TensorRT only supports CUDA devices, got: {device}\")\n\n    self.engine_path = Path(engine_path)\n    if not self.engine_path.exists():\n        raise FileNotFoundError(f\"TensorRT engine not found: {engine_path}\")\n\n    self.logger = trt.Logger(trt.Logger.WARNING)\n\n    # Load engine\n    with open(self.engine_path, \"rb\") as f:\n        self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n    if self.engine is None:\n        raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n    # Create execution context\n    self.context = self.engine.create_execution_context()\n\n    # Get input/output info\n    self.input_names: List[str] = []\n    self.output_names: List[str] = []\n    self.input_shapes: Dict[str, tuple] = {}\n    self.output_shapes: Dict[str, tuple] = {}\n\n    for i in range(self.engine.num_io_tensors):\n        name = self.engine.get_tensor_name(i)\n        shape = tuple(self.engine.get_tensor_shape(name))\n        if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n            self.input_names.append(name)\n            self.input_shapes[name] = shape\n        else:\n            self.output_names.append(name)\n            self.output_shapes[name] = shape\n\n    self.device = torch.device(\"cuda\")\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.TensorRTPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>","text":"<p>Benchmark inference performance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image(s) as numpy array [B, C, H, W].</p> required <code>n_warmup</code> <code>int</code> <p>Number of warmup runs (not timed).</p> <code>50</code> <code>n_runs</code> <code>int</code> <p>Number of timed runs.</p> <code>200</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict with timing statistics:     - mean_ms: Mean inference time in milliseconds     - std_ms: Standard deviation of inference time     - min_ms: Minimum inference time     - max_ms: Maximum inference time     - fps: Frames per second (based on mean time and batch size)</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference performance.\n\n    Args:\n        image: Input image(s) as numpy array [B, C, H, W].\n        n_warmup: Number of warmup runs (not timed).\n        n_runs: Number of timed runs.\n\n    Returns:\n        Dict with timing statistics:\n            - mean_ms: Mean inference time in milliseconds\n            - std_ms: Standard deviation of inference time\n            - min_ms: Minimum inference time\n            - max_ms: Maximum inference time\n            - fps: Frames per second (based on mean time and batch size)\n    \"\"\"\n    batch_size = image.shape[0]\n\n    # Warmup\n    for _ in range(n_warmup):\n        _ = self.predict(image)\n\n    # Timed runs\n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        _ = self.predict(image)\n        times.append((time.perf_counter() - start) * 1000)\n\n    times_arr = np.array(times)\n    mean_ms = float(np.mean(times_arr))\n\n    return {\n        \"mean_ms\": mean_ms,\n        \"std_ms\": float(np.std(times_arr)),\n        \"min_ms\": float(np.min(times_arr)),\n        \"max_ms\": float(np.max(times_arr)),\n        \"fps\": (batch_size * 1000) / mean_ms if mean_ms &gt; 0 else 0.0,\n    }\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.TensorRTPredictor.predict","title":"<code>predict(image)</code>","text":"<p>Run TensorRT inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image(s) as numpy array [B, C, H, W] with uint8 dtype.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict mapping output names to numpy arrays.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run TensorRT inference.\n\n    Args:\n        image: Input image(s) as numpy array [B, C, H, W] with uint8 dtype.\n\n    Returns:\n        Dict mapping output names to numpy arrays.\n    \"\"\"\n    import tensorrt as trt\n\n    # Convert input to torch tensor on GPU\n    input_tensor = torch.from_numpy(image).to(self.device)\n\n    # Check if engine expects uint8 or float32 and convert if needed\n    input_name = self.input_names[0]\n    expected_dtype = self.engine.get_tensor_dtype(input_name)\n    if expected_dtype == trt.DataType.UINT8:\n        # Engine expects uint8 - keep as uint8\n        if input_tensor.dtype != torch.uint8:\n            input_tensor = input_tensor.to(torch.uint8)\n    else:\n        # Engine expects float - convert uint8 to float32\n        if input_tensor.dtype == torch.uint8:\n            input_tensor = input_tensor.to(torch.float32)\n\n    # Ensure contiguous memory\n    input_tensor = input_tensor.contiguous()\n\n    # Set input shape for dynamic dimensions\n    input_name = self.input_names[0]\n    self.context.set_input_shape(input_name, tuple(input_tensor.shape))\n\n    # Allocate output tensors\n    outputs: Dict[str, torch.Tensor] = {}\n    bindings: Dict[str, int] = {}\n\n    # Set input binding\n    bindings[input_name] = input_tensor.data_ptr()\n\n    # Allocate outputs\n    for name in self.output_names:\n        shape = self.context.get_tensor_shape(name)\n        dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n        outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=self.device)\n        bindings[name] = outputs[name].data_ptr()\n\n    # Set tensor addresses\n    for name, ptr in bindings.items():\n        self.context.set_tensor_address(name, ptr)\n\n    # Run inference\n    stream = torch.cuda.current_stream().cuda_stream\n    success = self.context.execute_async_v3(stream)\n    torch.cuda.current_stream().synchronize()\n\n    if not success:\n        raise RuntimeError(\"TensorRT inference failed\")\n\n    # Convert outputs to numpy\n    return {name: tensor.cpu().numpy() for name, tensor in outputs.items()}\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.detect_runtime","title":"<code>detect_runtime(model_path)</code>","text":"<p>Auto-detect runtime from file extension or folder contents.</p> Source code in <code>sleap_nn/export/predictors/__init__.py</code> <pre><code>def detect_runtime(model_path: str | Path) -&gt; str:\n    \"\"\"Auto-detect runtime from file extension or folder contents.\"\"\"\n    model_path = Path(model_path)\n    if model_path.is_dir():\n        onnx_path = model_path / \"exported\" / \"model.onnx\"\n        trt_path = model_path / \"exported\" / \"model.trt\"\n        if trt_path.exists():\n            return \"tensorrt\"\n        if onnx_path.exists():\n            return \"onnx\"\n        raise ValueError(f\"No exported model found in {model_path}\")\n\n    ext = model_path.suffix.lower()\n    if ext == \".onnx\":\n        return \"onnx\"\n    if ext in (\".trt\", \".engine\"):\n        return \"tensorrt\"\n\n    raise ValueError(f\"Unknown model format: {ext}\")\n</code></pre>"},{"location":"api/export/predictors/#sleap_nn.export.predictors.load_exported_model","title":"<code>load_exported_model(model_path, runtime='auto', device='auto', **kwargs)</code>","text":"<p>Load an exported model and return a predictor instance.</p> Source code in <code>sleap_nn/export/predictors/__init__.py</code> <pre><code>def load_exported_model(\n    model_path: str | Path,\n    runtime: str = \"auto\",\n    device: str = \"auto\",\n    **kwargs,\n) -&gt; ExportPredictor:\n    \"\"\"Load an exported model and return a predictor instance.\"\"\"\n    if runtime == \"auto\":\n        runtime = detect_runtime(model_path)\n\n    if device == \"auto\":\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    if runtime == \"onnx\":\n        return ONNXPredictor(str(model_path), device=device, **kwargs)\n    if runtime == \"tensorrt\":\n        return TensorRTPredictor(str(model_path), device=device, **kwargs)\n\n    raise ValueError(f\"Unknown runtime: {runtime}\")\n</code></pre>"},{"location":"api/export/predictors/base/","title":"base","text":""},{"location":"api/export/predictors/base/#sleap_nn.export.predictors.base","title":"<code>sleap_nn.export.predictors.base</code>","text":"<p>Predictor base class for exported models.</p> <p>Classes:</p> Name Description <code>ExportPredictor</code> <p>Base interface for exported model inference.</p>"},{"location":"api/export/predictors/base/#sleap_nn.export.predictors.base.ExportPredictor","title":"<code>ExportPredictor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base interface for exported model inference.</p> <p>Methods:</p> Name Description <code>benchmark</code> <p>Benchmark inference latency and throughput.</p> <code>predict</code> <p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/base.py</code> <pre><code>class ExportPredictor(ABC):\n    \"\"\"Base interface for exported model inference.\"\"\"\n\n    @abstractmethod\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run inference on a batch of images.\"\"\"\n\n    @abstractmethod\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference latency and throughput.\"\"\"\n</code></pre>"},{"location":"api/export/predictors/base/#sleap_nn.export.predictors.base.ExportPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>  <code>abstractmethod</code>","text":"<p>Benchmark inference latency and throughput.</p> Source code in <code>sleap_nn/export/predictors/base.py</code> <pre><code>@abstractmethod\ndef benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference latency and throughput.\"\"\"\n</code></pre>"},{"location":"api/export/predictors/base/#sleap_nn.export.predictors.base.ExportPredictor.predict","title":"<code>predict(image)</code>  <code>abstractmethod</code>","text":"<p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/base.py</code> <pre><code>@abstractmethod\ndef predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run inference on a batch of images.\"\"\"\n</code></pre>"},{"location":"api/export/predictors/onnx/","title":"onnx","text":""},{"location":"api/export/predictors/onnx/#sleap_nn.export.predictors.onnx","title":"<code>sleap_nn.export.predictors.onnx</code>","text":"<p>ONNX Runtime predictor.</p> <p>Classes:</p> Name Description <code>ONNXPredictor</code> <p>ONNX Runtime inference with provider selection.</p>"},{"location":"api/export/predictors/onnx/#sleap_nn.export.predictors.onnx.ONNXPredictor","title":"<code>ONNXPredictor</code>","text":"<p>               Bases: <code>ExportPredictor</code></p> <p>ONNX Runtime inference with provider selection.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize ONNX predictor with execution providers.</p> <code>benchmark</code> <p>Benchmark inference latency and throughput.</p> <code>predict</code> <p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>class ONNXPredictor(ExportPredictor):\n    \"\"\"ONNX Runtime inference with provider selection.\"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        device: str = \"auto\",\n        providers: Optional[Iterable[str]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize ONNX predictor with execution providers.\n\n        Args:\n            model_path: Path to the ONNX model file.\n            device: Device for inference (\"auto\", \"cpu\", or \"cuda\").\n            providers: ONNX Runtime execution providers. Auto-selected if None.\n        \"\"\"\n        try:\n            import onnxruntime as ort\n        except ImportError as exc:\n            raise ImportError(\n                \"onnxruntime is required for ONNXPredictor. Install with \"\n                \"`pip install onnxruntime` or `onnxruntime-gpu`.\"\n            ) from exc\n\n        # Preload CUDA/cuDNN libraries from pip-installed nvidia packages\n        # This is required for onnxruntime-gpu to find the CUDA libraries\n        if hasattr(ort, \"preload_dlls\"):\n            ort.preload_dlls()\n\n        self.ort = ort\n        if providers is None:\n            providers = _select_providers(device, ort.get_available_providers())\n\n        self.session = ort.InferenceSession(model_path, providers=list(providers))\n        input_info = self.session.get_inputs()[0]\n        self.input_name = input_info.name\n        self.input_type = input_info.type\n        self.input_dtype = _onnx_type_to_numpy(self.input_type)\n        self.output_names = [out.name for out in self.session.get_outputs()]\n\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run inference on a batch of images.\"\"\"\n        image = _as_numpy(image, expected_dtype=self.input_dtype)\n        outputs = self.session.run(None, {self.input_name: image})\n        return dict(zip(self.output_names, outputs))\n\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference latency and throughput.\"\"\"\n        image = _as_numpy(image, expected_dtype=self.input_dtype)\n        for _ in range(n_warmup):\n            self.session.run(None, {self.input_name: image})\n\n        times = []\n        for _ in range(n_runs):\n            start = time.perf_counter()\n            self.session.run(None, {self.input_name: image})\n            times.append(time.perf_counter() - start)\n\n        times_ms = np.array(times) * 1000.0\n        mean_ms = float(times_ms.mean())\n        p50_ms = float(np.percentile(times_ms, 50))\n        p95_ms = float(np.percentile(times_ms, 95))\n        fps = float(1000.0 / mean_ms) if mean_ms &gt; 0 else 0.0\n\n        return {\n            \"latency_ms_mean\": mean_ms,\n            \"latency_ms_p50\": p50_ms,\n            \"latency_ms_p95\": p95_ms,\n            \"fps\": fps,\n        }\n</code></pre>"},{"location":"api/export/predictors/onnx/#sleap_nn.export.predictors.onnx.ONNXPredictor.__init__","title":"<code>__init__(model_path, device='auto', providers=None)</code>","text":"<p>Initialize ONNX predictor with execution providers.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the ONNX model file.</p> required <code>device</code> <code>str</code> <p>Device for inference (\"auto\", \"cpu\", or \"cuda\").</p> <code>'auto'</code> <code>providers</code> <code>Optional[Iterable[str]]</code> <p>ONNX Runtime execution providers. Auto-selected if None.</p> <code>None</code> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def __init__(\n    self,\n    model_path: str,\n    device: str = \"auto\",\n    providers: Optional[Iterable[str]] = None,\n) -&gt; None:\n    \"\"\"Initialize ONNX predictor with execution providers.\n\n    Args:\n        model_path: Path to the ONNX model file.\n        device: Device for inference (\"auto\", \"cpu\", or \"cuda\").\n        providers: ONNX Runtime execution providers. Auto-selected if None.\n    \"\"\"\n    try:\n        import onnxruntime as ort\n    except ImportError as exc:\n        raise ImportError(\n            \"onnxruntime is required for ONNXPredictor. Install with \"\n            \"`pip install onnxruntime` or `onnxruntime-gpu`.\"\n        ) from exc\n\n    # Preload CUDA/cuDNN libraries from pip-installed nvidia packages\n    # This is required for onnxruntime-gpu to find the CUDA libraries\n    if hasattr(ort, \"preload_dlls\"):\n        ort.preload_dlls()\n\n    self.ort = ort\n    if providers is None:\n        providers = _select_providers(device, ort.get_available_providers())\n\n    self.session = ort.InferenceSession(model_path, providers=list(providers))\n    input_info = self.session.get_inputs()[0]\n    self.input_name = input_info.name\n    self.input_type = input_info.type\n    self.input_dtype = _onnx_type_to_numpy(self.input_type)\n    self.output_names = [out.name for out in self.session.get_outputs()]\n</code></pre>"},{"location":"api/export/predictors/onnx/#sleap_nn.export.predictors.onnx.ONNXPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>","text":"<p>Benchmark inference latency and throughput.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference latency and throughput.\"\"\"\n    image = _as_numpy(image, expected_dtype=self.input_dtype)\n    for _ in range(n_warmup):\n        self.session.run(None, {self.input_name: image})\n\n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        self.session.run(None, {self.input_name: image})\n        times.append(time.perf_counter() - start)\n\n    times_ms = np.array(times) * 1000.0\n    mean_ms = float(times_ms.mean())\n    p50_ms = float(np.percentile(times_ms, 50))\n    p95_ms = float(np.percentile(times_ms, 95))\n    fps = float(1000.0 / mean_ms) if mean_ms &gt; 0 else 0.0\n\n    return {\n        \"latency_ms_mean\": mean_ms,\n        \"latency_ms_p50\": p50_ms,\n        \"latency_ms_p95\": p95_ms,\n        \"fps\": fps,\n    }\n</code></pre>"},{"location":"api/export/predictors/onnx/#sleap_nn.export.predictors.onnx.ONNXPredictor.predict","title":"<code>predict(image)</code>","text":"<p>Run inference on a batch of images.</p> Source code in <code>sleap_nn/export/predictors/onnx.py</code> <pre><code>def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run inference on a batch of images.\"\"\"\n    image = _as_numpy(image, expected_dtype=self.input_dtype)\n    outputs = self.session.run(None, {self.input_name: image})\n    return dict(zip(self.output_names, outputs))\n</code></pre>"},{"location":"api/export/predictors/tensorrt/","title":"tensorrt","text":""},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt","title":"<code>sleap_nn.export.predictors.tensorrt</code>","text":"<p>TensorRT predictor for exported models.</p> <p>Classes:</p> Name Description <code>TensorRTEngine</code> <p>Low-level wrapper for native TensorRT engine files (.trt).</p> <code>TensorRTPredictor</code> <p>TensorRT inference for exported models.</p>"},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt.TensorRTEngine","title":"<code>TensorRTEngine</code>","text":"<p>Low-level wrapper for native TensorRT engine files (.trt).</p> <p>Provides a callable interface similar to PyTorch models, returning torch.Tensor outputs directly.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the TensorRT engine file (.trt).</p> required Example <p>engine = TensorRTEngine(\"model.trt\") input_tensor = torch.randn(1, 1, 512, 512, device=\"cuda\") outputs = engine(input_tensor)  # Dict[str, torch.Tensor]</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Run inference.</p> <code>__init__</code> <p>Initialize TensorRT engine wrapper.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>class TensorRTEngine:\n    \"\"\"Low-level wrapper for native TensorRT engine files (.trt).\n\n    Provides a callable interface similar to PyTorch models, returning\n    torch.Tensor outputs directly.\n\n    Args:\n        engine_path: Path to the TensorRT engine file (.trt).\n\n    Example:\n        &gt;&gt;&gt; engine = TensorRTEngine(\"model.trt\")\n        &gt;&gt;&gt; input_tensor = torch.randn(1, 1, 512, 512, device=\"cuda\")\n        &gt;&gt;&gt; outputs = engine(input_tensor)  # Dict[str, torch.Tensor]\n    \"\"\"\n\n    def __init__(self, engine_path: str | Path) -&gt; None:\n        \"\"\"Initialize TensorRT engine wrapper.\n\n        Args:\n            engine_path: Path to the serialized TensorRT engine file.\n        \"\"\"\n        import tensorrt as trt\n\n        self.engine_path = Path(engine_path)\n        self.logger = trt.Logger(trt.Logger.WARNING)\n\n        # Load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n        if self.engine is None:\n            raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n        # Create execution context\n        self.context = self.engine.create_execution_context()\n\n        # Get input/output info\n        self.input_names: List[str] = []\n        self.output_names: List[str] = []\n        self.input_shapes: Dict[str, tuple] = {}\n        self.output_shapes: Dict[str, tuple] = {}\n\n        for i in range(self.engine.num_io_tensors):\n            name = self.engine.get_tensor_name(i)\n            shape = tuple(self.engine.get_tensor_shape(name))\n            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                self.input_names.append(name)\n                self.input_shapes[name] = shape\n            else:\n                self.output_names.append(name)\n                self.output_shapes[name] = shape\n\n    def __call__(self, *args, **kwargs) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run inference.\n\n        Args:\n            *args: Input tensors (positional) or\n            **kwargs: Input tensors by name\n\n        Returns:\n            Dict of output tensors on the same device as input.\n        \"\"\"\n        import tensorrt as trt\n\n        # Handle inputs\n        if args:\n            inputs = {self.input_names[i]: arg for i, arg in enumerate(args)}\n        else:\n            inputs = kwargs\n\n        # Set input shapes and allocate outputs\n        outputs: Dict[str, torch.Tensor] = {}\n        bindings: Dict[str, int] = {}\n\n        for name in self.input_names:\n            tensor = inputs[name]\n            if not isinstance(tensor, torch.Tensor):\n                raise ValueError(f\"Input {name} must be a torch.Tensor\")\n            # Set actual shape for dynamic dimensions\n            self.context.set_input_shape(name, tuple(tensor.shape))\n            bindings[name] = tensor.contiguous().data_ptr()\n\n        # Allocate output tensors\n        device = next(iter(inputs.values())).device\n        for name in self.output_names:\n            shape = self.context.get_tensor_shape(name)\n            dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n            outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=device)\n            bindings[name] = outputs[name].data_ptr()\n\n        # Set tensor addresses\n        for name, ptr in bindings.items():\n            self.context.set_tensor_address(name, ptr)\n\n        # Run inference\n        stream = torch.cuda.current_stream().cuda_stream\n        self.context.execute_async_v3(stream)\n        torch.cuda.current_stream().synchronize()\n\n        return outputs\n\n    @staticmethod\n    def _trt_dtype_to_torch(trt_dtype):\n        \"\"\"Convert TensorRT dtype to PyTorch dtype.\"\"\"\n        import tensorrt as trt\n\n        mapping = {\n            trt.DataType.FLOAT: torch.float32,\n            trt.DataType.HALF: torch.float16,\n            trt.DataType.INT32: torch.int32,\n            trt.DataType.INT8: torch.int8,\n            trt.DataType.BOOL: torch.bool,\n        }\n        return mapping.get(trt_dtype, torch.float32)\n</code></pre>"},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt.TensorRTEngine.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Run inference.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Input tensors (positional) or</p> <code>()</code> <code>**kwargs</code> <p>Input tensors by name</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict of output tensors on the same device as input.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run inference.\n\n    Args:\n        *args: Input tensors (positional) or\n        **kwargs: Input tensors by name\n\n    Returns:\n        Dict of output tensors on the same device as input.\n    \"\"\"\n    import tensorrt as trt\n\n    # Handle inputs\n    if args:\n        inputs = {self.input_names[i]: arg for i, arg in enumerate(args)}\n    else:\n        inputs = kwargs\n\n    # Set input shapes and allocate outputs\n    outputs: Dict[str, torch.Tensor] = {}\n    bindings: Dict[str, int] = {}\n\n    for name in self.input_names:\n        tensor = inputs[name]\n        if not isinstance(tensor, torch.Tensor):\n            raise ValueError(f\"Input {name} must be a torch.Tensor\")\n        # Set actual shape for dynamic dimensions\n        self.context.set_input_shape(name, tuple(tensor.shape))\n        bindings[name] = tensor.contiguous().data_ptr()\n\n    # Allocate output tensors\n    device = next(iter(inputs.values())).device\n    for name in self.output_names:\n        shape = self.context.get_tensor_shape(name)\n        dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n        outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=device)\n        bindings[name] = outputs[name].data_ptr()\n\n    # Set tensor addresses\n    for name, ptr in bindings.items():\n        self.context.set_tensor_address(name, ptr)\n\n    # Run inference\n    stream = torch.cuda.current_stream().cuda_stream\n    self.context.execute_async_v3(stream)\n    torch.cuda.current_stream().synchronize()\n\n    return outputs\n</code></pre>"},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt.TensorRTEngine.__init__","title":"<code>__init__(engine_path)</code>","text":"<p>Initialize TensorRT engine wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the serialized TensorRT engine file.</p> required Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def __init__(self, engine_path: str | Path) -&gt; None:\n    \"\"\"Initialize TensorRT engine wrapper.\n\n    Args:\n        engine_path: Path to the serialized TensorRT engine file.\n    \"\"\"\n    import tensorrt as trt\n\n    self.engine_path = Path(engine_path)\n    self.logger = trt.Logger(trt.Logger.WARNING)\n\n    # Load engine\n    with open(engine_path, \"rb\") as f:\n        self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n    if self.engine is None:\n        raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n    # Create execution context\n    self.context = self.engine.create_execution_context()\n\n    # Get input/output info\n    self.input_names: List[str] = []\n    self.output_names: List[str] = []\n    self.input_shapes: Dict[str, tuple] = {}\n    self.output_shapes: Dict[str, tuple] = {}\n\n    for i in range(self.engine.num_io_tensors):\n        name = self.engine.get_tensor_name(i)\n        shape = tuple(self.engine.get_tensor_shape(name))\n        if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n            self.input_names.append(name)\n            self.input_shapes[name] = shape\n        else:\n            self.output_names.append(name)\n            self.output_shapes[name] = shape\n</code></pre>"},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt.TensorRTPredictor","title":"<code>TensorRTPredictor</code>","text":"<p>               Bases: <code>ExportPredictor</code></p> <p>TensorRT inference for exported models.</p> <p>This predictor loads a native TensorRT engine file (.trt) and provides inference capabilities using CUDA for high-throughput predictions.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the TensorRT engine file (.trt).</p> required <code>device</code> <code>str</code> <p>Device to run inference on (only \"cuda\" supported for TRT).</p> <code>'cuda'</code> Example <p>predictor = TensorRTPredictor(\"model.trt\") outputs = predictor.predict(images)  # uint8 [B, C, H, W]</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize TensorRT predictor with a serialized engine.</p> <code>benchmark</code> <p>Benchmark inference performance.</p> <code>predict</code> <p>Run TensorRT inference.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>class TensorRTPredictor(ExportPredictor):\n    \"\"\"TensorRT inference for exported models.\n\n    This predictor loads a native TensorRT engine file (.trt) and provides\n    inference capabilities using CUDA for high-throughput predictions.\n\n    Args:\n        engine_path: Path to the TensorRT engine file (.trt).\n        device: Device to run inference on (only \"cuda\" supported for TRT).\n\n    Example:\n        &gt;&gt;&gt; predictor = TensorRTPredictor(\"model.trt\")\n        &gt;&gt;&gt; outputs = predictor.predict(images)  # uint8 [B, C, H, W]\n    \"\"\"\n\n    def __init__(\n        self,\n        engine_path: str | Path,\n        device: str = \"cuda\",\n    ) -&gt; None:\n        \"\"\"Initialize TensorRT predictor with a serialized engine.\n\n        Args:\n            engine_path: Path to the TensorRT engine file.\n            device: Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.\n        \"\"\"\n        import tensorrt as trt\n\n        if device not in (\"cuda\", \"auto\"):\n            raise ValueError(f\"TensorRT only supports CUDA devices, got: {device}\")\n\n        self.engine_path = Path(engine_path)\n        if not self.engine_path.exists():\n            raise FileNotFoundError(f\"TensorRT engine not found: {engine_path}\")\n\n        self.logger = trt.Logger(trt.Logger.WARNING)\n\n        # Load engine\n        with open(self.engine_path, \"rb\") as f:\n            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n        if self.engine is None:\n            raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n        # Create execution context\n        self.context = self.engine.create_execution_context()\n\n        # Get input/output info\n        self.input_names: List[str] = []\n        self.output_names: List[str] = []\n        self.input_shapes: Dict[str, tuple] = {}\n        self.output_shapes: Dict[str, tuple] = {}\n\n        for i in range(self.engine.num_io_tensors):\n            name = self.engine.get_tensor_name(i)\n            shape = tuple(self.engine.get_tensor_shape(name))\n            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                self.input_names.append(name)\n                self.input_shapes[name] = shape\n            else:\n                self.output_names.append(name)\n                self.output_shapes[name] = shape\n\n        self.device = torch.device(\"cuda\")\n\n    def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Run TensorRT inference.\n\n        Args:\n            image: Input image(s) as numpy array [B, C, H, W] with uint8 dtype.\n\n        Returns:\n            Dict mapping output names to numpy arrays.\n        \"\"\"\n        import tensorrt as trt\n\n        # Convert input to torch tensor on GPU\n        input_tensor = torch.from_numpy(image).to(self.device)\n\n        # Check if engine expects uint8 or float32 and convert if needed\n        input_name = self.input_names[0]\n        expected_dtype = self.engine.get_tensor_dtype(input_name)\n        if expected_dtype == trt.DataType.UINT8:\n            # Engine expects uint8 - keep as uint8\n            if input_tensor.dtype != torch.uint8:\n                input_tensor = input_tensor.to(torch.uint8)\n        else:\n            # Engine expects float - convert uint8 to float32\n            if input_tensor.dtype == torch.uint8:\n                input_tensor = input_tensor.to(torch.float32)\n\n        # Ensure contiguous memory\n        input_tensor = input_tensor.contiguous()\n\n        # Set input shape for dynamic dimensions\n        input_name = self.input_names[0]\n        self.context.set_input_shape(input_name, tuple(input_tensor.shape))\n\n        # Allocate output tensors\n        outputs: Dict[str, torch.Tensor] = {}\n        bindings: Dict[str, int] = {}\n\n        # Set input binding\n        bindings[input_name] = input_tensor.data_ptr()\n\n        # Allocate outputs\n        for name in self.output_names:\n            shape = self.context.get_tensor_shape(name)\n            dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n            outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=self.device)\n            bindings[name] = outputs[name].data_ptr()\n\n        # Set tensor addresses\n        for name, ptr in bindings.items():\n            self.context.set_tensor_address(name, ptr)\n\n        # Run inference\n        stream = torch.cuda.current_stream().cuda_stream\n        success = self.context.execute_async_v3(stream)\n        torch.cuda.current_stream().synchronize()\n\n        if not success:\n            raise RuntimeError(\"TensorRT inference failed\")\n\n        # Convert outputs to numpy\n        return {name: tensor.cpu().numpy() for name, tensor in outputs.items()}\n\n    def benchmark(\n        self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n    ) -&gt; Dict[str, float]:\n        \"\"\"Benchmark inference performance.\n\n        Args:\n            image: Input image(s) as numpy array [B, C, H, W].\n            n_warmup: Number of warmup runs (not timed).\n            n_runs: Number of timed runs.\n\n        Returns:\n            Dict with timing statistics:\n                - mean_ms: Mean inference time in milliseconds\n                - std_ms: Standard deviation of inference time\n                - min_ms: Minimum inference time\n                - max_ms: Maximum inference time\n                - fps: Frames per second (based on mean time and batch size)\n        \"\"\"\n        batch_size = image.shape[0]\n\n        # Warmup\n        for _ in range(n_warmup):\n            _ = self.predict(image)\n\n        # Timed runs\n        times = []\n        for _ in range(n_runs):\n            start = time.perf_counter()\n            _ = self.predict(image)\n            times.append((time.perf_counter() - start) * 1000)\n\n        times_arr = np.array(times)\n        mean_ms = float(np.mean(times_arr))\n\n        return {\n            \"mean_ms\": mean_ms,\n            \"std_ms\": float(np.std(times_arr)),\n            \"min_ms\": float(np.min(times_arr)),\n            \"max_ms\": float(np.max(times_arr)),\n            \"fps\": (batch_size * 1000) / mean_ms if mean_ms &gt; 0 else 0.0,\n        }\n\n    @staticmethod\n    def _trt_dtype_to_torch(trt_dtype):\n        \"\"\"Convert TensorRT dtype to PyTorch dtype.\"\"\"\n        import tensorrt as trt\n\n        mapping = {\n            trt.DataType.FLOAT: torch.float32,\n            trt.DataType.HALF: torch.float16,\n            trt.DataType.INT32: torch.int32,\n            trt.DataType.INT8: torch.int8,\n            trt.DataType.BOOL: torch.bool,\n        }\n        return mapping.get(trt_dtype, torch.float32)\n</code></pre>"},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt.TensorRTPredictor.__init__","title":"<code>__init__(engine_path, device='cuda')</code>","text":"<p>Initialize TensorRT predictor with a serialized engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_path</code> <code>str | Path</code> <p>Path to the TensorRT engine file.</p> required <code>device</code> <code>str</code> <p>Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.</p> <code>'cuda'</code> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def __init__(\n    self,\n    engine_path: str | Path,\n    device: str = \"cuda\",\n) -&gt; None:\n    \"\"\"Initialize TensorRT predictor with a serialized engine.\n\n    Args:\n        engine_path: Path to the TensorRT engine file.\n        device: Device for inference (\"cuda\" or \"auto\"). TensorRT requires CUDA.\n    \"\"\"\n    import tensorrt as trt\n\n    if device not in (\"cuda\", \"auto\"):\n        raise ValueError(f\"TensorRT only supports CUDA devices, got: {device}\")\n\n    self.engine_path = Path(engine_path)\n    if not self.engine_path.exists():\n        raise FileNotFoundError(f\"TensorRT engine not found: {engine_path}\")\n\n    self.logger = trt.Logger(trt.Logger.WARNING)\n\n    # Load engine\n    with open(self.engine_path, \"rb\") as f:\n        self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n    if self.engine is None:\n        raise RuntimeError(f\"Failed to load TensorRT engine: {engine_path}\")\n\n    # Create execution context\n    self.context = self.engine.create_execution_context()\n\n    # Get input/output info\n    self.input_names: List[str] = []\n    self.output_names: List[str] = []\n    self.input_shapes: Dict[str, tuple] = {}\n    self.output_shapes: Dict[str, tuple] = {}\n\n    for i in range(self.engine.num_io_tensors):\n        name = self.engine.get_tensor_name(i)\n        shape = tuple(self.engine.get_tensor_shape(name))\n        if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n            self.input_names.append(name)\n            self.input_shapes[name] = shape\n        else:\n            self.output_names.append(name)\n            self.output_shapes[name] = shape\n\n    self.device = torch.device(\"cuda\")\n</code></pre>"},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt.TensorRTPredictor.benchmark","title":"<code>benchmark(image, n_warmup=50, n_runs=200)</code>","text":"<p>Benchmark inference performance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image(s) as numpy array [B, C, H, W].</p> required <code>n_warmup</code> <code>int</code> <p>Number of warmup runs (not timed).</p> <code>50</code> <code>n_runs</code> <code>int</code> <p>Number of timed runs.</p> <code>200</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict with timing statistics:     - mean_ms: Mean inference time in milliseconds     - std_ms: Standard deviation of inference time     - min_ms: Minimum inference time     - max_ms: Maximum inference time     - fps: Frames per second (based on mean time and batch size)</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def benchmark(\n    self, image: np.ndarray, n_warmup: int = 50, n_runs: int = 200\n) -&gt; Dict[str, float]:\n    \"\"\"Benchmark inference performance.\n\n    Args:\n        image: Input image(s) as numpy array [B, C, H, W].\n        n_warmup: Number of warmup runs (not timed).\n        n_runs: Number of timed runs.\n\n    Returns:\n        Dict with timing statistics:\n            - mean_ms: Mean inference time in milliseconds\n            - std_ms: Standard deviation of inference time\n            - min_ms: Minimum inference time\n            - max_ms: Maximum inference time\n            - fps: Frames per second (based on mean time and batch size)\n    \"\"\"\n    batch_size = image.shape[0]\n\n    # Warmup\n    for _ in range(n_warmup):\n        _ = self.predict(image)\n\n    # Timed runs\n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        _ = self.predict(image)\n        times.append((time.perf_counter() - start) * 1000)\n\n    times_arr = np.array(times)\n    mean_ms = float(np.mean(times_arr))\n\n    return {\n        \"mean_ms\": mean_ms,\n        \"std_ms\": float(np.std(times_arr)),\n        \"min_ms\": float(np.min(times_arr)),\n        \"max_ms\": float(np.max(times_arr)),\n        \"fps\": (batch_size * 1000) / mean_ms if mean_ms &gt; 0 else 0.0,\n    }\n</code></pre>"},{"location":"api/export/predictors/tensorrt/#sleap_nn.export.predictors.tensorrt.TensorRTPredictor.predict","title":"<code>predict(image)</code>","text":"<p>Run TensorRT inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image(s) as numpy array [B, C, H, W] with uint8 dtype.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dict mapping output names to numpy arrays.</p> Source code in <code>sleap_nn/export/predictors/tensorrt.py</code> <pre><code>def predict(self, image: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run TensorRT inference.\n\n    Args:\n        image: Input image(s) as numpy array [B, C, H, W] with uint8 dtype.\n\n    Returns:\n        Dict mapping output names to numpy arrays.\n    \"\"\"\n    import tensorrt as trt\n\n    # Convert input to torch tensor on GPU\n    input_tensor = torch.from_numpy(image).to(self.device)\n\n    # Check if engine expects uint8 or float32 and convert if needed\n    input_name = self.input_names[0]\n    expected_dtype = self.engine.get_tensor_dtype(input_name)\n    if expected_dtype == trt.DataType.UINT8:\n        # Engine expects uint8 - keep as uint8\n        if input_tensor.dtype != torch.uint8:\n            input_tensor = input_tensor.to(torch.uint8)\n    else:\n        # Engine expects float - convert uint8 to float32\n        if input_tensor.dtype == torch.uint8:\n            input_tensor = input_tensor.to(torch.float32)\n\n    # Ensure contiguous memory\n    input_tensor = input_tensor.contiguous()\n\n    # Set input shape for dynamic dimensions\n    input_name = self.input_names[0]\n    self.context.set_input_shape(input_name, tuple(input_tensor.shape))\n\n    # Allocate output tensors\n    outputs: Dict[str, torch.Tensor] = {}\n    bindings: Dict[str, int] = {}\n\n    # Set input binding\n    bindings[input_name] = input_tensor.data_ptr()\n\n    # Allocate outputs\n    for name in self.output_names:\n        shape = self.context.get_tensor_shape(name)\n        dtype = self._trt_dtype_to_torch(self.engine.get_tensor_dtype(name))\n        outputs[name] = torch.empty(tuple(shape), dtype=dtype, device=self.device)\n        bindings[name] = outputs[name].data_ptr()\n\n    # Set tensor addresses\n    for name, ptr in bindings.items():\n        self.context.set_tensor_address(name, ptr)\n\n    # Run inference\n    stream = torch.cuda.current_stream().cuda_stream\n    success = self.context.execute_async_v3(stream)\n    torch.cuda.current_stream().synchronize()\n\n    if not success:\n        raise RuntimeError(\"TensorRT inference failed\")\n\n    # Convert outputs to numpy\n    return {name: tensor.cpu().numpy() for name, tensor in outputs.items()}\n</code></pre>"},{"location":"api/export/wrappers/","title":"wrappers","text":""},{"location":"api/export/wrappers/#sleap_nn.export.wrappers","title":"<code>sleap_nn.export.wrappers</code>","text":"<p>ONNX/TensorRT export wrappers.</p> <p>Modules:</p> Name Description <code>base</code> <p>Base classes and shared helpers for export wrappers.</p> <code>bottomup</code> <p>Bottom-up ONNX wrapper.</p> <code>bottomup_multiclass</code> <p>ONNX wrapper for bottom-up multiclass (supervised ID) models.</p> <code>centered_instance</code> <p>Centered-instance ONNX wrapper.</p> <code>centroid</code> <p>Centroid ONNX wrapper.</p> <code>single_instance</code> <p>Single-instance ONNX wrapper.</p> <code>topdown</code> <p>Top-down ONNX wrapper.</p> <code>topdown_multiclass</code> <p>ONNX wrapper for top-down multiclass (supervised ID) models.</p> <p>Classes:</p> Name Description <code>BaseExportWrapper</code> <p>Base class for ONNX-exportable wrappers.</p> <code>BottomUpMultiClassONNXWrapper</code> <p>ONNX-exportable wrapper for bottom-up multiclass (supervised ID) models.</p> <code>BottomUpONNXWrapper</code> <p>ONNX-exportable wrapper for bottom-up inference up to PAF scoring.</p> <code>CenteredInstanceONNXWrapper</code> <p>ONNX-exportable wrapper for centered-instance models.</p> <code>CentroidONNXWrapper</code> <p>ONNX-exportable wrapper for centroid models.</p> <code>SingleInstanceONNXWrapper</code> <p>ONNX-exportable wrapper for single-instance models.</p> <code>TopDownMultiClassCombinedONNXWrapper</code> <p>ONNX-exportable wrapper for combined centroid + multiclass instance models.</p> <code>TopDownMultiClassONNXWrapper</code> <p>ONNX-exportable wrapper for top-down multiclass (supervised ID) models.</p> <code>TopDownONNXWrapper</code> <p>ONNX-exportable wrapper for top-down (centroid + centered-instance) inference.</p>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BaseExportWrapper","title":"<code>BaseExportWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for ONNX-exportable wrappers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize wrapper with the underlying model.</p> Source code in <code>sleap_nn/export/wrappers/base.py</code> <pre><code>class BaseExportWrapper(nn.Module):\n    \"\"\"Base class for ONNX-exportable wrappers.\"\"\"\n\n    def __init__(self, model: nn.Module):\n        \"\"\"Initialize wrapper with the underlying model.\n\n        Args:\n            model: The PyTorch model to wrap for export.\n        \"\"\"\n        super().__init__()\n        self.model = model\n\n    @staticmethod\n    def _normalize_uint8(image: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Normalize unnormalized uint8 (or [0, 255] float) images to [0, 1].\"\"\"\n        if image.dtype != torch.float32:\n            image = image.float()\n        return image / 255.0\n\n    @staticmethod\n    def _extract_tensor(output, key_hints: Iterable[str]) -&gt; torch.Tensor:\n        if isinstance(output, dict):\n            for key in output:\n                for hint in key_hints:\n                    if hint.lower() in key.lower():\n                        return output[key]\n            return next(iter(output.values()))\n        return output\n\n    @staticmethod\n    def _find_topk_peaks(\n        confmaps: torch.Tensor, k: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Top-K peak finding with NMS via max pooling.\"\"\"\n        batch_size, _, height, width = confmaps.shape\n        pooled = F.max_pool2d(confmaps, kernel_size=3, stride=1, padding=1)\n        is_peak = (confmaps == pooled) &amp; (confmaps &gt; 0)\n\n        confmaps_flat = confmaps.reshape(batch_size, height * width)\n        is_peak_flat = is_peak.reshape(batch_size, height * width)\n        masked = torch.where(\n            is_peak_flat, confmaps_flat, torch.full_like(confmaps_flat, -1e9)\n        )\n        values, indices = torch.topk(masked, k=k, dim=1)\n\n        y = indices // width\n        x = indices % width\n        peaks = torch.stack([x.float(), y.float()], dim=-1)\n        valid = values &gt; 0\n        return peaks, values, valid\n\n    @staticmethod\n    def _find_topk_peaks_per_node(\n        confmaps: torch.Tensor, k: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Top-K peak finding per channel with NMS via max pooling.\"\"\"\n        batch_size, n_nodes, height, width = confmaps.shape\n        pooled = F.max_pool2d(confmaps, kernel_size=3, stride=1, padding=1)\n        is_peak = (confmaps == pooled) &amp; (confmaps &gt; 0)\n\n        confmaps_flat = confmaps.reshape(batch_size, n_nodes, height * width)\n        is_peak_flat = is_peak.reshape(batch_size, n_nodes, height * width)\n        masked = torch.where(\n            is_peak_flat, confmaps_flat, torch.full_like(confmaps_flat, -1e9)\n        )\n        values, indices = torch.topk(masked, k=k, dim=2)\n\n        y = indices // width\n        x = indices % width\n        peaks = torch.stack([x.float(), y.float()], dim=-1)\n        valid = values &gt; 0\n        return peaks, values, valid\n\n    @staticmethod\n    def _find_global_peaks(\n        confmaps: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Find global maxima per channel.\"\"\"\n        batch_size, channels, height, width = confmaps.shape\n        flat = confmaps.reshape(batch_size, channels, height * width)\n        values, indices = flat.max(dim=-1)\n        y = indices // width\n        x = indices % width\n        peaks = torch.stack([x.float(), y.float()], dim=-1)\n        return peaks, values\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BaseExportWrapper.__init__","title":"<code>__init__(model)</code>","text":"<p>Initialize wrapper with the underlying model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to wrap for export.</p> required Source code in <code>sleap_nn/export/wrappers/base.py</code> <pre><code>def __init__(self, model: nn.Module):\n    \"\"\"Initialize wrapper with the underlying model.\n\n    Args:\n        model: The PyTorch model to wrap for export.\n    \"\"\"\n    super().__init__()\n    self.model = model\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BottomUpMultiClassONNXWrapper","title":"<code>BottomUpMultiClassONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for bottom-up multiclass (supervised ID) models.</p> <p>This wrapper handles models that output both confidence maps for keypoint detection and class maps for identity classification. Unlike PAF-based bottom-up models, multiclass models use class maps to assign identity to each detected peak, then group peaks by identity.</p> <p>The wrapper performs: 1. Peak detection in confidence maps (GPU) 2. Class probability sampling at peak locations (GPU) 3. Returns fixed-size tensors for CPU-side grouping</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The underlying PyTorch model.</p> <code>n_nodes</code> <p>Number of keypoint nodes in the skeleton.</p> <code>n_classes</code> <p>Number of identity classes.</p> <code>max_peaks_per_node</code> <p>Maximum number of peaks to detect per node.</p> <code>cms_output_stride</code> <p>Output stride of the confidence map head.</p> <code>class_maps_output_stride</code> <p>Output stride of the class maps head.</p> <code>input_scale</code> <p>Scale factor applied to input images before inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the wrapper.</p> <code>forward</code> <p>Run bottom-up multiclass inference.</p> Source code in <code>sleap_nn/export/wrappers/bottomup_multiclass.py</code> <pre><code>class BottomUpMultiClassONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for bottom-up multiclass (supervised ID) models.\n\n    This wrapper handles models that output both confidence maps for keypoint\n    detection and class maps for identity classification. Unlike PAF-based\n    bottom-up models, multiclass models use class maps to assign identity to\n    each detected peak, then group peaks by identity.\n\n    The wrapper performs:\n    1. Peak detection in confidence maps (GPU)\n    2. Class probability sampling at peak locations (GPU)\n    3. Returns fixed-size tensors for CPU-side grouping\n\n    Expects input images as uint8 tensors in [0, 255].\n\n    Attributes:\n        model: The underlying PyTorch model.\n        n_nodes: Number of keypoint nodes in the skeleton.\n        n_classes: Number of identity classes.\n        max_peaks_per_node: Maximum number of peaks to detect per node.\n        cms_output_stride: Output stride of the confidence map head.\n        class_maps_output_stride: Output stride of the class maps head.\n        input_scale: Scale factor applied to input images before inference.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        n_nodes: int,\n        n_classes: int = 2,\n        max_peaks_per_node: int = 20,\n        cms_output_stride: int = 4,\n        class_maps_output_stride: int = 8,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize the wrapper.\n\n        Args:\n            model: The underlying PyTorch model.\n            n_nodes: Number of keypoint nodes.\n            n_classes: Number of identity classes (e.g., 2 for male/female).\n            max_peaks_per_node: Maximum peaks per node to detect.\n            cms_output_stride: Output stride of confidence maps.\n            class_maps_output_stride: Output stride of class maps.\n            input_scale: Scale factor for input images.\n        \"\"\"\n        super().__init__(model)\n        self.n_nodes = n_nodes\n        self.n_classes = n_classes\n        self.max_peaks_per_node = max_peaks_per_node\n        self.cms_output_stride = cms_output_stride\n        self.class_maps_output_stride = class_maps_output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run bottom-up multiclass inference.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                   Expected to be uint8 in [0, 255].\n\n        Returns:\n            Dictionary with keys:\n                - \"peaks\": Detected peak coordinates (batch, n_nodes, max_peaks, 2).\n                    Coordinates are in input image space (x, y).\n                - \"peak_vals\": Peak confidence values (batch, n_nodes, max_peaks).\n                - \"peak_mask\": Boolean mask for valid peaks (batch, n_nodes, max_peaks).\n                - \"class_probs\": Class probabilities at each peak location\n                    (batch, n_nodes, max_peaks, n_classes).\n\n            Postprocessing on CPU uses `classify_peaks_from_maps()` to group\n            peaks by identity using Hungarian matching.\n        \"\"\"\n        # Normalize uint8 [0, 255] to float32 [0, 1]\n        image = self._normalize_uint8(image)\n\n        # Apply input scaling if needed\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        batch_size = image.shape[0]\n\n        # Forward pass\n        out = self.model(image)\n\n        # Extract outputs\n        # Note: Use \"classmaps\" as a single hint to avoid \"map\" matching \"confmaps\"\n        confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n        class_maps = self._extract_tensor(out, [\"classmaps\", \"classmapshead\"])\n\n        # Find top-k peaks per node\n        peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n            confmaps, self.max_peaks_per_node\n        )\n\n        # Scale peaks to input image space\n        peaks = peaks * self.cms_output_stride\n\n        # Sample class maps at peak locations\n        class_probs = self._sample_class_maps_at_peaks(class_maps, peaks, peak_mask)\n\n        # Scale peaks for output (accounting for input scale)\n        if self.input_scale != 1.0:\n            peaks = peaks / self.input_scale\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"peak_mask\": peak_mask,\n            \"class_probs\": class_probs,\n        }\n\n    def _sample_class_maps_at_peaks(\n        self,\n        class_maps: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_mask: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Sample class map values at peak locations.\n\n        Args:\n            class_maps: Class maps of shape (batch, n_classes, height, width).\n            peaks: Peak coordinates in cms_output_stride space,\n                shape (batch, n_nodes, max_peaks, 2) in (x, y) order.\n            peak_mask: Boolean mask for valid peaks (batch, n_nodes, max_peaks).\n\n        Returns:\n            Class probabilities at each peak location,\n            shape (batch, n_nodes, max_peaks, n_classes).\n        \"\"\"\n        batch_size, n_classes, cm_height, cm_width = class_maps.shape\n        _, n_nodes, max_peaks, _ = peaks.shape\n        device = peaks.device\n\n        # Initialize output tensor\n        class_probs = torch.zeros(\n            (batch_size, n_nodes, max_peaks, n_classes),\n            device=device,\n            dtype=class_maps.dtype,\n        )\n\n        # Convert peak coordinates to class map space\n        # peaks are in full image space (after cms_output_stride scaling)\n        peaks_cm = peaks / self.class_maps_output_stride\n\n        # Clamp coordinates to valid range\n        peaks_cm_x = peaks_cm[..., 0].clamp(0, cm_width - 1)\n        peaks_cm_y = peaks_cm[..., 1].clamp(0, cm_height - 1)\n\n        # Use grid_sample for bilinear interpolation\n        # Normalize coordinates to [-1, 1] for grid_sample\n        grid_x = (peaks_cm_x / (cm_width - 1)) * 2 - 1\n        grid_y = (peaks_cm_y / (cm_height - 1)) * 2 - 1\n\n        # Reshape for grid_sample: (batch, n_nodes * max_peaks, 1, 2)\n        grid = torch.stack([grid_x, grid_y], dim=-1)\n        grid_flat = grid.reshape(batch_size, n_nodes * max_peaks, 1, 2)\n\n        # Sample class maps: (batch, n_classes, n_nodes * max_peaks, 1)\n        sampled = F.grid_sample(\n            class_maps,\n            grid_flat,\n            mode=\"bilinear\",\n            padding_mode=\"zeros\",\n            align_corners=True,\n        )\n\n        # Reshape to (batch, n_nodes, max_peaks, n_classes)\n        sampled = sampled.squeeze(-1)  # (batch, n_classes, n_nodes * max_peaks)\n        sampled = sampled.permute(0, 2, 1)  # (batch, n_nodes * max_peaks, n_classes)\n        sampled = sampled.reshape(batch_size, n_nodes, max_peaks, n_classes)\n\n        # Apply softmax to get probabilities (optional - depends on training)\n        # For now, return raw values as the grouping function expects logits\n        class_probs = sampled\n\n        # Mask invalid peaks\n        class_probs = class_probs * peak_mask.unsqueeze(-1).float()\n\n        return class_probs\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BottomUpMultiClassONNXWrapper.__init__","title":"<code>__init__(model, n_nodes, n_classes=2, max_peaks_per_node=20, cms_output_stride=4, class_maps_output_stride=8, input_scale=1.0)</code>","text":"<p>Initialize the wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The underlying PyTorch model.</p> required <code>n_nodes</code> <code>int</code> <p>Number of keypoint nodes.</p> required <code>n_classes</code> <code>int</code> <p>Number of identity classes (e.g., 2 for male/female).</p> <code>2</code> <code>max_peaks_per_node</code> <code>int</code> <p>Maximum peaks per node to detect.</p> <code>20</code> <code>cms_output_stride</code> <code>int</code> <p>Output stride of confidence maps.</p> <code>4</code> <code>class_maps_output_stride</code> <code>int</code> <p>Output stride of class maps.</p> <code>8</code> <code>input_scale</code> <code>float</code> <p>Scale factor for input images.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/bottomup_multiclass.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    n_nodes: int,\n    n_classes: int = 2,\n    max_peaks_per_node: int = 20,\n    cms_output_stride: int = 4,\n    class_maps_output_stride: int = 8,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize the wrapper.\n\n    Args:\n        model: The underlying PyTorch model.\n        n_nodes: Number of keypoint nodes.\n        n_classes: Number of identity classes (e.g., 2 for male/female).\n        max_peaks_per_node: Maximum peaks per node to detect.\n        cms_output_stride: Output stride of confidence maps.\n        class_maps_output_stride: Output stride of class maps.\n        input_scale: Scale factor for input images.\n    \"\"\"\n    super().__init__(model)\n    self.n_nodes = n_nodes\n    self.n_classes = n_classes\n    self.max_peaks_per_node = max_peaks_per_node\n    self.cms_output_stride = cms_output_stride\n    self.class_maps_output_stride = class_maps_output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BottomUpMultiClassONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run bottom-up multiclass inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width).    Expected to be uint8 in [0, 255].</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with keys:     - \"peaks\": Detected peak coordinates (batch, n_nodes, max_peaks, 2).         Coordinates are in input image space (x, y).     - \"peak_vals\": Peak confidence values (batch, n_nodes, max_peaks).     - \"peak_mask\": Boolean mask for valid peaks (batch, n_nodes, max_peaks).     - \"class_probs\": Class probabilities at each peak location         (batch, n_nodes, max_peaks, n_classes).</p> <p>Postprocessing on CPU uses <code>classify_peaks_from_maps()</code> to group peaks by identity using Hungarian matching.</p> Source code in <code>sleap_nn/export/wrappers/bottomup_multiclass.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run bottom-up multiclass inference.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n               Expected to be uint8 in [0, 255].\n\n    Returns:\n        Dictionary with keys:\n            - \"peaks\": Detected peak coordinates (batch, n_nodes, max_peaks, 2).\n                Coordinates are in input image space (x, y).\n            - \"peak_vals\": Peak confidence values (batch, n_nodes, max_peaks).\n            - \"peak_mask\": Boolean mask for valid peaks (batch, n_nodes, max_peaks).\n            - \"class_probs\": Class probabilities at each peak location\n                (batch, n_nodes, max_peaks, n_classes).\n\n        Postprocessing on CPU uses `classify_peaks_from_maps()` to group\n        peaks by identity using Hungarian matching.\n    \"\"\"\n    # Normalize uint8 [0, 255] to float32 [0, 1]\n    image = self._normalize_uint8(image)\n\n    # Apply input scaling if needed\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    batch_size = image.shape[0]\n\n    # Forward pass\n    out = self.model(image)\n\n    # Extract outputs\n    # Note: Use \"classmaps\" as a single hint to avoid \"map\" matching \"confmaps\"\n    confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n    class_maps = self._extract_tensor(out, [\"classmaps\", \"classmapshead\"])\n\n    # Find top-k peaks per node\n    peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n        confmaps, self.max_peaks_per_node\n    )\n\n    # Scale peaks to input image space\n    peaks = peaks * self.cms_output_stride\n\n    # Sample class maps at peak locations\n    class_probs = self._sample_class_maps_at_peaks(class_maps, peaks, peak_mask)\n\n    # Scale peaks for output (accounting for input scale)\n    if self.input_scale != 1.0:\n        peaks = peaks / self.input_scale\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"peak_mask\": peak_mask,\n        \"class_probs\": class_probs,\n    }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BottomUpONNXWrapper","title":"<code>BottomUpONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for bottom-up inference up to PAF scoring.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize bottom-up ONNX wrapper.</p> <code>forward</code> <p>Run bottom-up inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/bottomup.py</code> <pre><code>class BottomUpONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for bottom-up inference up to PAF scoring.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        skeleton_edges: list,\n        n_nodes: int,\n        max_peaks_per_node: int = 20,\n        n_line_points: int = 10,\n        cms_output_stride: int = 4,\n        pafs_output_stride: int = 8,\n        max_edge_length_ratio: float = 0.25,\n        dist_penalty_weight: float = 1.0,\n        input_scale: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize bottom-up ONNX wrapper.\n\n        Args:\n            model: Bottom-up model producing confidence maps and PAFs.\n            skeleton_edges: List of (src, dst) edge tuples defining skeleton.\n            n_nodes: Number of nodes in the skeleton.\n            max_peaks_per_node: Maximum peaks to detect per node type.\n            n_line_points: Points to sample along PAF edges.\n            cms_output_stride: Confidence map output stride.\n            pafs_output_stride: PAF output stride.\n            max_edge_length_ratio: Maximum edge length as ratio of image size.\n            dist_penalty_weight: Weight for distance penalty in scoring.\n            input_scale: Input scaling factor.\n        \"\"\"\n        super().__init__(model)\n        self.n_nodes = n_nodes\n        self.n_edges = len(skeleton_edges)\n        self.max_peaks_per_node = max_peaks_per_node\n        self.n_line_points = n_line_points\n        self.cms_output_stride = cms_output_stride\n        self.pafs_output_stride = pafs_output_stride\n        self.max_edge_length_ratio = max_edge_length_ratio\n        self.dist_penalty_weight = dist_penalty_weight\n        self.input_scale = input_scale\n\n        edge_src = torch.tensor([e[0] for e in skeleton_edges], dtype=torch.long)\n        edge_dst = torch.tensor([e[1] for e in skeleton_edges], dtype=torch.long)\n        self.register_buffer(\"edge_src\", edge_src)\n        self.register_buffer(\"edge_dst\", edge_dst)\n\n        line_samples = torch.linspace(0, 1, n_line_points, dtype=torch.float32)\n        self.register_buffer(\"line_samples\", line_samples)\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run bottom-up inference and return fixed-size outputs.\n\n        Note: confmaps and pafs are NOT returned to avoid D2H transfer bottleneck.\n        Peak detection and PAF scoring are performed on GPU within this wrapper.\n        \"\"\"\n        image = self._normalize_uint8(image)\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        batch_size, _, height, width = image.shape\n\n        out = self.model(image)\n        if isinstance(out, dict):\n            confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n            pafs = self._extract_tensor(out, [\"paf\", \"affinity\"])\n        else:\n            confmaps, pafs = out[:2]\n\n        peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n            confmaps, self.max_peaks_per_node\n        )\n\n        peaks = peaks * self.cms_output_stride\n\n        # Compute max_edge_length to match PyTorch implementation:\n        # max_edge_length = ratio * max(paf_dims) * pafs_stride\n        # PAFs shape is (batch, 2*edges, H, W)\n        _, n_paf_channels, paf_height, paf_width = pafs.shape\n        max_paf_dim = max(n_paf_channels, paf_height, paf_width)\n        max_edge_length = torch.tensor(\n            self.max_edge_length_ratio * max_paf_dim * self.pafs_output_stride,\n            dtype=peaks.dtype,\n            device=peaks.device,\n        )\n\n        line_scores, candidate_mask = self._score_all_candidates(\n            pafs, peaks, peak_mask, max_edge_length\n        )\n\n        # Only return final outputs needed for CPU-side grouping.\n        # Do NOT return confmaps/pafs - they are large (~29 MB/batch) and\n        # cause D2H transfer bottleneck. Peak detection and PAF scoring\n        # are already done on GPU above.\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"peak_mask\": peak_mask,\n            \"line_scores\": line_scores,\n            \"candidate_mask\": candidate_mask,\n        }\n\n    def _score_all_candidates(\n        self,\n        pafs: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_mask: torch.Tensor,\n        max_edge_length: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Score all K*K candidate connections for each edge.\"\"\"\n        batch_size = peaks.shape[0]\n        k = self.max_peaks_per_node\n        n_edges = self.n_edges\n\n        _, _, paf_height, paf_width = pafs.shape\n\n        src_peaks = peaks[:, self.edge_src, :, :]\n        dst_peaks = peaks[:, self.edge_dst, :, :]\n\n        src_mask = peak_mask[:, self.edge_src, :]\n        dst_mask = peak_mask[:, self.edge_dst, :]\n\n        src_peaks_exp = src_peaks.unsqueeze(3).expand(-1, -1, -1, k, -1)\n        dst_peaks_exp = dst_peaks.unsqueeze(2).expand(-1, -1, k, -1, -1)\n\n        src_mask_exp = src_mask.unsqueeze(3).expand(-1, -1, -1, k)\n        dst_mask_exp = dst_mask.unsqueeze(2).expand(-1, -1, k, -1)\n        candidate_mask = src_mask_exp &amp; dst_mask_exp\n\n        src_peaks_flat = src_peaks_exp.reshape(batch_size, n_edges, k * k, 2)\n        dst_peaks_flat = dst_peaks_exp.reshape(batch_size, n_edges, k * k, 2)\n        candidate_mask_flat = candidate_mask.reshape(batch_size, n_edges, k * k)\n\n        spatial_vecs = dst_peaks_flat - src_peaks_flat\n        spatial_lengths = torch.norm(spatial_vecs, dim=-1, keepdim=True).clamp(min=1e-6)\n        spatial_vecs_norm = spatial_vecs / spatial_lengths\n\n        line_samples = self.line_samples.view(1, 1, 1, -1, 1)\n        src_exp = src_peaks_flat.unsqueeze(3)\n        dst_exp = dst_peaks_flat.unsqueeze(3)\n        line_points = src_exp + line_samples * (dst_exp - src_exp)\n\n        line_points_paf = line_points / self.pafs_output_stride\n        line_x = line_points_paf[..., 0].clamp(0, paf_width - 1)\n        line_y = line_points_paf[..., 1].clamp(0, paf_height - 1)\n\n        line_scores = self._sample_and_score_lines(\n            pafs,\n            line_x,\n            line_y,\n            spatial_vecs_norm,\n            spatial_lengths.squeeze(-1),\n            max_edge_length,\n        )\n\n        line_scores = line_scores.masked_fill(~candidate_mask_flat, -2.0)\n        return line_scores, candidate_mask_flat\n\n    def _sample_and_score_lines(\n        self,\n        pafs: torch.Tensor,\n        line_x: torch.Tensor,\n        line_y: torch.Tensor,\n        spatial_vecs_norm: torch.Tensor,\n        spatial_lengths: torch.Tensor,\n        max_edge_length: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Sample PAF values along lines and compute scores.\"\"\"\n        batch_size, n_edges, k2, n_points = line_x.shape\n        _, _, paf_height, paf_width = pafs.shape\n\n        all_scores = []\n        for edge_idx in range(n_edges):\n            paf_x = pafs[:, 2 * edge_idx, :, :]\n            paf_y = pafs[:, 2 * edge_idx + 1, :, :]\n\n            lx = line_x[:, edge_idx, :, :]\n            ly = line_y[:, edge_idx, :, :]\n\n            lx_norm = (lx / (paf_width - 1)) * 2 - 1\n            ly_norm = (ly / (paf_height - 1)) * 2 - 1\n\n            grid = torch.stack([lx_norm, ly_norm], dim=-1)\n\n            paf_x_samples = F.grid_sample(\n                paf_x.unsqueeze(1),\n                grid,\n                mode=\"bilinear\",\n                padding_mode=\"zeros\",\n                align_corners=True,\n            ).squeeze(1)\n\n            paf_y_samples = F.grid_sample(\n                paf_y.unsqueeze(1),\n                grid,\n                mode=\"bilinear\",\n                padding_mode=\"zeros\",\n                align_corners=True,\n            ).squeeze(1)\n\n            paf_samples = torch.stack([paf_x_samples, paf_y_samples], dim=-1)\n            disp_vec = spatial_vecs_norm[:, edge_idx, :, :]\n\n            dot_products = (paf_samples * disp_vec.unsqueeze(2)).sum(dim=-1)\n            mean_scores = dot_products.mean(dim=-1)\n\n            edge_lengths = spatial_lengths[:, edge_idx, :]\n            dist_penalty = self._compute_distance_penalty(edge_lengths, max_edge_length)\n\n            all_scores.append(mean_scores + dist_penalty)\n\n        return torch.stack(all_scores, dim=1)\n\n    def _compute_distance_penalty(\n        self, distances: torch.Tensor, max_edge_length: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute distance penalty for edge candidates.\n\n        Matches the PyTorch implementation in sleap_nn.inference.paf_grouping.\n        Penalty is 0 when distance &lt;= max_edge_length, and negative when longer.\n        \"\"\"\n        # Match PyTorch: penalty = clamp((max_edge_length / distance) - 1, max=0) * weight\n        penalty = torch.clamp((max_edge_length / distances) - 1, max=0)\n        return penalty * self.dist_penalty_weight\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BottomUpONNXWrapper.__init__","title":"<code>__init__(model, skeleton_edges, n_nodes, max_peaks_per_node=20, n_line_points=10, cms_output_stride=4, pafs_output_stride=8, max_edge_length_ratio=0.25, dist_penalty_weight=1.0, input_scale=1.0)</code>","text":"<p>Initialize bottom-up ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Bottom-up model producing confidence maps and PAFs.</p> required <code>skeleton_edges</code> <code>list</code> <p>List of (src, dst) edge tuples defining skeleton.</p> required <code>n_nodes</code> <code>int</code> <p>Number of nodes in the skeleton.</p> required <code>max_peaks_per_node</code> <code>int</code> <p>Maximum peaks to detect per node type.</p> <code>20</code> <code>n_line_points</code> <code>int</code> <p>Points to sample along PAF edges.</p> <code>10</code> <code>cms_output_stride</code> <code>int</code> <p>Confidence map output stride.</p> <code>4</code> <code>pafs_output_stride</code> <code>int</code> <p>PAF output stride.</p> <code>8</code> <code>max_edge_length_ratio</code> <code>float</code> <p>Maximum edge length as ratio of image size.</p> <code>0.25</code> <code>dist_penalty_weight</code> <code>float</code> <p>Weight for distance penalty in scoring.</p> <code>1.0</code> <code>input_scale</code> <code>float</code> <p>Input scaling factor.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/bottomup.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    skeleton_edges: list,\n    n_nodes: int,\n    max_peaks_per_node: int = 20,\n    n_line_points: int = 10,\n    cms_output_stride: int = 4,\n    pafs_output_stride: int = 8,\n    max_edge_length_ratio: float = 0.25,\n    dist_penalty_weight: float = 1.0,\n    input_scale: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize bottom-up ONNX wrapper.\n\n    Args:\n        model: Bottom-up model producing confidence maps and PAFs.\n        skeleton_edges: List of (src, dst) edge tuples defining skeleton.\n        n_nodes: Number of nodes in the skeleton.\n        max_peaks_per_node: Maximum peaks to detect per node type.\n        n_line_points: Points to sample along PAF edges.\n        cms_output_stride: Confidence map output stride.\n        pafs_output_stride: PAF output stride.\n        max_edge_length_ratio: Maximum edge length as ratio of image size.\n        dist_penalty_weight: Weight for distance penalty in scoring.\n        input_scale: Input scaling factor.\n    \"\"\"\n    super().__init__(model)\n    self.n_nodes = n_nodes\n    self.n_edges = len(skeleton_edges)\n    self.max_peaks_per_node = max_peaks_per_node\n    self.n_line_points = n_line_points\n    self.cms_output_stride = cms_output_stride\n    self.pafs_output_stride = pafs_output_stride\n    self.max_edge_length_ratio = max_edge_length_ratio\n    self.dist_penalty_weight = dist_penalty_weight\n    self.input_scale = input_scale\n\n    edge_src = torch.tensor([e[0] for e in skeleton_edges], dtype=torch.long)\n    edge_dst = torch.tensor([e[1] for e in skeleton_edges], dtype=torch.long)\n    self.register_buffer(\"edge_src\", edge_src)\n    self.register_buffer(\"edge_dst\", edge_dst)\n\n    line_samples = torch.linspace(0, 1, n_line_points, dtype=torch.float32)\n    self.register_buffer(\"line_samples\", line_samples)\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.BottomUpONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run bottom-up inference and return fixed-size outputs.</p> <p>Note: confmaps and pafs are NOT returned to avoid D2H transfer bottleneck. Peak detection and PAF scoring are performed on GPU within this wrapper.</p> Source code in <code>sleap_nn/export/wrappers/bottomup.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run bottom-up inference and return fixed-size outputs.\n\n    Note: confmaps and pafs are NOT returned to avoid D2H transfer bottleneck.\n    Peak detection and PAF scoring are performed on GPU within this wrapper.\n    \"\"\"\n    image = self._normalize_uint8(image)\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    batch_size, _, height, width = image.shape\n\n    out = self.model(image)\n    if isinstance(out, dict):\n        confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n        pafs = self._extract_tensor(out, [\"paf\", \"affinity\"])\n    else:\n        confmaps, pafs = out[:2]\n\n    peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n        confmaps, self.max_peaks_per_node\n    )\n\n    peaks = peaks * self.cms_output_stride\n\n    # Compute max_edge_length to match PyTorch implementation:\n    # max_edge_length = ratio * max(paf_dims) * pafs_stride\n    # PAFs shape is (batch, 2*edges, H, W)\n    _, n_paf_channels, paf_height, paf_width = pafs.shape\n    max_paf_dim = max(n_paf_channels, paf_height, paf_width)\n    max_edge_length = torch.tensor(\n        self.max_edge_length_ratio * max_paf_dim * self.pafs_output_stride,\n        dtype=peaks.dtype,\n        device=peaks.device,\n    )\n\n    line_scores, candidate_mask = self._score_all_candidates(\n        pafs, peaks, peak_mask, max_edge_length\n    )\n\n    # Only return final outputs needed for CPU-side grouping.\n    # Do NOT return confmaps/pafs - they are large (~29 MB/batch) and\n    # cause D2H transfer bottleneck. Peak detection and PAF scoring\n    # are already done on GPU above.\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"peak_mask\": peak_mask,\n        \"line_scores\": line_scores,\n        \"candidate_mask\": candidate_mask,\n    }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.CenteredInstanceONNXWrapper","title":"<code>CenteredInstanceONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for centered-instance models.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize centered instance ONNX wrapper.</p> <code>forward</code> <p>Run centered-instance inference on crops.</p> Source code in <code>sleap_nn/export/wrappers/centered_instance.py</code> <pre><code>class CenteredInstanceONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for centered-instance models.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        output_stride: int = 4,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize centered instance ONNX wrapper.\n\n        Args:\n            model: Centered instance model for pose estimation.\n            output_stride: Output stride for confidence maps.\n            input_scale: Input scaling factor.\n        \"\"\"\n        super().__init__(model)\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run centered-instance inference on crops.\"\"\"\n        image = self._normalize_uint8(image)\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        confmaps = self._extract_tensor(\n            self.model(image), [\"centered\", \"instance\", \"confmap\"]\n        )\n        peaks, values = self._find_global_peaks(confmaps)\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": values,\n        }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.CenteredInstanceONNXWrapper.__init__","title":"<code>__init__(model, output_stride=4, input_scale=1.0)</code>","text":"<p>Initialize centered instance ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Centered instance model for pose estimation.</p> required <code>output_stride</code> <code>int</code> <p>Output stride for confidence maps.</p> <code>4</code> <code>input_scale</code> <code>float</code> <p>Input scaling factor.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/centered_instance.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    output_stride: int = 4,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize centered instance ONNX wrapper.\n\n    Args:\n        model: Centered instance model for pose estimation.\n        output_stride: Output stride for confidence maps.\n        input_scale: Input scaling factor.\n    \"\"\"\n    super().__init__(model)\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.CenteredInstanceONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run centered-instance inference on crops.</p> Source code in <code>sleap_nn/export/wrappers/centered_instance.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run centered-instance inference on crops.\"\"\"\n    image = self._normalize_uint8(image)\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    confmaps = self._extract_tensor(\n        self.model(image), [\"centered\", \"instance\", \"confmap\"]\n    )\n    peaks, values = self._find_global_peaks(confmaps)\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": values,\n    }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.CentroidONNXWrapper","title":"<code>CentroidONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for centroid models.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize centroid ONNX wrapper.</p> <code>forward</code> <p>Run centroid inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/centroid.py</code> <pre><code>class CentroidONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for centroid models.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        max_instances: int = 20,\n        output_stride: int = 2,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize centroid ONNX wrapper.\n\n        Args:\n            model: Centroid detection model.\n            max_instances: Maximum number of instances to detect.\n            output_stride: Output stride for confidence maps.\n            input_scale: Input scaling factor.\n        \"\"\"\n        super().__init__(model)\n        self.max_instances = max_instances\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run centroid inference and return fixed-size outputs.\"\"\"\n        image = self._normalize_uint8(image)\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        confmaps = self._extract_tensor(self.model(image), [\"centroid\", \"confmap\"])\n        peaks, values, valid = self._find_topk_peaks(confmaps, self.max_instances)\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"centroids\": peaks,\n            \"centroid_vals\": values,\n            \"instance_valid\": valid,\n        }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.CentroidONNXWrapper.__init__","title":"<code>__init__(model, max_instances=20, output_stride=2, input_scale=1.0)</code>","text":"<p>Initialize centroid ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Centroid detection model.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances to detect.</p> <code>20</code> <code>output_stride</code> <code>int</code> <p>Output stride for confidence maps.</p> <code>2</code> <code>input_scale</code> <code>float</code> <p>Input scaling factor.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/centroid.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    max_instances: int = 20,\n    output_stride: int = 2,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize centroid ONNX wrapper.\n\n    Args:\n        model: Centroid detection model.\n        max_instances: Maximum number of instances to detect.\n        output_stride: Output stride for confidence maps.\n        input_scale: Input scaling factor.\n    \"\"\"\n    super().__init__(model)\n    self.max_instances = max_instances\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.CentroidONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run centroid inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/centroid.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run centroid inference and return fixed-size outputs.\"\"\"\n    image = self._normalize_uint8(image)\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    confmaps = self._extract_tensor(self.model(image), [\"centroid\", \"confmap\"])\n    peaks, values, valid = self._find_topk_peaks(confmaps, self.max_instances)\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"centroids\": peaks,\n        \"centroid_vals\": values,\n        \"instance_valid\": valid,\n    }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.SingleInstanceONNXWrapper","title":"<code>SingleInstanceONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for single-instance models.</p> <p>This wrapper handles full-frame inference assuming a single instance per frame. For each body part (channel), it finds the global maximum in the confidence map.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The trained backbone model that outputs confidence maps.</p> <code>output_stride</code> <p>Output stride of the model (e.g., 4 means confmaps are \u00bc the input resolution).</p> <code>input_scale</code> <p>Factor to scale input images before inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the single-instance wrapper.</p> <code>forward</code> <p>Run single-instance inference.</p> Source code in <code>sleap_nn/export/wrappers/single_instance.py</code> <pre><code>class SingleInstanceONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for single-instance models.\n\n    This wrapper handles full-frame inference assuming a single instance per frame.\n    For each body part (channel), it finds the global maximum in the confidence map.\n\n    Expects input images as uint8 tensors in [0, 255].\n\n    Attributes:\n        model: The trained backbone model that outputs confidence maps.\n        output_stride: Output stride of the model (e.g., 4 means confmaps are 1/4 the\n            input resolution).\n        input_scale: Factor to scale input images before inference.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        output_stride: int = 4,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize the single-instance wrapper.\n\n        Args:\n            model: The trained backbone model.\n            output_stride: Output stride of the model. Default: 4.\n            input_scale: Factor to scale input images. Default: 1.0.\n        \"\"\"\n        super().__init__(model)\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run single-instance inference.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                Expected as uint8 [0, 255] values.\n\n        Returns:\n            Dictionary with:\n                peaks: Peak coordinates of shape (batch, n_nodes, 2) in (x, y) format.\n                peak_vals: Peak confidence values of shape (batch, n_nodes).\n        \"\"\"\n        # Normalize uint8 [0, 255] to float32 [0, 1]\n        image = self._normalize_uint8(image)\n\n        # Apply input scaling if needed\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        # Run model to get confidence maps: (batch, n_nodes, height, width)\n        confmaps = self._extract_tensor(\n            self.model(image), [\"single\", \"instance\", \"confmap\"]\n        )\n\n        # Find global peak for each channel: (batch, n_nodes, 2), (batch, n_nodes)\n        peaks, values = self._find_global_peaks(confmaps)\n\n        # Scale peaks from confmap coordinates to image coordinates\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": values,\n        }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.SingleInstanceONNXWrapper.__init__","title":"<code>__init__(model, output_stride=4, input_scale=1.0)</code>","text":"<p>Initialize the single-instance wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The trained backbone model.</p> required <code>output_stride</code> <code>int</code> <p>Output stride of the model. Default: 4.</p> <code>4</code> <code>input_scale</code> <code>float</code> <p>Factor to scale input images. Default: 1.0.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/single_instance.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    output_stride: int = 4,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize the single-instance wrapper.\n\n    Args:\n        model: The trained backbone model.\n        output_stride: Output stride of the model. Default: 4.\n        input_scale: Factor to scale input images. Default: 1.0.\n    \"\"\"\n    super().__init__(model)\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.SingleInstanceONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run single-instance inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width). Expected as uint8 [0, 255] values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with:     peaks: Peak coordinates of shape (batch, n_nodes, 2) in (x, y) format.     peak_vals: Peak confidence values of shape (batch, n_nodes).</p> Source code in <code>sleap_nn/export/wrappers/single_instance.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run single-instance inference.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n            Expected as uint8 [0, 255] values.\n\n    Returns:\n        Dictionary with:\n            peaks: Peak coordinates of shape (batch, n_nodes, 2) in (x, y) format.\n            peak_vals: Peak confidence values of shape (batch, n_nodes).\n    \"\"\"\n    # Normalize uint8 [0, 255] to float32 [0, 1]\n    image = self._normalize_uint8(image)\n\n    # Apply input scaling if needed\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    # Run model to get confidence maps: (batch, n_nodes, height, width)\n    confmaps = self._extract_tensor(\n        self.model(image), [\"single\", \"instance\", \"confmap\"]\n    )\n\n    # Find global peak for each channel: (batch, n_nodes, 2), (batch, n_nodes)\n    peaks, values = self._find_global_peaks(confmaps)\n\n    # Scale peaks from confmap coordinates to image coordinates\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": values,\n    }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownMultiClassCombinedONNXWrapper","title":"<code>TopDownMultiClassCombinedONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for combined centroid + multiclass instance models.</p> <p>This wrapper combines a centroid detection model with a centered instance multiclass model. It performs: 1. Centroid detection on full images 2. Cropping around each centroid using vectorized grid_sample 3. Instance keypoint detection + identity classification on each crop</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the combined wrapper.</p> <code>forward</code> <p>Run combined top-down multiclass inference.</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>class TopDownMultiClassCombinedONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for combined centroid + multiclass instance models.\n\n    This wrapper combines a centroid detection model with a centered instance\n    multiclass model. It performs:\n    1. Centroid detection on full images\n    2. Cropping around each centroid using vectorized grid_sample\n    3. Instance keypoint detection + identity classification on each crop\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        centroid_model: nn.Module,\n        instance_model: nn.Module,\n        max_instances: int = 20,\n        crop_size: tuple = (192, 192),\n        centroid_output_stride: int = 4,\n        instance_output_stride: int = 2,\n        centroid_input_scale: float = 1.0,\n        instance_input_scale: float = 1.0,\n        n_nodes: int = 13,\n        n_classes: int = 2,\n    ):\n        \"\"\"Initialize the combined wrapper.\n\n        Args:\n            centroid_model: Model for centroid detection.\n            instance_model: Model for instance keypoints + class prediction.\n            max_instances: Maximum number of instances to detect.\n            crop_size: Size of crops around centroids (height, width).\n            centroid_output_stride: Output stride of centroid model.\n            instance_output_stride: Output stride of instance model.\n            centroid_input_scale: Input scale for centroid model.\n            instance_input_scale: Input scale for instance model.\n            n_nodes: Number of keypoint nodes per instance.\n            n_classes: Number of identity classes.\n        \"\"\"\n        super().__init__(centroid_model)  # Primary model is centroid\n        self.instance_model = instance_model\n        self.max_instances = max_instances\n        self.crop_size = crop_size\n        self.centroid_output_stride = centroid_output_stride\n        self.instance_output_stride = instance_output_stride\n        self.centroid_input_scale = centroid_input_scale\n        self.instance_input_scale = instance_input_scale\n        self.n_nodes = n_nodes\n        self.n_classes = n_classes\n\n        # Pre-compute base grid for crop extraction (same as TopDownONNXWrapper)\n        crop_h, crop_w = crop_size\n        y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n        x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n        grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n        base_grid = torch.stack([grid_x, grid_y], dim=-1)\n        self.register_buffer(\"base_grid\", base_grid, persistent=False)\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run combined top-down multiclass inference.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                   Expected to be uint8 in [0, 255].\n\n        Returns:\n            Dictionary with keys:\n                - \"centroids\": Detected centroids (batch, max_instances, 2).\n                - \"centroid_vals\": Centroid confidence values (batch, max_instances).\n                - \"peaks\": Instance peaks (batch, max_instances, n_nodes, 2).\n                - \"peak_vals\": Peak values (batch, max_instances, n_nodes).\n                - \"class_logits\": Class logits per instance (batch, max_instances, n_classes).\n                - \"instance_valid\": Validity mask (batch, max_instances).\n        \"\"\"\n        # Normalize input\n        image = self._normalize_uint8(image)\n        batch_size, channels, height, width = image.shape\n\n        # Apply centroid input scaling\n        scaled_image = image\n        if self.centroid_input_scale != 1.0:\n            scaled_h = int(height * self.centroid_input_scale)\n            scaled_w = int(width * self.centroid_input_scale)\n            scaled_image = F.interpolate(\n                scaled_image,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        # Centroid detection\n        centroid_out = self.model(scaled_image)\n        centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n        centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n            centroid_cms, self.max_instances\n        )\n        centroids = centroids * (\n            self.centroid_output_stride / self.centroid_input_scale\n        )\n\n        # Extract crops using vectorized grid_sample (same as TopDownONNXWrapper)\n        crops = self._extract_crops(image, centroids)\n        crops_flat = crops.reshape(\n            batch_size * self.max_instances,\n            channels,\n            self.crop_size[0],\n            self.crop_size[1],\n        )\n\n        # Apply instance input scaling if needed\n        if self.instance_input_scale != 1.0:\n            scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n            scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n            crops_flat = F.interpolate(\n                crops_flat,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        # Instance model forward (batch all crops)\n        instance_out = self.instance_model(crops_flat)\n        instance_cms = self._extract_tensor(\n            instance_out, [\"centered\", \"instance\", \"confmap\"]\n        )\n        instance_class = self._extract_tensor(instance_out, [\"class\", \"vector\"])\n\n        # Find peaks in all crops\n        crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n        crop_peaks = crop_peaks * (\n            self.instance_output_stride / self.instance_input_scale\n        )\n\n        # Reshape to batch x instances x nodes x 2\n        crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n        peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n        # Reshape class logits\n        class_logits = instance_class.reshape(\n            batch_size, self.max_instances, self.n_classes\n        )\n\n        # Transform peaks from crop coordinates to full image coordinates\n        crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n            [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n        )\n        peaks = crop_peaks + crop_offset\n\n        # Zero out invalid instances\n        invalid_mask = ~instance_valid\n        centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n        centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n        peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n        peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n        class_logits = class_logits.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n        return {\n            \"centroids\": centroids,\n            \"centroid_vals\": centroid_vals,\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"class_logits\": class_logits,\n            \"instance_valid\": instance_valid,\n        }\n\n    def _extract_crops(\n        self,\n        image: torch.Tensor,\n        centroids: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extract crops around centroids using grid_sample.\n\n        This is the same vectorized implementation as TopDownONNXWrapper.\n        \"\"\"\n        batch_size, channels, height, width = image.shape\n        crop_h, crop_w = self.crop_size\n        n_instances = centroids.shape[1]\n\n        scale_x = crop_w / width\n        scale_y = crop_h / height\n        scale = image.new_tensor([scale_x, scale_y])\n        base_grid = self.base_grid.to(device=image.device, dtype=image.dtype)\n        scaled_grid = base_grid * scale\n\n        scaled_grid = scaled_grid.unsqueeze(0).unsqueeze(0)\n        scaled_grid = scaled_grid.expand(batch_size, n_instances, -1, -1, -1)\n\n        norm_centroids = torch.zeros_like(centroids)\n        norm_centroids[..., 0] = (centroids[..., 0] / (width - 1)) * 2 - 1\n        norm_centroids[..., 1] = (centroids[..., 1] / (height - 1)) * 2 - 1\n        offset = norm_centroids.unsqueeze(2).unsqueeze(2)\n\n        sample_grid = scaled_grid + offset\n\n        image_expanded = image.unsqueeze(1).expand(-1, n_instances, -1, -1, -1)\n        image_flat = image_expanded.reshape(\n            batch_size * n_instances, channels, height, width\n        )\n        grid_flat = sample_grid.reshape(batch_size * n_instances, crop_h, crop_w, 2)\n\n        crops_flat = F.grid_sample(\n            image_flat,\n            grid_flat,\n            mode=\"bilinear\",\n            padding_mode=\"zeros\",\n            align_corners=True,\n        )\n\n        crops = crops_flat.reshape(batch_size, n_instances, channels, crop_h, crop_w)\n        return crops\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownMultiClassCombinedONNXWrapper.__init__","title":"<code>__init__(centroid_model, instance_model, max_instances=20, crop_size=(192, 192), centroid_output_stride=4, instance_output_stride=2, centroid_input_scale=1.0, instance_input_scale=1.0, n_nodes=13, n_classes=2)</code>","text":"<p>Initialize the combined wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_model</code> <code>Module</code> <p>Model for centroid detection.</p> required <code>instance_model</code> <code>Module</code> <p>Model for instance keypoints + class prediction.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances to detect.</p> <code>20</code> <code>crop_size</code> <code>tuple</code> <p>Size of crops around centroids (height, width).</p> <code>(192, 192)</code> <code>centroid_output_stride</code> <code>int</code> <p>Output stride of centroid model.</p> <code>4</code> <code>instance_output_stride</code> <code>int</code> <p>Output stride of instance model.</p> <code>2</code> <code>centroid_input_scale</code> <code>float</code> <p>Input scale for centroid model.</p> <code>1.0</code> <code>instance_input_scale</code> <code>float</code> <p>Input scale for instance model.</p> <code>1.0</code> <code>n_nodes</code> <code>int</code> <p>Number of keypoint nodes per instance.</p> <code>13</code> <code>n_classes</code> <code>int</code> <p>Number of identity classes.</p> <code>2</code> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def __init__(\n    self,\n    centroid_model: nn.Module,\n    instance_model: nn.Module,\n    max_instances: int = 20,\n    crop_size: tuple = (192, 192),\n    centroid_output_stride: int = 4,\n    instance_output_stride: int = 2,\n    centroid_input_scale: float = 1.0,\n    instance_input_scale: float = 1.0,\n    n_nodes: int = 13,\n    n_classes: int = 2,\n):\n    \"\"\"Initialize the combined wrapper.\n\n    Args:\n        centroid_model: Model for centroid detection.\n        instance_model: Model for instance keypoints + class prediction.\n        max_instances: Maximum number of instances to detect.\n        crop_size: Size of crops around centroids (height, width).\n        centroid_output_stride: Output stride of centroid model.\n        instance_output_stride: Output stride of instance model.\n        centroid_input_scale: Input scale for centroid model.\n        instance_input_scale: Input scale for instance model.\n        n_nodes: Number of keypoint nodes per instance.\n        n_classes: Number of identity classes.\n    \"\"\"\n    super().__init__(centroid_model)  # Primary model is centroid\n    self.instance_model = instance_model\n    self.max_instances = max_instances\n    self.crop_size = crop_size\n    self.centroid_output_stride = centroid_output_stride\n    self.instance_output_stride = instance_output_stride\n    self.centroid_input_scale = centroid_input_scale\n    self.instance_input_scale = instance_input_scale\n    self.n_nodes = n_nodes\n    self.n_classes = n_classes\n\n    # Pre-compute base grid for crop extraction (same as TopDownONNXWrapper)\n    crop_h, crop_w = crop_size\n    y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n    x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n    grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n    base_grid = torch.stack([grid_x, grid_y], dim=-1)\n    self.register_buffer(\"base_grid\", base_grid, persistent=False)\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownMultiClassCombinedONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run combined top-down multiclass inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width).    Expected to be uint8 in [0, 255].</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with keys:     - \"centroids\": Detected centroids (batch, max_instances, 2).     - \"centroid_vals\": Centroid confidence values (batch, max_instances).     - \"peaks\": Instance peaks (batch, max_instances, n_nodes, 2).     - \"peak_vals\": Peak values (batch, max_instances, n_nodes).     - \"class_logits\": Class logits per instance (batch, max_instances, n_classes).     - \"instance_valid\": Validity mask (batch, max_instances).</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run combined top-down multiclass inference.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n               Expected to be uint8 in [0, 255].\n\n    Returns:\n        Dictionary with keys:\n            - \"centroids\": Detected centroids (batch, max_instances, 2).\n            - \"centroid_vals\": Centroid confidence values (batch, max_instances).\n            - \"peaks\": Instance peaks (batch, max_instances, n_nodes, 2).\n            - \"peak_vals\": Peak values (batch, max_instances, n_nodes).\n            - \"class_logits\": Class logits per instance (batch, max_instances, n_classes).\n            - \"instance_valid\": Validity mask (batch, max_instances).\n    \"\"\"\n    # Normalize input\n    image = self._normalize_uint8(image)\n    batch_size, channels, height, width = image.shape\n\n    # Apply centroid input scaling\n    scaled_image = image\n    if self.centroid_input_scale != 1.0:\n        scaled_h = int(height * self.centroid_input_scale)\n        scaled_w = int(width * self.centroid_input_scale)\n        scaled_image = F.interpolate(\n            scaled_image,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    # Centroid detection\n    centroid_out = self.model(scaled_image)\n    centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n    centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n        centroid_cms, self.max_instances\n    )\n    centroids = centroids * (\n        self.centroid_output_stride / self.centroid_input_scale\n    )\n\n    # Extract crops using vectorized grid_sample (same as TopDownONNXWrapper)\n    crops = self._extract_crops(image, centroids)\n    crops_flat = crops.reshape(\n        batch_size * self.max_instances,\n        channels,\n        self.crop_size[0],\n        self.crop_size[1],\n    )\n\n    # Apply instance input scaling if needed\n    if self.instance_input_scale != 1.0:\n        scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n        scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n        crops_flat = F.interpolate(\n            crops_flat,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    # Instance model forward (batch all crops)\n    instance_out = self.instance_model(crops_flat)\n    instance_cms = self._extract_tensor(\n        instance_out, [\"centered\", \"instance\", \"confmap\"]\n    )\n    instance_class = self._extract_tensor(instance_out, [\"class\", \"vector\"])\n\n    # Find peaks in all crops\n    crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n    crop_peaks = crop_peaks * (\n        self.instance_output_stride / self.instance_input_scale\n    )\n\n    # Reshape to batch x instances x nodes x 2\n    crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n    peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n    # Reshape class logits\n    class_logits = instance_class.reshape(\n        batch_size, self.max_instances, self.n_classes\n    )\n\n    # Transform peaks from crop coordinates to full image coordinates\n    crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n        [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n    )\n    peaks = crop_peaks + crop_offset\n\n    # Zero out invalid instances\n    invalid_mask = ~instance_valid\n    centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n    centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n    peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n    peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n    class_logits = class_logits.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n    return {\n        \"centroids\": centroids,\n        \"centroid_vals\": centroid_vals,\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"class_logits\": class_logits,\n        \"instance_valid\": instance_valid,\n    }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownMultiClassONNXWrapper","title":"<code>TopDownMultiClassONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for top-down multiclass (supervised ID) models.</p> <p>This wrapper handles models that output both confidence maps for keypoint detection and class logits for identity classification. It runs on instance crops (centered around detected centroids).</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The underlying PyTorch model (centered instance + class vectors heads).</p> <code>output_stride</code> <p>Output stride of the confmap head.</p> <code>input_scale</code> <p>Scale factor applied to input images before inference.</p> <code>n_classes</code> <p>Number of identity classes.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the wrapper.</p> <code>forward</code> <p>Run top-down multiclass inference on crops.</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>class TopDownMultiClassONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for top-down multiclass (supervised ID) models.\n\n    This wrapper handles models that output both confidence maps for keypoint\n    detection and class logits for identity classification. It runs on instance\n    crops (centered around detected centroids).\n\n    Expects input images as uint8 tensors in [0, 255].\n\n    Attributes:\n        model: The underlying PyTorch model (centered instance + class vectors heads).\n        output_stride: Output stride of the confmap head.\n        input_scale: Scale factor applied to input images before inference.\n        n_classes: Number of identity classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        output_stride: int = 2,\n        input_scale: float = 1.0,\n        n_classes: int = 2,\n    ):\n        \"\"\"Initialize the wrapper.\n\n        Args:\n            model: The underlying PyTorch model.\n            output_stride: Output stride of the confidence maps.\n            input_scale: Scale factor for input images.\n            n_classes: Number of identity classes (e.g., 2 for male/female).\n        \"\"\"\n        super().__init__(model)\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n        self.n_classes = n_classes\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run top-down multiclass inference on crops.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                   Expected to be uint8 in [0, 255].\n\n        Returns:\n            Dictionary with keys:\n                - \"peaks\": Predicted peak coordinates (batch, n_nodes, 2) in (x, y).\n                - \"peak_vals\": Peak confidence values (batch, n_nodes).\n                - \"class_logits\": Raw class logits (batch, n_classes).\n\n            The class assignment is done on CPU using Hungarian matching\n            via `get_class_inds_from_vectors()`.\n        \"\"\"\n        # Normalize uint8 [0, 255] to float32 [0, 1]\n        image = self._normalize_uint8(image)\n\n        # Apply input scaling if needed\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        # Forward pass\n        out = self.model(image)\n\n        # Extract outputs\n        confmaps = self._extract_tensor(out, [\"centered\", \"instance\", \"confmap\"])\n        class_logits = self._extract_tensor(out, [\"class\", \"vector\"])\n\n        # Find global peaks (one per node)\n        peaks, peak_vals = self._find_global_peaks(confmaps)\n\n        # Scale peaks back to input coordinates\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"class_logits\": class_logits,\n        }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownMultiClassONNXWrapper.__init__","title":"<code>__init__(model, output_stride=2, input_scale=1.0, n_classes=2)</code>","text":"<p>Initialize the wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The underlying PyTorch model.</p> required <code>output_stride</code> <code>int</code> <p>Output stride of the confidence maps.</p> <code>2</code> <code>input_scale</code> <code>float</code> <p>Scale factor for input images.</p> <code>1.0</code> <code>n_classes</code> <code>int</code> <p>Number of identity classes (e.g., 2 for male/female).</p> <code>2</code> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    output_stride: int = 2,\n    input_scale: float = 1.0,\n    n_classes: int = 2,\n):\n    \"\"\"Initialize the wrapper.\n\n    Args:\n        model: The underlying PyTorch model.\n        output_stride: Output stride of the confidence maps.\n        input_scale: Scale factor for input images.\n        n_classes: Number of identity classes (e.g., 2 for male/female).\n    \"\"\"\n    super().__init__(model)\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n    self.n_classes = n_classes\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownMultiClassONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run top-down multiclass inference on crops.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width).    Expected to be uint8 in [0, 255].</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with keys:     - \"peaks\": Predicted peak coordinates (batch, n_nodes, 2) in (x, y).     - \"peak_vals\": Peak confidence values (batch, n_nodes).     - \"class_logits\": Raw class logits (batch, n_classes).</p> <p>The class assignment is done on CPU using Hungarian matching via <code>get_class_inds_from_vectors()</code>.</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run top-down multiclass inference on crops.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n               Expected to be uint8 in [0, 255].\n\n    Returns:\n        Dictionary with keys:\n            - \"peaks\": Predicted peak coordinates (batch, n_nodes, 2) in (x, y).\n            - \"peak_vals\": Peak confidence values (batch, n_nodes).\n            - \"class_logits\": Raw class logits (batch, n_classes).\n\n        The class assignment is done on CPU using Hungarian matching\n        via `get_class_inds_from_vectors()`.\n    \"\"\"\n    # Normalize uint8 [0, 255] to float32 [0, 1]\n    image = self._normalize_uint8(image)\n\n    # Apply input scaling if needed\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    # Forward pass\n    out = self.model(image)\n\n    # Extract outputs\n    confmaps = self._extract_tensor(out, [\"centered\", \"instance\", \"confmap\"])\n    class_logits = self._extract_tensor(out, [\"class\", \"vector\"])\n\n    # Find global peaks (one per node)\n    peaks, peak_vals = self._find_global_peaks(confmaps)\n\n    # Scale peaks back to input coordinates\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"class_logits\": class_logits,\n    }\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownONNXWrapper","title":"<code>TopDownONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for top-down (centroid + centered-instance) inference.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize top-down ONNX wrapper.</p> <code>forward</code> <p>Run top-down inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/topdown.py</code> <pre><code>class TopDownONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for top-down (centroid + centered-instance) inference.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        centroid_model: nn.Module,\n        instance_model: nn.Module,\n        max_instances: int = 20,\n        crop_size: Tuple[int, int] = (192, 192),\n        centroid_output_stride: int = 2,\n        instance_output_stride: int = 4,\n        centroid_input_scale: float = 1.0,\n        instance_input_scale: float = 1.0,\n        n_nodes: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize top-down ONNX wrapper.\n\n        Args:\n            centroid_model: Centroid detection model.\n            instance_model: Instance pose estimation model.\n            max_instances: Maximum number of instances to detect.\n            crop_size: Size of instance crops (height, width).\n            centroid_output_stride: Centroid model output stride.\n            instance_output_stride: Instance model output stride.\n            centroid_input_scale: Centroid input scaling factor.\n            instance_input_scale: Instance input scaling factor.\n            n_nodes: Number of skeleton nodes.\n        \"\"\"\n        super().__init__(centroid_model)\n        self.centroid_model = centroid_model\n        self.instance_model = instance_model\n        self.max_instances = max_instances\n        self.crop_size = crop_size\n        self.centroid_output_stride = centroid_output_stride\n        self.instance_output_stride = instance_output_stride\n        self.centroid_input_scale = centroid_input_scale\n        self.instance_input_scale = instance_input_scale\n        self.n_nodes = n_nodes\n\n        crop_h, crop_w = crop_size\n        y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n        x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n        grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n        base_grid = torch.stack([grid_x, grid_y], dim=-1)\n        self.register_buffer(\"base_grid\", base_grid, persistent=False)\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run top-down inference and return fixed-size outputs.\"\"\"\n        image = self._normalize_uint8(image)\n        batch_size, channels, height, width = image.shape\n\n        scaled_image = image\n        if self.centroid_input_scale != 1.0:\n            scaled_h = int(height * self.centroid_input_scale)\n            scaled_w = int(width * self.centroid_input_scale)\n            scaled_image = F.interpolate(\n                scaled_image,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        centroid_out = self.centroid_model(scaled_image)\n        centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n\n        centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n            centroid_cms, self.max_instances\n        )\n        centroids = centroids * (\n            self.centroid_output_stride / self.centroid_input_scale\n        )\n\n        crops = self._extract_crops(image, centroids)\n        crops_flat = crops.reshape(\n            batch_size * self.max_instances,\n            channels,\n            self.crop_size[0],\n            self.crop_size[1],\n        )\n\n        if self.instance_input_scale != 1.0:\n            scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n            scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n            crops_flat = F.interpolate(\n                crops_flat,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        instance_out = self.instance_model(crops_flat)\n        instance_cms = self._extract_tensor(\n            instance_out, [\"centered\", \"instance\", \"confmap\"]\n        )\n\n        crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n        crop_peaks = crop_peaks * (\n            self.instance_output_stride / self.instance_input_scale\n        )\n\n        crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n        peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n        crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n            [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n        )\n        peaks = crop_peaks + crop_offset\n\n        invalid_mask = ~instance_valid\n        centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n        centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n        peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n        peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n        return {\n            \"centroids\": centroids,\n            \"centroid_vals\": centroid_vals,\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"instance_valid\": instance_valid,\n        }\n\n    def _extract_crops(\n        self,\n        image: torch.Tensor,\n        centroids: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extract crops around centroids using grid_sample.\"\"\"\n        batch_size, channels, height, width = image.shape\n        crop_h, crop_w = self.crop_size\n        n_instances = centroids.shape[1]\n\n        scale_x = crop_w / width\n        scale_y = crop_h / height\n        scale = image.new_tensor([scale_x, scale_y])\n        base_grid = self.base_grid.to(device=image.device, dtype=image.dtype)\n        scaled_grid = base_grid * scale\n\n        scaled_grid = scaled_grid.unsqueeze(0).unsqueeze(0)\n        scaled_grid = scaled_grid.expand(batch_size, n_instances, -1, -1, -1)\n\n        norm_centroids = torch.zeros_like(centroids)\n        norm_centroids[..., 0] = (centroids[..., 0] / (width - 1)) * 2 - 1\n        norm_centroids[..., 1] = (centroids[..., 1] / (height - 1)) * 2 - 1\n        offset = norm_centroids.unsqueeze(2).unsqueeze(2)\n\n        sample_grid = scaled_grid + offset\n\n        image_expanded = image.unsqueeze(1).expand(-1, n_instances, -1, -1, -1)\n        image_flat = image_expanded.reshape(\n            batch_size * n_instances, channels, height, width\n        )\n        grid_flat = sample_grid.reshape(batch_size * n_instances, crop_h, crop_w, 2)\n\n        crops_flat = F.grid_sample(\n            image_flat,\n            grid_flat,\n            mode=\"bilinear\",\n            padding_mode=\"zeros\",\n            align_corners=True,\n        )\n\n        crops = crops_flat.reshape(batch_size, n_instances, channels, crop_h, crop_w)\n        return crops\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownONNXWrapper.__init__","title":"<code>__init__(centroid_model, instance_model, max_instances=20, crop_size=(192, 192), centroid_output_stride=2, instance_output_stride=4, centroid_input_scale=1.0, instance_input_scale=1.0, n_nodes=1)</code>","text":"<p>Initialize top-down ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_model</code> <code>Module</code> <p>Centroid detection model.</p> required <code>instance_model</code> <code>Module</code> <p>Instance pose estimation model.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances to detect.</p> <code>20</code> <code>crop_size</code> <code>Tuple[int, int]</code> <p>Size of instance crops (height, width).</p> <code>(192, 192)</code> <code>centroid_output_stride</code> <code>int</code> <p>Centroid model output stride.</p> <code>2</code> <code>instance_output_stride</code> <code>int</code> <p>Instance model output stride.</p> <code>4</code> <code>centroid_input_scale</code> <code>float</code> <p>Centroid input scaling factor.</p> <code>1.0</code> <code>instance_input_scale</code> <code>float</code> <p>Instance input scaling factor.</p> <code>1.0</code> <code>n_nodes</code> <code>int</code> <p>Number of skeleton nodes.</p> <code>1</code> Source code in <code>sleap_nn/export/wrappers/topdown.py</code> <pre><code>def __init__(\n    self,\n    centroid_model: nn.Module,\n    instance_model: nn.Module,\n    max_instances: int = 20,\n    crop_size: Tuple[int, int] = (192, 192),\n    centroid_output_stride: int = 2,\n    instance_output_stride: int = 4,\n    centroid_input_scale: float = 1.0,\n    instance_input_scale: float = 1.0,\n    n_nodes: int = 1,\n) -&gt; None:\n    \"\"\"Initialize top-down ONNX wrapper.\n\n    Args:\n        centroid_model: Centroid detection model.\n        instance_model: Instance pose estimation model.\n        max_instances: Maximum number of instances to detect.\n        crop_size: Size of instance crops (height, width).\n        centroid_output_stride: Centroid model output stride.\n        instance_output_stride: Instance model output stride.\n        centroid_input_scale: Centroid input scaling factor.\n        instance_input_scale: Instance input scaling factor.\n        n_nodes: Number of skeleton nodes.\n    \"\"\"\n    super().__init__(centroid_model)\n    self.centroid_model = centroid_model\n    self.instance_model = instance_model\n    self.max_instances = max_instances\n    self.crop_size = crop_size\n    self.centroid_output_stride = centroid_output_stride\n    self.instance_output_stride = instance_output_stride\n    self.centroid_input_scale = centroid_input_scale\n    self.instance_input_scale = instance_input_scale\n    self.n_nodes = n_nodes\n\n    crop_h, crop_w = crop_size\n    y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n    x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n    grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n    base_grid = torch.stack([grid_x, grid_y], dim=-1)\n    self.register_buffer(\"base_grid\", base_grid, persistent=False)\n</code></pre>"},{"location":"api/export/wrappers/#sleap_nn.export.wrappers.TopDownONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run top-down inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/topdown.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run top-down inference and return fixed-size outputs.\"\"\"\n    image = self._normalize_uint8(image)\n    batch_size, channels, height, width = image.shape\n\n    scaled_image = image\n    if self.centroid_input_scale != 1.0:\n        scaled_h = int(height * self.centroid_input_scale)\n        scaled_w = int(width * self.centroid_input_scale)\n        scaled_image = F.interpolate(\n            scaled_image,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    centroid_out = self.centroid_model(scaled_image)\n    centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n\n    centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n        centroid_cms, self.max_instances\n    )\n    centroids = centroids * (\n        self.centroid_output_stride / self.centroid_input_scale\n    )\n\n    crops = self._extract_crops(image, centroids)\n    crops_flat = crops.reshape(\n        batch_size * self.max_instances,\n        channels,\n        self.crop_size[0],\n        self.crop_size[1],\n    )\n\n    if self.instance_input_scale != 1.0:\n        scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n        scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n        crops_flat = F.interpolate(\n            crops_flat,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    instance_out = self.instance_model(crops_flat)\n    instance_cms = self._extract_tensor(\n        instance_out, [\"centered\", \"instance\", \"confmap\"]\n    )\n\n    crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n    crop_peaks = crop_peaks * (\n        self.instance_output_stride / self.instance_input_scale\n    )\n\n    crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n    peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n    crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n        [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n    )\n    peaks = crop_peaks + crop_offset\n\n    invalid_mask = ~instance_valid\n    centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n    centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n    peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n    peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n    return {\n        \"centroids\": centroids,\n        \"centroid_vals\": centroid_vals,\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"instance_valid\": instance_valid,\n    }\n</code></pre>"},{"location":"api/export/wrappers/base/","title":"base","text":""},{"location":"api/export/wrappers/base/#sleap_nn.export.wrappers.base","title":"<code>sleap_nn.export.wrappers.base</code>","text":"<p>Base classes and shared helpers for export wrappers.</p> <p>Classes:</p> Name Description <code>BaseExportWrapper</code> <p>Base class for ONNX-exportable wrappers.</p>"},{"location":"api/export/wrappers/base/#sleap_nn.export.wrappers.base.BaseExportWrapper","title":"<code>BaseExportWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for ONNX-exportable wrappers.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize wrapper with the underlying model.</p> Source code in <code>sleap_nn/export/wrappers/base.py</code> <pre><code>class BaseExportWrapper(nn.Module):\n    \"\"\"Base class for ONNX-exportable wrappers.\"\"\"\n\n    def __init__(self, model: nn.Module):\n        \"\"\"Initialize wrapper with the underlying model.\n\n        Args:\n            model: The PyTorch model to wrap for export.\n        \"\"\"\n        super().__init__()\n        self.model = model\n\n    @staticmethod\n    def _normalize_uint8(image: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Normalize unnormalized uint8 (or [0, 255] float) images to [0, 1].\"\"\"\n        if image.dtype != torch.float32:\n            image = image.float()\n        return image / 255.0\n\n    @staticmethod\n    def _extract_tensor(output, key_hints: Iterable[str]) -&gt; torch.Tensor:\n        if isinstance(output, dict):\n            for key in output:\n                for hint in key_hints:\n                    if hint.lower() in key.lower():\n                        return output[key]\n            return next(iter(output.values()))\n        return output\n\n    @staticmethod\n    def _find_topk_peaks(\n        confmaps: torch.Tensor, k: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Top-K peak finding with NMS via max pooling.\"\"\"\n        batch_size, _, height, width = confmaps.shape\n        pooled = F.max_pool2d(confmaps, kernel_size=3, stride=1, padding=1)\n        is_peak = (confmaps == pooled) &amp; (confmaps &gt; 0)\n\n        confmaps_flat = confmaps.reshape(batch_size, height * width)\n        is_peak_flat = is_peak.reshape(batch_size, height * width)\n        masked = torch.where(\n            is_peak_flat, confmaps_flat, torch.full_like(confmaps_flat, -1e9)\n        )\n        values, indices = torch.topk(masked, k=k, dim=1)\n\n        y = indices // width\n        x = indices % width\n        peaks = torch.stack([x.float(), y.float()], dim=-1)\n        valid = values &gt; 0\n        return peaks, values, valid\n\n    @staticmethod\n    def _find_topk_peaks_per_node(\n        confmaps: torch.Tensor, k: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Top-K peak finding per channel with NMS via max pooling.\"\"\"\n        batch_size, n_nodes, height, width = confmaps.shape\n        pooled = F.max_pool2d(confmaps, kernel_size=3, stride=1, padding=1)\n        is_peak = (confmaps == pooled) &amp; (confmaps &gt; 0)\n\n        confmaps_flat = confmaps.reshape(batch_size, n_nodes, height * width)\n        is_peak_flat = is_peak.reshape(batch_size, n_nodes, height * width)\n        masked = torch.where(\n            is_peak_flat, confmaps_flat, torch.full_like(confmaps_flat, -1e9)\n        )\n        values, indices = torch.topk(masked, k=k, dim=2)\n\n        y = indices // width\n        x = indices % width\n        peaks = torch.stack([x.float(), y.float()], dim=-1)\n        valid = values &gt; 0\n        return peaks, values, valid\n\n    @staticmethod\n    def _find_global_peaks(\n        confmaps: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Find global maxima per channel.\"\"\"\n        batch_size, channels, height, width = confmaps.shape\n        flat = confmaps.reshape(batch_size, channels, height * width)\n        values, indices = flat.max(dim=-1)\n        y = indices // width\n        x = indices % width\n        peaks = torch.stack([x.float(), y.float()], dim=-1)\n        return peaks, values\n</code></pre>"},{"location":"api/export/wrappers/base/#sleap_nn.export.wrappers.base.BaseExportWrapper.__init__","title":"<code>__init__(model)</code>","text":"<p>Initialize wrapper with the underlying model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to wrap for export.</p> required Source code in <code>sleap_nn/export/wrappers/base.py</code> <pre><code>def __init__(self, model: nn.Module):\n    \"\"\"Initialize wrapper with the underlying model.\n\n    Args:\n        model: The PyTorch model to wrap for export.\n    \"\"\"\n    super().__init__()\n    self.model = model\n</code></pre>"},{"location":"api/export/wrappers/bottomup/","title":"bottomup","text":""},{"location":"api/export/wrappers/bottomup/#sleap_nn.export.wrappers.bottomup","title":"<code>sleap_nn.export.wrappers.bottomup</code>","text":"<p>Bottom-up ONNX wrapper.</p> <p>Classes:</p> Name Description <code>BottomUpONNXWrapper</code> <p>ONNX-exportable wrapper for bottom-up inference up to PAF scoring.</p>"},{"location":"api/export/wrappers/bottomup/#sleap_nn.export.wrappers.bottomup.BottomUpONNXWrapper","title":"<code>BottomUpONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for bottom-up inference up to PAF scoring.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize bottom-up ONNX wrapper.</p> <code>forward</code> <p>Run bottom-up inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/bottomup.py</code> <pre><code>class BottomUpONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for bottom-up inference up to PAF scoring.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        skeleton_edges: list,\n        n_nodes: int,\n        max_peaks_per_node: int = 20,\n        n_line_points: int = 10,\n        cms_output_stride: int = 4,\n        pafs_output_stride: int = 8,\n        max_edge_length_ratio: float = 0.25,\n        dist_penalty_weight: float = 1.0,\n        input_scale: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize bottom-up ONNX wrapper.\n\n        Args:\n            model: Bottom-up model producing confidence maps and PAFs.\n            skeleton_edges: List of (src, dst) edge tuples defining skeleton.\n            n_nodes: Number of nodes in the skeleton.\n            max_peaks_per_node: Maximum peaks to detect per node type.\n            n_line_points: Points to sample along PAF edges.\n            cms_output_stride: Confidence map output stride.\n            pafs_output_stride: PAF output stride.\n            max_edge_length_ratio: Maximum edge length as ratio of image size.\n            dist_penalty_weight: Weight for distance penalty in scoring.\n            input_scale: Input scaling factor.\n        \"\"\"\n        super().__init__(model)\n        self.n_nodes = n_nodes\n        self.n_edges = len(skeleton_edges)\n        self.max_peaks_per_node = max_peaks_per_node\n        self.n_line_points = n_line_points\n        self.cms_output_stride = cms_output_stride\n        self.pafs_output_stride = pafs_output_stride\n        self.max_edge_length_ratio = max_edge_length_ratio\n        self.dist_penalty_weight = dist_penalty_weight\n        self.input_scale = input_scale\n\n        edge_src = torch.tensor([e[0] for e in skeleton_edges], dtype=torch.long)\n        edge_dst = torch.tensor([e[1] for e in skeleton_edges], dtype=torch.long)\n        self.register_buffer(\"edge_src\", edge_src)\n        self.register_buffer(\"edge_dst\", edge_dst)\n\n        line_samples = torch.linspace(0, 1, n_line_points, dtype=torch.float32)\n        self.register_buffer(\"line_samples\", line_samples)\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run bottom-up inference and return fixed-size outputs.\n\n        Note: confmaps and pafs are NOT returned to avoid D2H transfer bottleneck.\n        Peak detection and PAF scoring are performed on GPU within this wrapper.\n        \"\"\"\n        image = self._normalize_uint8(image)\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        batch_size, _, height, width = image.shape\n\n        out = self.model(image)\n        if isinstance(out, dict):\n            confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n            pafs = self._extract_tensor(out, [\"paf\", \"affinity\"])\n        else:\n            confmaps, pafs = out[:2]\n\n        peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n            confmaps, self.max_peaks_per_node\n        )\n\n        peaks = peaks * self.cms_output_stride\n\n        # Compute max_edge_length to match PyTorch implementation:\n        # max_edge_length = ratio * max(paf_dims) * pafs_stride\n        # PAFs shape is (batch, 2*edges, H, W)\n        _, n_paf_channels, paf_height, paf_width = pafs.shape\n        max_paf_dim = max(n_paf_channels, paf_height, paf_width)\n        max_edge_length = torch.tensor(\n            self.max_edge_length_ratio * max_paf_dim * self.pafs_output_stride,\n            dtype=peaks.dtype,\n            device=peaks.device,\n        )\n\n        line_scores, candidate_mask = self._score_all_candidates(\n            pafs, peaks, peak_mask, max_edge_length\n        )\n\n        # Only return final outputs needed for CPU-side grouping.\n        # Do NOT return confmaps/pafs - they are large (~29 MB/batch) and\n        # cause D2H transfer bottleneck. Peak detection and PAF scoring\n        # are already done on GPU above.\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"peak_mask\": peak_mask,\n            \"line_scores\": line_scores,\n            \"candidate_mask\": candidate_mask,\n        }\n\n    def _score_all_candidates(\n        self,\n        pafs: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_mask: torch.Tensor,\n        max_edge_length: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Score all K*K candidate connections for each edge.\"\"\"\n        batch_size = peaks.shape[0]\n        k = self.max_peaks_per_node\n        n_edges = self.n_edges\n\n        _, _, paf_height, paf_width = pafs.shape\n\n        src_peaks = peaks[:, self.edge_src, :, :]\n        dst_peaks = peaks[:, self.edge_dst, :, :]\n\n        src_mask = peak_mask[:, self.edge_src, :]\n        dst_mask = peak_mask[:, self.edge_dst, :]\n\n        src_peaks_exp = src_peaks.unsqueeze(3).expand(-1, -1, -1, k, -1)\n        dst_peaks_exp = dst_peaks.unsqueeze(2).expand(-1, -1, k, -1, -1)\n\n        src_mask_exp = src_mask.unsqueeze(3).expand(-1, -1, -1, k)\n        dst_mask_exp = dst_mask.unsqueeze(2).expand(-1, -1, k, -1)\n        candidate_mask = src_mask_exp &amp; dst_mask_exp\n\n        src_peaks_flat = src_peaks_exp.reshape(batch_size, n_edges, k * k, 2)\n        dst_peaks_flat = dst_peaks_exp.reshape(batch_size, n_edges, k * k, 2)\n        candidate_mask_flat = candidate_mask.reshape(batch_size, n_edges, k * k)\n\n        spatial_vecs = dst_peaks_flat - src_peaks_flat\n        spatial_lengths = torch.norm(spatial_vecs, dim=-1, keepdim=True).clamp(min=1e-6)\n        spatial_vecs_norm = spatial_vecs / spatial_lengths\n\n        line_samples = self.line_samples.view(1, 1, 1, -1, 1)\n        src_exp = src_peaks_flat.unsqueeze(3)\n        dst_exp = dst_peaks_flat.unsqueeze(3)\n        line_points = src_exp + line_samples * (dst_exp - src_exp)\n\n        line_points_paf = line_points / self.pafs_output_stride\n        line_x = line_points_paf[..., 0].clamp(0, paf_width - 1)\n        line_y = line_points_paf[..., 1].clamp(0, paf_height - 1)\n\n        line_scores = self._sample_and_score_lines(\n            pafs,\n            line_x,\n            line_y,\n            spatial_vecs_norm,\n            spatial_lengths.squeeze(-1),\n            max_edge_length,\n        )\n\n        line_scores = line_scores.masked_fill(~candidate_mask_flat, -2.0)\n        return line_scores, candidate_mask_flat\n\n    def _sample_and_score_lines(\n        self,\n        pafs: torch.Tensor,\n        line_x: torch.Tensor,\n        line_y: torch.Tensor,\n        spatial_vecs_norm: torch.Tensor,\n        spatial_lengths: torch.Tensor,\n        max_edge_length: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Sample PAF values along lines and compute scores.\"\"\"\n        batch_size, n_edges, k2, n_points = line_x.shape\n        _, _, paf_height, paf_width = pafs.shape\n\n        all_scores = []\n        for edge_idx in range(n_edges):\n            paf_x = pafs[:, 2 * edge_idx, :, :]\n            paf_y = pafs[:, 2 * edge_idx + 1, :, :]\n\n            lx = line_x[:, edge_idx, :, :]\n            ly = line_y[:, edge_idx, :, :]\n\n            lx_norm = (lx / (paf_width - 1)) * 2 - 1\n            ly_norm = (ly / (paf_height - 1)) * 2 - 1\n\n            grid = torch.stack([lx_norm, ly_norm], dim=-1)\n\n            paf_x_samples = F.grid_sample(\n                paf_x.unsqueeze(1),\n                grid,\n                mode=\"bilinear\",\n                padding_mode=\"zeros\",\n                align_corners=True,\n            ).squeeze(1)\n\n            paf_y_samples = F.grid_sample(\n                paf_y.unsqueeze(1),\n                grid,\n                mode=\"bilinear\",\n                padding_mode=\"zeros\",\n                align_corners=True,\n            ).squeeze(1)\n\n            paf_samples = torch.stack([paf_x_samples, paf_y_samples], dim=-1)\n            disp_vec = spatial_vecs_norm[:, edge_idx, :, :]\n\n            dot_products = (paf_samples * disp_vec.unsqueeze(2)).sum(dim=-1)\n            mean_scores = dot_products.mean(dim=-1)\n\n            edge_lengths = spatial_lengths[:, edge_idx, :]\n            dist_penalty = self._compute_distance_penalty(edge_lengths, max_edge_length)\n\n            all_scores.append(mean_scores + dist_penalty)\n\n        return torch.stack(all_scores, dim=1)\n\n    def _compute_distance_penalty(\n        self, distances: torch.Tensor, max_edge_length: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute distance penalty for edge candidates.\n\n        Matches the PyTorch implementation in sleap_nn.inference.paf_grouping.\n        Penalty is 0 when distance &lt;= max_edge_length, and negative when longer.\n        \"\"\"\n        # Match PyTorch: penalty = clamp((max_edge_length / distance) - 1, max=0) * weight\n        penalty = torch.clamp((max_edge_length / distances) - 1, max=0)\n        return penalty * self.dist_penalty_weight\n</code></pre>"},{"location":"api/export/wrappers/bottomup/#sleap_nn.export.wrappers.bottomup.BottomUpONNXWrapper.__init__","title":"<code>__init__(model, skeleton_edges, n_nodes, max_peaks_per_node=20, n_line_points=10, cms_output_stride=4, pafs_output_stride=8, max_edge_length_ratio=0.25, dist_penalty_weight=1.0, input_scale=1.0)</code>","text":"<p>Initialize bottom-up ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Bottom-up model producing confidence maps and PAFs.</p> required <code>skeleton_edges</code> <code>list</code> <p>List of (src, dst) edge tuples defining skeleton.</p> required <code>n_nodes</code> <code>int</code> <p>Number of nodes in the skeleton.</p> required <code>max_peaks_per_node</code> <code>int</code> <p>Maximum peaks to detect per node type.</p> <code>20</code> <code>n_line_points</code> <code>int</code> <p>Points to sample along PAF edges.</p> <code>10</code> <code>cms_output_stride</code> <code>int</code> <p>Confidence map output stride.</p> <code>4</code> <code>pafs_output_stride</code> <code>int</code> <p>PAF output stride.</p> <code>8</code> <code>max_edge_length_ratio</code> <code>float</code> <p>Maximum edge length as ratio of image size.</p> <code>0.25</code> <code>dist_penalty_weight</code> <code>float</code> <p>Weight for distance penalty in scoring.</p> <code>1.0</code> <code>input_scale</code> <code>float</code> <p>Input scaling factor.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/bottomup.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    skeleton_edges: list,\n    n_nodes: int,\n    max_peaks_per_node: int = 20,\n    n_line_points: int = 10,\n    cms_output_stride: int = 4,\n    pafs_output_stride: int = 8,\n    max_edge_length_ratio: float = 0.25,\n    dist_penalty_weight: float = 1.0,\n    input_scale: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize bottom-up ONNX wrapper.\n\n    Args:\n        model: Bottom-up model producing confidence maps and PAFs.\n        skeleton_edges: List of (src, dst) edge tuples defining skeleton.\n        n_nodes: Number of nodes in the skeleton.\n        max_peaks_per_node: Maximum peaks to detect per node type.\n        n_line_points: Points to sample along PAF edges.\n        cms_output_stride: Confidence map output stride.\n        pafs_output_stride: PAF output stride.\n        max_edge_length_ratio: Maximum edge length as ratio of image size.\n        dist_penalty_weight: Weight for distance penalty in scoring.\n        input_scale: Input scaling factor.\n    \"\"\"\n    super().__init__(model)\n    self.n_nodes = n_nodes\n    self.n_edges = len(skeleton_edges)\n    self.max_peaks_per_node = max_peaks_per_node\n    self.n_line_points = n_line_points\n    self.cms_output_stride = cms_output_stride\n    self.pafs_output_stride = pafs_output_stride\n    self.max_edge_length_ratio = max_edge_length_ratio\n    self.dist_penalty_weight = dist_penalty_weight\n    self.input_scale = input_scale\n\n    edge_src = torch.tensor([e[0] for e in skeleton_edges], dtype=torch.long)\n    edge_dst = torch.tensor([e[1] for e in skeleton_edges], dtype=torch.long)\n    self.register_buffer(\"edge_src\", edge_src)\n    self.register_buffer(\"edge_dst\", edge_dst)\n\n    line_samples = torch.linspace(0, 1, n_line_points, dtype=torch.float32)\n    self.register_buffer(\"line_samples\", line_samples)\n</code></pre>"},{"location":"api/export/wrappers/bottomup/#sleap_nn.export.wrappers.bottomup.BottomUpONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run bottom-up inference and return fixed-size outputs.</p> <p>Note: confmaps and pafs are NOT returned to avoid D2H transfer bottleneck. Peak detection and PAF scoring are performed on GPU within this wrapper.</p> Source code in <code>sleap_nn/export/wrappers/bottomup.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run bottom-up inference and return fixed-size outputs.\n\n    Note: confmaps and pafs are NOT returned to avoid D2H transfer bottleneck.\n    Peak detection and PAF scoring are performed on GPU within this wrapper.\n    \"\"\"\n    image = self._normalize_uint8(image)\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    batch_size, _, height, width = image.shape\n\n    out = self.model(image)\n    if isinstance(out, dict):\n        confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n        pafs = self._extract_tensor(out, [\"paf\", \"affinity\"])\n    else:\n        confmaps, pafs = out[:2]\n\n    peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n        confmaps, self.max_peaks_per_node\n    )\n\n    peaks = peaks * self.cms_output_stride\n\n    # Compute max_edge_length to match PyTorch implementation:\n    # max_edge_length = ratio * max(paf_dims) * pafs_stride\n    # PAFs shape is (batch, 2*edges, H, W)\n    _, n_paf_channels, paf_height, paf_width = pafs.shape\n    max_paf_dim = max(n_paf_channels, paf_height, paf_width)\n    max_edge_length = torch.tensor(\n        self.max_edge_length_ratio * max_paf_dim * self.pafs_output_stride,\n        dtype=peaks.dtype,\n        device=peaks.device,\n    )\n\n    line_scores, candidate_mask = self._score_all_candidates(\n        pafs, peaks, peak_mask, max_edge_length\n    )\n\n    # Only return final outputs needed for CPU-side grouping.\n    # Do NOT return confmaps/pafs - they are large (~29 MB/batch) and\n    # cause D2H transfer bottleneck. Peak detection and PAF scoring\n    # are already done on GPU above.\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"peak_mask\": peak_mask,\n        \"line_scores\": line_scores,\n        \"candidate_mask\": candidate_mask,\n    }\n</code></pre>"},{"location":"api/export/wrappers/bottomup_multiclass/","title":"bottomup_multiclass","text":""},{"location":"api/export/wrappers/bottomup_multiclass/#sleap_nn.export.wrappers.bottomup_multiclass","title":"<code>sleap_nn.export.wrappers.bottomup_multiclass</code>","text":"<p>ONNX wrapper for bottom-up multiclass (supervised ID) models.</p> <p>Classes:</p> Name Description <code>BottomUpMultiClassONNXWrapper</code> <p>ONNX-exportable wrapper for bottom-up multiclass (supervised ID) models.</p>"},{"location":"api/export/wrappers/bottomup_multiclass/#sleap_nn.export.wrappers.bottomup_multiclass.BottomUpMultiClassONNXWrapper","title":"<code>BottomUpMultiClassONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for bottom-up multiclass (supervised ID) models.</p> <p>This wrapper handles models that output both confidence maps for keypoint detection and class maps for identity classification. Unlike PAF-based bottom-up models, multiclass models use class maps to assign identity to each detected peak, then group peaks by identity.</p> <p>The wrapper performs: 1. Peak detection in confidence maps (GPU) 2. Class probability sampling at peak locations (GPU) 3. Returns fixed-size tensors for CPU-side grouping</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The underlying PyTorch model.</p> <code>n_nodes</code> <p>Number of keypoint nodes in the skeleton.</p> <code>n_classes</code> <p>Number of identity classes.</p> <code>max_peaks_per_node</code> <p>Maximum number of peaks to detect per node.</p> <code>cms_output_stride</code> <p>Output stride of the confidence map head.</p> <code>class_maps_output_stride</code> <p>Output stride of the class maps head.</p> <code>input_scale</code> <p>Scale factor applied to input images before inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the wrapper.</p> <code>forward</code> <p>Run bottom-up multiclass inference.</p> Source code in <code>sleap_nn/export/wrappers/bottomup_multiclass.py</code> <pre><code>class BottomUpMultiClassONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for bottom-up multiclass (supervised ID) models.\n\n    This wrapper handles models that output both confidence maps for keypoint\n    detection and class maps for identity classification. Unlike PAF-based\n    bottom-up models, multiclass models use class maps to assign identity to\n    each detected peak, then group peaks by identity.\n\n    The wrapper performs:\n    1. Peak detection in confidence maps (GPU)\n    2. Class probability sampling at peak locations (GPU)\n    3. Returns fixed-size tensors for CPU-side grouping\n\n    Expects input images as uint8 tensors in [0, 255].\n\n    Attributes:\n        model: The underlying PyTorch model.\n        n_nodes: Number of keypoint nodes in the skeleton.\n        n_classes: Number of identity classes.\n        max_peaks_per_node: Maximum number of peaks to detect per node.\n        cms_output_stride: Output stride of the confidence map head.\n        class_maps_output_stride: Output stride of the class maps head.\n        input_scale: Scale factor applied to input images before inference.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        n_nodes: int,\n        n_classes: int = 2,\n        max_peaks_per_node: int = 20,\n        cms_output_stride: int = 4,\n        class_maps_output_stride: int = 8,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize the wrapper.\n\n        Args:\n            model: The underlying PyTorch model.\n            n_nodes: Number of keypoint nodes.\n            n_classes: Number of identity classes (e.g., 2 for male/female).\n            max_peaks_per_node: Maximum peaks per node to detect.\n            cms_output_stride: Output stride of confidence maps.\n            class_maps_output_stride: Output stride of class maps.\n            input_scale: Scale factor for input images.\n        \"\"\"\n        super().__init__(model)\n        self.n_nodes = n_nodes\n        self.n_classes = n_classes\n        self.max_peaks_per_node = max_peaks_per_node\n        self.cms_output_stride = cms_output_stride\n        self.class_maps_output_stride = class_maps_output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run bottom-up multiclass inference.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                   Expected to be uint8 in [0, 255].\n\n        Returns:\n            Dictionary with keys:\n                - \"peaks\": Detected peak coordinates (batch, n_nodes, max_peaks, 2).\n                    Coordinates are in input image space (x, y).\n                - \"peak_vals\": Peak confidence values (batch, n_nodes, max_peaks).\n                - \"peak_mask\": Boolean mask for valid peaks (batch, n_nodes, max_peaks).\n                - \"class_probs\": Class probabilities at each peak location\n                    (batch, n_nodes, max_peaks, n_classes).\n\n            Postprocessing on CPU uses `classify_peaks_from_maps()` to group\n            peaks by identity using Hungarian matching.\n        \"\"\"\n        # Normalize uint8 [0, 255] to float32 [0, 1]\n        image = self._normalize_uint8(image)\n\n        # Apply input scaling if needed\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        batch_size = image.shape[0]\n\n        # Forward pass\n        out = self.model(image)\n\n        # Extract outputs\n        # Note: Use \"classmaps\" as a single hint to avoid \"map\" matching \"confmaps\"\n        confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n        class_maps = self._extract_tensor(out, [\"classmaps\", \"classmapshead\"])\n\n        # Find top-k peaks per node\n        peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n            confmaps, self.max_peaks_per_node\n        )\n\n        # Scale peaks to input image space\n        peaks = peaks * self.cms_output_stride\n\n        # Sample class maps at peak locations\n        class_probs = self._sample_class_maps_at_peaks(class_maps, peaks, peak_mask)\n\n        # Scale peaks for output (accounting for input scale)\n        if self.input_scale != 1.0:\n            peaks = peaks / self.input_scale\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"peak_mask\": peak_mask,\n            \"class_probs\": class_probs,\n        }\n\n    def _sample_class_maps_at_peaks(\n        self,\n        class_maps: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_mask: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Sample class map values at peak locations.\n\n        Args:\n            class_maps: Class maps of shape (batch, n_classes, height, width).\n            peaks: Peak coordinates in cms_output_stride space,\n                shape (batch, n_nodes, max_peaks, 2) in (x, y) order.\n            peak_mask: Boolean mask for valid peaks (batch, n_nodes, max_peaks).\n\n        Returns:\n            Class probabilities at each peak location,\n            shape (batch, n_nodes, max_peaks, n_classes).\n        \"\"\"\n        batch_size, n_classes, cm_height, cm_width = class_maps.shape\n        _, n_nodes, max_peaks, _ = peaks.shape\n        device = peaks.device\n\n        # Initialize output tensor\n        class_probs = torch.zeros(\n            (batch_size, n_nodes, max_peaks, n_classes),\n            device=device,\n            dtype=class_maps.dtype,\n        )\n\n        # Convert peak coordinates to class map space\n        # peaks are in full image space (after cms_output_stride scaling)\n        peaks_cm = peaks / self.class_maps_output_stride\n\n        # Clamp coordinates to valid range\n        peaks_cm_x = peaks_cm[..., 0].clamp(0, cm_width - 1)\n        peaks_cm_y = peaks_cm[..., 1].clamp(0, cm_height - 1)\n\n        # Use grid_sample for bilinear interpolation\n        # Normalize coordinates to [-1, 1] for grid_sample\n        grid_x = (peaks_cm_x / (cm_width - 1)) * 2 - 1\n        grid_y = (peaks_cm_y / (cm_height - 1)) * 2 - 1\n\n        # Reshape for grid_sample: (batch, n_nodes * max_peaks, 1, 2)\n        grid = torch.stack([grid_x, grid_y], dim=-1)\n        grid_flat = grid.reshape(batch_size, n_nodes * max_peaks, 1, 2)\n\n        # Sample class maps: (batch, n_classes, n_nodes * max_peaks, 1)\n        sampled = F.grid_sample(\n            class_maps,\n            grid_flat,\n            mode=\"bilinear\",\n            padding_mode=\"zeros\",\n            align_corners=True,\n        )\n\n        # Reshape to (batch, n_nodes, max_peaks, n_classes)\n        sampled = sampled.squeeze(-1)  # (batch, n_classes, n_nodes * max_peaks)\n        sampled = sampled.permute(0, 2, 1)  # (batch, n_nodes * max_peaks, n_classes)\n        sampled = sampled.reshape(batch_size, n_nodes, max_peaks, n_classes)\n\n        # Apply softmax to get probabilities (optional - depends on training)\n        # For now, return raw values as the grouping function expects logits\n        class_probs = sampled\n\n        # Mask invalid peaks\n        class_probs = class_probs * peak_mask.unsqueeze(-1).float()\n\n        return class_probs\n</code></pre>"},{"location":"api/export/wrappers/bottomup_multiclass/#sleap_nn.export.wrappers.bottomup_multiclass.BottomUpMultiClassONNXWrapper.__init__","title":"<code>__init__(model, n_nodes, n_classes=2, max_peaks_per_node=20, cms_output_stride=4, class_maps_output_stride=8, input_scale=1.0)</code>","text":"<p>Initialize the wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The underlying PyTorch model.</p> required <code>n_nodes</code> <code>int</code> <p>Number of keypoint nodes.</p> required <code>n_classes</code> <code>int</code> <p>Number of identity classes (e.g., 2 for male/female).</p> <code>2</code> <code>max_peaks_per_node</code> <code>int</code> <p>Maximum peaks per node to detect.</p> <code>20</code> <code>cms_output_stride</code> <code>int</code> <p>Output stride of confidence maps.</p> <code>4</code> <code>class_maps_output_stride</code> <code>int</code> <p>Output stride of class maps.</p> <code>8</code> <code>input_scale</code> <code>float</code> <p>Scale factor for input images.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/bottomup_multiclass.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    n_nodes: int,\n    n_classes: int = 2,\n    max_peaks_per_node: int = 20,\n    cms_output_stride: int = 4,\n    class_maps_output_stride: int = 8,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize the wrapper.\n\n    Args:\n        model: The underlying PyTorch model.\n        n_nodes: Number of keypoint nodes.\n        n_classes: Number of identity classes (e.g., 2 for male/female).\n        max_peaks_per_node: Maximum peaks per node to detect.\n        cms_output_stride: Output stride of confidence maps.\n        class_maps_output_stride: Output stride of class maps.\n        input_scale: Scale factor for input images.\n    \"\"\"\n    super().__init__(model)\n    self.n_nodes = n_nodes\n    self.n_classes = n_classes\n    self.max_peaks_per_node = max_peaks_per_node\n    self.cms_output_stride = cms_output_stride\n    self.class_maps_output_stride = class_maps_output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/bottomup_multiclass/#sleap_nn.export.wrappers.bottomup_multiclass.BottomUpMultiClassONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run bottom-up multiclass inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width).    Expected to be uint8 in [0, 255].</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with keys:     - \"peaks\": Detected peak coordinates (batch, n_nodes, max_peaks, 2).         Coordinates are in input image space (x, y).     - \"peak_vals\": Peak confidence values (batch, n_nodes, max_peaks).     - \"peak_mask\": Boolean mask for valid peaks (batch, n_nodes, max_peaks).     - \"class_probs\": Class probabilities at each peak location         (batch, n_nodes, max_peaks, n_classes).</p> <p>Postprocessing on CPU uses <code>classify_peaks_from_maps()</code> to group peaks by identity using Hungarian matching.</p> Source code in <code>sleap_nn/export/wrappers/bottomup_multiclass.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run bottom-up multiclass inference.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n               Expected to be uint8 in [0, 255].\n\n    Returns:\n        Dictionary with keys:\n            - \"peaks\": Detected peak coordinates (batch, n_nodes, max_peaks, 2).\n                Coordinates are in input image space (x, y).\n            - \"peak_vals\": Peak confidence values (batch, n_nodes, max_peaks).\n            - \"peak_mask\": Boolean mask for valid peaks (batch, n_nodes, max_peaks).\n            - \"class_probs\": Class probabilities at each peak location\n                (batch, n_nodes, max_peaks, n_classes).\n\n        Postprocessing on CPU uses `classify_peaks_from_maps()` to group\n        peaks by identity using Hungarian matching.\n    \"\"\"\n    # Normalize uint8 [0, 255] to float32 [0, 1]\n    image = self._normalize_uint8(image)\n\n    # Apply input scaling if needed\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    batch_size = image.shape[0]\n\n    # Forward pass\n    out = self.model(image)\n\n    # Extract outputs\n    # Note: Use \"classmaps\" as a single hint to avoid \"map\" matching \"confmaps\"\n    confmaps = self._extract_tensor(out, [\"confmap\", \"multiinstance\"])\n    class_maps = self._extract_tensor(out, [\"classmaps\", \"classmapshead\"])\n\n    # Find top-k peaks per node\n    peaks, peak_vals, peak_mask = self._find_topk_peaks_per_node(\n        confmaps, self.max_peaks_per_node\n    )\n\n    # Scale peaks to input image space\n    peaks = peaks * self.cms_output_stride\n\n    # Sample class maps at peak locations\n    class_probs = self._sample_class_maps_at_peaks(class_maps, peaks, peak_mask)\n\n    # Scale peaks for output (accounting for input scale)\n    if self.input_scale != 1.0:\n        peaks = peaks / self.input_scale\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"peak_mask\": peak_mask,\n        \"class_probs\": class_probs,\n    }\n</code></pre>"},{"location":"api/export/wrappers/centered_instance/","title":"centered_instance","text":""},{"location":"api/export/wrappers/centered_instance/#sleap_nn.export.wrappers.centered_instance","title":"<code>sleap_nn.export.wrappers.centered_instance</code>","text":"<p>Centered-instance ONNX wrapper.</p> <p>Classes:</p> Name Description <code>CenteredInstanceONNXWrapper</code> <p>ONNX-exportable wrapper for centered-instance models.</p>"},{"location":"api/export/wrappers/centered_instance/#sleap_nn.export.wrappers.centered_instance.CenteredInstanceONNXWrapper","title":"<code>CenteredInstanceONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for centered-instance models.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize centered instance ONNX wrapper.</p> <code>forward</code> <p>Run centered-instance inference on crops.</p> Source code in <code>sleap_nn/export/wrappers/centered_instance.py</code> <pre><code>class CenteredInstanceONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for centered-instance models.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        output_stride: int = 4,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize centered instance ONNX wrapper.\n\n        Args:\n            model: Centered instance model for pose estimation.\n            output_stride: Output stride for confidence maps.\n            input_scale: Input scaling factor.\n        \"\"\"\n        super().__init__(model)\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run centered-instance inference on crops.\"\"\"\n        image = self._normalize_uint8(image)\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        confmaps = self._extract_tensor(\n            self.model(image), [\"centered\", \"instance\", \"confmap\"]\n        )\n        peaks, values = self._find_global_peaks(confmaps)\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": values,\n        }\n</code></pre>"},{"location":"api/export/wrappers/centered_instance/#sleap_nn.export.wrappers.centered_instance.CenteredInstanceONNXWrapper.__init__","title":"<code>__init__(model, output_stride=4, input_scale=1.0)</code>","text":"<p>Initialize centered instance ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Centered instance model for pose estimation.</p> required <code>output_stride</code> <code>int</code> <p>Output stride for confidence maps.</p> <code>4</code> <code>input_scale</code> <code>float</code> <p>Input scaling factor.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/centered_instance.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    output_stride: int = 4,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize centered instance ONNX wrapper.\n\n    Args:\n        model: Centered instance model for pose estimation.\n        output_stride: Output stride for confidence maps.\n        input_scale: Input scaling factor.\n    \"\"\"\n    super().__init__(model)\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/centered_instance/#sleap_nn.export.wrappers.centered_instance.CenteredInstanceONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run centered-instance inference on crops.</p> Source code in <code>sleap_nn/export/wrappers/centered_instance.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run centered-instance inference on crops.\"\"\"\n    image = self._normalize_uint8(image)\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    confmaps = self._extract_tensor(\n        self.model(image), [\"centered\", \"instance\", \"confmap\"]\n    )\n    peaks, values = self._find_global_peaks(confmaps)\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": values,\n    }\n</code></pre>"},{"location":"api/export/wrappers/centroid/","title":"centroid","text":""},{"location":"api/export/wrappers/centroid/#sleap_nn.export.wrappers.centroid","title":"<code>sleap_nn.export.wrappers.centroid</code>","text":"<p>Centroid ONNX wrapper.</p> <p>Classes:</p> Name Description <code>CentroidONNXWrapper</code> <p>ONNX-exportable wrapper for centroid models.</p>"},{"location":"api/export/wrappers/centroid/#sleap_nn.export.wrappers.centroid.CentroidONNXWrapper","title":"<code>CentroidONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for centroid models.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize centroid ONNX wrapper.</p> <code>forward</code> <p>Run centroid inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/centroid.py</code> <pre><code>class CentroidONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for centroid models.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        max_instances: int = 20,\n        output_stride: int = 2,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize centroid ONNX wrapper.\n\n        Args:\n            model: Centroid detection model.\n            max_instances: Maximum number of instances to detect.\n            output_stride: Output stride for confidence maps.\n            input_scale: Input scaling factor.\n        \"\"\"\n        super().__init__(model)\n        self.max_instances = max_instances\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run centroid inference and return fixed-size outputs.\"\"\"\n        image = self._normalize_uint8(image)\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        confmaps = self._extract_tensor(self.model(image), [\"centroid\", \"confmap\"])\n        peaks, values, valid = self._find_topk_peaks(confmaps, self.max_instances)\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"centroids\": peaks,\n            \"centroid_vals\": values,\n            \"instance_valid\": valid,\n        }\n</code></pre>"},{"location":"api/export/wrappers/centroid/#sleap_nn.export.wrappers.centroid.CentroidONNXWrapper.__init__","title":"<code>__init__(model, max_instances=20, output_stride=2, input_scale=1.0)</code>","text":"<p>Initialize centroid ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Centroid detection model.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances to detect.</p> <code>20</code> <code>output_stride</code> <code>int</code> <p>Output stride for confidence maps.</p> <code>2</code> <code>input_scale</code> <code>float</code> <p>Input scaling factor.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/centroid.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    max_instances: int = 20,\n    output_stride: int = 2,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize centroid ONNX wrapper.\n\n    Args:\n        model: Centroid detection model.\n        max_instances: Maximum number of instances to detect.\n        output_stride: Output stride for confidence maps.\n        input_scale: Input scaling factor.\n    \"\"\"\n    super().__init__(model)\n    self.max_instances = max_instances\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/centroid/#sleap_nn.export.wrappers.centroid.CentroidONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run centroid inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/centroid.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run centroid inference and return fixed-size outputs.\"\"\"\n    image = self._normalize_uint8(image)\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    confmaps = self._extract_tensor(self.model(image), [\"centroid\", \"confmap\"])\n    peaks, values, valid = self._find_topk_peaks(confmaps, self.max_instances)\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"centroids\": peaks,\n        \"centroid_vals\": values,\n        \"instance_valid\": valid,\n    }\n</code></pre>"},{"location":"api/export/wrappers/single_instance/","title":"single_instance","text":""},{"location":"api/export/wrappers/single_instance/#sleap_nn.export.wrappers.single_instance","title":"<code>sleap_nn.export.wrappers.single_instance</code>","text":"<p>Single-instance ONNX wrapper.</p> <p>Classes:</p> Name Description <code>SingleInstanceONNXWrapper</code> <p>ONNX-exportable wrapper for single-instance models.</p>"},{"location":"api/export/wrappers/single_instance/#sleap_nn.export.wrappers.single_instance.SingleInstanceONNXWrapper","title":"<code>SingleInstanceONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for single-instance models.</p> <p>This wrapper handles full-frame inference assuming a single instance per frame. For each body part (channel), it finds the global maximum in the confidence map.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The trained backbone model that outputs confidence maps.</p> <code>output_stride</code> <p>Output stride of the model (e.g., 4 means confmaps are \u00bc the input resolution).</p> <code>input_scale</code> <p>Factor to scale input images before inference.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the single-instance wrapper.</p> <code>forward</code> <p>Run single-instance inference.</p> Source code in <code>sleap_nn/export/wrappers/single_instance.py</code> <pre><code>class SingleInstanceONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for single-instance models.\n\n    This wrapper handles full-frame inference assuming a single instance per frame.\n    For each body part (channel), it finds the global maximum in the confidence map.\n\n    Expects input images as uint8 tensors in [0, 255].\n\n    Attributes:\n        model: The trained backbone model that outputs confidence maps.\n        output_stride: Output stride of the model (e.g., 4 means confmaps are 1/4 the\n            input resolution).\n        input_scale: Factor to scale input images before inference.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        output_stride: int = 4,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialize the single-instance wrapper.\n\n        Args:\n            model: The trained backbone model.\n            output_stride: Output stride of the model. Default: 4.\n            input_scale: Factor to scale input images. Default: 1.0.\n        \"\"\"\n        super().__init__(model)\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run single-instance inference.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                Expected as uint8 [0, 255] values.\n\n        Returns:\n            Dictionary with:\n                peaks: Peak coordinates of shape (batch, n_nodes, 2) in (x, y) format.\n                peak_vals: Peak confidence values of shape (batch, n_nodes).\n        \"\"\"\n        # Normalize uint8 [0, 255] to float32 [0, 1]\n        image = self._normalize_uint8(image)\n\n        # Apply input scaling if needed\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        # Run model to get confidence maps: (batch, n_nodes, height, width)\n        confmaps = self._extract_tensor(\n            self.model(image), [\"single\", \"instance\", \"confmap\"]\n        )\n\n        # Find global peak for each channel: (batch, n_nodes, 2), (batch, n_nodes)\n        peaks, values = self._find_global_peaks(confmaps)\n\n        # Scale peaks from confmap coordinates to image coordinates\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": values,\n        }\n</code></pre>"},{"location":"api/export/wrappers/single_instance/#sleap_nn.export.wrappers.single_instance.SingleInstanceONNXWrapper.__init__","title":"<code>__init__(model, output_stride=4, input_scale=1.0)</code>","text":"<p>Initialize the single-instance wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The trained backbone model.</p> required <code>output_stride</code> <code>int</code> <p>Output stride of the model. Default: 4.</p> <code>4</code> <code>input_scale</code> <code>float</code> <p>Factor to scale input images. Default: 1.0.</p> <code>1.0</code> Source code in <code>sleap_nn/export/wrappers/single_instance.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    output_stride: int = 4,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialize the single-instance wrapper.\n\n    Args:\n        model: The trained backbone model.\n        output_stride: Output stride of the model. Default: 4.\n        input_scale: Factor to scale input images. Default: 1.0.\n    \"\"\"\n    super().__init__(model)\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/export/wrappers/single_instance/#sleap_nn.export.wrappers.single_instance.SingleInstanceONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run single-instance inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width). Expected as uint8 [0, 255] values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with:     peaks: Peak coordinates of shape (batch, n_nodes, 2) in (x, y) format.     peak_vals: Peak confidence values of shape (batch, n_nodes).</p> Source code in <code>sleap_nn/export/wrappers/single_instance.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run single-instance inference.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n            Expected as uint8 [0, 255] values.\n\n    Returns:\n        Dictionary with:\n            peaks: Peak coordinates of shape (batch, n_nodes, 2) in (x, y) format.\n            peak_vals: Peak confidence values of shape (batch, n_nodes).\n    \"\"\"\n    # Normalize uint8 [0, 255] to float32 [0, 1]\n    image = self._normalize_uint8(image)\n\n    # Apply input scaling if needed\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    # Run model to get confidence maps: (batch, n_nodes, height, width)\n    confmaps = self._extract_tensor(\n        self.model(image), [\"single\", \"instance\", \"confmap\"]\n    )\n\n    # Find global peak for each channel: (batch, n_nodes, 2), (batch, n_nodes)\n    peaks, values = self._find_global_peaks(confmaps)\n\n    # Scale peaks from confmap coordinates to image coordinates\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": values,\n    }\n</code></pre>"},{"location":"api/export/wrappers/topdown/","title":"topdown","text":""},{"location":"api/export/wrappers/topdown/#sleap_nn.export.wrappers.topdown","title":"<code>sleap_nn.export.wrappers.topdown</code>","text":"<p>Top-down ONNX wrapper.</p> <p>Classes:</p> Name Description <code>TopDownONNXWrapper</code> <p>ONNX-exportable wrapper for top-down (centroid + centered-instance) inference.</p>"},{"location":"api/export/wrappers/topdown/#sleap_nn.export.wrappers.topdown.TopDownONNXWrapper","title":"<code>TopDownONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for top-down (centroid + centered-instance) inference.</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize top-down ONNX wrapper.</p> <code>forward</code> <p>Run top-down inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/topdown.py</code> <pre><code>class TopDownONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for top-down (centroid + centered-instance) inference.\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        centroid_model: nn.Module,\n        instance_model: nn.Module,\n        max_instances: int = 20,\n        crop_size: Tuple[int, int] = (192, 192),\n        centroid_output_stride: int = 2,\n        instance_output_stride: int = 4,\n        centroid_input_scale: float = 1.0,\n        instance_input_scale: float = 1.0,\n        n_nodes: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize top-down ONNX wrapper.\n\n        Args:\n            centroid_model: Centroid detection model.\n            instance_model: Instance pose estimation model.\n            max_instances: Maximum number of instances to detect.\n            crop_size: Size of instance crops (height, width).\n            centroid_output_stride: Centroid model output stride.\n            instance_output_stride: Instance model output stride.\n            centroid_input_scale: Centroid input scaling factor.\n            instance_input_scale: Instance input scaling factor.\n            n_nodes: Number of skeleton nodes.\n        \"\"\"\n        super().__init__(centroid_model)\n        self.centroid_model = centroid_model\n        self.instance_model = instance_model\n        self.max_instances = max_instances\n        self.crop_size = crop_size\n        self.centroid_output_stride = centroid_output_stride\n        self.instance_output_stride = instance_output_stride\n        self.centroid_input_scale = centroid_input_scale\n        self.instance_input_scale = instance_input_scale\n        self.n_nodes = n_nodes\n\n        crop_h, crop_w = crop_size\n        y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n        x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n        grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n        base_grid = torch.stack([grid_x, grid_y], dim=-1)\n        self.register_buffer(\"base_grid\", base_grid, persistent=False)\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run top-down inference and return fixed-size outputs.\"\"\"\n        image = self._normalize_uint8(image)\n        batch_size, channels, height, width = image.shape\n\n        scaled_image = image\n        if self.centroid_input_scale != 1.0:\n            scaled_h = int(height * self.centroid_input_scale)\n            scaled_w = int(width * self.centroid_input_scale)\n            scaled_image = F.interpolate(\n                scaled_image,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        centroid_out = self.centroid_model(scaled_image)\n        centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n\n        centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n            centroid_cms, self.max_instances\n        )\n        centroids = centroids * (\n            self.centroid_output_stride / self.centroid_input_scale\n        )\n\n        crops = self._extract_crops(image, centroids)\n        crops_flat = crops.reshape(\n            batch_size * self.max_instances,\n            channels,\n            self.crop_size[0],\n            self.crop_size[1],\n        )\n\n        if self.instance_input_scale != 1.0:\n            scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n            scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n            crops_flat = F.interpolate(\n                crops_flat,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        instance_out = self.instance_model(crops_flat)\n        instance_cms = self._extract_tensor(\n            instance_out, [\"centered\", \"instance\", \"confmap\"]\n        )\n\n        crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n        crop_peaks = crop_peaks * (\n            self.instance_output_stride / self.instance_input_scale\n        )\n\n        crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n        peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n        crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n            [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n        )\n        peaks = crop_peaks + crop_offset\n\n        invalid_mask = ~instance_valid\n        centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n        centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n        peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n        peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n        return {\n            \"centroids\": centroids,\n            \"centroid_vals\": centroid_vals,\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"instance_valid\": instance_valid,\n        }\n\n    def _extract_crops(\n        self,\n        image: torch.Tensor,\n        centroids: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extract crops around centroids using grid_sample.\"\"\"\n        batch_size, channels, height, width = image.shape\n        crop_h, crop_w = self.crop_size\n        n_instances = centroids.shape[1]\n\n        scale_x = crop_w / width\n        scale_y = crop_h / height\n        scale = image.new_tensor([scale_x, scale_y])\n        base_grid = self.base_grid.to(device=image.device, dtype=image.dtype)\n        scaled_grid = base_grid * scale\n\n        scaled_grid = scaled_grid.unsqueeze(0).unsqueeze(0)\n        scaled_grid = scaled_grid.expand(batch_size, n_instances, -1, -1, -1)\n\n        norm_centroids = torch.zeros_like(centroids)\n        norm_centroids[..., 0] = (centroids[..., 0] / (width - 1)) * 2 - 1\n        norm_centroids[..., 1] = (centroids[..., 1] / (height - 1)) * 2 - 1\n        offset = norm_centroids.unsqueeze(2).unsqueeze(2)\n\n        sample_grid = scaled_grid + offset\n\n        image_expanded = image.unsqueeze(1).expand(-1, n_instances, -1, -1, -1)\n        image_flat = image_expanded.reshape(\n            batch_size * n_instances, channels, height, width\n        )\n        grid_flat = sample_grid.reshape(batch_size * n_instances, crop_h, crop_w, 2)\n\n        crops_flat = F.grid_sample(\n            image_flat,\n            grid_flat,\n            mode=\"bilinear\",\n            padding_mode=\"zeros\",\n            align_corners=True,\n        )\n\n        crops = crops_flat.reshape(batch_size, n_instances, channels, crop_h, crop_w)\n        return crops\n</code></pre>"},{"location":"api/export/wrappers/topdown/#sleap_nn.export.wrappers.topdown.TopDownONNXWrapper.__init__","title":"<code>__init__(centroid_model, instance_model, max_instances=20, crop_size=(192, 192), centroid_output_stride=2, instance_output_stride=4, centroid_input_scale=1.0, instance_input_scale=1.0, n_nodes=1)</code>","text":"<p>Initialize top-down ONNX wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_model</code> <code>Module</code> <p>Centroid detection model.</p> required <code>instance_model</code> <code>Module</code> <p>Instance pose estimation model.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances to detect.</p> <code>20</code> <code>crop_size</code> <code>Tuple[int, int]</code> <p>Size of instance crops (height, width).</p> <code>(192, 192)</code> <code>centroid_output_stride</code> <code>int</code> <p>Centroid model output stride.</p> <code>2</code> <code>instance_output_stride</code> <code>int</code> <p>Instance model output stride.</p> <code>4</code> <code>centroid_input_scale</code> <code>float</code> <p>Centroid input scaling factor.</p> <code>1.0</code> <code>instance_input_scale</code> <code>float</code> <p>Instance input scaling factor.</p> <code>1.0</code> <code>n_nodes</code> <code>int</code> <p>Number of skeleton nodes.</p> <code>1</code> Source code in <code>sleap_nn/export/wrappers/topdown.py</code> <pre><code>def __init__(\n    self,\n    centroid_model: nn.Module,\n    instance_model: nn.Module,\n    max_instances: int = 20,\n    crop_size: Tuple[int, int] = (192, 192),\n    centroid_output_stride: int = 2,\n    instance_output_stride: int = 4,\n    centroid_input_scale: float = 1.0,\n    instance_input_scale: float = 1.0,\n    n_nodes: int = 1,\n) -&gt; None:\n    \"\"\"Initialize top-down ONNX wrapper.\n\n    Args:\n        centroid_model: Centroid detection model.\n        instance_model: Instance pose estimation model.\n        max_instances: Maximum number of instances to detect.\n        crop_size: Size of instance crops (height, width).\n        centroid_output_stride: Centroid model output stride.\n        instance_output_stride: Instance model output stride.\n        centroid_input_scale: Centroid input scaling factor.\n        instance_input_scale: Instance input scaling factor.\n        n_nodes: Number of skeleton nodes.\n    \"\"\"\n    super().__init__(centroid_model)\n    self.centroid_model = centroid_model\n    self.instance_model = instance_model\n    self.max_instances = max_instances\n    self.crop_size = crop_size\n    self.centroid_output_stride = centroid_output_stride\n    self.instance_output_stride = instance_output_stride\n    self.centroid_input_scale = centroid_input_scale\n    self.instance_input_scale = instance_input_scale\n    self.n_nodes = n_nodes\n\n    crop_h, crop_w = crop_size\n    y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n    x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n    grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n    base_grid = torch.stack([grid_x, grid_y], dim=-1)\n    self.register_buffer(\"base_grid\", base_grid, persistent=False)\n</code></pre>"},{"location":"api/export/wrappers/topdown/#sleap_nn.export.wrappers.topdown.TopDownONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run top-down inference and return fixed-size outputs.</p> Source code in <code>sleap_nn/export/wrappers/topdown.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run top-down inference and return fixed-size outputs.\"\"\"\n    image = self._normalize_uint8(image)\n    batch_size, channels, height, width = image.shape\n\n    scaled_image = image\n    if self.centroid_input_scale != 1.0:\n        scaled_h = int(height * self.centroid_input_scale)\n        scaled_w = int(width * self.centroid_input_scale)\n        scaled_image = F.interpolate(\n            scaled_image,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    centroid_out = self.centroid_model(scaled_image)\n    centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n\n    centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n        centroid_cms, self.max_instances\n    )\n    centroids = centroids * (\n        self.centroid_output_stride / self.centroid_input_scale\n    )\n\n    crops = self._extract_crops(image, centroids)\n    crops_flat = crops.reshape(\n        batch_size * self.max_instances,\n        channels,\n        self.crop_size[0],\n        self.crop_size[1],\n    )\n\n    if self.instance_input_scale != 1.0:\n        scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n        scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n        crops_flat = F.interpolate(\n            crops_flat,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    instance_out = self.instance_model(crops_flat)\n    instance_cms = self._extract_tensor(\n        instance_out, [\"centered\", \"instance\", \"confmap\"]\n    )\n\n    crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n    crop_peaks = crop_peaks * (\n        self.instance_output_stride / self.instance_input_scale\n    )\n\n    crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n    peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n    crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n        [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n    )\n    peaks = crop_peaks + crop_offset\n\n    invalid_mask = ~instance_valid\n    centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n    centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n    peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n    peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n    return {\n        \"centroids\": centroids,\n        \"centroid_vals\": centroid_vals,\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"instance_valid\": instance_valid,\n    }\n</code></pre>"},{"location":"api/export/wrappers/topdown_multiclass/","title":"topdown_multiclass","text":""},{"location":"api/export/wrappers/topdown_multiclass/#sleap_nn.export.wrappers.topdown_multiclass","title":"<code>sleap_nn.export.wrappers.topdown_multiclass</code>","text":"<p>ONNX wrapper for top-down multiclass (supervised ID) models.</p> <p>Classes:</p> Name Description <code>TopDownMultiClassCombinedONNXWrapper</code> <p>ONNX-exportable wrapper for combined centroid + multiclass instance models.</p> <code>TopDownMultiClassONNXWrapper</code> <p>ONNX-exportable wrapper for top-down multiclass (supervised ID) models.</p>"},{"location":"api/export/wrappers/topdown_multiclass/#sleap_nn.export.wrappers.topdown_multiclass.TopDownMultiClassCombinedONNXWrapper","title":"<code>TopDownMultiClassCombinedONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for combined centroid + multiclass instance models.</p> <p>This wrapper combines a centroid detection model with a centered instance multiclass model. It performs: 1. Centroid detection on full images 2. Cropping around each centroid using vectorized grid_sample 3. Instance keypoint detection + identity classification on each crop</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the combined wrapper.</p> <code>forward</code> <p>Run combined top-down multiclass inference.</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>class TopDownMultiClassCombinedONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for combined centroid + multiclass instance models.\n\n    This wrapper combines a centroid detection model with a centered instance\n    multiclass model. It performs:\n    1. Centroid detection on full images\n    2. Cropping around each centroid using vectorized grid_sample\n    3. Instance keypoint detection + identity classification on each crop\n\n    Expects input images as uint8 tensors in [0, 255].\n    \"\"\"\n\n    def __init__(\n        self,\n        centroid_model: nn.Module,\n        instance_model: nn.Module,\n        max_instances: int = 20,\n        crop_size: tuple = (192, 192),\n        centroid_output_stride: int = 4,\n        instance_output_stride: int = 2,\n        centroid_input_scale: float = 1.0,\n        instance_input_scale: float = 1.0,\n        n_nodes: int = 13,\n        n_classes: int = 2,\n    ):\n        \"\"\"Initialize the combined wrapper.\n\n        Args:\n            centroid_model: Model for centroid detection.\n            instance_model: Model for instance keypoints + class prediction.\n            max_instances: Maximum number of instances to detect.\n            crop_size: Size of crops around centroids (height, width).\n            centroid_output_stride: Output stride of centroid model.\n            instance_output_stride: Output stride of instance model.\n            centroid_input_scale: Input scale for centroid model.\n            instance_input_scale: Input scale for instance model.\n            n_nodes: Number of keypoint nodes per instance.\n            n_classes: Number of identity classes.\n        \"\"\"\n        super().__init__(centroid_model)  # Primary model is centroid\n        self.instance_model = instance_model\n        self.max_instances = max_instances\n        self.crop_size = crop_size\n        self.centroid_output_stride = centroid_output_stride\n        self.instance_output_stride = instance_output_stride\n        self.centroid_input_scale = centroid_input_scale\n        self.instance_input_scale = instance_input_scale\n        self.n_nodes = n_nodes\n        self.n_classes = n_classes\n\n        # Pre-compute base grid for crop extraction (same as TopDownONNXWrapper)\n        crop_h, crop_w = crop_size\n        y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n        x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n        grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n        base_grid = torch.stack([grid_x, grid_y], dim=-1)\n        self.register_buffer(\"base_grid\", base_grid, persistent=False)\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run combined top-down multiclass inference.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                   Expected to be uint8 in [0, 255].\n\n        Returns:\n            Dictionary with keys:\n                - \"centroids\": Detected centroids (batch, max_instances, 2).\n                - \"centroid_vals\": Centroid confidence values (batch, max_instances).\n                - \"peaks\": Instance peaks (batch, max_instances, n_nodes, 2).\n                - \"peak_vals\": Peak values (batch, max_instances, n_nodes).\n                - \"class_logits\": Class logits per instance (batch, max_instances, n_classes).\n                - \"instance_valid\": Validity mask (batch, max_instances).\n        \"\"\"\n        # Normalize input\n        image = self._normalize_uint8(image)\n        batch_size, channels, height, width = image.shape\n\n        # Apply centroid input scaling\n        scaled_image = image\n        if self.centroid_input_scale != 1.0:\n            scaled_h = int(height * self.centroid_input_scale)\n            scaled_w = int(width * self.centroid_input_scale)\n            scaled_image = F.interpolate(\n                scaled_image,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        # Centroid detection\n        centroid_out = self.model(scaled_image)\n        centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n        centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n            centroid_cms, self.max_instances\n        )\n        centroids = centroids * (\n            self.centroid_output_stride / self.centroid_input_scale\n        )\n\n        # Extract crops using vectorized grid_sample (same as TopDownONNXWrapper)\n        crops = self._extract_crops(image, centroids)\n        crops_flat = crops.reshape(\n            batch_size * self.max_instances,\n            channels,\n            self.crop_size[0],\n            self.crop_size[1],\n        )\n\n        # Apply instance input scaling if needed\n        if self.instance_input_scale != 1.0:\n            scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n            scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n            crops_flat = F.interpolate(\n                crops_flat,\n                size=(scaled_h, scaled_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n\n        # Instance model forward (batch all crops)\n        instance_out = self.instance_model(crops_flat)\n        instance_cms = self._extract_tensor(\n            instance_out, [\"centered\", \"instance\", \"confmap\"]\n        )\n        instance_class = self._extract_tensor(instance_out, [\"class\", \"vector\"])\n\n        # Find peaks in all crops\n        crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n        crop_peaks = crop_peaks * (\n            self.instance_output_stride / self.instance_input_scale\n        )\n\n        # Reshape to batch x instances x nodes x 2\n        crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n        peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n        # Reshape class logits\n        class_logits = instance_class.reshape(\n            batch_size, self.max_instances, self.n_classes\n        )\n\n        # Transform peaks from crop coordinates to full image coordinates\n        crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n            [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n        )\n        peaks = crop_peaks + crop_offset\n\n        # Zero out invalid instances\n        invalid_mask = ~instance_valid\n        centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n        centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n        peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n        peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n        class_logits = class_logits.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n        return {\n            \"centroids\": centroids,\n            \"centroid_vals\": centroid_vals,\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"class_logits\": class_logits,\n            \"instance_valid\": instance_valid,\n        }\n\n    def _extract_crops(\n        self,\n        image: torch.Tensor,\n        centroids: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extract crops around centroids using grid_sample.\n\n        This is the same vectorized implementation as TopDownONNXWrapper.\n        \"\"\"\n        batch_size, channels, height, width = image.shape\n        crop_h, crop_w = self.crop_size\n        n_instances = centroids.shape[1]\n\n        scale_x = crop_w / width\n        scale_y = crop_h / height\n        scale = image.new_tensor([scale_x, scale_y])\n        base_grid = self.base_grid.to(device=image.device, dtype=image.dtype)\n        scaled_grid = base_grid * scale\n\n        scaled_grid = scaled_grid.unsqueeze(0).unsqueeze(0)\n        scaled_grid = scaled_grid.expand(batch_size, n_instances, -1, -1, -1)\n\n        norm_centroids = torch.zeros_like(centroids)\n        norm_centroids[..., 0] = (centroids[..., 0] / (width - 1)) * 2 - 1\n        norm_centroids[..., 1] = (centroids[..., 1] / (height - 1)) * 2 - 1\n        offset = norm_centroids.unsqueeze(2).unsqueeze(2)\n\n        sample_grid = scaled_grid + offset\n\n        image_expanded = image.unsqueeze(1).expand(-1, n_instances, -1, -1, -1)\n        image_flat = image_expanded.reshape(\n            batch_size * n_instances, channels, height, width\n        )\n        grid_flat = sample_grid.reshape(batch_size * n_instances, crop_h, crop_w, 2)\n\n        crops_flat = F.grid_sample(\n            image_flat,\n            grid_flat,\n            mode=\"bilinear\",\n            padding_mode=\"zeros\",\n            align_corners=True,\n        )\n\n        crops = crops_flat.reshape(batch_size, n_instances, channels, crop_h, crop_w)\n        return crops\n</code></pre>"},{"location":"api/export/wrappers/topdown_multiclass/#sleap_nn.export.wrappers.topdown_multiclass.TopDownMultiClassCombinedONNXWrapper.__init__","title":"<code>__init__(centroid_model, instance_model, max_instances=20, crop_size=(192, 192), centroid_output_stride=4, instance_output_stride=2, centroid_input_scale=1.0, instance_input_scale=1.0, n_nodes=13, n_classes=2)</code>","text":"<p>Initialize the combined wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_model</code> <code>Module</code> <p>Model for centroid detection.</p> required <code>instance_model</code> <code>Module</code> <p>Model for instance keypoints + class prediction.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of instances to detect.</p> <code>20</code> <code>crop_size</code> <code>tuple</code> <p>Size of crops around centroids (height, width).</p> <code>(192, 192)</code> <code>centroid_output_stride</code> <code>int</code> <p>Output stride of centroid model.</p> <code>4</code> <code>instance_output_stride</code> <code>int</code> <p>Output stride of instance model.</p> <code>2</code> <code>centroid_input_scale</code> <code>float</code> <p>Input scale for centroid model.</p> <code>1.0</code> <code>instance_input_scale</code> <code>float</code> <p>Input scale for instance model.</p> <code>1.0</code> <code>n_nodes</code> <code>int</code> <p>Number of keypoint nodes per instance.</p> <code>13</code> <code>n_classes</code> <code>int</code> <p>Number of identity classes.</p> <code>2</code> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def __init__(\n    self,\n    centroid_model: nn.Module,\n    instance_model: nn.Module,\n    max_instances: int = 20,\n    crop_size: tuple = (192, 192),\n    centroid_output_stride: int = 4,\n    instance_output_stride: int = 2,\n    centroid_input_scale: float = 1.0,\n    instance_input_scale: float = 1.0,\n    n_nodes: int = 13,\n    n_classes: int = 2,\n):\n    \"\"\"Initialize the combined wrapper.\n\n    Args:\n        centroid_model: Model for centroid detection.\n        instance_model: Model for instance keypoints + class prediction.\n        max_instances: Maximum number of instances to detect.\n        crop_size: Size of crops around centroids (height, width).\n        centroid_output_stride: Output stride of centroid model.\n        instance_output_stride: Output stride of instance model.\n        centroid_input_scale: Input scale for centroid model.\n        instance_input_scale: Input scale for instance model.\n        n_nodes: Number of keypoint nodes per instance.\n        n_classes: Number of identity classes.\n    \"\"\"\n    super().__init__(centroid_model)  # Primary model is centroid\n    self.instance_model = instance_model\n    self.max_instances = max_instances\n    self.crop_size = crop_size\n    self.centroid_output_stride = centroid_output_stride\n    self.instance_output_stride = instance_output_stride\n    self.centroid_input_scale = centroid_input_scale\n    self.instance_input_scale = instance_input_scale\n    self.n_nodes = n_nodes\n    self.n_classes = n_classes\n\n    # Pre-compute base grid for crop extraction (same as TopDownONNXWrapper)\n    crop_h, crop_w = crop_size\n    y_crop = torch.linspace(-1, 1, crop_h, dtype=torch.float32)\n    x_crop = torch.linspace(-1, 1, crop_w, dtype=torch.float32)\n    grid_y, grid_x = torch.meshgrid(y_crop, x_crop, indexing=\"ij\")\n    base_grid = torch.stack([grid_x, grid_y], dim=-1)\n    self.register_buffer(\"base_grid\", base_grid, persistent=False)\n</code></pre>"},{"location":"api/export/wrappers/topdown_multiclass/#sleap_nn.export.wrappers.topdown_multiclass.TopDownMultiClassCombinedONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run combined top-down multiclass inference.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width).    Expected to be uint8 in [0, 255].</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with keys:     - \"centroids\": Detected centroids (batch, max_instances, 2).     - \"centroid_vals\": Centroid confidence values (batch, max_instances).     - \"peaks\": Instance peaks (batch, max_instances, n_nodes, 2).     - \"peak_vals\": Peak values (batch, max_instances, n_nodes).     - \"class_logits\": Class logits per instance (batch, max_instances, n_classes).     - \"instance_valid\": Validity mask (batch, max_instances).</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run combined top-down multiclass inference.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n               Expected to be uint8 in [0, 255].\n\n    Returns:\n        Dictionary with keys:\n            - \"centroids\": Detected centroids (batch, max_instances, 2).\n            - \"centroid_vals\": Centroid confidence values (batch, max_instances).\n            - \"peaks\": Instance peaks (batch, max_instances, n_nodes, 2).\n            - \"peak_vals\": Peak values (batch, max_instances, n_nodes).\n            - \"class_logits\": Class logits per instance (batch, max_instances, n_classes).\n            - \"instance_valid\": Validity mask (batch, max_instances).\n    \"\"\"\n    # Normalize input\n    image = self._normalize_uint8(image)\n    batch_size, channels, height, width = image.shape\n\n    # Apply centroid input scaling\n    scaled_image = image\n    if self.centroid_input_scale != 1.0:\n        scaled_h = int(height * self.centroid_input_scale)\n        scaled_w = int(width * self.centroid_input_scale)\n        scaled_image = F.interpolate(\n            scaled_image,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    # Centroid detection\n    centroid_out = self.model(scaled_image)\n    centroid_cms = self._extract_tensor(centroid_out, [\"centroid\", \"confmap\"])\n    centroids, centroid_vals, instance_valid = self._find_topk_peaks(\n        centroid_cms, self.max_instances\n    )\n    centroids = centroids * (\n        self.centroid_output_stride / self.centroid_input_scale\n    )\n\n    # Extract crops using vectorized grid_sample (same as TopDownONNXWrapper)\n    crops = self._extract_crops(image, centroids)\n    crops_flat = crops.reshape(\n        batch_size * self.max_instances,\n        channels,\n        self.crop_size[0],\n        self.crop_size[1],\n    )\n\n    # Apply instance input scaling if needed\n    if self.instance_input_scale != 1.0:\n        scaled_h = int(self.crop_size[0] * self.instance_input_scale)\n        scaled_w = int(self.crop_size[1] * self.instance_input_scale)\n        crops_flat = F.interpolate(\n            crops_flat,\n            size=(scaled_h, scaled_w),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    # Instance model forward (batch all crops)\n    instance_out = self.instance_model(crops_flat)\n    instance_cms = self._extract_tensor(\n        instance_out, [\"centered\", \"instance\", \"confmap\"]\n    )\n    instance_class = self._extract_tensor(instance_out, [\"class\", \"vector\"])\n\n    # Find peaks in all crops\n    crop_peaks, crop_peak_vals = self._find_global_peaks(instance_cms)\n    crop_peaks = crop_peaks * (\n        self.instance_output_stride / self.instance_input_scale\n    )\n\n    # Reshape to batch x instances x nodes x 2\n    crop_peaks = crop_peaks.reshape(batch_size, self.max_instances, self.n_nodes, 2)\n    peak_vals = crop_peak_vals.reshape(batch_size, self.max_instances, self.n_nodes)\n\n    # Reshape class logits\n    class_logits = instance_class.reshape(\n        batch_size, self.max_instances, self.n_classes\n    )\n\n    # Transform peaks from crop coordinates to full image coordinates\n    crop_offset = centroids.unsqueeze(2) - image.new_tensor(\n        [self.crop_size[1] / 2.0, self.crop_size[0] / 2.0]\n    )\n    peaks = crop_peaks + crop_offset\n\n    # Zero out invalid instances\n    invalid_mask = ~instance_valid\n    centroids = centroids.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n    centroid_vals = centroid_vals.masked_fill(invalid_mask, 0.0)\n    peaks = peaks.masked_fill(invalid_mask.unsqueeze(-1).unsqueeze(-1), 0.0)\n    peak_vals = peak_vals.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n    class_logits = class_logits.masked_fill(invalid_mask.unsqueeze(-1), 0.0)\n\n    return {\n        \"centroids\": centroids,\n        \"centroid_vals\": centroid_vals,\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"class_logits\": class_logits,\n        \"instance_valid\": instance_valid,\n    }\n</code></pre>"},{"location":"api/export/wrappers/topdown_multiclass/#sleap_nn.export.wrappers.topdown_multiclass.TopDownMultiClassONNXWrapper","title":"<code>TopDownMultiClassONNXWrapper</code>","text":"<p>               Bases: <code>BaseExportWrapper</code></p> <p>ONNX-exportable wrapper for top-down multiclass (supervised ID) models.</p> <p>This wrapper handles models that output both confidence maps for keypoint detection and class logits for identity classification. It runs on instance crops (centered around detected centroids).</p> <p>Expects input images as uint8 tensors in [0, 255].</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The underlying PyTorch model (centered instance + class vectors heads).</p> <code>output_stride</code> <p>Output stride of the confmap head.</p> <code>input_scale</code> <p>Scale factor applied to input images before inference.</p> <code>n_classes</code> <p>Number of identity classes.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the wrapper.</p> <code>forward</code> <p>Run top-down multiclass inference on crops.</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>class TopDownMultiClassONNXWrapper(BaseExportWrapper):\n    \"\"\"ONNX-exportable wrapper for top-down multiclass (supervised ID) models.\n\n    This wrapper handles models that output both confidence maps for keypoint\n    detection and class logits for identity classification. It runs on instance\n    crops (centered around detected centroids).\n\n    Expects input images as uint8 tensors in [0, 255].\n\n    Attributes:\n        model: The underlying PyTorch model (centered instance + class vectors heads).\n        output_stride: Output stride of the confmap head.\n        input_scale: Scale factor applied to input images before inference.\n        n_classes: Number of identity classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        output_stride: int = 2,\n        input_scale: float = 1.0,\n        n_classes: int = 2,\n    ):\n        \"\"\"Initialize the wrapper.\n\n        Args:\n            model: The underlying PyTorch model.\n            output_stride: Output stride of the confidence maps.\n            input_scale: Scale factor for input images.\n            n_classes: Number of identity classes (e.g., 2 for male/female).\n        \"\"\"\n        super().__init__(model)\n        self.output_stride = output_stride\n        self.input_scale = input_scale\n        self.n_classes = n_classes\n\n    def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Run top-down multiclass inference on crops.\n\n        Args:\n            image: Input image tensor of shape (batch, channels, height, width).\n                   Expected to be uint8 in [0, 255].\n\n        Returns:\n            Dictionary with keys:\n                - \"peaks\": Predicted peak coordinates (batch, n_nodes, 2) in (x, y).\n                - \"peak_vals\": Peak confidence values (batch, n_nodes).\n                - \"class_logits\": Raw class logits (batch, n_classes).\n\n            The class assignment is done on CPU using Hungarian matching\n            via `get_class_inds_from_vectors()`.\n        \"\"\"\n        # Normalize uint8 [0, 255] to float32 [0, 1]\n        image = self._normalize_uint8(image)\n\n        # Apply input scaling if needed\n        if self.input_scale != 1.0:\n            height = int(image.shape[-2] * self.input_scale)\n            width = int(image.shape[-1] * self.input_scale)\n            image = F.interpolate(\n                image, size=(height, width), mode=\"bilinear\", align_corners=False\n            )\n\n        # Forward pass\n        out = self.model(image)\n\n        # Extract outputs\n        confmaps = self._extract_tensor(out, [\"centered\", \"instance\", \"confmap\"])\n        class_logits = self._extract_tensor(out, [\"class\", \"vector\"])\n\n        # Find global peaks (one per node)\n        peaks, peak_vals = self._find_global_peaks(confmaps)\n\n        # Scale peaks back to input coordinates\n        peaks = peaks * (self.output_stride / self.input_scale)\n\n        return {\n            \"peaks\": peaks,\n            \"peak_vals\": peak_vals,\n            \"class_logits\": class_logits,\n        }\n</code></pre>"},{"location":"api/export/wrappers/topdown_multiclass/#sleap_nn.export.wrappers.topdown_multiclass.TopDownMultiClassONNXWrapper.__init__","title":"<code>__init__(model, output_stride=2, input_scale=1.0, n_classes=2)</code>","text":"<p>Initialize the wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The underlying PyTorch model.</p> required <code>output_stride</code> <code>int</code> <p>Output stride of the confidence maps.</p> <code>2</code> <code>input_scale</code> <code>float</code> <p>Scale factor for input images.</p> <code>1.0</code> <code>n_classes</code> <code>int</code> <p>Number of identity classes (e.g., 2 for male/female).</p> <code>2</code> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    output_stride: int = 2,\n    input_scale: float = 1.0,\n    n_classes: int = 2,\n):\n    \"\"\"Initialize the wrapper.\n\n    Args:\n        model: The underlying PyTorch model.\n        output_stride: Output stride of the confidence maps.\n        input_scale: Scale factor for input images.\n        n_classes: Number of identity classes (e.g., 2 for male/female).\n    \"\"\"\n    super().__init__(model)\n    self.output_stride = output_stride\n    self.input_scale = input_scale\n    self.n_classes = n_classes\n</code></pre>"},{"location":"api/export/wrappers/topdown_multiclass/#sleap_nn.export.wrappers.topdown_multiclass.TopDownMultiClassONNXWrapper.forward","title":"<code>forward(image)</code>","text":"<p>Run top-down multiclass inference on crops.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (batch, channels, height, width).    Expected to be uint8 in [0, 255].</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with keys:     - \"peaks\": Predicted peak coordinates (batch, n_nodes, 2) in (x, y).     - \"peak_vals\": Peak confidence values (batch, n_nodes).     - \"class_logits\": Raw class logits (batch, n_classes).</p> <p>The class assignment is done on CPU using Hungarian matching via <code>get_class_inds_from_vectors()</code>.</p> Source code in <code>sleap_nn/export/wrappers/topdown_multiclass.py</code> <pre><code>def forward(self, image: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Run top-down multiclass inference on crops.\n\n    Args:\n        image: Input image tensor of shape (batch, channels, height, width).\n               Expected to be uint8 in [0, 255].\n\n    Returns:\n        Dictionary with keys:\n            - \"peaks\": Predicted peak coordinates (batch, n_nodes, 2) in (x, y).\n            - \"peak_vals\": Peak confidence values (batch, n_nodes).\n            - \"class_logits\": Raw class logits (batch, n_classes).\n\n        The class assignment is done on CPU using Hungarian matching\n        via `get_class_inds_from_vectors()`.\n    \"\"\"\n    # Normalize uint8 [0, 255] to float32 [0, 1]\n    image = self._normalize_uint8(image)\n\n    # Apply input scaling if needed\n    if self.input_scale != 1.0:\n        height = int(image.shape[-2] * self.input_scale)\n        width = int(image.shape[-1] * self.input_scale)\n        image = F.interpolate(\n            image, size=(height, width), mode=\"bilinear\", align_corners=False\n        )\n\n    # Forward pass\n    out = self.model(image)\n\n    # Extract outputs\n    confmaps = self._extract_tensor(out, [\"centered\", \"instance\", \"confmap\"])\n    class_logits = self._extract_tensor(out, [\"class\", \"vector\"])\n\n    # Find global peaks (one per node)\n    peaks, peak_vals = self._find_global_peaks(confmaps)\n\n    # Scale peaks back to input coordinates\n    peaks = peaks * (self.output_stride / self.input_scale)\n\n    return {\n        \"peaks\": peaks,\n        \"peak_vals\": peak_vals,\n        \"class_logits\": class_logits,\n    }\n</code></pre>"},{"location":"api/inference/","title":"inference","text":""},{"location":"api/inference/#sleap_nn.inference","title":"<code>sleap_nn.inference</code>","text":"<p>Inference-related modules.</p> <p>Modules:</p> Name Description <code>bottomup</code> <p>Inference modules for BottomUp models.</p> <code>identity</code> <p>Utilities for models that learn identity.</p> <code>paf_grouping</code> <p>This module provides a set of utilities for grouping peaks based on PAFs.</p> <code>peak_finding</code> <p>Peak finding for inference.</p> <code>postprocessing</code> <p>Inference-level postprocessing filters for pose predictions.</p> <code>predictors</code> <p>Predictors for running inference.</p> <code>provenance</code> <p>Provenance metadata utilities for inference outputs.</p> <code>single_instance</code> <p>Inference modules for SingleInstance models.</p> <code>topdown</code> <p>Inference modules for TopDown centroid and centered-instance models.</p> <code>utils</code> <p>Miscellaneous utility functions for Inference modules.</p>"},{"location":"api/inference/bottomup/","title":"bottomup","text":""},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup","title":"<code>sleap_nn.inference.bottomup</code>","text":"<p>Inference modules for BottomUp models.</p> <p>Classes:</p> Name Description <code>BottomUpInferenceModel</code> <p>BottomUp Inference model.</p> <code>BottomUpMultiClassInferenceModel</code> <p>BottomUp Inference model for multi-class models.</p>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpInferenceModel","title":"<code>BottomUpInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>BottomUp Inference model.</p> <p>This model encapsulates the bottom-up approach. The images are passed to a peak detector to get the predicted instances and then fed into PAF to combine nodes belonging to the same instance.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on MultiInstanceConfMaps.</p> <code>paf_scorer</code> <p>A <code>sleap_nn.inference.paf_grouping.PAFScorer</code> instance configured to group instances based on peaks and PAFs produced by the model.</p> <code>cms_output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>pafs_output_stride</code> <p>Output stride of the model, denoting the scale of the output pafs relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>return_pafs</code> <p>If <code>True</code>, the part affinity fields will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>return_paf_graph</code> <p>If <code>True</code>, the part affinity field graph will be returned together with the predicted instances. The graph is obtained by parsing the part affinity fields with the <code>paf_scorer</code> instance and is an intermediate representation used during instance grouping.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>class BottomUpInferenceModel(L.LightningModule):\n    \"\"\"BottomUp Inference model.\n\n    This model encapsulates the bottom-up approach. The images are passed to a peak detector\n    to get the predicted instances and then fed into PAF to combine nodes belonging to\n    the same instance.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            MultiInstanceConfMaps.\n        paf_scorer: A `sleap_nn.inference.paf_grouping.PAFScorer` instance configured to group\n            instances based on peaks and PAFs produced by the model.\n        cms_output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        pafs_output_stride: Output stride of the model, denoting the scale of the output\n            pafs relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        return_pafs: If `True`, the part affinity fields will be returned together with\n            the predicted instances. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        return_paf_graph: If `True`, the part affinity field graph will be returned\n            together with the predicted instances. The graph is obtained by parsing the\n            part affinity fields with the `paf_scorer` instance and is an intermediate\n            representation used during instance grouping.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        paf_scorer: PAFScorer,\n        cms_output_stride: Optional[int] = None,\n        pafs_output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = \"integral\",\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        return_pafs: Optional[bool] = False,\n        return_paf_graph: Optional[bool] = False,\n        input_scale: float = 1.0,\n        max_peaks_per_node: Optional[int] = None,\n    ):\n        \"\"\"Initialise the model attributes.\n\n        Args:\n            torch_model: A `nn.Module` that accepts images and predicts confidence maps.\n            paf_scorer: A `PAFScorer` instance for grouping instances.\n            cms_output_stride: Output stride of confidence maps relative to images.\n            pafs_output_stride: Output stride of PAFs relative to images.\n            peak_threshold: Minimum confidence map value for valid peaks.\n            refinement: Peak refinement method: None, \"integral\", or \"local\".\n            integral_patch_size: Size of patches for integral refinement.\n            return_confmaps: If True, return confidence maps in output.\n            return_pafs: If True, return PAFs in output.\n            return_paf_graph: If True, return intermediate PAF graph in output.\n            input_scale: Scale factor applied to input images.\n            max_peaks_per_node: Maximum number of peaks allowed per node before\n                skipping PAF scoring. If any node has more peaks than this limit,\n                empty predictions are returned. This prevents combinatorial explosion\n                during early training when confidence maps are noisy. Set to None to\n                disable this check (default). Recommended value: 100.\n        \"\"\"\n        super().__init__()\n        self.torch_model = torch_model\n        self.paf_scorer = paf_scorer\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.cms_output_stride = cms_output_stride\n        self.pafs_output_stride = pafs_output_stride\n        self.return_confmaps = return_confmaps\n        self.return_pafs = return_pafs\n        self.return_paf_graph = return_paf_graph\n        self.input_scale = input_scale\n        self.max_peaks_per_node = max_peaks_per_node\n\n    def _generate_cms_peaks(self, cms):\n        # TODO: append nans to batch them -&gt; tensor (vectorize the initial paf grouping steps)\n        peaks, peak_vals, sample_inds, peak_channel_inds = find_local_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n        # Adjust for stride and scale.\n        peaks = peaks * self.cms_output_stride  # (n_centroids, 2)\n\n        cms_peaks, cms_peak_vals, cms_peak_channel_inds = [], [], []\n\n        for b in range(self.batch_size):\n            cms_peaks.append(peaks[sample_inds == b])\n            cms_peak_vals.append(peak_vals[sample_inds == b].to(torch.float32))\n            cms_peak_channel_inds.append(peak_channel_inds[sample_inds == b])\n\n        # cms_peaks: [(#nodes, 2), ...]\n        return cms_peaks, cms_peak_vals, cms_peak_channel_inds\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        Args:\n            inputs: Dictionary with \"image\" as one of the keys.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n                as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n        \"\"\"\n        # Network forward pass.\n        self.batch_size = inputs[\"image\"].shape[0]\n        output = self.torch_model(inputs[\"image\"])\n        cms = output[\"MultiInstanceConfmapsHead\"]\n        pafs = output[\"PartAffinityFieldsHead\"].permute(\n            0, 2, 3, 1\n        )  # (batch, h, w, 2*edges)\n        cms_peaks, cms_peak_vals, cms_peak_channel_inds = self._generate_cms_peaks(cms)\n\n        # Check if too many peaks per node (prevents combinatorial explosion)\n        skip_paf_scoring = False\n        if self.max_peaks_per_node is not None:\n            n_nodes = cms.shape[1]\n            for b in range(self.batch_size):\n                for node_idx in range(n_nodes):\n                    n_peaks = int((cms_peak_channel_inds[b] == node_idx).sum().item())\n                    if n_peaks &gt; self.max_peaks_per_node:\n                        logger.warning(\n                            f\"Skipping PAF scoring: node {node_idx} has {n_peaks} peaks \"\n                            f\"(max_peaks_per_node={self.max_peaks_per_node}). \"\n                            f\"Model may need more training.\"\n                        )\n                        skip_paf_scoring = True\n                        break\n                if skip_paf_scoring:\n                    break\n\n        if skip_paf_scoring:\n            # Return empty predictions for each sample\n            device = cms.device\n            n_nodes = cms.shape[1]\n            predicted_instances_adjusted = []\n            predicted_peak_scores = []\n            predicted_instance_scores = []\n            for _ in range(self.batch_size):\n                predicted_instances_adjusted.append(\n                    torch.full((0, n_nodes, 2), float(\"nan\"), device=device)\n                )\n                predicted_peak_scores.append(\n                    torch.full((0, n_nodes), float(\"nan\"), device=device)\n                )\n                predicted_instance_scores.append(torch.tensor([], device=device))\n            edge_inds = [\n                torch.tensor([], dtype=torch.int32, device=device)\n            ] * self.batch_size\n            edge_peak_inds = [\n                torch.tensor([], dtype=torch.int32, device=device).reshape(0, 2)\n            ] * self.batch_size\n            line_scores = [torch.tensor([], device=device)] * self.batch_size\n        else:\n            (\n                predicted_instances,\n                predicted_peak_scores,\n                predicted_instance_scores,\n                edge_inds,\n                edge_peak_inds,\n                line_scores,\n            ) = self.paf_scorer.predict(\n                pafs=pafs,\n                peaks=cms_peaks,\n                peak_vals=cms_peak_vals,\n                peak_channel_inds=cms_peak_channel_inds,\n            )\n\n            predicted_instances = [p / self.input_scale for p in predicted_instances]\n            predicted_instances_adjusted = []\n            for idx, p in enumerate(predicted_instances):\n                predicted_instances_adjusted.append(\n                    p / inputs[\"eff_scale\"][idx].to(p.device)\n                )\n\n        out = {\n            \"pred_instance_peaks\": predicted_instances_adjusted,\n            \"pred_peak_values\": predicted_peak_scores,\n            \"instance_scores\": predicted_instance_scores,\n        }\n\n        if self.return_confmaps:\n            out[\"pred_confmaps\"] = cms.detach()\n        if self.return_pafs:\n            out[\"pred_part_affinity_fields\"] = pafs.detach()\n        if self.return_paf_graph:\n            out[\"peaks\"] = cms_peaks\n            out[\"peak_vals\"] = cms_peak_vals\n            out[\"peak_channel_inds\"] = cms_peak_channel_inds\n            out[\"edge_inds\"] = edge_inds\n            out[\"edge_peak_inds\"] = edge_peak_inds\n            out[\"line_scores\"] = line_scores\n\n        inputs.update(out)\n        return [inputs]\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpInferenceModel.__init__","title":"<code>__init__(torch_model, paf_scorer, cms_output_stride=None, pafs_output_stride=None, peak_threshold=0.0, refinement='integral', integral_patch_size=5, return_confmaps=False, return_pafs=False, return_paf_graph=False, input_scale=1.0, max_peaks_per_node=None)</code>","text":"<p>Initialise the model attributes.</p> <p>Parameters:</p> Name Type Description Default <code>torch_model</code> <code>LightningModule</code> <p>A <code>nn.Module</code> that accepts images and predicts confidence maps.</p> required <code>paf_scorer</code> <code>PAFScorer</code> <p>A <code>PAFScorer</code> instance for grouping instances.</p> required <code>cms_output_stride</code> <code>Optional[int]</code> <p>Output stride of confidence maps relative to images.</p> <code>None</code> <code>pafs_output_stride</code> <code>Optional[int]</code> <p>Output stride of PAFs relative to images.</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>Minimum confidence map value for valid peaks.</p> <code>0.0</code> <code>refinement</code> <code>Optional[str]</code> <p>Peak refinement method: None, \"integral\", or \"local\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>Size of patches for integral refinement.</p> <code>5</code> <code>return_confmaps</code> <code>Optional[bool]</code> <p>If True, return confidence maps in output.</p> <code>False</code> <code>return_pafs</code> <code>Optional[bool]</code> <p>If True, return PAFs in output.</p> <code>False</code> <code>return_paf_graph</code> <code>Optional[bool]</code> <p>If True, return intermediate PAF graph in output.</p> <code>False</code> <code>input_scale</code> <code>float</code> <p>Scale factor applied to input images.</p> <code>1.0</code> <code>max_peaks_per_node</code> <code>Optional[int]</code> <p>Maximum number of peaks allowed per node before skipping PAF scoring. If any node has more peaks than this limit, empty predictions are returned. This prevents combinatorial explosion during early training when confidence maps are noisy. Set to None to disable this check (default). Recommended value: 100.</p> <code>None</code> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    paf_scorer: PAFScorer,\n    cms_output_stride: Optional[int] = None,\n    pafs_output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    return_pafs: Optional[bool] = False,\n    return_paf_graph: Optional[bool] = False,\n    input_scale: float = 1.0,\n    max_peaks_per_node: Optional[int] = None,\n):\n    \"\"\"Initialise the model attributes.\n\n    Args:\n        torch_model: A `nn.Module` that accepts images and predicts confidence maps.\n        paf_scorer: A `PAFScorer` instance for grouping instances.\n        cms_output_stride: Output stride of confidence maps relative to images.\n        pafs_output_stride: Output stride of PAFs relative to images.\n        peak_threshold: Minimum confidence map value for valid peaks.\n        refinement: Peak refinement method: None, \"integral\", or \"local\".\n        integral_patch_size: Size of patches for integral refinement.\n        return_confmaps: If True, return confidence maps in output.\n        return_pafs: If True, return PAFs in output.\n        return_paf_graph: If True, return intermediate PAF graph in output.\n        input_scale: Scale factor applied to input images.\n        max_peaks_per_node: Maximum number of peaks allowed per node before\n            skipping PAF scoring. If any node has more peaks than this limit,\n            empty predictions are returned. This prevents combinatorial explosion\n            during early training when confidence maps are noisy. Set to None to\n            disable this check (default). Recommended value: 100.\n    \"\"\"\n    super().__init__()\n    self.torch_model = torch_model\n    self.paf_scorer = paf_scorer\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.cms_output_stride = cms_output_stride\n    self.pafs_output_stride = pafs_output_stride\n    self.return_confmaps = return_confmaps\n    self.return_pafs = return_pafs\n    self.return_paf_graph = return_paf_graph\n    self.input_scale = input_scale\n    self.max_peaks_per_node = max_peaks_per_node\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpInferenceModel.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with \"image\" as one of the keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch     as a <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    Args:\n        inputs: Dictionary with \"image\" as one of the keys.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n            as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n    \"\"\"\n    # Network forward pass.\n    self.batch_size = inputs[\"image\"].shape[0]\n    output = self.torch_model(inputs[\"image\"])\n    cms = output[\"MultiInstanceConfmapsHead\"]\n    pafs = output[\"PartAffinityFieldsHead\"].permute(\n        0, 2, 3, 1\n    )  # (batch, h, w, 2*edges)\n    cms_peaks, cms_peak_vals, cms_peak_channel_inds = self._generate_cms_peaks(cms)\n\n    # Check if too many peaks per node (prevents combinatorial explosion)\n    skip_paf_scoring = False\n    if self.max_peaks_per_node is not None:\n        n_nodes = cms.shape[1]\n        for b in range(self.batch_size):\n            for node_idx in range(n_nodes):\n                n_peaks = int((cms_peak_channel_inds[b] == node_idx).sum().item())\n                if n_peaks &gt; self.max_peaks_per_node:\n                    logger.warning(\n                        f\"Skipping PAF scoring: node {node_idx} has {n_peaks} peaks \"\n                        f\"(max_peaks_per_node={self.max_peaks_per_node}). \"\n                        f\"Model may need more training.\"\n                    )\n                    skip_paf_scoring = True\n                    break\n            if skip_paf_scoring:\n                break\n\n    if skip_paf_scoring:\n        # Return empty predictions for each sample\n        device = cms.device\n        n_nodes = cms.shape[1]\n        predicted_instances_adjusted = []\n        predicted_peak_scores = []\n        predicted_instance_scores = []\n        for _ in range(self.batch_size):\n            predicted_instances_adjusted.append(\n                torch.full((0, n_nodes, 2), float(\"nan\"), device=device)\n            )\n            predicted_peak_scores.append(\n                torch.full((0, n_nodes), float(\"nan\"), device=device)\n            )\n            predicted_instance_scores.append(torch.tensor([], device=device))\n        edge_inds = [\n            torch.tensor([], dtype=torch.int32, device=device)\n        ] * self.batch_size\n        edge_peak_inds = [\n            torch.tensor([], dtype=torch.int32, device=device).reshape(0, 2)\n        ] * self.batch_size\n        line_scores = [torch.tensor([], device=device)] * self.batch_size\n    else:\n        (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n            edge_inds,\n            edge_peak_inds,\n            line_scores,\n        ) = self.paf_scorer.predict(\n            pafs=pafs,\n            peaks=cms_peaks,\n            peak_vals=cms_peak_vals,\n            peak_channel_inds=cms_peak_channel_inds,\n        )\n\n        predicted_instances = [p / self.input_scale for p in predicted_instances]\n        predicted_instances_adjusted = []\n        for idx, p in enumerate(predicted_instances):\n            predicted_instances_adjusted.append(\n                p / inputs[\"eff_scale\"][idx].to(p.device)\n            )\n\n    out = {\n        \"pred_instance_peaks\": predicted_instances_adjusted,\n        \"pred_peak_values\": predicted_peak_scores,\n        \"instance_scores\": predicted_instance_scores,\n    }\n\n    if self.return_confmaps:\n        out[\"pred_confmaps\"] = cms.detach()\n    if self.return_pafs:\n        out[\"pred_part_affinity_fields\"] = pafs.detach()\n    if self.return_paf_graph:\n        out[\"peaks\"] = cms_peaks\n        out[\"peak_vals\"] = cms_peak_vals\n        out[\"peak_channel_inds\"] = cms_peak_channel_inds\n        out[\"edge_inds\"] = edge_inds\n        out[\"edge_peak_inds\"] = edge_peak_inds\n        out[\"line_scores\"] = line_scores\n\n    inputs.update(out)\n    return [inputs]\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpMultiClassInferenceModel","title":"<code>BottomUpMultiClassInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>BottomUp Inference model for multi-class models.</p> <p>This model encapsulates the bottom-up approach. The images are passed to a local peak detector to get the predicted instances and then grouped into instances by their identity classifications.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on MultiInstanceConfMaps.</p> <code>cms_output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>class_maps_output_stride</code> <p>Output stride of the model, denoting the scale of the output pafs relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>return_class_maps</code> <p>If <code>True</code>, the class maps will be returned together with the predicted instances. This will result in slower inference times since the data must be copied off of the GPU, but is useful for visualizing the raw output of the model.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>class BottomUpMultiClassInferenceModel(L.LightningModule):\n    \"\"\"BottomUp Inference model for multi-class models.\n\n    This model encapsulates the bottom-up approach. The images are passed to a local peak detector\n    to get the predicted instances and then grouped into instances by their identity\n    classifications.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            MultiInstanceConfMaps.\n        cms_output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        class_maps_output_stride: Output stride of the model, denoting the scale of the output\n            pafs relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        return_class_maps: If `True`, the class maps will be returned together with\n            the predicted instances. This will result in slower inference times since\n            the data must be copied off of the GPU, but is useful for visualizing the\n            raw output of the model.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        cms_output_stride: Optional[int] = None,\n        class_maps_output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = \"integral\",\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        return_class_maps: Optional[bool] = False,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__()\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.cms_output_stride = cms_output_stride\n        self.class_maps_output_stride = class_maps_output_stride\n        self.return_confmaps = return_confmaps\n        self.return_class_maps = return_class_maps\n        self.input_scale = input_scale\n\n    def _generate_cms_peaks(self, cms):\n        # TODO: append nans to batch them -&gt; tensor (vectorize the initial paf grouping steps)\n        peaks, peak_vals, sample_inds, peak_channel_inds = find_local_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n        # Adjust for stride and scale.\n        peaks = peaks * self.cms_output_stride  # (n_centroids, 2)\n\n        return peaks, peak_vals, sample_inds, peak_channel_inds\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        Args:\n            inputs: Dictionary with \"image\" as one of the keys.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n                as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n        \"\"\"\n        # Network forward pass.\n        self.batch_size = inputs[\"image\"].shape[0]\n        output = self.torch_model(inputs[\"image\"])\n        cms = output[\"MultiInstanceConfmapsHead\"]\n        class_maps = output[\"ClassMapsHead\"]  # (batch, n_classes, h, w)\n        cms_peaks, cms_peak_vals, cms_peak_sample_inds, cms_peak_channel_inds = (\n            self._generate_cms_peaks(cms.detach())\n        )\n\n        cms_peaks = cms_peaks / self.class_maps_output_stride\n        (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n        ) = classify_peaks_from_maps(\n            class_maps.detach(),\n            cms_peaks,\n            cms_peak_vals,\n            cms_peak_sample_inds,\n            cms_peak_channel_inds,\n            n_channels=cms.shape[-3],\n        )\n        predicted_instances = [\n            p * self.class_maps_output_stride for p in predicted_instances\n        ]\n\n        # Adjust for input scaling.\n        if self.input_scale != 1.0:\n            predicted_instances = [p / self.input_scale for p in predicted_instances]\n\n        predicted_instances_adjusted = []\n        for idx, p in enumerate(predicted_instances):\n            predicted_instances_adjusted.append(\n                p / inputs[\"eff_scale\"][idx].to(p.device)\n            )\n        out = {\n            \"pred_instance_peaks\": predicted_instances_adjusted,\n            \"pred_peak_values\": predicted_peak_scores,\n            \"instance_scores\": predicted_instance_scores,\n        }\n\n        if self.return_confmaps:\n            out[\"pred_confmaps\"] = cms.detach()\n        if self.return_class_maps:\n            out[\"pred_class_maps\"] = class_maps.detach()\n\n        inputs.update(out)\n        return [inputs]\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpMultiClassInferenceModel.__init__","title":"<code>__init__(torch_model, cms_output_stride=None, class_maps_output_stride=None, peak_threshold=0.0, refinement='integral', integral_patch_size=5, return_confmaps=False, return_class_maps=False, input_scale=1.0)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    cms_output_stride: Optional[int] = None,\n    class_maps_output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    return_class_maps: Optional[bool] = False,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__()\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.cms_output_stride = cms_output_stride\n    self.class_maps_output_stride = class_maps_output_stride\n    self.return_confmaps = return_confmaps\n    self.return_class_maps = return_class_maps\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/inference/bottomup/#sleap_nn.inference.bottomup.BottomUpMultiClassInferenceModel.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with \"image\" as one of the keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch     as a <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> Source code in <code>sleap_nn/inference/bottomup.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    Args:\n        inputs: Dictionary with \"image\" as one of the keys.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n            as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n    \"\"\"\n    # Network forward pass.\n    self.batch_size = inputs[\"image\"].shape[0]\n    output = self.torch_model(inputs[\"image\"])\n    cms = output[\"MultiInstanceConfmapsHead\"]\n    class_maps = output[\"ClassMapsHead\"]  # (batch, n_classes, h, w)\n    cms_peaks, cms_peak_vals, cms_peak_sample_inds, cms_peak_channel_inds = (\n        self._generate_cms_peaks(cms.detach())\n    )\n\n    cms_peaks = cms_peaks / self.class_maps_output_stride\n    (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n    ) = classify_peaks_from_maps(\n        class_maps.detach(),\n        cms_peaks,\n        cms_peak_vals,\n        cms_peak_sample_inds,\n        cms_peak_channel_inds,\n        n_channels=cms.shape[-3],\n    )\n    predicted_instances = [\n        p * self.class_maps_output_stride for p in predicted_instances\n    ]\n\n    # Adjust for input scaling.\n    if self.input_scale != 1.0:\n        predicted_instances = [p / self.input_scale for p in predicted_instances]\n\n    predicted_instances_adjusted = []\n    for idx, p in enumerate(predicted_instances):\n        predicted_instances_adjusted.append(\n            p / inputs[\"eff_scale\"][idx].to(p.device)\n        )\n    out = {\n        \"pred_instance_peaks\": predicted_instances_adjusted,\n        \"pred_peak_values\": predicted_peak_scores,\n        \"instance_scores\": predicted_instance_scores,\n    }\n\n    if self.return_confmaps:\n        out[\"pred_confmaps\"] = cms.detach()\n    if self.return_class_maps:\n        out[\"pred_class_maps\"] = class_maps.detach()\n\n    inputs.update(out)\n    return [inputs]\n</code></pre>"},{"location":"api/inference/identity/","title":"identity","text":""},{"location":"api/inference/identity/#sleap_nn.inference.identity","title":"<code>sleap_nn.inference.identity</code>","text":"<p>Utilities for models that learn identity.</p> <p>These functions implement the inference logic for classifying peaks using class maps or classification vectors.</p> <p>Functions:</p> Name Description <code>classify_peaks_from_maps</code> <p>Classify and group local peaks by their class map probability.</p> <code>get_class_inds_from_vectors</code> <p>Get class indices from the probability scores.</p> <code>group_class_peaks</code> <p>Group local peaks using class probabilities, matching peaks to classes using the Hungarian algorithm, per (sample, channel) pair.</p>"},{"location":"api/inference/identity/#sleap_nn.inference.identity.classify_peaks_from_maps","title":"<code>classify_peaks_from_maps(class_maps, peak_points, peak_vals, peak_sample_inds, peak_channel_inds, n_channels)</code>","text":"<p>Classify and group local peaks by their class map probability.</p> <p>Parameters:</p> Name Type Description Default <code>class_maps</code> <code>Tensor</code> <p>Class maps with shape <code>(n_samples, n_classes, height, width, )</code>.</p> required <code>peak_points</code> <code>Tensor</code> <p>Local peak coordinates of shape <code>(n_peaks,)</code>. These should be in the same scale as the class maps.</p> required <code>peak_vals</code> <code>Tensor</code> <p>Confidence map value with shape <code>(n_peaks,)</code>.</p> required <code>peak_sample_inds</code> <code>Tensor</code> <p>Sample index for each peak with shape <code>(n_peaks,)</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>Channel index for each peak with shape <code>(n_peaks,)</code>.</p> required <code>n_channels</code> <code>int</code> <p>Integer number of channels (nodes) the instances should have.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of <code>(points, point_vals, class_probs)</code> containing the grouped peaks.</p> <p><code>points</code>: Predicted instances <code>(n_samples, n_classes, n_peaks, 2)</code>. Missing points will be denoted by     NaNs.</p> <p><code>point_vals</code>: The confidence map values for each point with shape <code>(n_samples, n_classes, n_peaks)</code>.</p> <p><code>class_probs</code>: Classification probabilities for matched points with shape <code>(n_samples, n_classes, n_peaks)</code>.</p> <p>See also: group_class_peaks</p> Source code in <code>sleap_nn/inference/identity.py</code> <pre><code>def classify_peaks_from_maps(\n    class_maps: torch.Tensor,\n    peak_points: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_sample_inds: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    n_channels: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Classify and group local peaks by their class map probability.\n\n    Args:\n        class_maps: Class maps with shape `(n_samples, n_classes, height, width, )`.\n        peak_points: Local peak coordinates of shape `(n_peaks,)`. These should be in the same scale as the class maps.\n        peak_vals: Confidence map value with shape `(n_peaks,)`.\n        peak_sample_inds: Sample index for each peak with shape `(n_peaks,)`.\n        peak_channel_inds: Channel index for each peak with shape `(n_peaks,)`.\n        n_channels: Integer number of channels (nodes) the instances should have.\n\n    Returns:\n        A tuple of `(points, point_vals, class_probs)` containing the grouped peaks.\n\n        `points`: Predicted instances `(n_samples, n_classes, n_peaks, 2)`. Missing points will be denoted by\n            NaNs.\n\n        `point_vals`: The confidence map values for each point with shape `(n_samples, n_classes, n_peaks)`.\n\n        `class_probs`: Classification probabilities for matched points with shape `(n_samples, n_classes, n_peaks)`.\n\n    See also: group_class_peaks\n    \"\"\"\n    # Build subscripts and pull out class probabilities for each peak from class maps.\n    n_samples, n_instances, h, w = class_maps.shape\n    peak_sample_inds = peak_sample_inds.to(torch.int32)\n    peak_channel_inds = peak_channel_inds.to(torch.int32)\n\n    subs = torch.cat(\n        [\n            peak_sample_inds.view(-1, 1),\n            torch.round(torch.flip(peak_points, dims=[1])).to(torch.int32),\n        ],\n        dim=1,\n    )\n    subs[:, 1] = subs[:, 1].clamp(0, h - 1)\n    subs[:, 2] = subs[:, 2].clamp(0, w - 1)\n\n    peak_class_probs = class_maps[subs[:, 0], :, subs[:, 1], subs[:, 2]]\n\n    # Classify the peaks.\n    peak_inds, class_inds = group_class_peaks(\n        peak_class_probs, peak_sample_inds, peak_channel_inds, n_samples, n_channels\n    )\n\n    # Assign the results to fixed size tensors.\n    subs = torch.stack(\n        [peak_sample_inds[peak_inds], class_inds, peak_channel_inds[peak_inds]], dim=1\n    )\n\n    points = torch.full(\n        (n_samples, n_instances, n_channels, 2), float(\"nan\"), device=class_maps.device\n    )\n    point_vals = torch.full(\n        (n_samples, n_instances, n_channels), float(\"nan\"), device=class_maps.device\n    )\n    class_probs = torch.full(\n        (n_samples, n_instances, n_channels), float(\"nan\"), device=class_maps.device\n    )\n\n    points[subs[:, 0], subs[:, 1], subs[:, 2]] = peak_points[peak_inds]\n    point_vals[subs[:, 0], subs[:, 1], subs[:, 2]] = peak_vals[peak_inds]\n\n    gather_inds = torch.stack([peak_inds, class_inds], dim=1)\n    gathered_class_probs = peak_class_probs[gather_inds[:, 0], gather_inds[:, 1]]\n\n    class_probs[subs[:, 0], subs[:, 1], subs[:, 2]] = gathered_class_probs\n\n    return points, point_vals, class_probs\n</code></pre>"},{"location":"api/inference/identity/#sleap_nn.inference.identity.get_class_inds_from_vectors","title":"<code>get_class_inds_from_vectors(peak_class_probs)</code>","text":"<p>Get class indices from the probability scores.</p> <p>Parameters:</p> Name Type Description Default <code>peak_class_probs</code> <code>Tensor</code> <p>(n_samples, n_classes) softmax output for each sample</p> required <p>Returns:</p> Name Type Description <code>class_inds</code> <p>(n_samples,) class index assigned to each sample class_probs: (n_samples,) the probability of the assigned class</p> Source code in <code>sleap_nn/inference/identity.py</code> <pre><code>def get_class_inds_from_vectors(peak_class_probs: torch.Tensor):\n    \"\"\"Get class indices from the probability scores.\n\n    Args:\n        peak_class_probs: (n_samples, n_classes) softmax output for each sample\n\n    Returns:\n        class_inds: (n_samples,) class index assigned to each sample\n        class_probs: (n_samples,) the probability of the assigned class\n    \"\"\"\n    n_samples, n_classes = peak_class_probs.shape\n\n    # Run Hungarian matching on negative probabilities (maximize total confidence)\n    row_inds, col_inds = linear_sum_assignment(-peak_class_probs.cpu().numpy())\n\n    # Initialize result tensors\n    class_inds = torch.full((n_samples,), -1, dtype=torch.int64)\n    class_probs = torch.full((n_samples,), float(\"nan\"))\n\n    # Assign class IDs and probabilities to samples\n    for sample_idx, class_idx in zip(row_inds, col_inds):\n        class_inds[sample_idx] = class_idx\n        class_probs[sample_idx] = peak_class_probs[sample_idx, class_idx]\n\n    return class_inds, class_probs\n</code></pre>"},{"location":"api/inference/identity/#sleap_nn.inference.identity.group_class_peaks","title":"<code>group_class_peaks(peak_class_probs, peak_sample_inds, peak_channel_inds, n_samples, n_channels)</code>","text":"<p>Group local peaks using class probabilities, matching peaks to classes using the Hungarian algorithm, per (sample, channel) pair.</p> Source code in <code>sleap_nn/inference/identity.py</code> <pre><code>def group_class_peaks(\n    peak_class_probs: torch.Tensor,\n    peak_sample_inds: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    n_samples: int,\n    n_channels: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Group local peaks using class probabilities, matching peaks to classes using the Hungarian algorithm, per (sample, channel) pair.\"\"\"\n    peak_inds_list = []\n    class_inds_list = []\n\n    for sample in range(n_samples):\n        for channel in range(n_channels):\n            # Mask to find peaks belonging to this (sample, channel) pair\n            mask = (peak_sample_inds == sample) &amp; (peak_channel_inds == channel)\n            if not torch.any(mask):\n                continue\n\n            # Extract probabilities for current group\n            probs = peak_class_probs[mask]  # (n_peaks_sc, n_classes)\n            if probs.numel() == 0:\n                continue\n\n            # Run Hungarian algorithm (note: maximize =&gt; minimize negative cost)\n            cost = -probs.detach().cpu().numpy()\n            row_ind, col_ind = linear_sum_assignment(cost)\n\n            # Get original indices in peak_class_probs\n            masked_indices = (\n                torch.nonzero(mask, as_tuple=False)\n                .squeeze(1)\n                .to(peak_sample_inds.device)\n            )\n            peak_inds_sc = masked_indices[row_ind]\n            class_inds_sc = torch.tensor(col_ind, dtype=torch.int64).to(\n                peak_sample_inds.device\n            )\n\n            peak_inds_list.append(peak_inds_sc)\n            class_inds_list.append(class_inds_sc)\n\n    if not peak_inds_list:\n        return (\n            torch.empty(0, dtype=torch.int64).to(peak_sample_inds.device),\n            torch.empty(0, dtype=torch.int64).to(peak_sample_inds.device),\n        )\n\n    peak_inds = torch.cat(peak_inds_list, dim=0).to(peak_sample_inds.device)\n    class_inds = torch.cat(class_inds_list, dim=0).to(peak_sample_inds.device)\n\n    # Filter to keep only best class per peak\n    matched_probs = peak_class_probs[peak_inds, class_inds]\n    best_probs = peak_class_probs[peak_inds].max(dim=1).values\n    is_best = matched_probs == best_probs\n\n    return peak_inds[is_best], class_inds[is_best]\n</code></pre>"},{"location":"api/inference/paf_grouping/","title":"paf_grouping","text":""},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping","title":"<code>sleap_nn.inference.paf_grouping</code>","text":"<p>This module provides a set of utilities for grouping peaks based on PAFs.</p> <p>Part affinity fields (PAFs) are a representation used to resolve the peak grouping problem for multi-instance pose estimation [1].</p> <p>They are a convenient way to represent directed graphs with support in image space. For each edge, a PAF can be represented by an image with two channels, corresponding to the x and y components of a unit vector pointing along the direction of the underlying directed graph formed by the connections of the landmarks belonging to an instance.</p> <p>Given a pair of putatively connected landmarks, the agreement between the line segment that connects them and the PAF vectors found at the coordinates along the same line can be used as a measure of \"connectedness\". These scores can then be used to guide the instance-wise grouping of landmarks.</p> <p>This image space representation is particularly useful as it is amenable to neural network-based prediction from unlabeled images.</p> <p>A high-level API for grouping based on PAFs is provided through the <code>PAFScorer</code> class.</p> References <p>.. [1] Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh. Realtime Multi-Person 2D    Pose Estimation using Part Affinity Fields. In CVPR, 2017.</p> <p>Classes:</p> Name Description <code>EdgeConnection</code> <p>Indices to specify a matched connection between two peaks.</p> <code>EdgeType</code> <p>Indices to uniquely identify a single edge type.</p> <code>PAFScorer</code> <p>Scoring pipeline based on part affinity fields.</p> <code>PeakID</code> <p>Indices to uniquely identify a single peak.</p> <p>Functions:</p> Name Description <code>assign_connections_to_instances</code> <p>Assign connected edges to instances via greedy graph partitioning.</p> <code>compute_distance_penalty</code> <p>Compute the distance penalty component of the PAF line integral score.</p> <code>get_connection_candidates</code> <p>Find the indices of all the possible connections formed by the detected peaks.</p> <code>get_paf_lines</code> <p>Get the PAF values at the lines formed between all detected peaks in a sample.</p> <code>group_instances_batch</code> <p>Group matched connections into full instances for a batch.</p> <code>group_instances_sample</code> <p>Group matched connections into full instances for a single sample.</p> <code>make_line_subs</code> <p>Create the lines between candidate connections for evaluating the PAFs.</p> <code>make_predicted_instances</code> <p>Group peaks by assignments and accumulate scores.</p> <code>match_candidates_batch</code> <p>Match candidate connections for a batch based on PAF scores.</p> <code>match_candidates_sample</code> <p>Match candidate connections for a sample based on PAF scores.</p> <code>score_paf_lines</code> <p>Compute the connectivity score for each PAF line in a sample.</p> <code>score_paf_lines_batch</code> <p>Process a batch of images to score the Part Affinity Fields (PAFs) lines formed between connection candidates for each sample.</p> <code>toposort_edges</code> <p>Find a topological ordering for a list of edge types.</p>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.EdgeConnection","title":"<code>EdgeConnection</code>","text":"<p>Indices to specify a matched connection between two peaks.</p> <p>This is a convenience named tuple for use in the matching pipeline.</p> <p>Attributes:</p> Name Type Description <code>src_peak_ind</code> <code>int</code> <p>Index of the source peak within all peaks.</p> <code>dst_peak_ind</code> <code>int</code> <p>Index of the destination peak within all peaks.</p> <code>score</code> <code>float</code> <p>Score of the match.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define(auto_attribs=True)\nclass EdgeConnection:\n    \"\"\"Indices to specify a matched connection between two peaks.\n\n    This is a convenience named tuple for use in the matching pipeline.\n\n    Attributes:\n        src_peak_ind: Index of the source peak within all peaks.\n        dst_peak_ind: Index of the destination peak within all peaks.\n        score: Score of the match.\n    \"\"\"\n\n    src_peak_ind: int\n    dst_peak_ind: int\n    score: float\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.EdgeType","title":"<code>EdgeType</code>","text":"<p>Indices to uniquely identify a single edge type.</p> <p>This is a convenience named tuple for use in the matching pipeline.</p> <p>Attributes:</p> Name Type Description <code>src_node_ind</code> <code>int</code> <p>Index of the source node type within the skeleton edges.</p> <code>dst_node_ind</code> <code>int</code> <p>Index of the destination node type within the skeleton edges.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define(auto_attribs=True, frozen=True)\nclass EdgeType:\n    \"\"\"Indices to uniquely identify a single edge type.\n\n    This is a convenience named tuple for use in the matching pipeline.\n\n    Attributes:\n        src_node_ind: Index of the source node type within the skeleton edges.\n        dst_node_ind: Index of the destination node type within the skeleton edges.\n    \"\"\"\n\n    src_node_ind: int\n    dst_node_ind: int\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer","title":"<code>PAFScorer</code>","text":"<p>Scoring pipeline based on part affinity fields.</p> <p>This class facilitates grouping of predicted peaks based on PAFs. It holds a set of common parameters that are used across different steps of the pipeline.</p> <p>Attributes:</p> Name Type Description <code>part_names</code> <code>List[Text]</code> <p>List of string node names in the skeleton.</p> <code>edges</code> <code>List[Tuple[Text, Text]]</code> <p>List of (src_node, dst_node) names in the skeleton.</p> <code>pafs_stride</code> <code>int</code> <p>Output stride of the part affinity fields. This will be used to adjust the peak coordinates from full image to PAF subscripts.</p> <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching.</p> <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>n_points</code> <code>int</code> <p>Number of points to sample along the line integral.</p> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Minimum number of peaks the instance should have to be considered a real instance. Instances with fewer peaks than this will be discarded (useful for filtering spurious detections).</p> <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones.</p> <code>edge_inds</code> <code>List[Tuple[int, int]]</code> <p>The edges of the skeleton defined as a list of (source, destination) tuples of node indices. This is created automatically on initialization.</p> <code>edge_types</code> <code>List[EdgeType]</code> <p>A list of <code>EdgeType</code> instances representing the edges of the skeleton. This is created automatically on initialization.</p> <code>n_nodes</code> <code>int</code> <p>The number of nodes in the skeleton as a scalar <code>int</code>. This is created automatically on initialization.</p> <code>n_edges</code> <code>int</code> <p>The number of edges in the skeleton as a scalar <code>int</code>. This is created automatically on initialization.</p> <code>sorted_edge_inds</code> <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> Notes <p>This class provides high level APIs for grouping peaks into instances using PAFs.</p> <p>The algorithm has three steps:</p> <pre><code>1. Find all candidate connections between peaks and compute their matching\nscore based on the PAFs.\n\n2. Match candidate connections using the connectivity score such that no\npeak is used in two connections of the same type.\n\n3. Group matched connections into complete instances.\n</code></pre> <p>In general, the output from a peak finder (such as multi-peak confidence map prediction network) can be passed into <code>PAFScorer.predict()</code> to get back complete instances.</p> <p>For finer control over the grouping pipeline steps, use the instance methods in this class or the lower level functions in <code>sleap_nn.paf_grouping</code>.</p> <p>Methods:</p> Name Description <code>__attrs_post_init__</code> <p>Cache some computed attributes on initialization.</p> <code>from_config</code> <p>Initialize the PAF scorer from a <code>MultiInstanceConfig</code> head config.</p> <code>group_instances</code> <p>Group matched connections into full instances for a batch.</p> <code>match_candidates</code> <p>Match candidate connections for a batch based on PAF scores.</p> <code>predict</code> <p>Group a batch of predicted peaks into full instance predictions using PAFs.</p> <code>score_paf_lines</code> <p>Create and score PAF lines formed between connection candidates.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define\nclass PAFScorer:\n    \"\"\"Scoring pipeline based on part affinity fields.\n\n    This class facilitates grouping of predicted peaks based on PAFs. It holds a set of\n    common parameters that are used across different steps of the pipeline.\n\n    Attributes:\n        part_names: List of string node names in the skeleton.\n        edges: List of (src_node, dst_node) names in the skeleton.\n        pafs_stride: Output stride of the part affinity fields. This will be used to\n            adjust the peak coordinates from full image to PAF subscripts.\n        max_edge_length_ratio: The maximum expected length of a connected pair of points\n            as a fraction of the image size. Candidate connections longer than this\n            length will be penalized during matching.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n        n_points: Number of points to sample along the line integral.\n        min_instance_peaks: Minimum number of peaks the instance should have to be\n            considered a real instance. Instances with fewer peaks than this will be\n            discarded (useful for filtering spurious detections).\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs. Useful for rejecting spurious detections when\n            there are no better ones.\n        edge_inds: The edges of the skeleton defined as a list of (source, destination)\n            tuples of node indices. This is created automatically on initialization.\n        edge_types: A list of `EdgeType` instances representing the edges of the\n            skeleton. This is created automatically on initialization.\n        n_nodes: The number of nodes in the skeleton as a scalar `int`. This is created\n            automatically on initialization.\n        n_edges: The number of edges in the skeleton as a scalar `int`. This is created\n            automatically on initialization.\n        sorted_edge_inds: A tuple of indices specifying the topological order that the\n            edge types should be accessed in during instance assembly\n            (`assign_connections_to_instances`).\n\n    Notes:\n        This class provides high level APIs for grouping peaks into instances using\n        PAFs.\n\n        The algorithm has three steps:\n\n            1. Find all candidate connections between peaks and compute their matching\n            score based on the PAFs.\n\n            2. Match candidate connections using the connectivity score such that no\n            peak is used in two connections of the same type.\n\n            3. Group matched connections into complete instances.\n\n        In general, the output from a peak finder (such as multi-peak confidence map\n        prediction network) can be passed into `PAFScorer.predict()` to get back\n        complete instances.\n\n        For finer control over the grouping pipeline steps, use the instance methods in\n        this class or the lower level functions in `sleap_nn.paf_grouping`.\n    \"\"\"\n\n    part_names: List[Text]\n    edges: List[Tuple[Text, Text]]\n    pafs_stride: int\n    max_edge_length_ratio: float = 0.25\n    dist_penalty_weight: float = 1.0\n    n_points: int = 10\n    min_instance_peaks: Union[int, float] = 0\n    min_line_scores: float = 0.25\n\n    edge_inds: List[Tuple[int, int]] = attr.ib(init=False)\n    edge_types: List[EdgeType] = attr.ib(init=False)\n    n_nodes: int = attr.ib(init=False)\n    n_edges: int = attr.ib(init=False)\n    sorted_edge_inds: Tuple[int] = attr.ib(init=False)\n\n    def __attrs_post_init__(self):\n        \"\"\"Cache some computed attributes on initialization.\"\"\"\n        self.edge_inds = [\n            (self.part_names.index(src), self.part_names.index(dst))\n            for (src, dst) in self.edges\n        ]\n        self.edge_types = [\n            EdgeType(src_node, dst_node) for src_node, dst_node in self.edge_inds\n        ]\n\n        self.n_nodes = len(self.part_names)\n        self.n_edges = len(self.edges)\n        self.sorted_edge_inds = toposort_edges(self.edge_types)\n\n    @classmethod\n    def from_config(\n        cls,\n        config: OmegaConf,\n        max_edge_length_ratio: float = 0.25,\n        dist_penalty_weight: float = 1.0,\n        n_points: int = 10,\n        min_instance_peaks: Union[int, float] = 0,\n        min_line_scores: float = 0.25,\n    ) -&gt; \"PAFScorer\":\n        \"\"\"Initialize the PAF scorer from a `MultiInstanceConfig` head config.\n\n        Args:\n            config: An `OmegaConf` instance.\n            max_edge_length_ratio: The maximum expected length of a connected pair of\n                points as a fraction of the image size. Candidate connections longer\n                than this length will be penalized during matching.\n            dist_penalty_weight: A coefficient to scale weight of the distance penalty\n                as a scalar float. Set to values greater than 1.0 to enforce the\n                distance penalty more strictly.\n            min_edge_score: Minimum score required to classify a connection as correct.\n            n_points: Number of points to sample along the line integral.\n            min_instance_peaks: Minimum number of peaks the instance should have to be\n                considered a real instance. Instances with fewer peaks than this will be\n                discarded (useful for filtering spurious detections).\n            min_line_scores: Minimum line score (between -1 and 1) required to form a\n                match between candidate point pairs. Useful for rejecting spurious\n                detections when there are no better ones.\n\n        Returns:\n            The initialized instance of `PAFScorer`.\n        \"\"\"\n        return cls(\n            part_names=config.confmaps.part_names,\n            edges=config.pafs.edges,\n            pafs_stride=config.pafs.output_stride,\n            max_edge_length_ratio=max_edge_length_ratio,\n            dist_penalty_weight=dist_penalty_weight,\n            n_points=n_points,\n            min_instance_peaks=min_instance_peaks,\n            min_line_scores=min_line_scores,\n        )\n\n    def score_paf_lines(\n        self,\n        pafs: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_channel_inds: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Create and score PAF lines formed between connection candidates.\n\n        Args:\n            pafs: A nested torch tensor of shape `(n_samples, height, width, 2 * n_edges)`\n                containing the part affinity fields for each sample in the batch.\n            peaks: A nested torch tensor of shape `(n_samples, (n_peaks), 2)` containing the\n                (x, y) coordinates of the detected peaks for each sample.\n            peak_channel_inds: A nested torch tensor of shape `(n_samples, (n_peaks))` indicating\n                the channel (node) index that each peak corresponds to.\n\n        Returns:\n            A tuple containing three lists for each sample in the batch:\n                - A nested torch tensor of shape `(n_samples, (n_connections,))` indicating the indices\n                of the edges that each connection corresponds to.\n                - A nested torch tensor of shape `(n_samples, (n_connections, 2))` containing the indices\n                of the source and destination peaks forming each connection.\n                - A nested torch tensor of shape `(n_samples, (n_connections,))` containing the scores\n                for each connection based on the PAFs.\n\n        Notes:\n            This is a convenience wrapper for the standalone `score_paf_lines_batch()`.\n\n        See also: score_paf_lines_batch\n        \"\"\"\n        return score_paf_lines_batch(\n            pafs,\n            peaks,\n            peak_channel_inds,\n            self.edge_inds,\n            self.n_points,\n            self.pafs_stride,\n            self.max_edge_length_ratio,\n            self.dist_penalty_weight,\n            self.n_nodes,\n        )\n\n    def match_candidates(\n        self,\n        edge_inds: torch.Tensor,\n        edge_peak_inds: torch.Tensor,\n        line_scores: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Match candidate connections for a batch based on PAF scores.\n\n        Args:\n            edge_inds: Sample-grouped edge indices as a nested `torch.Tensor` of shape\n                `(n_samples, (n_candidates))` and dtype `torch.int32` indicating the\n                indices of the edge that each of the candidate connections belongs to.\n                Can be generated using `PAFScorer.score_paf_lines()`.\n            edge_peak_inds: Sample-grouped indices of the peaks that form the source and\n                destination of each candidate connection as a nested `torch.Tensor` of shape\n                `(n_samples, (n_candidates), 2)` and dtype `torch.int32`. Can be generated\n                using `PAFScorer.score_paf_lines()`.\n            line_scores: Sample-grouped scores for each candidate connection as a\n                nested `torch.Tensor` of shape `(n_samples, (n_candidates))` and dtype\n                `torch.float32`. Can be generated using `PAFScorer.score_paf_lines()`.\n\n        Returns:\n            The connection peaks for each edge matched based on score as tuple of\n            `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n            `match_edge_inds`: Sample-grouped indices of the skeleton edge for each\n            connection as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n            and dtype `torch.int32`.\n\n            `match_src_peak_inds`: Sample-grouped indices of the source peaks that form\n            each connection as a nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks in\n            the sample.\n\n            `match_dst_peak_inds`: Sample-grouped indices of the destination peaks that\n            form each connection as a nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks in\n            the sample.\n\n            `match_line_scores`: Sample-grouped PAF line scores of the matched\n            connections as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n            and dtype `torch.float32`.\n\n        Notes:\n            This is a convenience wrapper for the standalone `match_candidates_batch()`.\n\n        See also: PAFScorer.score_paf_lines, match_candidates_batch\n        \"\"\"\n        return match_candidates_batch(\n            edge_inds, edge_peak_inds, line_scores, self.n_edges\n        )\n\n    def group_instances(\n        self,\n        peaks: torch.Tensor,\n        peak_vals: torch.Tensor,\n        peak_channel_inds: torch.Tensor,\n        match_edge_inds: torch.Tensor,\n        match_src_peak_inds: torch.Tensor,\n        match_dst_peak_inds: torch.Tensor,\n        match_line_scores: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Group matched connections into full instances for a batch.\n\n        Args:\n            peaks: The sample-grouped detected peaks in a batch as a nested tensor `torch.Tensor`\n                of shape `(n_samples, (n_peaks), 2)` and dtype `torch.float32`. These\n                should be `(x, y)` coordinates of each peak in the image scale.\n            peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n                nested tensor `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n                `torch.float32`.\n            peak_channel_inds: The sample-grouped indices of the channel (node) that\n                each detected peak is associated with as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_peaks))` and dtype `torch.int32`.\n            match_edge_inds: Sample-grouped indices of the skeleton edge that each\n                connection corresponds to as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_connections))` and dtype `torch.int32`. This can be\n                generated by `PAFScorer.match_candidates()`.\n            match_src_peak_inds: Sample-grouped indices of the source peaks that form\n                each connection as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n                indices correspond to the edge-grouped peaks, not the set of all peaks\n                in each sample. This can be generated by `PAFScorer.match_candidates()`.\n            match_dst_peak_inds: Sample-grouped indices of the destination peaks that\n                form each connection as a nested tensor `torch.Tensor` of shape\n                `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n                indices correspond to the edge-grouped peaks, not the set of all peaks\n                in the sample. This can be generated by `PAFScorer.match_candidates()`.\n            match_line_scores: Sample-grouped PAF line scores of the matched connections\n                as a nested tensor `torch.Tensor` of shape `(n_samples, (n_connections))` and dtype\n                `torch.float32`. This can be generated by `PAFScorer.match_candidates()`.\n\n        Returns:\n            A tuple of arrays with the grouped instances for the whole batch grouped by\n            sample:\n\n            `predicted_instances`: The sample- and instance-grouped coordinates for each\n            instance as nested `torch.Tensor` of shape\n            `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n            peaks are represented by `NaN`s.\n\n            `predicted_peak_scores`: The sample- and instance-grouped confidence map\n            values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n            and dtype `torch.float32`.\n\n            `predicted_instance_scores`: The sample-grouped instance grouping score for\n            each instance as an array of shape `(n_samples, (n_instances))` and dtype\n            `torch.float32`.\n\n        Notes:\n            This is a convenience wrapper for the standalone `group_instances_batch()`.\n\n        See also: PAFScorer.match_candidates, group_instances_batch\n        \"\"\"\n        return group_instances_batch(\n            peaks,\n            peak_vals,\n            peak_channel_inds,\n            match_edge_inds,\n            match_src_peak_inds,\n            match_dst_peak_inds,\n            match_line_scores,\n            self.n_nodes,\n            self.sorted_edge_inds,\n            self.edge_types,\n            self.min_instance_peaks,\n            min_line_scores=self.min_line_scores,\n        )\n\n    def predict(\n        self,\n        pafs: torch.Tensor,\n        peaks: torch.Tensor,\n        peak_vals: torch.Tensor,\n        peak_channel_inds: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Group a batch of predicted peaks into full instance predictions using PAFs.\n\n        Args:\n            pafs: The batch of part affinity fields as a `torch.Tensor` of shape\n                `(n_samples, height, width, 2 * n_edges)` and type `torch.float32`.\n            peaks: The coordinates of the peaks grouped by sample as a nested `torch.Tensor`\n                of shape `(n_samples, (n_peaks), 2)`.\n            peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n                nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n                `torch.float32`.\n            peak_channel_inds: The channel (node) that each peak in `peaks` corresponds\n                to as a nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n                `torch.int32`.\n\n        Returns:\n            A tuple of arrays with the grouped instances for the whole batch grouped by\n            sample:\n\n            `predicted_instances`: The sample- and instance-grouped coordinates for each\n            instance as nested `torch.Tensor` of shape\n            `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n            peaks are represented by `NaN`s.\n\n            `predicted_peak_scores`: The sample- and instance-grouped confidence map\n            values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n            and dtype `torch.float32`.\n\n            `predicted_instance_scores`: The sample-grouped instance grouping score for\n            each instance as an array of shape `(n_samples, (n_instances))` and dtype\n            `torch.float32`.\n\n        Notes:\n            This is a high level API for grouping peaks into instances using PAFs.\n\n            See the `PAFScorer` class documentation for more details on the algorithm.\n\n        See Also:\n            PAFScorer.score_paf_lines, PAFScorer.match_candidates,\n            PAFScorer.group_instances\n        \"\"\"\n        edge_inds, edge_peak_inds, line_scores = self.score_paf_lines(\n            pafs, peaks, peak_channel_inds\n        )\n        (\n            match_edge_inds,\n            match_src_peak_inds,\n            match_dst_peak_inds,\n            match_line_scores,\n        ) = self.match_candidates(edge_inds, edge_peak_inds, line_scores)\n        (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n        ) = self.group_instances(\n            peaks,\n            peak_vals,\n            peak_channel_inds,\n            match_edge_inds,\n            match_src_peak_inds,\n            match_dst_peak_inds,\n            match_line_scores,\n        )\n        return (\n            predicted_instances,\n            predicted_peak_scores,\n            predicted_instance_scores,\n            edge_inds,\n            edge_peak_inds,\n            line_scores,\n        )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.__attrs_post_init__","title":"<code>__attrs_post_init__()</code>","text":"<p>Cache some computed attributes on initialization.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def __attrs_post_init__(self):\n    \"\"\"Cache some computed attributes on initialization.\"\"\"\n    self.edge_inds = [\n        (self.part_names.index(src), self.part_names.index(dst))\n        for (src, dst) in self.edges\n    ]\n    self.edge_types = [\n        EdgeType(src_node, dst_node) for src_node, dst_node in self.edge_inds\n    ]\n\n    self.n_nodes = len(self.part_names)\n    self.n_edges = len(self.edges)\n    self.sorted_edge_inds = toposort_edges(self.edge_types)\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.from_config","title":"<code>from_config(config, max_edge_length_ratio=0.25, dist_penalty_weight=1.0, n_points=10, min_instance_peaks=0, min_line_scores=0.25)</code>  <code>classmethod</code>","text":"<p>Initialize the PAF scorer from a <code>MultiInstanceConfig</code> head config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>OmegaConf</code> <p>An <code>OmegaConf</code> instance.</p> required <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching.</p> <code>0.25</code> <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>1.0</code> <code>min_edge_score</code> <p>Minimum score required to classify a connection as correct.</p> required <code>n_points</code> <code>int</code> <p>Number of points to sample along the line integral.</p> <code>10</code> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Minimum number of peaks the instance should have to be considered a real instance. Instances with fewer peaks than this will be discarded (useful for filtering spurious detections).</p> <code>0</code> <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>PAFScorer</code> <p>The initialized instance of <code>PAFScorer</code>.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: OmegaConf,\n    max_edge_length_ratio: float = 0.25,\n    dist_penalty_weight: float = 1.0,\n    n_points: int = 10,\n    min_instance_peaks: Union[int, float] = 0,\n    min_line_scores: float = 0.25,\n) -&gt; \"PAFScorer\":\n    \"\"\"Initialize the PAF scorer from a `MultiInstanceConfig` head config.\n\n    Args:\n        config: An `OmegaConf` instance.\n        max_edge_length_ratio: The maximum expected length of a connected pair of\n            points as a fraction of the image size. Candidate connections longer\n            than this length will be penalized during matching.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty\n            as a scalar float. Set to values greater than 1.0 to enforce the\n            distance penalty more strictly.\n        min_edge_score: Minimum score required to classify a connection as correct.\n        n_points: Number of points to sample along the line integral.\n        min_instance_peaks: Minimum number of peaks the instance should have to be\n            considered a real instance. Instances with fewer peaks than this will be\n            discarded (useful for filtering spurious detections).\n        min_line_scores: Minimum line score (between -1 and 1) required to form a\n            match between candidate point pairs. Useful for rejecting spurious\n            detections when there are no better ones.\n\n    Returns:\n        The initialized instance of `PAFScorer`.\n    \"\"\"\n    return cls(\n        part_names=config.confmaps.part_names,\n        edges=config.pafs.edges,\n        pafs_stride=config.pafs.output_stride,\n        max_edge_length_ratio=max_edge_length_ratio,\n        dist_penalty_weight=dist_penalty_weight,\n        n_points=n_points,\n        min_instance_peaks=min_instance_peaks,\n        min_line_scores=min_line_scores,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.group_instances","title":"<code>group_instances(peaks, peak_vals, peak_channel_inds, match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code>","text":"<p>Group matched connections into full instances for a batch.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <code>Tensor</code> <p>The sample-grouped detected peaks in a batch as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks), 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>peak_vals</code> <code>Tensor</code> <p>The sample-grouped scores of the detected peaks in a batch as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>The sample-grouped indices of the channel (node) that each detected peak is associated with as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.int32</code>.</p> required <code>match_edge_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the skeleton edge that each connection corresponds to as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <code>match_src_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the source peaks that form each connection as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in each sample. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <code>match_dst_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the destination peaks that form each connection as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <code>match_line_scores</code> <code>Tensor</code> <p>Sample-grouped PAF line scores of the matched connections as a nested tensor <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>. This can be generated by <code>PAFScorer.match_candidates()</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of arrays with the grouped instances for the whole batch grouped by sample:</p> <p><code>predicted_instances</code>: The sample- and instance-grouped coordinates for each instance as nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes, 2)</code> and dtype <code>torch.float32</code>. Missing peaks are represented by <code>NaN</code>s.</p> <p><code>predicted_peak_scores</code>: The sample- and instance-grouped confidence map values for each peak as an array of <code>(n_samples, (n_instances), n_nodes)</code> and dtype <code>torch.float32</code>.</p> <p><code>predicted_instance_scores</code>: The sample-grouped instance grouping score for each instance as an array of shape <code>(n_samples, (n_instances))</code> and dtype <code>torch.float32</code>.</p> Notes <p>This is a convenience wrapper for the standalone <code>group_instances_batch()</code>.</p> <p>See also: PAFScorer.match_candidates, group_instances_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def group_instances(\n    self,\n    peaks: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    match_edge_inds: torch.Tensor,\n    match_src_peak_inds: torch.Tensor,\n    match_dst_peak_inds: torch.Tensor,\n    match_line_scores: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Group matched connections into full instances for a batch.\n\n    Args:\n        peaks: The sample-grouped detected peaks in a batch as a nested tensor `torch.Tensor`\n            of shape `(n_samples, (n_peaks), 2)` and dtype `torch.float32`. These\n            should be `(x, y)` coordinates of each peak in the image scale.\n        peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n            nested tensor `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n            `torch.float32`.\n        peak_channel_inds: The sample-grouped indices of the channel (node) that\n            each detected peak is associated with as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_peaks))` and dtype `torch.int32`.\n        match_edge_inds: Sample-grouped indices of the skeleton edge that each\n            connection corresponds to as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. This can be\n            generated by `PAFScorer.match_candidates()`.\n        match_src_peak_inds: Sample-grouped indices of the source peaks that form\n            each connection as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks\n            in each sample. This can be generated by `PAFScorer.match_candidates()`.\n        match_dst_peak_inds: Sample-grouped indices of the destination peaks that\n            form each connection as a nested tensor `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks\n            in the sample. This can be generated by `PAFScorer.match_candidates()`.\n        match_line_scores: Sample-grouped PAF line scores of the matched connections\n            as a nested tensor `torch.Tensor` of shape `(n_samples, (n_connections))` and dtype\n            `torch.float32`. This can be generated by `PAFScorer.match_candidates()`.\n\n    Returns:\n        A tuple of arrays with the grouped instances for the whole batch grouped by\n        sample:\n\n        `predicted_instances`: The sample- and instance-grouped coordinates for each\n        instance as nested `torch.Tensor` of shape\n        `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n        peaks are represented by `NaN`s.\n\n        `predicted_peak_scores`: The sample- and instance-grouped confidence map\n        values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n        and dtype `torch.float32`.\n\n        `predicted_instance_scores`: The sample-grouped instance grouping score for\n        each instance as an array of shape `(n_samples, (n_instances))` and dtype\n        `torch.float32`.\n\n    Notes:\n        This is a convenience wrapper for the standalone `group_instances_batch()`.\n\n    See also: PAFScorer.match_candidates, group_instances_batch\n    \"\"\"\n    return group_instances_batch(\n        peaks,\n        peak_vals,\n        peak_channel_inds,\n        match_edge_inds,\n        match_src_peak_inds,\n        match_dst_peak_inds,\n        match_line_scores,\n        self.n_nodes,\n        self.sorted_edge_inds,\n        self.edge_types,\n        self.min_instance_peaks,\n        min_line_scores=self.min_line_scores,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.match_candidates","title":"<code>match_candidates(edge_inds, edge_peak_inds, line_scores)</code>","text":"<p>Match candidate connections for a batch based on PAF scores.</p> <p>Parameters:</p> Name Type Description Default <code>edge_inds</code> <code>Tensor</code> <p>Sample-grouped edge indices as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>PAFScorer.score_paf_lines()</code>.</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the peaks that form the source and destination of each candidate connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_candidates), 2)</code> and dtype <code>torch.int32</code>. Can be generated using <code>PAFScorer.score_paf_lines()</code>.</p> required <code>line_scores</code> <code>Tensor</code> <p>Sample-grouped scores for each candidate connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.float32</code>. Can be generated using <code>PAFScorer.score_paf_lines()</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>The connection peaks for each edge matched based on score as tuple of <code>(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code></p> <p><code>match_edge_inds</code>: Sample-grouped indices of the skeleton edge for each connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>.</p> <p><code>match_src_peak_inds</code>: Sample-grouped indices of the source peaks that form each connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_dst_peak_inds</code>: Sample-grouped indices of the destination peaks that form each connection as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_line_scores</code>: Sample-grouped PAF line scores of the matched connections as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>.</p> Notes <p>This is a convenience wrapper for the standalone <code>match_candidates_batch()</code>.</p> <p>See also: PAFScorer.score_paf_lines, match_candidates_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def match_candidates(\n    self,\n    edge_inds: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    line_scores: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Match candidate connections for a batch based on PAF scores.\n\n    Args:\n        edge_inds: Sample-grouped edge indices as a nested `torch.Tensor` of shape\n            `(n_samples, (n_candidates))` and dtype `torch.int32` indicating the\n            indices of the edge that each of the candidate connections belongs to.\n            Can be generated using `PAFScorer.score_paf_lines()`.\n        edge_peak_inds: Sample-grouped indices of the peaks that form the source and\n            destination of each candidate connection as a nested `torch.Tensor` of shape\n            `(n_samples, (n_candidates), 2)` and dtype `torch.int32`. Can be generated\n            using `PAFScorer.score_paf_lines()`.\n        line_scores: Sample-grouped scores for each candidate connection as a\n            nested `torch.Tensor` of shape `(n_samples, (n_candidates))` and dtype\n            `torch.float32`. Can be generated using `PAFScorer.score_paf_lines()`.\n\n    Returns:\n        The connection peaks for each edge matched based on score as tuple of\n        `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n        `match_edge_inds`: Sample-grouped indices of the skeleton edge for each\n        connection as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n        and dtype `torch.int32`.\n\n        `match_src_peak_inds`: Sample-grouped indices of the source peaks that form\n        each connection as a nested `torch.Tensor` of shape\n        `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n        indices correspond to the edge-grouped peaks, not the set of all peaks in\n        the sample.\n\n        `match_dst_peak_inds`: Sample-grouped indices of the destination peaks that\n        form each connection as a nested `torch.Tensor` of shape\n        `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n        indices correspond to the edge-grouped peaks, not the set of all peaks in\n        the sample.\n\n        `match_line_scores`: Sample-grouped PAF line scores of the matched\n        connections as a nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n        and dtype `torch.float32`.\n\n    Notes:\n        This is a convenience wrapper for the standalone `match_candidates_batch()`.\n\n    See also: PAFScorer.score_paf_lines, match_candidates_batch\n    \"\"\"\n    return match_candidates_batch(\n        edge_inds, edge_peak_inds, line_scores, self.n_edges\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.predict","title":"<code>predict(pafs, peaks, peak_vals, peak_channel_inds)</code>","text":"<p>Group a batch of predicted peaks into full instance predictions using PAFs.</p> <p>Parameters:</p> Name Type Description Default <code>pafs</code> <code>Tensor</code> <p>The batch of part affinity fields as a <code>torch.Tensor</code> of shape <code>(n_samples, height, width, 2 * n_edges)</code> and type <code>torch.float32</code>.</p> required <code>peaks</code> <code>Tensor</code> <p>The coordinates of the peaks grouped by sample as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks), 2)</code>.</p> required <code>peak_vals</code> <code>Tensor</code> <p>The sample-grouped scores of the detected peaks in a batch as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>The channel (node) that each peak in <code>peaks</code> corresponds to as a nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.int32</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of arrays with the grouped instances for the whole batch grouped by sample:</p> <p><code>predicted_instances</code>: The sample- and instance-grouped coordinates for each instance as nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes, 2)</code> and dtype <code>torch.float32</code>. Missing peaks are represented by <code>NaN</code>s.</p> <p><code>predicted_peak_scores</code>: The sample- and instance-grouped confidence map values for each peak as an array of <code>(n_samples, (n_instances), n_nodes)</code> and dtype <code>torch.float32</code>.</p> <p><code>predicted_instance_scores</code>: The sample-grouped instance grouping score for each instance as an array of shape <code>(n_samples, (n_instances))</code> and dtype <code>torch.float32</code>.</p> Notes <p>This is a high level API for grouping peaks into instances using PAFs.</p> <p>See the <code>PAFScorer</code> class documentation for more details on the algorithm.</p> See Also <p>PAFScorer.score_paf_lines, PAFScorer.match_candidates, PAFScorer.group_instances</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def predict(\n    self,\n    pafs: torch.Tensor,\n    peaks: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Group a batch of predicted peaks into full instance predictions using PAFs.\n\n    Args:\n        pafs: The batch of part affinity fields as a `torch.Tensor` of shape\n            `(n_samples, height, width, 2 * n_edges)` and type `torch.float32`.\n        peaks: The coordinates of the peaks grouped by sample as a nested `torch.Tensor`\n            of shape `(n_samples, (n_peaks), 2)`.\n        peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n            nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n            `torch.float32`.\n        peak_channel_inds: The channel (node) that each peak in `peaks` corresponds\n            to as a nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype\n            `torch.int32`.\n\n    Returns:\n        A tuple of arrays with the grouped instances for the whole batch grouped by\n        sample:\n\n        `predicted_instances`: The sample- and instance-grouped coordinates for each\n        instance as nested `torch.Tensor` of shape\n        `(n_samples, (n_instances), n_nodes, 2)` and dtype `torch.float32`. Missing\n        peaks are represented by `NaN`s.\n\n        `predicted_peak_scores`: The sample- and instance-grouped confidence map\n        values for each peak as an array of `(n_samples, (n_instances), n_nodes)`\n        and dtype `torch.float32`.\n\n        `predicted_instance_scores`: The sample-grouped instance grouping score for\n        each instance as an array of shape `(n_samples, (n_instances))` and dtype\n        `torch.float32`.\n\n    Notes:\n        This is a high level API for grouping peaks into instances using PAFs.\n\n        See the `PAFScorer` class documentation for more details on the algorithm.\n\n    See Also:\n        PAFScorer.score_paf_lines, PAFScorer.match_candidates,\n        PAFScorer.group_instances\n    \"\"\"\n    edge_inds, edge_peak_inds, line_scores = self.score_paf_lines(\n        pafs, peaks, peak_channel_inds\n    )\n    (\n        match_edge_inds,\n        match_src_peak_inds,\n        match_dst_peak_inds,\n        match_line_scores,\n    ) = self.match_candidates(edge_inds, edge_peak_inds, line_scores)\n    (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n    ) = self.group_instances(\n        peaks,\n        peak_vals,\n        peak_channel_inds,\n        match_edge_inds,\n        match_src_peak_inds,\n        match_dst_peak_inds,\n        match_line_scores,\n    )\n    return (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n        edge_inds,\n        edge_peak_inds,\n        line_scores,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PAFScorer.score_paf_lines","title":"<code>score_paf_lines(pafs, peaks, peak_channel_inds)</code>","text":"<p>Create and score PAF lines formed between connection candidates.</p> <p>Parameters:</p> Name Type Description Default <code>pafs</code> <code>Tensor</code> <p>A nested torch tensor of shape <code>(n_samples, height, width, 2 * n_edges)</code> containing the part affinity fields for each sample in the batch.</p> required <code>peaks</code> <code>Tensor</code> <p>A nested torch tensor of shape <code>(n_samples, (n_peaks), 2)</code> containing the (x, y) coordinates of the detected peaks for each sample.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>A nested torch tensor of shape <code>(n_samples, (n_peaks))</code> indicating the channel (node) index that each peak corresponds to.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple containing three lists for each sample in the batch:     - A nested torch tensor of shape <code>(n_samples, (n_connections,))</code> indicating the indices     of the edges that each connection corresponds to.     - A nested torch tensor of shape <code>(n_samples, (n_connections, 2))</code> containing the indices     of the source and destination peaks forming each connection.     - A nested torch tensor of shape <code>(n_samples, (n_connections,))</code> containing the scores     for each connection based on the PAFs.</p> Notes <p>This is a convenience wrapper for the standalone <code>score_paf_lines_batch()</code>.</p> <p>See also: score_paf_lines_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def score_paf_lines(\n    self,\n    pafs: torch.Tensor,\n    peaks: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Create and score PAF lines formed between connection candidates.\n\n    Args:\n        pafs: A nested torch tensor of shape `(n_samples, height, width, 2 * n_edges)`\n            containing the part affinity fields for each sample in the batch.\n        peaks: A nested torch tensor of shape `(n_samples, (n_peaks), 2)` containing the\n            (x, y) coordinates of the detected peaks for each sample.\n        peak_channel_inds: A nested torch tensor of shape `(n_samples, (n_peaks))` indicating\n            the channel (node) index that each peak corresponds to.\n\n    Returns:\n        A tuple containing three lists for each sample in the batch:\n            - A nested torch tensor of shape `(n_samples, (n_connections,))` indicating the indices\n            of the edges that each connection corresponds to.\n            - A nested torch tensor of shape `(n_samples, (n_connections, 2))` containing the indices\n            of the source and destination peaks forming each connection.\n            - A nested torch tensor of shape `(n_samples, (n_connections,))` containing the scores\n            for each connection based on the PAFs.\n\n    Notes:\n        This is a convenience wrapper for the standalone `score_paf_lines_batch()`.\n\n    See also: score_paf_lines_batch\n    \"\"\"\n    return score_paf_lines_batch(\n        pafs,\n        peaks,\n        peak_channel_inds,\n        self.edge_inds,\n        self.n_points,\n        self.pafs_stride,\n        self.max_edge_length_ratio,\n        self.dist_penalty_weight,\n        self.n_nodes,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.PeakID","title":"<code>PeakID</code>","text":"<p>Indices to uniquely identify a single peak.</p> <p>This is a convenience named tuple for use in the matching pipeline.</p> <p>Attributes:</p> Name Type Description <code>node_ind</code> <code>int</code> <p>Index of the node type (channel) of the peak.</p> <code>peak_ind</code> <code>int</code> <p>Index of the peak within its node type.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>@attrs.define(auto_attribs=True, frozen=True)\nclass PeakID:\n    \"\"\"Indices to uniquely identify a single peak.\n\n    This is a convenience named tuple for use in the matching pipeline.\n\n    Attributes:\n        node_ind: Index of the node type (channel) of the peak.\n        peak_ind: Index of the peak within its node type.\n    \"\"\"\n\n    node_ind: int\n    peak_ind: int\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.assign_connections_to_instances","title":"<code>assign_connections_to_instances(connections, min_instance_peaks=0, n_nodes=None)</code>","text":"<p>Assign connected edges to instances via greedy graph partitioning.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Dict[EdgeType, List[EdgeConnection]]</code> <p>A dict that maps EdgeType to a list of EdgeConnections found through connection scoring. This can be generated by the filter_connection_candidates function.</p> required <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>If this is greater than 0, grouped instances with fewer assigned peaks than this threshold will be excluded. If a float in the range (0., 1.] is provided, this is interpreted as a fraction of the total number of nodes in the skeleton. If an integer is provided, this is the absolute minimum number of peaks.</p> <code>0</code> <code>n_nodes</code> <code>int</code> <p>Total node type count. Used to convert min_instance_peaks to an absolute number when a fraction is specified. If not provided, the node count is inferred from the unique node inds in connections.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>instance_assignments</code> <code>Dict[PeakID, int]</code> <p>A dict mapping PeakID to a unique instance ID specified as an integer.</p> <p>A PeakID is a tuple of (node_type_ind, peak_ind), where the peak_ind is the index or identifier specified in a EdgeConnection as a src_peak_ind or dst_peak_ind.</p> Note <p>Instance IDs are not necessarily consecutive since some instances may be filtered out during the partitioning or filtering.</p> <p>This function expects connections from a single sample/frame!</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def assign_connections_to_instances(\n    connections: Dict[EdgeType, List[EdgeConnection]],\n    min_instance_peaks: Union[int, float] = 0,\n    n_nodes: int = None,\n) -&gt; Dict[PeakID, int]:\n    \"\"\"Assign connected edges to instances via greedy graph partitioning.\n\n    Args:\n        connections: A dict that maps EdgeType to a list of EdgeConnections found\n            through connection scoring. This can be generated by the\n            filter_connection_candidates function.\n        min_instance_peaks: If this is greater than 0, grouped instances with fewer\n            assigned peaks than this threshold will be excluded. If a float in the\n            range (0., 1.] is provided, this is interpreted as a fraction of the total\n            number of nodes in the skeleton. If an integer is provided, this is the\n            absolute minimum number of peaks.\n        n_nodes: Total node type count. Used to convert min_instance_peaks to an\n            absolute number when a fraction is specified. If not provided, the node\n            count is inferred from the unique node inds in connections.\n\n    Returns:\n        instance_assignments: A dict mapping PeakID to a unique instance ID specified\n        as an integer.\n\n        A PeakID is a tuple of (node_type_ind, peak_ind), where the peak_ind is the\n        index or identifier specified in a EdgeConnection as a src_peak_ind or\n        dst_peak_ind.\n\n    Note:\n        Instance IDs are not necessarily consecutive since some instances may be\n        filtered out during the partitioning or filtering.\n\n        This function expects connections from a single sample/frame!\n    \"\"\"\n    # Grouping table that maps PeakID(node_ind, peak_ind) to an instance_id.\n    instance_assignments = dict()\n\n    # Loop through edge types.\n    for edge_type, edge_connections in connections.items():\n        # Loop through connections for the current edge.\n        for connection in edge_connections:\n            # Notation: specific peaks are identified by (node_ind, peak_ind).\n            src_id = PeakID(edge_type.src_node_ind, connection.src_peak_ind)\n            dst_id = PeakID(edge_type.dst_node_ind, connection.dst_peak_ind)\n\n            # Get instance assignments for the connection peaks.\n            src_instance = instance_assignments.get(src_id, None)\n            dst_instance = instance_assignments.get(dst_id, None)\n\n            if src_instance is None and dst_instance is None:\n                # Case 1: Neither peak is assigned to an instance yet. We'll create a\n                # new instance to hold both.\n                new_instance = max(instance_assignments.values(), default=-1) + 1\n                instance_assignments[src_id] = new_instance\n                instance_assignments[dst_id] = new_instance\n\n            elif src_instance is not None and dst_instance is None:\n                # Case 2: The source peak is assigned already, but not the destination\n                # peak. We'll assign the destination peak to the same instance as the\n                # source.\n                instance_assignments[dst_id] = src_instance\n\n            elif src_instance is not None and dst_instance is not None:\n                # Case 3: Both peaks have been assigned. We'll update the destination\n                # peak to be a part of the source peak instance.\n                instance_assignments[dst_id] = src_instance\n\n                # We'll also check if they form disconnected subgraphs, in which case\n                # we'll merge them by assigning all peaks belonging to the destination\n                # peak's instance to the source peak's instance.\n                src_instance_nodes = set(\n                    peak_id.node_ind\n                    for peak_id, instance in instance_assignments.items()\n                    if instance == src_instance\n                )\n                dst_instance_nodes = set(\n                    peak_id.node_ind\n                    for peak_id, instance in instance_assignments.items()\n                    if instance == dst_instance\n                )\n\n                if len(src_instance_nodes.intersection(dst_instance_nodes)) == 0:\n                    for peak_id in instance_assignments:\n                        if instance_assignments[peak_id] == dst_instance:\n                            instance_assignments[peak_id] = src_instance\n\n    if min_instance_peaks &gt; 0:\n        if isinstance(min_instance_peaks, float):\n            if n_nodes is None:\n                # Infer number of nodes if not specified.\n                all_node_types = set()\n                for edge_type in connections:\n                    all_node_types.add(edge_type.src_node_ind)\n                    all_node_types.add(edge_type.dst_node_ind)\n                n_nodes = len(all_node_types)\n\n            # Calculate minimum threshold.\n            min_instance_peaks = int(min_instance_peaks * n_nodes)\n\n        # Compute instance peak counts.\n        instance_ids, instance_peak_counts = np.unique(\n            list(instance_assignments.values()), return_counts=True\n        )\n        instance_peak_counts = {\n            instance: peaks_count\n            for instance, peaks_count in zip(instance_ids, instance_peak_counts)\n        }\n\n        # Filter out small instances.\n        instance_assignments = {\n            peak_id: instance\n            for peak_id, instance in instance_assignments.items()\n            if instance_peak_counts[instance] &gt;= min_instance_peaks\n        }\n\n    return instance_assignments\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.compute_distance_penalty","title":"<code>compute_distance_penalty(spatial_vec_lengths, max_edge_length, dist_penalty_weight=1.0)</code>","text":"<p>Compute the distance penalty component of the PAF line integral score.</p> <p>Parameters:</p> Name Type Description Default <code>spatial_vec_lengths</code> <code>Tensor</code> <p>Euclidean distance between candidate source and destination points as a <code>torch.float32</code> tensor of any shape (typically <code>(n_candidates, 1)</code>).</p> required <code>max_edge_length</code> <code>float</code> <p>Maximum length expected for any connection as a scalar <code>float</code> in units of pixels (corresponding to <code>peaks_sample</code>). Scores of lines longer than this will be penalized. Useful for ignoring spurious connections that are far apart in space.</p> required <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The distance penalty for each candidate as a <code>torch.float32</code> tensor of the same shape as <code>spatial_vec_lengths</code>.</p> <p>The penalty will be 0 (when below the threshold) and -1 as the distance approaches infinity. This is then scaled by the <code>dist_penalty_weight</code>.</p> Notes <p>The penalty is computed from the distances scaled by the max length:</p> <pre><code>if distance &lt;= max_edge_length:\n    penalty = 0\nelse:\n    penalty = (max_edge_length / distance) - 1\n</code></pre> <p>For example, if the max length is 10 and the distance is 20, then the penalty will be: <code>(10 / 20) - 1 == 0.5 - 1 == -0.5</code>.</p> <p>See also: score_paf_lines</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def compute_distance_penalty(\n    spatial_vec_lengths: torch.Tensor,\n    max_edge_length: float,\n    dist_penalty_weight: float = 1.0,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the distance penalty component of the PAF line integral score.\n\n    Args:\n        spatial_vec_lengths: Euclidean distance between candidate source and\n            destination points as a `torch.float32` tensor of any shape (typically\n            `(n_candidates, 1)`).\n        max_edge_length: Maximum length expected for any connection as a scalar `float`\n            in units of pixels (corresponding to `peaks_sample`). Scores of lines\n            longer than this will be penalized. Useful for ignoring spurious\n            connections that are far apart in space.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n\n    Returns:\n        The distance penalty for each candidate as a `torch.float32` tensor of the same\n        shape as `spatial_vec_lengths`.\n\n        The penalty will be 0 (when below the threshold) and -1 as the distance\n        approaches infinity. This is then scaled by the `dist_penalty_weight`.\n\n    Notes:\n        The penalty is computed from the distances scaled by the max length:\n\n        ```\n        if distance &lt;= max_edge_length:\n            penalty = 0\n        else:\n            penalty = (max_edge_length / distance) - 1\n        ```\n\n        For example, if the max length is 10 and the distance is 20, then the penalty\n        will be: `(10 / 20) - 1 == 0.5 - 1 == -0.5`.\n\n    See also: score_paf_lines\n    \"\"\"\n    penalty = torch.clamp((max_edge_length / spatial_vec_lengths) - 1, max=0)\n    return penalty * dist_penalty_weight\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.get_connection_candidates","title":"<code>get_connection_candidates(peak_channel_inds_sample, skeleton_edges, n_nodes)</code>","text":"<p>Find the indices of all the possible connections formed by the detected peaks.</p> <p>Parameters:</p> Name Type Description Default <code>peak_channel_inds_sample</code> <code>Tensor</code> <p>The channel indices of the peaks found in a sample. This is a <code>torch.Tensor</code> of shape <code>(n_peaks,)</code> and dtype <code>torch.int32</code> that is used to represent a detected peak by its channel/node index in the skeleton.</p> required <code>skeleton_edges</code> <code>Tensor</code> <p>The indices of the nodes that form the skeleton graph as a <code>torch.Tensor</code> of shape <code>(n_edges, 2)</code> and dtype <code>torch.int32</code> where each row corresponds to the source and destination node indices.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton as a scalar integer.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of <code>(edge_inds, edge_peak_inds)</code>.</p> <p><code>edge_inds</code> is a <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> indicating the indices of the edge that each of the candidate connections belongs to.</p> <p><code>edge_peak_inds</code> is a <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peak_channel_inds_sample</code>.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def get_connection_candidates(\n    peak_channel_inds_sample: torch.Tensor, skeleton_edges: torch.Tensor, n_nodes: int\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the indices of all the possible connections formed by the detected peaks.\n\n    Args:\n        peak_channel_inds_sample: The channel indices of the peaks found in a sample.\n            This is a `torch.Tensor` of shape `(n_peaks,)` and dtype `torch.int32` that is\n            used to represent a detected peak by its channel/node index in the skeleton.\n        skeleton_edges: The indices of the nodes that form the skeleton graph as a\n            `torch.Tensor` of shape `(n_edges, 2)` and dtype `torch.int32` where each row\n            corresponds to the source and destination node indices.\n        n_nodes: The total number of nodes in the skeleton as a scalar integer.\n\n    Returns:\n        A tuple of `(edge_inds, edge_peak_inds)`.\n\n        `edge_inds` is a `torch.Tensor` of shape `(n_candidates,)` indicating the indices\n        of the edge that each of the candidate connections belongs to.\n\n        `edge_peak_inds` is a `torch.Tensor` of shape `(n_candidates, 2)` with the indices\n        of the peaks that form the source and destination of each candidate connection.\n        This indexes into the input `peak_channel_inds_sample`.\n    \"\"\"\n    peak_inds = torch.argsort(peak_channel_inds_sample)\n    node_inds = torch.gather(peak_channel_inds_sample, 0, peak_inds)\n    node_grouped_peak_inds = [\n        peak_inds[node_inds == k] for k in range(n_nodes)\n    ]  # (n_nodes, (n_peaks_k))\n    edge_grouped_peak_inds = [\n        (node_grouped_peak_inds[src], node_grouped_peak_inds[dst])\n        for src, dst in skeleton_edges\n    ]  # (n_edges, (n_src_peaks), (n_dst_peaks))\n\n    edge_inds = []  # (n_edges, (n_src * n_dst))\n    edge_peak_inds = []  # (n_edges, (n_src * n_dst), 2)\n    for k, (src_peaks, dst_peaks) in enumerate(edge_grouped_peak_inds):\n        grid_src, grid_dst = torch.meshgrid(src_peaks, dst_peaks, indexing=\"ij\")\n        grid_src_dst = torch.stack([grid_src.flatten(), grid_dst.flatten()], dim=1)\n\n        edge_inds.append(torch.full((grid_src_dst.size(0),), k, dtype=torch.int32))\n        edge_peak_inds.append(grid_src_dst)\n\n    edge_inds = torch.cat(edge_inds)  # (n_candidates,)\n    edge_peak_inds = torch.cat(edge_peak_inds)  # (n_candidates, 2)\n\n    return edge_inds, edge_peak_inds\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.get_paf_lines","title":"<code>get_paf_lines(pafs_sample, peaks_sample, edge_peak_inds, edge_inds, n_line_points, pafs_stride)</code>","text":"<p>Get the PAF values at the lines formed between all detected peaks in a sample.</p> <p>Parameters:</p> Name Type Description Default <code>pafs_sample</code> <code>Tensor</code> <p>The PAFs for the sample as a <code>torch.Tensor</code> of shape <code>(height, width, 2 * n_edges)</code>.</p> required <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale (they will be scaled by the <code>pafs_stride</code>).</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peaks_sample</code>. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>edge_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>n_line_points</code> <code>int</code> <p>The number of points to interpolate between source and destination peaks in each connection candidate as a scalar integer. Values ranging from 5 to 10 are pretty reasonable.</p> required <code>pafs_stride</code> <code>int</code> <p>The stride (1/scale) of the PAFs that these lines will need to index into relative to the image. Coordinates in <code>peaks_sample</code> will be divided by this value to adjust the indexing into the PAFs tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The PAF vectors at all of the line points as a <code>torch.Tensor</code> of shape <code>(n_candidates, n_line_points, 2, 3)</code> and dtype <code>torch.int32</code>.</p> <p>The last dimension of the line subscripts correspond to the full <code>[row, col, channel]</code> subscripts of each element of the lines. Axis -2 contains the same <code>[row, col]</code> for each line but <code>channel</code> is adjusted to match the channels in the PAFs tensor.</p> Notes <p>If only the subscripts are needed, use <code>make_line_subs()</code> to generate the lines without retrieving the PAF vector at the line points.</p> <p>See also: get_connection_candidates, make_line_subs, score_paf_lines</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def get_paf_lines(\n    pafs_sample: torch.Tensor,\n    peaks_sample: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    edge_inds: torch.Tensor,\n    n_line_points: int,\n    pafs_stride: int,\n) -&gt; torch.Tensor:\n    \"\"\"Get the PAF values at the lines formed between all detected peaks in a sample.\n\n    Args:\n        pafs_sample: The PAFs for the sample as a `torch.Tensor` of shape\n            `(height, width, 2 * n_edges)`.\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale (they will be scaled by the `pafs_stride`).\n        edge_peak_inds: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype `torch.int32`\n            with the indices of the peaks that form the source and destination of each\n            candidate connection. This indexes into the input `peaks_sample`. Can be\n            generated using `get_connection_candidates()`.\n        edge_inds: A `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.int32`\n            indicating the indices of the edge that each of the candidate connections\n            belongs to. Can be generated using `get_connection_candidates()`.\n        n_line_points: The number of points to interpolate between source and\n            destination peaks in each connection candidate as a scalar integer. Values\n            ranging from 5 to 10 are pretty reasonable.\n        pafs_stride: The stride (1/scale) of the PAFs that these lines will need to\n            index into relative to the image. Coordinates in `peaks_sample` will be\n            divided by this value to adjust the indexing into the PAFs tensor.\n\n    Returns:\n        The PAF vectors at all of the line points as a `torch.Tensor` of shape\n        `(n_candidates, n_line_points, 2, 3)` and dtype `torch.int32`.\n\n        The last dimension of the line subscripts correspond to the full\n        `[row, col, channel]` subscripts of each element of the lines. Axis -2 contains\n        the same `[row, col]` for each line but `channel` is adjusted to match the\n        channels in the PAFs tensor.\n\n    Notes:\n        If only the subscripts are needed, use `make_line_subs()` to generate the lines\n        without retrieving the PAF vector at the line points.\n\n    See also: get_connection_candidates, make_line_subs, score_paf_lines\n    \"\"\"\n    pafs_hw = pafs_sample.shape[:2]\n    line_subs = make_line_subs(\n        peaks_sample, edge_peak_inds, edge_inds, n_line_points, pafs_stride, pafs_hw\n    )\n    lines = pafs_sample[line_subs[..., 0], line_subs[..., 1], line_subs[..., 2]]\n    return lines\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.group_instances_batch","title":"<code>group_instances_batch(peaks, peak_vals, peak_channel_inds, match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores, n_nodes, sorted_edge_inds, edge_types, min_instance_peaks, min_line_scores=0.25)</code>","text":"<p>Group matched connections into full instances for a batch.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <code>Tensor</code> <p>The sample-grouped detected peaks in a batch as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks), 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>peak_vals</code> <code>Tensor</code> <p>The sample-grouped scores of the detected peaks in a batch as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>The sample-grouped indices of the channel (node) that each detected peak is associated with as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_peaks))</code> and dtype <code>torch.int32</code>.</p> required <code>match_edge_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the skeleton edge that each connection corresponds to as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>match_src_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the source peaks that form each connection as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in each sample. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>match_dst_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the destination peaks that form each connection as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>match_line_scores</code> <code>Tensor</code> <p>Sample-grouped PAF line scores of the matched connections as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>. This can be generated by <code>match_candidates_batch()</code>.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton as a scalar integer.</p> required <code>sorted_edge_inds</code> <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> required <code>edge_types</code> <code>List[EdgeType]</code> <p>A torch nested <code>EdgeType</code>s associated with the skeleton.</p> required <code>min_instance_peaks</code> <code>int</code> <p>If this is greater than 0, grouped instances with fewer assigned peaks than this threshold will be excluded. If a <code>float</code> in the range <code>(0., 1.]</code> is provided, this is interpreted as a fraction of the total number of nodes in the skeleton. If an <code>int</code> is provided, this is the absolute minimum number of peaks.</p> required <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple of <code>torch.Tensor</code> with the grouped instances for the whole batch grouped by sample:</p> <p><code>predicted_instances</code>: The sample- and instance-grouped coordinates for each instance as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes, 2)</code> and dtype <code>torch.float32</code>. Missing peaks are represented by <code>NaN</code>s.</p> <p><code>predicted_peak_scores</code>: The sample- and instance-grouped confidence map values for each peak as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances), n_nodes)</code> and dtype <code>torch.float32</code>.</p> <p><code>predicted_instance_scores</code>: The sample-grouped instance grouping score for each instance as a torch nested <code>torch.Tensor</code> of shape <code>(n_samples, (n_instances))</code> and dtype <code>torch.float32</code>.</p> <p>See also: match_candidates_batch, group_instances_sample</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def group_instances_batch(\n    peaks: torch.Tensor,\n    peak_vals: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    match_edge_inds: torch.Tensor,\n    match_src_peak_inds: torch.Tensor,\n    match_dst_peak_inds: torch.Tensor,\n    match_line_scores: torch.Tensor,\n    n_nodes: int,\n    sorted_edge_inds: Tuple[int],\n    edge_types: List[EdgeType],\n    min_instance_peaks: int,\n    min_line_scores: float = 0.25,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Group matched connections into full instances for a batch.\n\n    Args:\n        peaks: The sample-grouped detected peaks in a batch as a torch nested `torch.Tensor` of\n            shape `(n_samples, (n_peaks), 2)` and dtype `torch.float32`. These should be\n            `(x, y)` coordinates of each peak in the image scale.\n        peak_vals: The sample-grouped scores of the detected peaks in a batch as a\n            torch nested `torch.Tensor` of shape `(n_samples, (n_peaks))` and dtype `torch.float32`.\n        peak_channel_inds: The sample-grouped indices of the channel (node) that each\n            detected peak is associated with as a torch nested `torch.Tensor` of shape\n            `(n_samples, (n_peaks))` and dtype `torch.int32`.\n        match_edge_inds: Sample-grouped indices of the skeleton edge that each\n            connection corresponds to as a torch nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. This can be generated\n            by `match_candidates_batch()`.\n        match_src_peak_inds: Sample-grouped indices of the source peaks that form each\n            connection as a torch nested `torch.Tensor` of shape `(n_samples, (n_connections))`\n            and dtype `torch.int32`. Important: These indices correspond to the\n            edge-grouped peaks, not the set of all peaks in each sample. This can be\n            generated by `match_candidates_batch()`.\n        match_dst_peak_inds: Sample-grouped indices of the destination peaks that form\n            each connection as a torch nested `torch.Tensor` of shape\n            `(n_samples, (n_connections))` and dtype `torch.int32`. Important: These\n            indices correspond to the edge-grouped peaks, not the set of all peaks in\n            the sample. This can be generated by `match_candidates_batch()`.\n        match_line_scores: Sample-grouped PAF line scores of the matched connections as\n            a torch nested `torch.Tensor` of shape `(n_samples, (n_connections))` and dtype\n            `torch.float32`. This can be generated by `match_candidates_batch()`.\n        n_nodes: The total number of nodes in the skeleton as a scalar integer.\n        sorted_edge_inds: A tuple of indices specifying the topological order that the\n            edge types should be accessed in during instance assembly\n            (`assign_connections_to_instances`).\n        edge_types: A torch nested `EdgeType`s associated with the skeleton.\n        min_instance_peaks: If this is greater than 0, grouped instances with fewer\n            assigned peaks than this threshold will be excluded. If a `float` in the\n            range `(0., 1.]` is provided, this is interpreted as a fraction of the total\n            number of nodes in the skeleton. If an `int` is provided, this is the\n            absolute minimum number of peaks.\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs.\n\n    Returns:\n        A tuple of `torch.Tensor` with the grouped instances for the whole batch grouped by\n        sample:\n\n        `predicted_instances`: The sample- and instance-grouped coordinates for each\n        instance as a torch nested `torch.Tensor` of shape `(n_samples, (n_instances), n_nodes, 2)`\n        and dtype `torch.float32`. Missing peaks are represented by `NaN`s.\n\n        `predicted_peak_scores`: The sample- and instance-grouped confidence map values\n        for each peak as a torch nested `torch.Tensor` of shape `(n_samples, (n_instances), n_nodes)` and dtype\n        `torch.float32`.\n\n        `predicted_instance_scores`: The sample-grouped instance grouping score for each\n        instance as a torch nested `torch.Tensor` of shape `(n_samples, (n_instances))` and dtype\n        `torch.float32`.\n\n    See also: match_candidates_batch, group_instances_sample\n    \"\"\"\n    n_samples = len(peaks)\n    predicted_instances_batch = []\n    predicted_peak_scores_batch = []\n    predicted_instance_scores_batch = []\n\n    for sample in range(n_samples):\n        (\n            predicted_instances_sample,\n            predicted_peak_scores_sample,\n            predicted_instance_scores_sample,\n        ) = group_instances_sample(\n            peaks[sample],\n            peak_vals[sample],\n            peak_channel_inds[sample],\n            match_edge_inds[sample],\n            match_src_peak_inds[sample],\n            match_dst_peak_inds[sample],\n            match_line_scores[sample],\n            n_nodes,\n            sorted_edge_inds,\n            edge_types,\n            min_instance_peaks,\n            min_line_scores,\n        )\n\n        predicted_instances_batch.append(torch.tensor(predicted_instances_sample))\n        predicted_peak_scores_batch.append(torch.tensor(predicted_peak_scores_sample))\n        predicted_instance_scores_batch.append(\n            torch.tensor(predicted_instance_scores_sample)\n        )\n\n    return (\n        predicted_instances_batch,\n        predicted_peak_scores_batch,\n        predicted_instance_scores_batch,\n    )\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.group_instances_sample","title":"<code>group_instances_sample(peaks_sample, peak_scores_sample, peak_channel_inds_sample, match_edge_inds_sample, match_src_peak_inds_sample, match_dst_peak_inds_sample, match_line_scores_sample, n_nodes, sorted_edge_inds, edge_types, min_instance_peaks, min_line_scores=0.25)</code>","text":"<p>Group matched connections into full instances for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>peak_scores_sample</code> <code>Tensor</code> <p>The scores of the detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks,)</code> and dtype <code>torch.float32</code>.</p> required <code>peak_channel_inds_sample</code> <code>Tensor</code> <p>The indices of the channel (node) that each detected peak is associated with as a <code>torch.Tensor</code> of shape <code>(n_peaks,)</code> and dtype <code>torch.int32</code>.</p> required <code>match_edge_inds_sample</code> <code>Tensor</code> <p>Indices of the skeleton edge that each connection corresponds to as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>match_src_peak_inds_sample</code> <code>Tensor</code> <p>Indices of the source peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>match_dst_peak_inds_sample</code> <code>Tensor</code> <p>Indices of the destination peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>match_line_scores_sample</code> <code>Tensor</code> <p>PAF line scores of the matched connections as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.float32</code>. This can be generated by <code>match_candidates_sample()</code>.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton as a scalar integer.</p> required <code>sorted_edge_inds</code> <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> required <code>edge_types</code> <code>List[EdgeType]</code> <p>A list of <code>EdgeType</code>s associated with the skeleton.</p> required <code>min_instance_peaks</code> <code>int</code> <p>If this is greater than 0, grouped instances with fewer assigned peaks than this threshold will be excluded. If a <code>float</code> in the range <code>(0., 1.]</code> is provided, this is interpreted as a fraction of the total number of nodes in the skeleton. If an <code>int</code> is provided, this is the absolute minimum number of peaks.</p> required <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>A tuple of arrays with the grouped instances:</p> <p><code>predicted_instances</code>: The grouped coordinates for each instance as an array of shape <code>(n_instances, n_nodes, 2)</code> and dtype <code>float32</code>. Missing peaks are represented by <code>np.nan</code>s.</p> <p><code>predicted_peak_scores</code>: The confidence map values for each peak as an array of <code>(n_instances, n_nodes)</code> and dtype <code>float32</code>.</p> <p><code>predicted_instance_scores</code>: The grouping score for each instance as an array of shape <code>(n_instances,)</code> and dtype <code>float32</code>.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def group_instances_sample(\n    peaks_sample: torch.Tensor,\n    peak_scores_sample: torch.Tensor,\n    peak_channel_inds_sample: torch.Tensor,\n    match_edge_inds_sample: torch.Tensor,\n    match_src_peak_inds_sample: torch.Tensor,\n    match_dst_peak_inds_sample: torch.Tensor,\n    match_line_scores_sample: torch.Tensor,\n    n_nodes: int,\n    sorted_edge_inds: Tuple[int],\n    edge_types: List[EdgeType],\n    min_instance_peaks: int,\n    min_line_scores: float = 0.25,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Group matched connections into full instances for a single sample.\n\n    Args:\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale.\n        peak_scores_sample: The scores of the detected peaks in a sample as a\n            `torch.Tensor` of shape `(n_peaks,)` and dtype `torch.float32`.\n        peak_channel_inds_sample: The indices of the channel (node) that each detected\n            peak is associated with as a `torch.Tensor` of shape `(n_peaks,)` and dtype\n            `torch.int32`.\n        match_edge_inds_sample: Indices of the skeleton edge that each connection\n            corresponds to as a `torch.Tensor` of shape `(n_connections,)` and dtype\n            `torch.int32`. This can be generated by `match_candidates_sample()`.\n        match_src_peak_inds_sample: Indices of the source peaks that form each\n            connection as a `torch.Tensor` of shape `(n_connections,)` and dtype\n            `torch.int32`. Important: These indices correspond to the edge-grouped peaks,\n            not the set of all peaks in the sample. This can be generated by\n            `match_candidates_sample()`.\n        match_dst_peak_inds_sample: Indices of the destination peaks that form each\n            connection as a `torch.Tensor` of shape `(n_connections,)` and dtype\n            `torch.int32`. Important: These indices correspond to the edge-grouped peaks,\n            not the set of all peaks in the sample. This can be generated by\n            `match_candidates_sample()`.\n        match_line_scores_sample: PAF line scores of the matched connections as a\n            `torch.Tensor` of shape `(n_connections,)` and dtype `torch.float32`. This can be\n            generated by `match_candidates_sample()`.\n        n_nodes: The total number of nodes in the skeleton as a scalar integer.\n        sorted_edge_inds: A tuple of indices specifying the topological order that the\n            edge types should be accessed in during instance assembly\n            (`assign_connections_to_instances`).\n        edge_types: A list of `EdgeType`s associated with the skeleton.\n        min_instance_peaks: If this is greater than 0, grouped instances with fewer\n            assigned peaks than this threshold will be excluded. If a `float` in the\n            range `(0., 1.]` is provided, this is interpreted as a fraction of the total\n            number of nodes in the skeleton. If an `int` is provided, this is the\n            absolute minimum number of peaks.\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs.\n\n    Returns:\n        A tuple of arrays with the grouped instances:\n\n        `predicted_instances`: The grouped coordinates for each instance as an array of\n        shape `(n_instances, n_nodes, 2)` and dtype `float32`. Missing peaks are\n        represented by `np.nan`s.\n\n        `predicted_peak_scores`: The confidence map values for each peak as an array of\n        `(n_instances, n_nodes)` and dtype `float32`.\n\n        `predicted_instance_scores`: The grouping score for each instance as an array of\n        shape `(n_instances,)` and dtype `float32`.\n    \"\"\"\n    # Convert PyTorch tensors to NumPy arrays for non-tensor computations\n    if isinstance(peaks_sample, torch.Tensor):\n        peaks_sample = peaks_sample.cpu().numpy()\n        peak_scores_sample = peak_scores_sample.cpu().numpy()\n        peak_channel_inds_sample = peak_channel_inds_sample.cpu().numpy()\n        match_edge_inds_sample = match_edge_inds_sample.cpu().numpy()\n        match_src_peak_inds_sample = match_src_peak_inds_sample.cpu().numpy()\n        match_dst_peak_inds_sample = match_dst_peak_inds_sample.cpu().numpy()\n        match_line_scores_sample = match_line_scores_sample.cpu().numpy()\n\n    # Filter out low scoring matches.\n    is_valid_match = match_line_scores_sample &gt;= min_line_scores\n    match_edge_inds_sample = match_edge_inds_sample[is_valid_match]\n    match_src_peak_inds_sample = match_src_peak_inds_sample[is_valid_match]\n    match_dst_peak_inds_sample = match_dst_peak_inds_sample[is_valid_match]\n    match_line_scores_sample = match_line_scores_sample[is_valid_match]\n\n    # Group peaks by channel.\n    peaks = []\n    peak_scores = []\n    for i in range(n_nodes):\n        in_channel = peak_channel_inds_sample == i\n        peaks.append(peaks_sample[in_channel])\n        peak_scores.append(peak_scores_sample[in_channel])\n\n    # Group connection data by edge in sorted order.\n    # Note: This step is crucial since the instance assembly depends on the ordering\n    # of the edges.\n    connections = {}\n    for edge_ind in sorted_edge_inds:\n        in_edge = match_edge_inds_sample == edge_ind\n        edge_type = edge_types[edge_ind]\n\n        src_peak_inds = match_src_peak_inds_sample[in_edge]\n        dst_peak_inds = match_dst_peak_inds_sample[in_edge]\n        line_scores = match_line_scores_sample[in_edge]\n\n        connections[edge_type] = [\n            EdgeConnection(src, dst, score)\n            for src, dst, score in zip(src_peak_inds, dst_peak_inds, line_scores)\n        ]\n\n    # Bipartite graph partitioning to group connections into instances.\n    instance_assignments = assign_connections_to_instances(\n        connections,\n        min_instance_peaks=min_instance_peaks,\n        n_nodes=n_nodes,\n    )\n\n    # Gather the data by instance.\n    (\n        predicted_instances,\n        predicted_peak_scores,\n        predicted_instance_scores,\n    ) = make_predicted_instances(peaks, peak_scores, connections, instance_assignments)\n\n    return predicted_instances, predicted_peak_scores, predicted_instance_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.make_line_subs","title":"<code>make_line_subs(peaks_sample, edge_peak_inds, edge_inds, n_line_points, pafs_stride, pafs_hw)</code>","text":"<p>Create the lines between candidate connections for evaluating the PAFs.</p> <p>Parameters:</p> Name Type Description Default <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale (they will be scaled by the <code>pafs_stride</code>).</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peaks_sample</code>. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>edge_inds</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>n_line_points</code> <code>int</code> <p>The number of points to interpolate between source and destination peaks in each connection candidate as a scalar integer. Values ranging from 5 to 10 are pretty reasonable.</p> required <code>pafs_stride</code> <code>int</code> <p>The stride (1/scale) of the PAFs that these lines will need to index into relative to the image. Coordinates in <code>peaks_sample</code> will be divided by this value to adjust the indexing into the PAFs tensor.</p> required <code>pafs_hw</code> <code>tuple</code> <p>Tuple (height, width) with the dimension of PAFs tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The line subscripts as a <code>torch.Tensor</code> of shape <code>(n_candidates, n_line_points, 2, 3)</code> and dtype <code>torch.int32</code>.</p> <p>The last dimension of the line subscripts correspond to the full <code>[row, col, channel]</code> subscripts of each element of the lines. Axis -2 contains the same <code>[row, col]</code> for each line but <code>channel</code> is adjusted to match the channels in the PAFs tensor.</p> Notes <p>The subscripts are interpolated via nearest neighbor, so multiple fractional coordinates may map on to the same pixel if the line is short.</p> <p>See also: get_connection_candidates</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def make_line_subs(\n    peaks_sample: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    edge_inds: torch.Tensor,\n    n_line_points: int,\n    pafs_stride: int,\n    pafs_hw: tuple,\n) -&gt; torch.Tensor:\n    \"\"\"Create the lines between candidate connections for evaluating the PAFs.\n\n    Args:\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale (they will be scaled by the `pafs_stride`).\n        edge_peak_inds: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype `torch.int32`\n            with the indices of the peaks that form the source and destination of each\n            candidate connection. This indexes into the input `peaks_sample`. Can be\n            generated using `get_connection_candidates()`.\n        edge_inds: A `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.int32`\n            indicating the indices of the edge that each of the candidate connections\n            belongs to. Can be generated using `get_connection_candidates()`.\n        n_line_points: The number of points to interpolate between source and\n            destination peaks in each connection candidate as a scalar integer. Values\n            ranging from 5 to 10 are pretty reasonable.\n        pafs_stride: The stride (1/scale) of the PAFs that these lines will need to\n            index into relative to the image. Coordinates in `peaks_sample` will be\n            divided by this value to adjust the indexing into the PAFs tensor.\n        pafs_hw: Tuple (height, width) with the dimension of PAFs tensor.\n\n    Returns:\n        The line subscripts as a `torch.Tensor` of shape\n        `(n_candidates, n_line_points, 2, 3)` and dtype `torch.int32`.\n\n        The last dimension of the line subscripts correspond to the full\n        `[row, col, channel]` subscripts of each element of the lines. Axis -2 contains\n        the same `[row, col]` for each line but `channel` is adjusted to match the\n        channels in the PAFs tensor.\n\n    Notes:\n        The subscripts are interpolated via nearest neighbor, so multiple fractional\n        coordinates may map on to the same pixel if the line is short.\n\n    See also: get_connection_candidates\n    \"\"\"\n    src_peaks = torch.index_select(peaks_sample, 0, edge_peak_inds[:, 0])\n    dst_peaks = torch.index_select(peaks_sample, 0, edge_peak_inds[:, 1])\n    n_candidates = torch.tensor(src_peaks.shape[0], device=peaks_sample.device)\n\n    X = torch.cat(\n        (src_peaks[:, 0].unsqueeze(dim=-1), dst_peaks[:, 0].unsqueeze(dim=-1)), dim=-1\n    ).to(torch.float32)\n    Y = torch.cat(\n        (src_peaks[:, 1].unsqueeze(dim=-1), dst_peaks[:, 1].unsqueeze(dim=-1)), dim=-1\n    ).to(torch.float32)\n    samples = torch.tensor([0, 1], device=X.device).repeat(n_candidates, 1)\n    samples_new = torch.linspace(0, 1, steps=n_line_points, device=X.device).repeat(\n        n_candidates, 1\n    )\n\n    X = interp1d(samples, X, samples_new).unsqueeze(\n        dim=1\n    )  # (n_candidates, 1, n_line_points)\n    Y = interp1d(samples, Y, samples_new).unsqueeze(\n        dim=1\n    )  # (n_candidates, 1, n_line_points)\n    XY = torch.concat([X, Y], dim=1)\n\n    XY = (\n        (XY / pafs_stride).round().int()\n    )  # (n_candidates, 2, n_line_points)  # dim 1 is [x, y]\n    XY = XY[:, [1, 0], :]  # dim 1 is [row, col]\n\n    # clip coordinates for size of pafs tensor.\n    height, width = pafs_hw\n    XY[:, 0] = torch.clip(XY[:, 0], min=0, max=height - 1)\n    XY[:, 1] = torch.clip(XY[:, 1], min=0, max=width - 1)\n\n    edge_inds_expanded = (\n        edge_inds.view(-1, 1, 1)\n        .expand(-1, 1, n_line_points)\n        .to(device=peaks_sample.device)\n    )\n    line_subs = torch.cat((XY, edge_inds_expanded), dim=1)\n    line_subs = line_subs.permute(\n        0, 2, 1\n    )  # (n_candidates, n_line_points, 3) -- last dim is [row, col, edge_ind]\n\n    multiplier = torch.tensor(\n        [1, 1, 2], dtype=torch.int32, device=line_subs.device\n    ).view(1, 1, 3)\n    adder = torch.tensor([0, 0, 1], dtype=torch.int32, device=line_subs.device).view(\n        1, 1, 3\n    )\n\n    line_subs_first = line_subs * multiplier\n    line_subs_second = line_subs * multiplier + adder\n    line_subs = torch.stack(\n        (line_subs_first, line_subs_second), dim=2\n    )  # (n_candidates, n_line_points, 2, 3)\n    # The last dim is [row, col, edge_ind], but for both PAF (x and y) edge channels.\n\n    return line_subs\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.make_predicted_instances","title":"<code>make_predicted_instances(peaks, peak_scores, connections, instance_assignments)</code>","text":"<p>Group peaks by assignments and accumulate scores.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <code>array</code> <p>Node-grouped peaks</p> required <code>peak_scores</code> <code>array</code> <p>Node-grouped peak scores</p> required <code>connections</code> <code>List[EdgeConnection]</code> <p><code>EdgeConnection</code>s grouped by edge type</p> required <code>instance_assignments</code> <code>Dict[PeakID, int]</code> <p><code>PeakID</code> to instance ID mapping</p> required <p>Returns:</p> Type Description <code>Tuple[array, array, array]</code> <p>Tuple of (predicted_instances, predicted_peak_scores, predicted_instance_scores)</p> <p>predicted_instances: (n_instances, n_nodes, 2) array predicted_peak_scores: (n_instances, n_nodes) array predicted_instance_scores: (n_instances,) array</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def make_predicted_instances(\n    peaks: np.array,\n    peak_scores: np.array,\n    connections: List[EdgeConnection],\n    instance_assignments: Dict[PeakID, int],\n) -&gt; Tuple[np.array, np.array, np.array]:\n    \"\"\"Group peaks by assignments and accumulate scores.\n\n    Args:\n        peaks: Node-grouped peaks\n        peak_scores: Node-grouped peak scores\n        connections: `EdgeConnection`s grouped by edge type\n        instance_assignments: `PeakID` to instance ID mapping\n\n    Returns:\n        Tuple of (predicted_instances, predicted_peak_scores, predicted_instance_scores)\n\n        predicted_instances: (n_instances, n_nodes, 2) array\n        predicted_peak_scores: (n_instances, n_nodes) array\n        predicted_instance_scores: (n_instances,) array\n    \"\"\"\n    # Ensure instance IDs are contiguous.\n    instance_ids, instance_inds = np.unique(\n        list(instance_assignments.values()), return_inverse=True\n    )\n    for peak_id, instance_ind in zip(instance_assignments.keys(), instance_inds):\n        instance_assignments[peak_id] = instance_ind\n    n_instances = len(instance_ids)\n\n    # Compute instance scores as the sum of all edge scores.\n    predicted_instance_scores = np.full((n_instances,), 0.0, dtype=\"float32\")\n\n    for edge_type, edge_connections in connections.items():\n        # Loop over all connections for this edge type.\n        for edge_connection in edge_connections:\n            # Look up the source peak.\n            src_peak_id = PeakID(\n                node_ind=edge_type.src_node_ind, peak_ind=edge_connection.src_peak_ind\n            )\n            if src_peak_id in instance_assignments:\n                # Add to the total instance score.\n                instance_ind = instance_assignments[src_peak_id]\n                predicted_instance_scores[instance_ind] += edge_connection.score\n\n                # Sanity check: both peaks in the edge should have been assigned to the\n                # same instance.\n                dst_peak_id = PeakID(\n                    node_ind=edge_type.dst_node_ind,\n                    peak_ind=edge_connection.dst_peak_ind,\n                )\n                assert instance_ind == instance_assignments[dst_peak_id]\n\n    # Fill out instances and peak scores.\n    n_nodes = len(peaks)\n    predicted_instances = np.full((n_instances, n_nodes, 2), np.nan, dtype=\"float32\")\n    predicted_peak_scores = np.full((n_instances, n_nodes), np.nan, dtype=\"float32\")\n    for peak_id, instance_ind in instance_assignments.items():\n        predicted_instances[instance_ind, peak_id.node_ind, :] = peaks[\n            peak_id.node_ind\n        ][peak_id.peak_ind]\n        predicted_peak_scores[instance_ind, peak_id.node_ind] = peak_scores[\n            peak_id.node_ind\n        ][peak_id.peak_ind]\n\n    return predicted_instances, predicted_peak_scores, predicted_instance_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.match_candidates_batch","title":"<code>match_candidates_batch(edge_inds, edge_peak_inds, line_scores, n_edges)</code>","text":"<p>Match candidate connections for a batch based on PAF scores.</p> <p>Parameters:</p> Name Type Description Default <code>edge_inds</code> <code>Tensor</code> <p>Sample-grouped edge indices as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to. Can be generated using <code>score_paf_lines_batch()</code>.</p> required <code>edge_peak_inds</code> <code>Tensor</code> <p>Sample-grouped indices of the peaks that form the source and destination of each candidate connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_candidates), 2)</code> and dtype <code>torch.int32</code>. Can be generated using <code>score_paf_lines_batch()</code>.</p> required <code>line_scores</code> <code>Tensor</code> <p>Sample-grouped scores for each candidate connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_candidates))</code> and dtype <code>torch.float32</code>. Can be generated using <code>score_paf_lines_batch()</code>.</p> required <code>n_edges</code> <code>int</code> <p>A scalar <code>int</code> denoting the number of edges in the skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>The connection peaks for each edge matched based on score as tuple of <code>(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code></p> <p><code>match_edge_inds</code>: Sample-grouped indices of the skeleton edge for each connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>.</p> <p><code>match_src_peak_inds</code>: Sample-grouped indices of the source peaks that form each connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_dst_peak_inds</code>: Sample-grouped indices of the destination peaks that form each connection as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_line_scores</code>: Sample-grouped PAF line scores of the matched connections as a torch nested <code>torch.Tensor</code>s of shape <code>(n_samples, (n_connections))</code> and dtype <code>torch.float32</code>.</p> Notes <p>The matching is performed using the Munkres algorithm implemented in <code>scipy.optimize.linear_sum_assignment()</code>.</p> <p>See also: match_candidates_sample, score_paf_lines_batch, group_instances_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def match_candidates_batch(\n    edge_inds: torch.Tensor,\n    edge_peak_inds: torch.Tensor,\n    line_scores: torch.Tensor,\n    n_edges: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Match candidate connections for a batch based on PAF scores.\n\n    Args:\n        edge_inds: Sample-grouped edge indices as a torch nested `torch.Tensor`s of shape\n            `(n_samples, (n_candidates))` and dtype `torch.int32` indicating the indices\n            of the edge that each of the candidate connections belongs to. Can be\n            generated using `score_paf_lines_batch()`.\n        edge_peak_inds: Sample-grouped indices of the peaks that form the source and\n            destination of each candidate connection as a torch nested `torch.Tensor`s of shape\n            `(n_samples, (n_candidates), 2)` and dtype `torch.int32`. Can be generated\n            using `score_paf_lines_batch()`.\n        line_scores: Sample-grouped scores for each candidate connection as a\n            torch nested `torch.Tensor`s of shape `(n_samples, (n_candidates))` and dtype\n            `torch.float32`. Can be generated using `score_paf_lines_batch()`.\n        n_edges: A scalar `int` denoting the number of edges in the skeleton.\n\n    Returns:\n        The connection peaks for each edge matched based on score as tuple of\n        `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n        `match_edge_inds`: Sample-grouped indices of the skeleton edge for each\n        connection as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))` and\n        dtype `torch.int32`.\n\n        `match_src_peak_inds`: Sample-grouped indices of the source peaks that form each\n        connection as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))` and\n        dtype `torch.int32`. Important: These indices correspond to the edge-grouped peaks,\n        not the set of all peaks in the sample.\n\n        `match_dst_peak_inds`: Sample-grouped indices of the destination peaks that form\n        each connection as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))`\n        and dtype `torch.int32`. Important: These indices correspond to the edge-grouped\n        peaks, not the set of all peaks in the sample.\n\n        `match_line_scores`: Sample-grouped PAF line scores of the matched connections\n        as a torch nested `torch.Tensor`s of shape `(n_samples, (n_connections))` and dtype\n        `torch.float32`.\n\n    Notes:\n        The matching is performed using the Munkres algorithm implemented in\n        `scipy.optimize.linear_sum_assignment()`.\n\n    See also: match_candidates_sample, score_paf_lines_batch, group_instances_batch\n    \"\"\"\n    match_sample_inds = []\n    match_edge_inds = []\n    match_src_peak_inds = []\n    match_dst_peak_inds = []\n    match_line_scores = []\n\n    for sample in range(len(edge_inds)):\n        edge_inds_sample = edge_inds[sample]\n        edge_peak_inds_sample = edge_peak_inds[sample]\n        line_scores_sample = line_scores[sample]\n\n        matched_sample = match_candidates_sample(\n            edge_inds_sample, edge_peak_inds_sample, line_scores_sample, n_edges\n        )\n\n        (\n            match_edge_inds_sample,\n            match_src_peak_inds_sample,\n            match_dst_peak_inds_sample,\n            match_line_scores_sample,\n        ) = matched_sample\n\n        match_sample_inds.append(\n            torch.full_like(match_edge_inds_sample, sample, dtype=torch.int32)\n        )\n        match_edge_inds.append(match_edge_inds_sample)\n        match_src_peak_inds.append(match_src_peak_inds_sample)\n        match_dst_peak_inds.append(match_dst_peak_inds_sample)\n        match_line_scores.append(match_line_scores_sample)\n\n    return match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.match_candidates_sample","title":"<code>match_candidates_sample(edge_inds_sample, edge_peak_inds_sample, line_scores_sample, n_edges)</code>","text":"<p>Match candidate connections for a sample based on PAF scores.</p> <p>Parameters:</p> Name Type Description Default <code>edge_inds_sample</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.int32</code> indicating the indices of the edge that each of the candidate connections belongs to for the sample. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>edge_peak_inds_sample</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>line_scores_sample</code> <code>Tensor</code> <p>Scores for each candidate connection in the sample as a <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.float32</code>. Can be generated using <code>score_paf_lines()</code>.</p> required <code>n_edges</code> <code>int</code> <p>A scalar <code>int</code> denoting the number of edges in the skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>The connection peaks for each edge matched based on score as tuple of <code>(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)</code></p> <p><code>match_edge_inds</code>: Indices of the skeleton edge that each connection corresponds to as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>.</p> <p><code>match_src_peak_inds</code>: Indices of the source peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_dst_peak_inds</code>: Indices of the destination peaks that form each connection as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.int32</code>. Important: These indices correspond to the edge-grouped peaks, not the set of all peaks in the sample.</p> <p><code>match_line_scores</code>: PAF line scores of the matched connections as a <code>torch.Tensor</code> of shape <code>(n_connections,)</code> and dtype <code>torch.float32</code>.</p> Notes <p>The matching is performed using the Munkres algorithm implemented in <code>scipy.optimize.linear_sum_assignment()</code>.</p> <p>See also: match_candidates_batch</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def match_candidates_sample(\n    edge_inds_sample: torch.Tensor,\n    edge_peak_inds_sample: torch.Tensor,\n    line_scores_sample: torch.Tensor,\n    n_edges: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Match candidate connections for a sample based on PAF scores.\n\n    Args:\n        edge_inds_sample: A `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.int32`\n            indicating the indices of the edge that each of the candidate connections\n            belongs to for the sample. Can be generated using\n            `get_connection_candidates()`.\n        edge_peak_inds_sample: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype\n            `torch.int32` with the indices of the peaks that form the source and\n            destination of each candidate connection. Can be generated using\n            `get_connection_candidates()`.\n        line_scores_sample: Scores for each candidate connection in the sample as a\n            `torch.Tensor` of shape `(n_candidates,)` and dtype `torch.float32`. Can be\n            generated using `score_paf_lines()`.\n        n_edges: A scalar `int` denoting the number of edges in the skeleton.\n\n    Returns:\n        The connection peaks for each edge matched based on score as tuple of\n        `(match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores)`\n\n        `match_edge_inds`: Indices of the skeleton edge that each connection corresponds\n        to as a `torch.Tensor` of shape `(n_connections,)` and dtype `torch.int32`.\n\n        `match_src_peak_inds`: Indices of the source peaks that form each connection\n        as a `torch.Tensor` of shape `(n_connections,)` and dtype `torch.int32`. Important:\n        These indices correspond to the edge-grouped peaks, not the set of all peaks in\n        the sample.\n\n        `match_dst_peak_inds`: Indices of the destination peaks that form each\n        connection as a `torch.Tensor` of shape `(n_connections,)` and dtype `torch.int32`.\n        Important: These indices correspond to the edge-grouped peaks, not the set of\n        all peaks in the sample.\n\n        `match_line_scores`: PAF line scores of the matched connections as a `torch.Tensor`\n        of shape `(n_connections,)` and dtype `torch.float32`.\n\n    Notes:\n        The matching is performed using the Munkres algorithm implemented in\n        `scipy.optimize.linear_sum_assignment()`.\n\n    See also: match_candidates_batch\n    \"\"\"\n    # Move tensors to CPU once to avoid repeated device&lt;-&gt;host synchronizations\n    edge_inds_sample = edge_inds_sample.detach().cpu()\n    edge_peak_inds_sample = edge_peak_inds_sample.detach().cpu()\n    line_scores_sample = line_scores_sample.detach().cpu()\n\n    match_edge_inds = []\n    match_src_peak_inds = []\n    match_dst_peak_inds = []\n    match_line_scores = []\n\n    for k in range(n_edges):\n        is_edge_k = (edge_inds_sample == k).nonzero(as_tuple=True)[0]\n        edge_peak_inds_k = edge_peak_inds_sample[is_edge_k]\n        line_scores_k = line_scores_sample[is_edge_k]\n\n        # Get the unique peak indices.\n        src_peak_inds_k = torch.unique(edge_peak_inds_k[:, 0])\n        dst_peak_inds_k = torch.unique(edge_peak_inds_k[:, 1])\n\n        n_src = src_peak_inds_k.size(0)\n        n_dst = dst_peak_inds_k.size(0)\n\n        # Initialize cost matrix with infinite cost.\n        cost_matrix = torch.full((n_src, n_dst), np.inf)\n\n        # Update cost matrix with line scores.\n        for i, src_ind in enumerate(src_peak_inds_k):\n            for j, dst_ind in enumerate(dst_peak_inds_k):\n                mask = (edge_peak_inds_k[:, 0] == src_ind) &amp; (\n                    edge_peak_inds_k[:, 1] == dst_ind\n                )\n                if mask.any():\n                    # `line_scores_k` is already on CPU; `.item()` does not trigger\n                    # a device synchronization and matches the original behaviour.\n                    cost_matrix[i, j] = -line_scores_k[\n                        mask\n                    ].item()  # Flip sign for maximization.\n\n        # Convert cost matrix to numpy for use with scipy's linear_sum_assignment.\n        cost_matrix_np = cost_matrix.numpy()\n        cost_matrix_np[np.isnan(cost_matrix_np)] = np.inf\n\n        # Match.\n        match_src_inds, match_dst_inds = linear_sum_assignment(cost_matrix_np)\n\n        # Pull out matched scores from the numpy cost matrix.\n        match_line_scores_k = -cost_matrix_np[\n            match_src_inds, match_dst_inds\n        ]  # Flip sign back.\n\n        # Get the peak indices for the matched points (these index into peaks_sample).\n        # These index into the edge-grouped peaks.\n        match_src_peak_inds_k = match_src_inds\n        match_dst_peak_inds_k = match_dst_inds\n\n        # Save.\n        match_edge_inds.append(\n            torch.full((match_src_peak_inds_k.size,), k, dtype=torch.int32)\n        )\n        match_src_peak_inds.append(\n            torch.tensor(match_src_peak_inds_k, dtype=torch.int32)\n        )\n        match_dst_peak_inds.append(\n            torch.tensor(match_dst_peak_inds_k, dtype=torch.int32)\n        )\n        match_line_scores.append(torch.tensor(match_line_scores_k, dtype=torch.float32))\n\n    # Convert lists to tensors.\n    match_edge_inds = torch.cat(match_edge_inds)\n    match_src_peak_inds = torch.cat(match_src_peak_inds)\n    match_dst_peak_inds = torch.cat(match_dst_peak_inds)\n    match_line_scores = torch.cat(match_line_scores)\n\n    return match_edge_inds, match_src_peak_inds, match_dst_peak_inds, match_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.score_paf_lines","title":"<code>score_paf_lines(paf_lines_sample, peaks_sample, edge_peak_inds_sample, max_edge_length, dist_penalty_weight=1.0)</code>","text":"<p>Compute the connectivity score for each PAF line in a sample.</p> <p>Parameters:</p> Name Type Description Default <code>paf_lines_sample</code> <code>Tensor</code> <p>The PAF vectors evaluated at the lines formed between candidate connections as a <code>torch.Tensor</code> of shape <code>(n_candidates, n_line_points, 2, 3)</code> dtype <code>torch.int32</code>. This can be generated by <code>get_paf_lines()</code>.</p> required <code>peaks_sample</code> <code>Tensor</code> <p>The detected peaks in a sample as a <code>torch.Tensor</code> of shape <code>(n_peaks, 2)</code> and dtype <code>torch.float32</code>. These should be <code>(x, y)</code> coordinates of each peak in the image scale.</p> required <code>edge_peak_inds_sample</code> <code>Tensor</code> <p>A <code>torch.Tensor</code> of shape <code>(n_candidates, 2)</code> and dtype <code>torch.int32</code> with the indices of the peaks that form the source and destination of each candidate connection. This indexes into the input <code>peaks_sample</code>. Can be generated using <code>get_connection_candidates()</code>.</p> required <code>max_edge_length</code> <code>float</code> <p>Maximum length expected for any connection as a scalar <code>float</code> in units of pixels (corresponding to <code>peaks_sample</code>). Scores of lines longer than this will be penalized. Useful for ignoring spurious connections that are far apart in space.</p> required <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The line scores as a <code>torch.Tensor</code> of shape <code>(n_candidates,)</code> and dtype <code>torch.float32</code>. Each score value is the average dot product between the PAFs and the normalized displacement vector between source and destination peaks.</p> <p>Scores range from roughly -1.5 to 1.0, where larger values indicate a better connectivity score for the candidate. Values can be larger or smaller due to prediction error.</p> Notes <p>This function operates on a single sample (frame). For batches of multiple frames, use <code>score_paf_lines_batch()</code>.</p> <p>See also: get_paf_lines, score_paf_lines_batch, compute_distance_penalty</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def score_paf_lines(\n    paf_lines_sample: torch.Tensor,\n    peaks_sample: torch.Tensor,\n    edge_peak_inds_sample: torch.Tensor,\n    max_edge_length: float,\n    dist_penalty_weight: float = 1.0,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the connectivity score for each PAF line in a sample.\n\n    Args:\n        paf_lines_sample: The PAF vectors evaluated at the lines formed between\n            candidate connections as a `torch.Tensor` of shape\n            `(n_candidates, n_line_points, 2, 3)` dtype `torch.int32`. This can be\n            generated by `get_paf_lines()`.\n        peaks_sample: The detected peaks in a sample as a `torch.Tensor` of shape\n            `(n_peaks, 2)` and dtype `torch.float32`. These should be `(x, y)` coordinates\n            of each peak in the image scale.\n        edge_peak_inds_sample: A `torch.Tensor` of shape `(n_candidates, 2)` and dtype\n            `torch.int32` with the indices of the peaks that form the source and\n            destination of each candidate connection. This indexes into the input\n            `peaks_sample`. Can be generated using `get_connection_candidates()`.\n        max_edge_length: Maximum length expected for any connection as a scalar `float`\n            in units of pixels (corresponding to `peaks_sample`). Scores of lines\n            longer than this will be penalized. Useful for ignoring spurious\n            connections that are far apart in space.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n\n    Returns:\n        The line scores as a `torch.Tensor` of shape `(n_candidates,)` and dtype\n        `torch.float32`. Each score value is the average dot product between the PAFs and\n        the normalized displacement vector between source and destination peaks.\n\n        Scores range from roughly -1.5 to 1.0, where larger values indicate a better\n        connectivity score for the candidate. Values can be larger or smaller due to\n        prediction error.\n\n    Notes:\n        This function operates on a single sample (frame). For batches of multiple\n        frames, use `score_paf_lines_batch()`.\n\n    See also: get_paf_lines, score_paf_lines_batch, compute_distance_penalty\n    \"\"\"\n    # Pull out points using advanced indexing\n    src_peaks = peaks_sample[edge_peak_inds_sample[:, 0]]  # (n_candidates, 2)\n    dst_peaks = peaks_sample[edge_peak_inds_sample[:, 1]]  # (n_candidates, 2)\n\n    # Compute normalized spatial displacement vector\n    spatial_vecs = dst_peaks - src_peaks\n    spatial_vec_lengths = torch.norm(\n        spatial_vecs, dim=1, keepdim=True\n    )  # (n_candidates, 1)\n    spatial_vecs = spatial_vecs / spatial_vec_lengths  # Normalize\n\n    # Compute similarity scores\n    spatial_vecs = spatial_vecs.unsqueeze(2)  # Add dimension for matrix multiplication\n    line_scores = torch.squeeze(\n        paf_lines_sample @ spatial_vecs, dim=-1\n    )  # (n_candidates, n_line_points)\n\n    # Compute distance penalties\n    dist_penalties = torch.squeeze(\n        compute_distance_penalty(\n            spatial_vec_lengths,\n            max_edge_length,\n            dist_penalty_weight=dist_penalty_weight,\n        ),\n        dim=1,\n    )  # (n_candidates,)\n\n    # Compute average line scores with distance penalty.\n    mean_line_scores = torch.mean(line_scores, dim=1)\n    penalized_line_scores = mean_line_scores + dist_penalties  # (n_candidates,)\n\n    return penalized_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.score_paf_lines_batch","title":"<code>score_paf_lines_batch(pafs, peaks, peak_channel_inds, skeleton_edges, n_line_points, pafs_stride, max_edge_length_ratio, dist_penalty_weight, n_nodes)</code>","text":"<p>Process a batch of images to score the Part Affinity Fields (PAFs) lines formed between connection candidates for each sample.</p> <p>This function loops over each sample in the batch and applies the process of getting connection candidates, retrieving PAF vectors for each line, and computing the connectivity score for each candidate based on the PAF lines.</p> <p>Parameters:</p> Name Type Description Default <code>pafs</code> <code>Tensor</code> <p>A tensor of shape <code>(n_samples, height, width, 2 * n_edges)</code> containing the part affinity fields for each sample in the batch.</p> required <code>peaks</code> <code>Tensor</code> <p>A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_peaks), 2)</code> containing the (x, y) coordinates of the detected peaks for each sample.</p> required <code>peak_channel_inds</code> <code>Tensor</code> <p>A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_peaks))</code> indicating the channel (node) index that each peak corresponds to.</p> required <code>skeleton_edges</code> <code>Tensor</code> <p>A tensor of shape <code>(n_edges, 2)</code> indicating the indices of the nodes that form each edge of the skeleton.</p> required <code>n_line_points</code> <code>int</code> <p>The number of points used to interpolate between source and destination peaks in each connection candidate.</p> required <code>pafs_stride</code> <code>int</code> <p>The stride (1/scale) of the PAFs relative to the image scale.</p> required <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points relative to the image dimensions.</p> required <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale the weight of the distance penalty applied to the score of each line.</p> required <code>n_nodes</code> <code>int</code> <p>The total number of nodes in the skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple containing three lists for each sample in the batch:     - A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_connections,))</code> indicating the indices       of the edges that each connection corresponds to.     - A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_connections, 2))</code> containing the indices       of the source and destination peaks forming each connection.     - A list of tensors (torch nested tensors) of shape <code>(n_samples, (n_connections,))</code> containing the scores       for each connection based on the PAFs.</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def score_paf_lines_batch(\n    pafs: torch.Tensor,\n    peaks: torch.Tensor,\n    peak_channel_inds: torch.Tensor,\n    skeleton_edges: torch.Tensor,\n    n_line_points: int,\n    pafs_stride: int,\n    max_edge_length_ratio: float,\n    dist_penalty_weight: float,\n    n_nodes: int,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Process a batch of images to score the Part Affinity Fields (PAFs) lines formed between connection candidates for each sample.\n\n    This function loops over each sample in the batch and applies the process of\n    getting connection candidates, retrieving PAF vectors for each line, and\n    computing the connectivity score for each candidate based on the PAF lines.\n\n    Args:\n        pafs: A tensor of shape `(n_samples, height, width, 2 * n_edges)`\n            containing the part affinity fields for each sample in the batch.\n        peaks: A list of tensors (torch nested tensors) of shape `(n_samples, (n_peaks), 2)` containing the\n            (x, y) coordinates of the detected peaks for each sample.\n        peak_channel_inds: A list of tensors (torch nested tensors) of shape `(n_samples, (n_peaks))` indicating\n            the channel (node) index that each peak corresponds to.\n        skeleton_edges: A tensor of shape `(n_edges, 2)` indicating the indices\n            of the nodes that form each edge of the skeleton.\n        n_line_points: The number of points used to interpolate between source\n            and destination peaks in each connection candidate.\n        pafs_stride: The stride (1/scale) of the PAFs relative to the image scale.\n        max_edge_length_ratio: The maximum expected length of a connected pair\n            of points relative to the image dimensions.\n        dist_penalty_weight: A coefficient to scale the weight of the distance\n            penalty applied to the score of each line.\n        n_nodes: The total number of nodes in the skeleton.\n\n    Returns:\n        A tuple containing three lists for each sample in the batch:\n            - A list of tensors (torch nested tensors) of shape `(n_samples, (n_connections,))` indicating the indices\n              of the edges that each connection corresponds to.\n            - A list of tensors (torch nested tensors) of shape `(n_samples, (n_connections, 2))` containing the indices\n              of the source and destination peaks forming each connection.\n            - A list of tensors (torch nested tensors) of shape `(n_samples, (n_connections,))` containing the scores\n              for each connection based on the PAFs.\n    \"\"\"\n    max_edge_length = (\n        max_edge_length_ratio\n        * max(pafs.shape[-1], pafs.shape[-2], pafs.shape[-3])\n        * pafs_stride\n    )\n\n    n_samples = pafs.shape[0]\n    batch_edge_inds = []\n    batch_edge_peak_inds = []\n    batch_line_scores = []\n\n    for sample in range(n_samples):\n        pafs_sample = pafs[sample]\n        peaks_sample = peaks[sample]\n        peak_channel_inds_sample = peak_channel_inds[sample]\n\n        edge_inds_sample, edge_peak_inds_sample = get_connection_candidates(\n            peak_channel_inds_sample, skeleton_edges, n_nodes\n        )\n        paf_lines_sample = get_paf_lines(\n            pafs_sample,\n            peaks_sample,\n            edge_peak_inds_sample,\n            edge_inds_sample,\n            n_line_points,\n            pafs_stride,\n        )\n        line_scores_sample = score_paf_lines(\n            paf_lines_sample,\n            peaks_sample,\n            edge_peak_inds_sample,\n            max_edge_length,\n            dist_penalty_weight=dist_penalty_weight,\n        )\n\n        # Appending as lists to maintain the nested structure.\n        batch_edge_inds.append(edge_inds_sample)\n        batch_edge_peak_inds.append(edge_peak_inds_sample)\n        batch_line_scores.append(line_scores_sample)\n\n    return batch_edge_inds, batch_edge_peak_inds, batch_line_scores\n</code></pre>"},{"location":"api/inference/paf_grouping/#sleap_nn.inference.paf_grouping.toposort_edges","title":"<code>toposort_edges(edge_types)</code>","text":"<p>Find a topological ordering for a list of edge types.</p> <p>Parameters:</p> Name Type Description Default <code>edge_types</code> <code>List[EdgeType]</code> <p>A list of <code>EdgeType</code> instances describing a skeleton.</p> required <p>Returns:</p> Type Description <code>Tuple[int]</code> <p>A tuple of indices specifying the topological order that the edge types should be accessed in during instance assembly (<code>assign_connections_to_instances</code>).</p> <p>This is important to ensure that instances are assembled starting at the root of the skeleton and moving down.</p> <p>See also: assign_connections_to_instances</p> Source code in <code>sleap_nn/inference/paf_grouping.py</code> <pre><code>def toposort_edges(edge_types: List[EdgeType]) -&gt; Tuple[int]:\n    \"\"\"Find a topological ordering for a list of edge types.\n\n    Args:\n        edge_types: A list of `EdgeType` instances describing a skeleton.\n\n    Returns:\n        A tuple of indices specifying the topological order that the edge types should\n        be accessed in during instance assembly (`assign_connections_to_instances`).\n\n        This is important to ensure that instances are assembled starting at the root\n        of the skeleton and moving down.\n\n    See also: assign_connections_to_instances\n    \"\"\"\n    edges = [\n        (edge_type.src_node_ind, edge_type.dst_node_ind) for edge_type in edge_types\n    ]\n    dg = nx.DiGraph(edges)\n    root_ind = next(nx.topological_sort(dg))\n    sorted_edges = nx.bfs_edges(dg, root_ind)\n    sorted_edge_inds = tuple([edges.index(edge) for edge in sorted_edges])\n    return sorted_edge_inds\n</code></pre>"},{"location":"api/inference/peak_finding/","title":"peak_finding","text":""},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding","title":"<code>sleap_nn.inference.peak_finding</code>","text":"<p>Peak finding for inference.</p> <p>Functions:</p> Name Description <code>crop_bboxes</code> <p>Crop bounding boxes from a batch of images using fast tensor indexing.</p> <code>find_global_peaks</code> <p>Find global peaks with optional refinement.</p> <code>find_global_peaks_rough</code> <p>Find the global maximum for each sample and channel.</p> <code>find_local_peaks</code> <p>Find local peaks with optional refinement.</p> <code>find_local_peaks_rough</code> <p>Find local maxima via non-maximum suppression.</p> <code>integral_regression</code> <p>Compute regression by integrating over the confidence maps on a grid.</p> <code>morphological_dilation</code> <p>Apply morphological dilation using max pooling.</p>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.crop_bboxes","title":"<code>crop_bboxes(images, bboxes, sample_inds)</code>","text":"<p>Crop bounding boxes from a batch of images using fast tensor indexing.</p> <p>This uses tensor unfold operations to extract patches, which is significantly faster than kornia's crop_and_resize (17-51x speedup) as it avoids perspective transform computations.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Tensor</code> <p>Tensor of shape (samples, channels, height, width) of a batch of images.</p> required <code>bboxes</code> <code>Tensor</code> <p>Tensor of shape (n_bboxes, 4, 2) and dtype torch.float32, where n_bboxes is the number of centroids, and the second dimension represents the four corner points of the bounding boxes, each with x and y coordinates. The order of the corners follows a clockwise arrangement: top-left, top-right, bottom-right, and bottom-left. This can be generated from centroids using <code>make_centered_bboxes</code>.</p> required <code>sample_inds</code> <code>Tensor</code> <p>Tensor of shape (n_bboxes,) specifying which samples each bounding box should be cropped from.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (n_bboxes, channels, crop_height, crop_width) of the same dtype as the input image. The crop size is inferred from the bounding box coordinates.</p> Notes <p>This function expects bounding boxes with coordinates at the centers of the pixels in the box limits. Technically, the box will span (x1 - 0.5, x2 + 0.5) and (y1 - 0.5, y2 + 0.5).</p> <p>For example, a 3x3 patch centered at (1, 1) would be specified by (y1, x1, y2, x2) = (0, 0, 2, 2). This would be exactly equivalent to indexing the image with <code>image[:, :, 0:3, 0:3]</code>.</p> <p>See also: <code>make_centered_bboxes</code></p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def crop_bboxes(\n    images: torch.Tensor, bboxes: torch.Tensor, sample_inds: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Crop bounding boxes from a batch of images using fast tensor indexing.\n\n    This uses tensor unfold operations to extract patches, which is significantly\n    faster than kornia's crop_and_resize (17-51x speedup) as it avoids perspective\n    transform computations.\n\n    Args:\n        images: Tensor of shape (samples, channels, height, width) of a batch of images.\n        bboxes: Tensor of shape (n_bboxes, 4, 2) and dtype torch.float32, where n_bboxes\n            is the number of centroids, and the second dimension represents the four\n            corner points of the bounding boxes, each with x and y coordinates.\n            The order of the corners follows a clockwise arrangement: top-left,\n            top-right, bottom-right, and bottom-left. This can be generated from\n            centroids using `make_centered_bboxes`.\n        sample_inds: Tensor of shape (n_bboxes,) specifying which samples each bounding\n            box should be cropped from.\n\n    Returns:\n        A tensor of shape (n_bboxes, channels, crop_height, crop_width) of the same\n        dtype as the input image. The crop size is inferred from the bounding box\n        coordinates.\n\n    Notes:\n        This function expects bounding boxes with coordinates at the centers of the\n        pixels in the box limits. Technically, the box will span (x1 - 0.5, x2 + 0.5)\n        and (y1 - 0.5, y2 + 0.5).\n\n        For example, a 3x3 patch centered at (1, 1) would be specified by\n        (y1, x1, y2, x2) = (0, 0, 2, 2). This would be exactly equivalent to indexing\n        the image with `image[:, :, 0:3, 0:3]`.\n\n    See also: `make_centered_bboxes`\n    \"\"\"\n    n_crops = bboxes.shape[0]\n    if n_crops == 0:\n        # Return empty tensor; use default crop size since we can't infer from bboxes\n        return torch.empty(\n            0, images.shape[1], 0, 0, device=images.device, dtype=images.dtype\n        )\n\n    # Compute bounding box size to use for crops.\n    height = int(abs(bboxes[0, 3, 1] - bboxes[0, 0, 1]).item()) + 1\n    width = int(abs(bboxes[0, 1, 0] - bboxes[0, 0, 0]).item()) + 1\n\n    # Store original dtype for conversion back after cropping.\n    original_dtype = images.dtype\n    device = images.device\n    n_samples, channels, img_h, img_w = images.shape\n    half_h, half_w = height // 2, width // 2\n\n    # Pad images for edge handling.\n    images_padded = F.pad(\n        images.float(), (half_w, half_w, half_h, half_h), mode=\"constant\", value=0\n    )\n\n    # Extract all possible patches using unfold (creates a view, no copy).\n    # Shape after unfold: (n_samples, channels, img_h, img_w, height, width)\n    patches = images_padded.unfold(2, height, 1).unfold(3, width, 1)\n\n    # Get crop centers from bboxes.\n    # The bbox top-left is at index 0, with (x, y) coordinates.\n    # We need the center of the crop (peak location), which is top-left + half_size.\n    # Ensure bboxes are on the same device as images for index computation.\n    bboxes_on_device = bboxes.to(device)\n    crop_x = (bboxes_on_device[:, 0, 0] + half_w).to(torch.long)\n    crop_y = (bboxes_on_device[:, 0, 1] + half_h).to(torch.long)\n\n    # Clamp indices to valid bounds to handle edge cases where centroids\n    # might be at or beyond image boundaries.\n    crop_x = torch.clamp(crop_x, 0, patches.shape[3] - 1)\n    crop_y = torch.clamp(crop_y, 0, patches.shape[2] - 1)\n\n    # Select crops using advanced indexing.\n    # Convert sample_inds to tensor if it's a list.\n    if not isinstance(sample_inds, torch.Tensor):\n        sample_inds = torch.tensor(sample_inds, device=device)\n    sample_inds_long = sample_inds.to(device=device, dtype=torch.long)\n    crops = patches[sample_inds_long, :, crop_y, crop_x]\n    # Shape: (n_crops, channels, height, width)\n\n    # Cast back to original dtype and return.\n    crops = crops.to(original_dtype)\n    return crops\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_global_peaks","title":"<code>find_global_peaks(cms, threshold=0.2, refinement=None, integral_patch_size=5)</code>","text":"<p>Find global peaks with optional refinement.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Confidence maps. Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence threshold. Peaks with values below this will ignored.</p> <code>0.2</code> <code>refinement</code> <code>Optional[str]</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression.</p> <code>None</code> <code>integral_patch_size</code> <code>int</code> <p>Size of patches to crop around each rough peak as an integer scalar.</p> <code>5</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals).</p> <p>peak_points: float32 tensor of shape (samples, channels, 2), where the last axis indicates peak locations in xy order.</p> <p>peak_vals: float32 tensor of shape (samples, channels) containing the values at the peak points.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_global_peaks(\n    cms: torch.Tensor,\n    threshold: float = 0.2,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find global peaks with optional refinement.\n\n    Args:\n        cms: Confidence maps. Tensor of shape (samples, channels, height, width).\n        threshold: Minimum confidence threshold. Peaks with values below this will\n            ignored.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression.\n        integral_patch_size: Size of patches to crop around each rough peak as an\n            integer scalar.\n\n    Returns:\n        A tuple of (peak_points, peak_vals).\n\n        peak_points: float32 tensor of shape (samples, channels, 2), where the last axis\n        indicates peak locations in xy order.\n\n        peak_vals: float32 tensor of shape (samples, channels) containing the values at\n        the peak points.\n    \"\"\"\n    # Find grid aligned peaks.\n    rough_peaks, peak_vals = find_global_peaks_rough(\n        cms, threshold=threshold\n    )  # (samples, channels, 2)\n\n    # Return early if not refining or no rough peaks found.\n    if refinement is None or torch.isnan(rough_peaks).all():\n        return rough_peaks, peak_vals\n\n    if refinement == \"integral\":\n        crop_size = integral_patch_size\n    else:\n        return rough_peaks, peak_vals\n\n    # Flatten samples and channels to (n_peaks, 2).\n    samples = cms.size(0)\n    channels = cms.size(1)\n    rough_peaks = rough_peaks.view(samples * channels, 2)\n\n    # Keep only peaks that are not NaNs.\n    valid_idx = torch.where(~torch.isnan(rough_peaks[:, 0]))[0]\n    valid_peaks = rough_peaks[valid_idx]\n\n    # Make bounding boxes for cropping around peaks.\n    bboxes = make_centered_bboxes(\n        valid_peaks, box_height=crop_size, box_width=crop_size\n    )\n\n    # Crop patch around each grid-aligned peak.\n    cms = torch.reshape(\n        cms,\n        [samples * channels, 1, cms.size(2), cms.size(3)],\n    )\n    cm_crops = crop_bboxes(cms, bboxes, valid_idx)\n\n    # Compute offsets via integral regression on a local patch.\n    if refinement == \"integral\":\n        gv = torch.arange(crop_size, dtype=torch.float32) - ((crop_size - 1) / 2)\n        dx_hat, dy_hat = integral_regression(cm_crops, xv=gv, yv=gv)\n        offsets = torch.cat([dx_hat, dy_hat], dim=1)\n\n    # Apply offsets.\n    refined_peaks = rough_peaks.clone()\n    refined_peaks[valid_idx] += offsets\n\n    # Reshape to (samples, channels, 2).\n    refined_peaks = refined_peaks.reshape(samples, channels, 2)\n\n    return refined_peaks, peak_vals\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_global_peaks_rough","title":"<code>find_global_peaks_rough(cms, threshold=0.1)</code>","text":"<p>Find the global maximum for each sample and channel.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Scalar float specifying the minimum confidence value for peaks. Peaks with values below this threshold will be replaced with NaNs.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals). peak_points: float32 tensor of shape (samples, channels, 2), where the last axis indicates peak locations in xy order. peak_vals: float32 tensor of shape (samples, channels) containing the values at the peak points.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_global_peaks_rough(\n    cms: torch.Tensor, threshold: float = 0.1\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Find the global maximum for each sample and channel.\n\n    Args:\n        cms: Tensor of shape (samples, channels, height, width).\n        threshold: Scalar float specifying the minimum confidence value for peaks. Peaks\n            with values below this threshold will be replaced with NaNs.\n\n    Returns:\n        A tuple of (peak_points, peak_vals).\n        peak_points: float32 tensor of shape (samples, channels, 2), where the last axis\n        indicates peak locations in xy order.\n        peak_vals: float32 tensor of shape (samples, channels) containing the values at\n        the peak points.\n\n    \"\"\"\n    # Find the maximum values and their indices along the height and width axes.\n    max_values, max_indices_y = torch.max(cms, dim=2, keepdim=True)\n    max_values, max_indices_x = torch.max(max_values, dim=3, keepdim=True)\n    max_indices_x = max_indices_x.squeeze(dim=(2, 3))  # (samples, channels)\n    # Find the maximum values and their indices along the height and width axes.\n    amax_values, amax_indices_x = torch.max(cms, dim=3, keepdim=True)\n    amax_values, amax_indices_y = torch.max(amax_values, dim=2, keepdim=True)\n    amax_indices_y = amax_indices_y.squeeze(dim=(2, 3))\n    peak_points = torch.cat(\n        [max_indices_x.unsqueeze(-1), amax_indices_y.unsqueeze(-1)], dim=-1\n    ).to(torch.float32)\n    max_values = max_values.squeeze(-1).squeeze(-1)\n    # Create masks for values below the threshold.\n    below_threshold_mask = max_values &lt; threshold\n    # Replace values below the threshold with NaN.\n    peak_points[below_threshold_mask] = float(\"nan\")\n    max_values[below_threshold_mask] = float(0)\n    return peak_points, max_values\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_local_peaks","title":"<code>find_local_peaks(cms, threshold=0.2, refinement=None, integral_patch_size=5)</code>","text":"<p>Find local peaks with optional refinement.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Confidence maps. Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Minimum confidence threshold. Peaks with values below this will ignored.</p> <code>0.2</code> <code>refinement</code> <code>Optional[str]</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression.</p> <code>None</code> <code>integral_patch_size</code> <code>int</code> <p>Size of patches to crop around each rough peak as an integer scalar.</p> <code>5</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds).</p> <p>peak_points: float32 tensor of shape (n_peaks, 2), where the last axis indicates peak locations in xy order.</p> <p>peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak points.</p> <p>peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the sample each peak belongs to.</p> <p>peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of the channel each peak belongs to.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_local_peaks(\n    cms: torch.Tensor,\n    threshold: float = 0.2,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Find local peaks with optional refinement.\n\n    Args:\n        cms: Confidence maps. Tensor of shape (samples, channels, height, width).\n        threshold: Minimum confidence threshold. Peaks with values below this will\n            ignored.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression.\n        integral_patch_size: Size of patches to crop around each rough peak as an\n            integer scalar.\n\n    Returns:\n        A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds).\n\n        peak_points: float32 tensor of shape (n_peaks, 2), where the last axis\n        indicates peak locations in xy order.\n\n        peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak\n        points.\n\n        peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the\n        sample each peak belongs to.\n\n        peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of\n        the channel each peak belongs to.\n    \"\"\"\n    # Find grid aligned peaks.\n    (\n        rough_peaks,\n        peak_vals,\n        peak_sample_inds,\n        peak_channel_inds,\n    ) = find_local_peaks_rough(cms, threshold=threshold)\n\n    # Return early if no rough peaks found.\n    if rough_peaks.size(0) == 0 or refinement is None:\n        return rough_peaks, peak_vals, peak_sample_inds, peak_channel_inds\n\n    if refinement == \"integral\":\n        crop_size = integral_patch_size\n    else:\n        return rough_peaks, peak_vals, peak_sample_inds, peak_channel_inds\n\n    # Make bounding boxes for cropping around peaks.\n    bboxes = make_centered_bboxes(\n        rough_peaks, box_height=crop_size, box_width=crop_size\n    )\n\n    # Reshape to (samples * channels, height, width, 1).\n    samples = cms.size(0)\n    channels = cms.size(1)\n    cms = torch.reshape(\n        cms,\n        [samples * channels, 1, cms.size(2), cms.size(3)],\n    )\n    box_sample_inds = (peak_sample_inds * channels) + peak_channel_inds\n\n    # Crop patch around each grid-aligned peak.\n    cm_crops = crop_bboxes(cms, bboxes, sample_inds=box_sample_inds)\n\n    # Compute offsets via integral regression on a local patch.\n    if refinement == \"integral\":\n        gv = torch.arange(crop_size, dtype=torch.float32) - ((crop_size - 1) / 2)\n        dx_hat, dy_hat = integral_regression(cm_crops, xv=gv, yv=gv)\n        offsets = torch.cat([dx_hat, dy_hat], dim=1)\n\n    # Apply offsets.\n    refined_peaks = rough_peaks + offsets\n\n    return refined_peaks, peak_vals, peak_sample_inds, peak_channel_inds\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.find_local_peaks_rough","title":"<code>find_local_peaks_rough(cms, threshold=0.2)</code>","text":"<p>Find local maxima via non-maximum suppression.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Tensor of shape (samples, channels, height, width).</p> required <code>threshold</code> <code>float</code> <p>Scalar float specifying the minimum confidence value for peaks. Peaks with values below this threshold will not be returned.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds). peak_points: float32 tensor of shape (n_peaks, 2), where the last axis indicates peak locations in xy order.</p> <p>peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak points.</p> <p>peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the sample each peak belongs to.</p> <p>peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of the channel each peak belongs to.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def find_local_peaks_rough(\n    cms: torch.Tensor, threshold: float = 0.2\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Find local maxima via non-maximum suppression.\n\n    Args:\n        cms: Tensor of shape (samples, channels, height, width).\n        threshold: Scalar float specifying the minimum confidence value for peaks. Peaks\n            with values below this threshold will not be returned.\n\n    Returns:\n        A tuple of (peak_points, peak_vals, peak_sample_inds, peak_channel_inds).\n        peak_points: float32 tensor of shape (n_peaks, 2), where the last axis\n        indicates peak locations in xy order.\n\n        peak_vals: float32 tensor of shape (n_peaks,) containing the values at the peak\n        points.\n\n        peak_sample_inds: int32 tensor of shape (n_peaks,) containing the indices of the\n        sample each peak belongs to.\n\n        peak_channel_inds: int32 tensor of shape (n_peaks,) containing the indices of\n        the channel each peak belongs to.\n    \"\"\"\n    # Build custom local NMS kernel.\n    kernel = torch.tensor([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=torch.float32)\n\n    # Reshape to have singleton channels.\n    height = cms.size(2)\n    width = cms.size(3)\n    channels = cms.size(1)\n    flat_img = cms.reshape(-1, 1, height, width)\n\n    # Perform dilation filtering to find local maxima per channel and reshape back.\n    max_img = morphological_dilation(flat_img, kernel.to(flat_img.device))\n    max_img = max_img.reshape(-1, channels, height, width)\n\n    # Filter for maxima and threshold.\n    argmax_and_thresh_img = (cms &gt; max_img) &amp; (cms &gt; threshold)\n\n    # Convert to subscripts.\n    peak_subs = torch.stack(\n        torch.where(argmax_and_thresh_img.permute(0, 2, 3, 1)), axis=-1\n    )\n\n    # Get peak values.\n    peak_vals = cms[peak_subs[:, 0], peak_subs[:, 3], peak_subs[:, 1], peak_subs[:, 2]]\n\n    # Convert to points format.\n    peak_points = peak_subs[:, [2, 1]].to(torch.float32)\n\n    # Pull out indexing vectors.\n    peak_sample_inds = peak_subs[:, 0].to(torch.int32)\n    peak_channel_inds = peak_subs[:, 3].to(torch.int32)\n\n    return peak_points, peak_vals, peak_sample_inds, peak_channel_inds\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.integral_regression","title":"<code>integral_regression(cms, xv, yv)</code>","text":"<p>Compute regression by integrating over the confidence maps on a grid.</p> <p>Parameters:</p> Name Type Description Default <code>cms</code> <code>Tensor</code> <p>Confidence maps with shape (samples, channels, height, width).</p> required <code>xv</code> <code>Tensor</code> <p>X grid vector torch.float32 of grid coordinates to sample.</p> required <code>yv</code> <code>Tensor</code> <p>Y grid vector torch.float32 of grid coordinates to sample.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>A tuple of (x_hat, y_hat) with the regressed x- and y-coordinates for each channel of the confidence maps.</p> <p>x_hat and y_hat are of shape (samples, channels)</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def integral_regression(\n    cms: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute regression by integrating over the confidence maps on a grid.\n\n    Args:\n        cms: Confidence maps with shape (samples, channels, height, width).\n        xv: X grid vector torch.float32 of grid coordinates to sample.\n        yv: Y grid vector torch.float32 of grid coordinates to sample.\n\n    Returns:\n        A tuple of (x_hat, y_hat) with the regressed x- and y-coordinates for each\n        channel of the confidence maps.\n\n        x_hat and y_hat are of shape (samples, channels)\n    \"\"\"\n    # Compute normalizing factor.\n    z = torch.sum(cms, dim=[2, 3]).to(cms.device)\n    xv = xv.to(cms.device)\n    yv = yv.to(cms.device)\n\n    # Regress to expectation.\n    x_hat = torch.sum(xv.view(1, 1, 1, -1) * cms, dim=[2, 3]) / z\n    y_hat = torch.sum(yv.view(1, 1, -1, 1) * cms, dim=[2, 3]) / z\n\n    return x_hat, y_hat\n</code></pre>"},{"location":"api/inference/peak_finding/#sleap_nn.inference.peak_finding.morphological_dilation","title":"<code>morphological_dilation(image, kernel)</code>","text":"<p>Apply morphological dilation using max pooling.</p> <p>This is a pure PyTorch replacement for kornia.morphology.dilation. For non-maximum suppression, it computes the maximum of 8 neighbors (excluding the center pixel).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (B, 1, H, W).</p> required <code>kernel</code> <code>Tensor</code> <p>Dilation kernel (3x3 expected for NMS).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Dilated tensor of same shape as input.</p> Source code in <code>sleap_nn/inference/peak_finding.py</code> <pre><code>def morphological_dilation(image: torch.Tensor, kernel: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply morphological dilation using max pooling.\n\n    This is a pure PyTorch replacement for kornia.morphology.dilation.\n    For non-maximum suppression, it computes the maximum of 8 neighbors\n    (excluding the center pixel).\n\n    Args:\n        image: Input tensor of shape (B, 1, H, W).\n        kernel: Dilation kernel (3x3 expected for NMS).\n\n    Returns:\n        Dilated tensor of same shape as input.\n    \"\"\"\n    # Pad the image to handle border pixels\n    padded = F.pad(image, (1, 1, 1, 1), mode=\"constant\", value=float(\"-inf\"))\n\n    # Extract 3x3 patches using unfold\n    # Shape: (B, 1, H, W, 3, 3)\n    patches = padded.unfold(2, 3, 1).unfold(3, 3, 1)\n\n    # Reshape to (B, 1, H, W, 9)\n    b, c, h, w, kh, kw = patches.shape\n    patches = patches.reshape(b, c, h, w, -1)\n\n    # Apply kernel mask (kernel has 0 at center, 1 elsewhere for NMS)\n    # Reshape kernel to (1, 1, 1, 1, 9)\n    kernel_flat = kernel.reshape(-1).to(patches.device)\n    kernel_mask = kernel_flat &gt; 0\n\n    # Set non-kernel positions to -inf so they don't affect max\n    patches_masked = patches.clone()\n    patches_masked[..., ~kernel_mask] = float(\"-inf\")\n\n    # Take max over the kernel neighborhood\n    max_vals = patches_masked.max(dim=-1)[0]\n\n    return max_vals\n</code></pre>"},{"location":"api/inference/postprocessing/","title":"postprocessing","text":""},{"location":"api/inference/postprocessing/#sleap_nn.inference.postprocessing","title":"<code>sleap_nn.inference.postprocessing</code>","text":"<p>Inference-level postprocessing filters for pose predictions.</p> <p>This module provides filters that run after model inference but before tracking. These filters are independent of tracking configuration and can be used standalone.</p> <p>Functions:</p> Name Description <code>filter_overlapping_instances</code> <p>Filter overlapping instances using greedy non-maximum suppression.</p>"},{"location":"api/inference/postprocessing/#sleap_nn.inference.postprocessing.filter_overlapping_instances","title":"<code>filter_overlapping_instances(labels, threshold=0.8, method='iou')</code>","text":"<p>Filter overlapping instances using greedy non-maximum suppression.</p> <p>Removes duplicate/overlapping instances by applying greedy NMS based on either bounding box IOU or Object Keypoint Similarity (OKS). When two instances overlap above the threshold, the lower-scoring one is removed.</p> <p>This filter runs independently of tracking and can be used to clean up model outputs before saving or further processing.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Labels</code> <p>Labels object with predicted instances to filter.</p> required <code>threshold</code> <code>float</code> <p>Similarity threshold for considering instances as overlapping. Instances with similarity &gt; threshold are candidates for removal. Lower values are more aggressive (remove more). Typical values: 0.3 (aggressive) to 0.8 (permissive).</p> <code>0.8</code> <code>method</code> <code>Literal['iou', 'oks']</code> <p>Similarity metric to use for comparing instances. \"iou\": Bounding box intersection-over-union. \"oks\": Object Keypoint Similarity (pose-based).</p> <code>'iou'</code> <p>Returns:</p> Type Description <code>Labels</code> <p>The input Labels object with overlapping instances removed. Modification is done in place, but the object is also returned for convenience.</p> Example Note <ul> <li>Only affects frames with 2+ predicted instances</li> <li>Uses instance.score for ranking; higher scores are preferred</li> <li>For IOU: bounding boxes computed from non-NaN keypoints</li> <li>For OKS: uses standard COCO OKS formula with bbox-derived scale</li> </ul> Source code in <code>sleap_nn/inference/postprocessing.py</code> <pre><code>def filter_overlapping_instances(\n    labels: sio.Labels,\n    threshold: float = 0.8,\n    method: Literal[\"iou\", \"oks\"] = \"iou\",\n) -&gt; sio.Labels:\n    \"\"\"Filter overlapping instances using greedy non-maximum suppression.\n\n    Removes duplicate/overlapping instances by applying greedy NMS based on\n    either bounding box IOU or Object Keypoint Similarity (OKS). When two\n    instances overlap above the threshold, the lower-scoring one is removed.\n\n    This filter runs independently of tracking and can be used to clean up\n    model outputs before saving or further processing.\n\n    Args:\n        labels: Labels object with predicted instances to filter.\n        threshold: Similarity threshold for considering instances as overlapping.\n            Instances with similarity &gt; threshold are candidates for removal.\n            Lower values are more aggressive (remove more).\n            Typical values: 0.3 (aggressive) to 0.8 (permissive).\n        method: Similarity metric to use for comparing instances.\n            \"iou\": Bounding box intersection-over-union.\n            \"oks\": Object Keypoint Similarity (pose-based).\n\n    Returns:\n        The input Labels object with overlapping instances removed.\n        Modification is done in place, but the object is also returned\n        for convenience.\n\n    Example:\n        &gt;&gt;&gt; # Filter instances with &gt;80% bounding box overlap\n        &gt;&gt;&gt; labels = filter_overlapping_instances(labels, threshold=0.8, method=\"iou\")\n        &gt;&gt;&gt; # Filter using OKS similarity\n        &gt;&gt;&gt; labels = filter_overlapping_instances(labels, threshold=0.5, method=\"oks\")\n\n    Note:\n        - Only affects frames with 2+ predicted instances\n        - Uses instance.score for ranking; higher scores are preferred\n        - For IOU: bounding boxes computed from non-NaN keypoints\n        - For OKS: uses standard COCO OKS formula with bbox-derived scale\n    \"\"\"\n    for lf in labels.labeled_frames:\n        if len(lf.instances) &lt;= 1:\n            continue\n\n        # Separate predicted instances (have scores) from other instances\n        predicted = []\n        other = []\n        for inst in lf.instances:\n            if isinstance(inst, sio.PredictedInstance):\n                predicted.append(inst)\n            else:\n                other.append(inst)\n\n        # Only filter predicted instances\n        if len(predicted) &lt;= 1:\n            continue\n\n        # Get scores\n        scores = np.array([_instance_score(inst) for inst in predicted])\n\n        # Apply greedy NMS with selected method\n        if method == \"iou\":\n            bboxes = np.array([_instance_bbox(inst) for inst in predicted])\n            keep_indices = _nms_greedy_iou(bboxes, scores, threshold)\n        elif method == \"oks\":\n            points = [inst.numpy() for inst in predicted]\n            keep_indices = _nms_greedy_oks(points, scores, threshold)\n        else:\n            raise ValueError(f\"Unknown method: {method}. Use 'iou' or 'oks'.\")\n\n        # Reconstruct instance list: kept predicted + other instances\n        kept_predicted = [predicted[i] for i in keep_indices]\n        lf.instances = kept_predicted + other\n\n    return labels\n</code></pre>"},{"location":"api/inference/postprocessing/#sleap_nn.inference.postprocessing.filter_overlapping_instances--filter-instances-with-80-bounding-box-overlap","title":"Filter instances with &gt;80% bounding box overlap","text":"<p>labels = filter_overlapping_instances(labels, threshold=0.8, method=\"iou\")</p>"},{"location":"api/inference/postprocessing/#sleap_nn.inference.postprocessing.filter_overlapping_instances--filter-using-oks-similarity","title":"Filter using OKS similarity","text":"<p>labels = filter_overlapping_instances(labels, threshold=0.5, method=\"oks\")</p>"},{"location":"api/inference/predictors/","title":"predictors","text":""},{"location":"api/inference/predictors/#sleap_nn.inference.predictors","title":"<code>sleap_nn.inference.predictors</code>","text":"<p>Predictors for running inference.</p> <p>Classes:</p> Name Description <code>BottomUpMultiClassPredictor</code> <p>BottomUp ID model predictor.</p> <code>BottomUpPredictor</code> <p>BottomUp model predictor.</p> <code>Predictor</code> <p>Base interface class for predictors.</p> <code>RateColumn</code> <p>Renders the progress rate.</p> <code>SingleInstancePredictor</code> <p>Single-Instance predictor.</p> <code>TopDownMultiClassPredictor</code> <p>Top-down multi-class predictor.</p> <code>TopDownPredictor</code> <p>Top-down multi-instance predictor.</p>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpMultiClassPredictor","title":"<code>BottomUpMultiClassPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>BottomUp ID model predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained BottomUp SLEAP-NN model.</p> <p>This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>bottomup_config</code> <code>Optional[OmegaConf]</code> <p>A OmegaConfig dictionary with the configs used for training the             multi_class_bottomup model.</p> <code>bottomup_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights for            multi_class_bottomup model.</p> <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\".</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters     in the <code>data_config.preprocessing</code> section.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass BottomUpMultiClassPredictor(Predictor):\n    \"\"\"BottomUp ID model predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained BottomUp SLEAP-NN model.\n\n    This should be initialized using the `from_trained_models()` constructor.\n\n    Attributes:\n        bottomup_config: A OmegaConfig dictionary with the configs used for training the\n                        multi_class_bottomup model.\n        bottomup_model: A LightningModule instance created from the trained weights for\n                       multi_class_bottomup model.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\".\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    bottomup_config: Optional[OmegaConf] = attrs.field(default=None)\n    bottomup_model: Optional[L.LightningModule] = attrs.field(default=None)\n    backbone_type: str = \"unet\"\n    videos: Optional[List[sio.Video]] = attrs.field(default=None)\n    skeletons: Optional[List[sio.Skeleton]] = attrs.field(default=None)\n    peak_threshold: float = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # initialize the BottomUpMultiClassInferenceModel\n        self.inference_model = BottomUpMultiClassInferenceModel(\n            torch_model=self.bottomup_model,\n            peak_threshold=self.peak_threshold,\n            cms_output_stride=self.bottomup_config.model_config.head_configs.multi_class_bottomup.confmaps.output_stride,\n            class_maps_output_stride=self.bottomup_config.model_config.head_configs.multi_class_bottomup.class_maps.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            input_scale=self.bottomup_config.data_config.preprocessing.scale,\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        bottomup_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        max_stride: int = 16,\n    ) -&gt; \"BottomUpMultiClassPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            bottomup_ckpt_path: Path to a multi-class bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                    are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                    from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mps\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `BottomUpPredictor` with the loaded models.\n\n        \"\"\"\n        is_sleap_ckpt = False\n        if (\n            Path(bottomup_ckpt_path) / \"training_config.yaml\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            bottomup_config = OmegaConf.load(\n                (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(bottomup_ckpt_path) / \"training_config.json\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            bottomup_config = TrainingJobConfig.load_sleap_config(\n                (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in bottomup_config.model_config.backbone_config.items():\n            if v is not None:\n                backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n            bottomup_model = BottomUpMultiClassLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                backbone_type=backbone_type,\n                model_type=\"multi_class_bottomup\",\n                map_location=device,\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=bottomup_config.model_config.init_weights,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n                weights_only=False,\n            )\n        else:\n            bottomup_converted_model = load_legacy_model(\n                model_dir=f\"{bottomup_ckpt_path}\"\n            )\n            bottomup_model = BottomUpMultiClassLightningModule(\n                backbone_type=backbone_type,\n                model_type=\"multi_class_bottomup\",\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=bottomup_config.model_config.init_weights,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            )\n            bottomup_model.eval()\n            bottomup_model.model = bottomup_converted_model\n            bottomup_model.to(device)\n\n        bottomup_model.eval()\n        skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n        bottomup_model.to(device)\n\n        for k, v in preprocess_config.items():\n            if v is None:\n                preprocess_config[k] = (\n                    bottomup_config.data_config.preprocessing[k]\n                    if k in bottomup_config.data_config.preprocessing\n                    else None\n                )\n\n        # create an instance of SingleInstancePredictor class\n        obj = cls(\n            bottomup_config=bottomup_config,\n            backbone_type=backbone_type,\n            bottomup_model=bottomup_model,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            preprocess_config=preprocess_config,\n            max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        inference_object: Union[str, Path, sio.Labels, sio.Video],\n        queue_maxsize: int = 32,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n            frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n            only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        if isinstance(inference_object, str) or isinstance(inference_object, Path):\n            inference_object = (\n                sio.load_slp(inference_object)\n                if inference_object.endswith(\".slp\")\n                else sio.load_video(\n                    inference_object,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n            )\n\n        self.preprocess = True\n        # LabelsReader provider\n        if isinstance(inference_object, sio.Labels) and video_index is None:\n            provider = LabelsReader\n            max_stride = self.bottomup_config.model_config.backbone_config[\n                f\"{self.backbone_type}\"\n            ][\"max_stride\"]\n\n            frame_buffer = Queue(maxsize=queue_maxsize)\n\n            self.pipeline = provider(\n                labels=inference_object,\n                frame_buffer=frame_buffer,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n                exclude_user_labeled=exclude_user_labeled,\n                only_predicted_frames=only_predicted_frames,\n            )\n\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n\n            if isinstance(inference_object, sio.Labels) and video_index is not None:\n                labels = inference_object\n                video = labels.videos[video_index]\n                # Filter out user-labeled frames if requested\n                filtered_frames = _filter_user_labeled_frames(\n                    labels, video, frames, exclude_user_labeled\n                )\n                self.pipeline = provider.from_video(\n                    video=video,\n                    queue_maxsize=queue_maxsize,\n                    frames=filtered_frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                frame_buffer = Queue(maxsize=queue_maxsize)\n                self.pipeline = provider(\n                    video=inference_object,\n                    frame_buffer=frame_buffer,\n                    frames=frames,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        # open video backend for tracking\n        for video in self.videos:\n            if not video.open_backend:\n                video.open()\n\n        predicted_frames = []\n        tracks = [\n            sio.Track(name=x)\n            for x in self.bottomup_config.model_config.head_configs.multi_class_bottomup.class_maps.classes\n        ]\n\n        skeleton_idx = 0\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                pred_instances,\n                pred_values,\n                instance_score,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"instance_scores\"],\n            ):\n                # Loop over instances.\n                predicted_instances = []\n                for i, (pts, confs, score) in enumerate(\n                    zip(pred_instances, pred_values, instance_score)\n                ):\n                    if np.isnan(pts).all():\n                        continue\n\n                    track = None\n                    if tracks is not None and len(tracks) &gt;= (i - 1):\n                        track = tracks[i]\n\n                    predicted_instances.append(\n                        sio.PredictedInstance.from_numpy(\n                            points_data=pts,\n                            point_scores=confs,\n                            score=np.nanmean(confs),\n                            skeleton=self.skeletons[skeleton_idx],\n                            track=track,\n                            tracking_score=np.nanmean(score),\n                        )\n                    )\n\n                max_instances = (\n                    self.max_instances if self.max_instances is not None else None\n                )\n                if max_instances is not None:\n                    # Filter by score.\n                    predicted_instances = sorted(\n                        predicted_instances, key=lambda x: x.score, reverse=True\n                    )\n                    predicted_instances = predicted_instances[\n                        : min(max_instances, len(predicted_instances))\n                    ]\n\n                lf = sio.LabeledFrame(\n                    video=self.videos[video_idx],\n                    frame_idx=frame_idx,\n                    instances=predicted_instances,\n                )\n\n                predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpMultiClassPredictor.from_trained_models","title":"<code>from_trained_models(bottomup_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>bottomup_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a multi-class bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights     are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt     from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>BottomUpMultiClassPredictor</code> <p>An instance of <code>BottomUpPredictor</code> with the loaded models.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    bottomup_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    max_stride: int = 16,\n) -&gt; \"BottomUpMultiClassPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        bottomup_ckpt_path: Path to a multi-class bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `BottomUpPredictor` with the loaded models.\n\n    \"\"\"\n    is_sleap_ckpt = False\n    if (\n        Path(bottomup_ckpt_path) / \"training_config.yaml\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        bottomup_config = OmegaConf.load(\n            (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n        )\n    elif (\n        Path(bottomup_ckpt_path) / \"training_config.json\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        is_sleap_ckpt = True\n        bottomup_config = TrainingJobConfig.load_sleap_config(\n            (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n        )\n\n    # check which backbone architecture\n    for k, v in bottomup_config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n\n    if not is_sleap_ckpt:\n        ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n        bottomup_model = BottomUpMultiClassLightningModule.load_from_checkpoint(\n            checkpoint_path=ckpt_path,\n            backbone_type=backbone_type,\n            model_type=\"multi_class_bottomup\",\n            map_location=device,\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=None,\n            pretrained_head_weights=None,\n            init_weights=bottomup_config.model_config.init_weights,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            weights_only=False,\n        )\n    else:\n        bottomup_converted_model = load_legacy_model(\n            model_dir=f\"{bottomup_ckpt_path}\"\n        )\n        bottomup_model = BottomUpMultiClassLightningModule(\n            backbone_type=backbone_type,\n            model_type=\"multi_class_bottomup\",\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=None,\n            pretrained_head_weights=None,\n            init_weights=bottomup_config.model_config.init_weights,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n        )\n        bottomup_model.eval()\n        bottomup_model.model = bottomup_converted_model\n        bottomup_model.to(device)\n\n    bottomup_model.eval()\n    skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n    if backbone_ckpt_path is not None and head_ckpt_path is not None:\n        logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path,\n            map_location=device,\n            weights_only=False,\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".backbone\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    elif backbone_ckpt_path is not None:\n        logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path,\n            map_location=device,\n            weights_only=False,\n        )\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    if head_ckpt_path is not None:\n        logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n        ckpt = torch.load(\n            head_ckpt_path,\n            map_location=device,\n            weights_only=False,\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".head_layers\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n    bottomup_model.to(device)\n\n    for k, v in preprocess_config.items():\n        if v is None:\n            preprocess_config[k] = (\n                bottomup_config.data_config.preprocessing[k]\n                if k in bottomup_config.data_config.preprocessing\n                else None\n            )\n\n    # create an instance of SingleInstancePredictor class\n    obj = cls(\n        bottomup_config=bottomup_config,\n        backbone_type=backbone_type,\n        bottomup_model=bottomup_model,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        preprocess_config=preprocess_config,\n        max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n            \"max_stride\"\n        ],\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpMultiClassPredictor.make_pipeline","title":"<code>make_pipeline(inference_object, queue_maxsize=32, frames=None, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inference_object</code> <code>Union[str, Path, Labels, Video]</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> or sio.Labels or sio.Video to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 32.</p> <code>32</code> <code>frames</code> <code>Optional[list]</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>exclude_user_labeled</code> <code>bool</code> <p>(bool) <code>True</code> to skip frames that have user-labeled instances. Default: <code>False</code>.</p> <code>False</code> <code>only_predicted_frames</code> <code>bool</code> <p>(bool) <code>True</code> to run inference only on frames that already have predictions. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    inference_object: Union[str, Path, sio.Labels, sio.Video],\n    queue_maxsize: int = 32,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n        frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n        only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    if isinstance(inference_object, str) or isinstance(inference_object, Path):\n        inference_object = (\n            sio.load_slp(inference_object)\n            if inference_object.endswith(\".slp\")\n            else sio.load_video(\n                inference_object,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n        )\n\n    self.preprocess = True\n    # LabelsReader provider\n    if isinstance(inference_object, sio.Labels) and video_index is None:\n        provider = LabelsReader\n        max_stride = self.bottomup_config.model_config.backbone_config[\n            f\"{self.backbone_type}\"\n        ][\"max_stride\"]\n\n        frame_buffer = Queue(maxsize=queue_maxsize)\n\n        self.pipeline = provider(\n            labels=inference_object,\n            frame_buffer=frame_buffer,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n            exclude_user_labeled=exclude_user_labeled,\n            only_predicted_frames=only_predicted_frames,\n        )\n\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n\n        if isinstance(inference_object, sio.Labels) and video_index is not None:\n            labels = inference_object\n            video = labels.videos[video_index]\n            # Filter out user-labeled frames if requested\n            filtered_frames = _filter_user_labeled_frames(\n                labels, video, frames, exclude_user_labeled\n            )\n            self.pipeline = provider.from_video(\n                video=video,\n                queue_maxsize=queue_maxsize,\n                frames=filtered_frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            frame_buffer = Queue(maxsize=queue_maxsize)\n            self.pipeline = provider(\n                video=inference_object,\n                frame_buffer=frame_buffer,\n                frames=frames,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpPredictor","title":"<code>BottomUpPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>BottomUp model predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained BottomUp SLEAP-NN model.</p> <p>This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>bottomup_config</code> <code>Optional[OmegaConf]</code> <p>A OmegaConfig dictionary with the configs used for training the             bottom-up model.</p> <code>bottomup_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights for            bottom-up model.</p> <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>max_edge_length_ratio</code> <code>float</code> <p>The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than this length will be penalized during matching.</p> <code>dist_penalty_weight</code> <code>float</code> <p>A coefficient to scale weight of the distance penalty as a scalar float. Set to values greater than 1.0 to enforce the distance penalty more strictly.</p> <code>n_points</code> <code>int</code> <p>Number of points to sample along the line integral.</p> <code>min_instance_peaks</code> <code>Union[int, float]</code> <p>Minimum number of peaks the instance should have to be     considered a real instance. Instances with fewer peaks than this will be     discarded (useful for filtering spurious detections).</p> <code>min_line_scores</code> <code>float</code> <p>Minimum line score (between -1 and 1) required to form a match between candidate point pairs. Useful for rejecting spurious detections when there are no better ones.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\".</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters     in the <code>data_config.preprocessing</code> section.</p> <code>tracker</code> <code>Optional[Tracker]</code> <p>A <code>sleap.nn.tracking.Tracker</code> that will be called to associate detections over time. Predicted instances will not be assigned to tracks if if this is <code>None</code>.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass BottomUpPredictor(Predictor):\n    \"\"\"BottomUp model predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained BottomUp SLEAP-NN model.\n\n    This should be initialized using the `from_trained_models()` constructor.\n\n    Attributes:\n        bottomup_config: A OmegaConfig dictionary with the configs used for training the\n                        bottom-up model.\n        bottomup_model: A LightningModule instance created from the trained weights for\n                       bottom-up model.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        max_edge_length_ratio: The maximum expected length of a connected pair of points\n            as a fraction of the image size. Candidate connections longer than this\n            length will be penalized during matching.\n        dist_penalty_weight: A coefficient to scale weight of the distance penalty as\n            a scalar float. Set to values greater than 1.0 to enforce the distance\n            penalty more strictly.\n        n_points: Number of points to sample along the line integral.\n        min_instance_peaks: Minimum number of peaks the instance should have to be\n                considered a real instance. Instances with fewer peaks than this will be\n                discarded (useful for filtering spurious detections).\n        min_line_scores: Minimum line score (between -1 and 1) required to form a match\n            between candidate point pairs. Useful for rejecting spurious detections when\n            there are no better ones.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\".\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n        tracker: A `sleap.nn.tracking.Tracker` that will be called to associate\n            detections over time. Predicted instances will not be assigned to tracks if\n            if this is `None`.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    bottomup_config: Optional[OmegaConf] = attrs.field(default=None)\n    bottomup_model: Optional[L.LightningModule] = attrs.field(default=None)\n    backbone_type: str = \"unet\"\n    max_edge_length_ratio: float = 0.25\n    dist_penalty_weight: float = 1.0\n    n_points: int = 10\n    min_instance_peaks: Union[int, float] = 0\n    min_line_scores: float = 0.25\n    videos: Optional[List[sio.Video]] = attrs.field(default=None)\n    skeletons: Optional[List[sio.Skeleton]] = attrs.field(default=None)\n    peak_threshold: float = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    tracker: Optional[Tracker] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # initialize the paf scorer\n        paf_scorer = PAFScorer.from_config(\n            config=OmegaConf.create(\n                {\n                    \"confmaps\": self.bottomup_config.model_config.head_configs.bottomup[\n                        \"confmaps\"\n                    ],\n                    \"pafs\": self.bottomup_config.model_config.head_configs.bottomup[\n                        \"pafs\"\n                    ],\n                }\n            ),\n            max_edge_length_ratio=self.max_edge_length_ratio,\n            dist_penalty_weight=self.dist_penalty_weight,\n            n_points=self.n_points,\n            min_instance_peaks=self.min_instance_peaks,\n            min_line_scores=self.min_line_scores,\n        )\n\n        # initialize the BottomUpInferenceModel\n        self.inference_model = BottomUpInferenceModel(\n            torch_model=self.bottomup_model,\n            paf_scorer=paf_scorer,\n            peak_threshold=self.peak_threshold,\n            cms_output_stride=self.bottomup_config.model_config.head_configs.bottomup.confmaps.output_stride,\n            pafs_output_stride=self.bottomup_config.model_config.head_configs.bottomup.pafs.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            input_scale=self.bottomup_config.data_config.preprocessing.scale,\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        bottomup_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        max_stride: int = 16,\n    ) -&gt; \"BottomUpPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            bottomup_ckpt_path: Path to a bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                    are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                    from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mps\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `BottomUpPredictor` with the loaded models.\n\n        \"\"\"\n        is_sleap_ckpt = False\n        if (\n            Path(bottomup_ckpt_path) / \"training_config.yaml\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            bottomup_config = OmegaConf.load(\n                (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(bottomup_ckpt_path) / \"training_config.json\"\n            in Path(bottomup_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            bottomup_config = TrainingJobConfig.load_sleap_config(\n                (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in bottomup_config.model_config.backbone_config.items():\n            if v is not None:\n                backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n            bottomup_model = BottomUpLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=bottomup_config.model_config.init_weights,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n                backbone_type=backbone_type,\n                model_type=\"bottomup\",\n                map_location=device,\n                weights_only=False,\n            )\n        else:\n            bottomup_converted_model = load_legacy_model(\n                model_dir=f\"{bottomup_ckpt_path}\"\n            )\n            bottomup_model = BottomUpLightningModule(\n                backbone_config=bottomup_config.model_config.backbone_config,\n                head_configs=bottomup_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=bottomup_config.model_config.init_weights,\n                lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n                backbone_type=backbone_type,\n                model_type=\"bottomup\",\n                online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=bottomup_config.trainer_config.optimizer_name,\n                learning_rate=bottomup_config.trainer_config.optimizer.lr,\n                amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            )\n            bottomup_model.eval()\n            bottomup_model.model = bottomup_converted_model\n            bottomup_model.to(device)\n\n        bottomup_model.eval()\n        skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n        bottomup_model.to(device)\n\n        for k, v in preprocess_config.items():\n            if v is None:\n                preprocess_config[k] = (\n                    bottomup_config.data_config.preprocessing[k]\n                    if k in bottomup_config.data_config.preprocessing\n                    else None\n                )\n\n        # create an instance of BottomUpPredictor class\n        obj = cls(\n            bottomup_config=bottomup_config,\n            backbone_type=backbone_type,\n            bottomup_model=bottomup_model,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            preprocess_config=preprocess_config,\n            max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        inference_object: Union[str, Path, sio.Labels, sio.Video],\n        queue_maxsize: int = 32,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n            frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n            only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        if isinstance(inference_object, str) or isinstance(inference_object, Path):\n            inference_object = (\n                sio.load_slp(inference_object)\n                if inference_object.endswith(\".slp\")\n                else sio.load_video(\n                    inference_object,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n            )\n\n        self.preprocess = True\n\n        # LabelsReader provider\n        if isinstance(inference_object, sio.Labels) and video_index is None:\n            provider = LabelsReader\n\n            frame_buffer = Queue(maxsize=queue_maxsize)\n\n            self.pipeline = provider(\n                labels=inference_object,\n                frame_buffer=frame_buffer,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n                exclude_user_labeled=exclude_user_labeled,\n                only_predicted_frames=only_predicted_frames,\n            )\n\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n\n            if isinstance(inference_object, sio.Labels) and video_index is not None:\n                labels = inference_object\n                video = labels.videos[video_index]\n                # Filter out user-labeled frames if requested\n                filtered_frames = _filter_user_labeled_frames(\n                    labels, video, frames, exclude_user_labeled\n                )\n                self.pipeline = provider.from_video(\n                    video=video,\n                    queue_maxsize=queue_maxsize,\n                    frames=filtered_frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                frame_buffer = Queue(maxsize=queue_maxsize)\n                self.pipeline = provider(\n                    video=inference_object,\n                    frame_buffer=frame_buffer,\n                    frames=frames,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        # open video backend for tracking\n        for video in self.videos:\n            if not video.open_backend:\n                video.open()\n\n        predicted_frames = []\n\n        skeleton_idx = 0\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                pred_instances,\n                pred_values,\n                instance_score,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"instance_scores\"],\n            ):\n                # Loop over instances.\n                predicted_instances = []\n                for pts, confs, score in zip(\n                    pred_instances, pred_values, instance_score\n                ):\n                    if np.isnan(pts).all():\n                        continue\n\n                    predicted_instances.append(\n                        sio.PredictedInstance.from_numpy(\n                            points_data=pts,\n                            point_scores=confs,\n                            score=score,\n                            skeleton=self.skeletons[skeleton_idx],\n                        )\n                    )\n\n                max_instances = (\n                    self.max_instances if self.max_instances is not None else None\n                )\n                if max_instances is not None:\n                    # Filter by score.\n                    predicted_instances = sorted(\n                        predicted_instances, key=lambda x: x.score, reverse=True\n                    )\n                    predicted_instances = predicted_instances[\n                        : min(max_instances, len(predicted_instances))\n                    ]\n\n                lf = sio.LabeledFrame(\n                    video=self.videos[video_idx],\n                    frame_idx=frame_idx,\n                    instances=predicted_instances,\n                )\n\n                if self.tracker:\n                    lf.instances = self.tracker.track(\n                        untracked_instances=predicted_instances,\n                        frame_idx=frame_idx,\n                        image=lf.image,\n                    )\n\n                predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpPredictor.from_trained_models","title":"<code>from_trained_models(bottomup_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>bottomup_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights     are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt     from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>BottomUpPredictor</code> <p>An instance of <code>BottomUpPredictor</code> with the loaded models.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    bottomup_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    max_stride: int = 16,\n) -&gt; \"BottomUpPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        bottomup_ckpt_path: Path to a bottom-up ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `BottomUpPredictor` with the loaded models.\n\n    \"\"\"\n    is_sleap_ckpt = False\n    if (\n        Path(bottomup_ckpt_path) / \"training_config.yaml\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        bottomup_config = OmegaConf.load(\n            (Path(bottomup_ckpt_path) / \"training_config.yaml\").as_posix()\n        )\n    elif (\n        Path(bottomup_ckpt_path) / \"training_config.json\"\n        in Path(bottomup_ckpt_path).iterdir()\n    ):\n        is_sleap_ckpt = True\n        bottomup_config = TrainingJobConfig.load_sleap_config(\n            (Path(bottomup_ckpt_path) / \"training_config.json\").as_posix()\n        )\n\n    # check which backbone architecture\n    for k, v in bottomup_config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n\n    if not is_sleap_ckpt:\n        ckpt_path = (Path(bottomup_ckpt_path) / \"best.ckpt\").as_posix()\n\n        bottomup_model = BottomUpLightningModule.load_from_checkpoint(\n            checkpoint_path=ckpt_path,\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=None,\n            pretrained_head_weights=None,\n            init_weights=bottomup_config.model_config.init_weights,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n            backbone_type=backbone_type,\n            model_type=\"bottomup\",\n            map_location=device,\n            weights_only=False,\n        )\n    else:\n        bottomup_converted_model = load_legacy_model(\n            model_dir=f\"{bottomup_ckpt_path}\"\n        )\n        bottomup_model = BottomUpLightningModule(\n            backbone_config=bottomup_config.model_config.backbone_config,\n            head_configs=bottomup_config.model_config.head_configs,\n            pretrained_backbone_weights=None,\n            pretrained_head_weights=None,\n            init_weights=bottomup_config.model_config.init_weights,\n            lr_scheduler=bottomup_config.trainer_config.lr_scheduler,\n            backbone_type=backbone_type,\n            model_type=\"bottomup\",\n            online_mining=bottomup_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=bottomup_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=bottomup_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=bottomup_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=bottomup_config.trainer_config.optimizer_name,\n            learning_rate=bottomup_config.trainer_config.optimizer.lr,\n            amsgrad=bottomup_config.trainer_config.optimizer.amsgrad,\n        )\n        bottomup_model.eval()\n        bottomup_model.model = bottomup_converted_model\n        bottomup_model.to(device)\n\n    bottomup_model.eval()\n    skeletons = get_skeleton_from_config(bottomup_config.data_config.skeletons)\n\n    if backbone_ckpt_path is not None and head_ckpt_path is not None:\n        logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".backbone\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    elif backbone_ckpt_path is not None:\n        logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    if head_ckpt_path is not None:\n        logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n        ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".head_layers\" in k\n        }\n        bottomup_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n    bottomup_model.to(device)\n\n    for k, v in preprocess_config.items():\n        if v is None:\n            preprocess_config[k] = (\n                bottomup_config.data_config.preprocessing[k]\n                if k in bottomup_config.data_config.preprocessing\n                else None\n            )\n\n    # create an instance of BottomUpPredictor class\n    obj = cls(\n        bottomup_config=bottomup_config,\n        backbone_type=backbone_type,\n        bottomup_model=bottomup_model,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        preprocess_config=preprocess_config,\n        max_stride=bottomup_config.model_config.backbone_config[f\"{backbone_type}\"][\n            \"max_stride\"\n        ],\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.BottomUpPredictor.make_pipeline","title":"<code>make_pipeline(inference_object, queue_maxsize=32, frames=None, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inference_object</code> <code>Union[str, Path, Labels, Video]</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> or sio.Labels or sio.Video to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 32.</p> <code>32</code> <code>frames</code> <code>Optional[list]</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>exclude_user_labeled</code> <code>bool</code> <p>(bool) <code>True</code> to skip frames that have user-labeled instances. Default: <code>False</code>.</p> <code>False</code> <code>only_predicted_frames</code> <code>bool</code> <p>(bool) <code>True</code> to run inference only on frames that already have predictions. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    inference_object: Union[str, Path, sio.Labels, sio.Video],\n    queue_maxsize: int = 32,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n        frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n        only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    if isinstance(inference_object, str) or isinstance(inference_object, Path):\n        inference_object = (\n            sio.load_slp(inference_object)\n            if inference_object.endswith(\".slp\")\n            else sio.load_video(\n                inference_object,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n        )\n\n    self.preprocess = True\n\n    # LabelsReader provider\n    if isinstance(inference_object, sio.Labels) and video_index is None:\n        provider = LabelsReader\n\n        frame_buffer = Queue(maxsize=queue_maxsize)\n\n        self.pipeline = provider(\n            labels=inference_object,\n            frame_buffer=frame_buffer,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n            exclude_user_labeled=exclude_user_labeled,\n            only_predicted_frames=only_predicted_frames,\n        )\n\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n\n        if isinstance(inference_object, sio.Labels) and video_index is not None:\n            labels = inference_object\n            video = labels.videos[video_index]\n            # Filter out user-labeled frames if requested\n            filtered_frames = _filter_user_labeled_frames(\n                labels, video, frames, exclude_user_labeled\n            )\n            self.pipeline = provider.from_video(\n                video=video,\n                queue_maxsize=queue_maxsize,\n                frames=filtered_frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            frame_buffer = Queue(maxsize=queue_maxsize)\n            self.pipeline = provider(\n                video=inference_object,\n                frame_buffer=frame_buffer,\n                frames=frames,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor","title":"<code>Predictor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base interface class for predictors.</p> <p>This is the base predictor class for different types of models.</p> <p>Attributes:</p> Name Type Description <code>preprocess</code> <code>bool</code> <p>True if preprocessing (resizing and apply_pad_to_stride) should be applied on the frames read in the video reader. Default: True.</p> <code>preprocess_config</code> <code>dict</code> <p>Preprocessing config with keys: [<code>scale</code>, <code>ensure_rgb</code>, <code>ensure_grayscale</code>, <code>scale</code>, <code>max_height</code>, <code>max_width</code>, <code>crop_size</code>]. Default: {\"scale\": 1.0, \"ensure_rgb\": False, \"ensure_grayscale\": False, \"max_height\": None, \"max_width\": None, \"crop_size\": None}</p> <code>pipeline</code> <code>Optional[Union[LabelsReader, VideoReader]]</code> <p>If provider is LabelsReader, pipeline is a <code>DataLoader</code> object. If provider is VideoReader, pipeline is an instance of <code>sleap_nn.data.providers.VideoReader</code> class. Default: None.</p> <code>inference_model</code> <code>Optional[Union[TopDownInferenceModel, SingleInstanceInferenceModel, BottomUpInferenceModel]]</code> <p>Instance of one of the inference models [\"TopDownInferenceModel\", \"SingleInstanceInferenceModel\", \"BottomUpInferenceModel\"]. Default: None.</p> <code>instances_key</code> <code>bool</code> <p>If <code>True</code>, then instances are appended to the data samples.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>gui</code> <code>bool</code> <p>If True, outputs JSON progress lines for GUI integration instead of Rich progress bars. Default: False.</p> <p>Methods:</p> Name Description <code>from_model_paths</code> <p>Create the appropriate <code>Predictor</code> subclass from from the ckpt path.</p> <code>from_trained_models</code> <p>Initialize the Predictor class for certain type of model.</p> <code>make_pipeline</code> <p>Create the data pipeline.</p> <code>predict</code> <p>Run inference on a data source.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass Predictor(ABC):\n    \"\"\"Base interface class for predictors.\n\n    This is the base predictor class for different types of models.\n\n    Attributes:\n        preprocess: True if preprocessing (resizing and\n            apply_pad_to_stride) should be applied on the frames read in the video reader.\n            Default: True.\n        preprocess_config: Preprocessing config with keys: [`scale`,\n            `ensure_rgb`, `ensure_grayscale`, `scale`, `max_height`, `max_width`, `crop_size`]. Default: {\"scale\": 1.0,\n            \"ensure_rgb\": False, \"ensure_grayscale\": False, \"max_height\": None, \"max_width\": None, \"crop_size\": None}\n        pipeline: If provider is LabelsReader, pipeline is a `DataLoader` object. If provider\n            is VideoReader, pipeline is an instance of `sleap_nn.data.providers.VideoReader`\n            class. Default: None.\n        inference_model: Instance of one of the inference models [\"TopDownInferenceModel\",\n            \"SingleInstanceInferenceModel\", \"BottomUpInferenceModel\"]. Default: None.\n        instances_key: If `True`, then instances are appended to the data samples.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n        gui: If True, outputs JSON progress lines for GUI integration instead of\n            Rich progress bars. Default: False.\n    \"\"\"\n\n    preprocess: bool = True\n    preprocess_config: dict = {\n        \"scale\": 1.0,\n        \"ensure_rgb\": False,\n        \"ensure_grayscale\": False,\n        \"crop_size\": None,\n        \"max_height\": None,\n        \"max_width\": None,\n    }\n    pipeline: Optional[Union[LabelsReader, VideoReader]] = None\n    inference_model: Optional[\n        Union[\n            TopDownInferenceModel, SingleInstanceInferenceModel, BottomUpInferenceModel\n        ]\n    ] = None\n    instances_key: bool = False\n    max_stride: int = 16\n    gui: bool = False\n\n    @classmethod\n    def from_model_paths(\n        cls,\n        model_paths: List[Text],\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: Union[float, List[float]] = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        anchor_part: Optional[str] = None,\n    ) -&gt; \"Predictor\":\n        \"\"\"Create the appropriate `Predictor` subclass from from the ckpt path.\n\n        Args:\n            model_paths: (List[str]) List of paths to the directory where the best.ckpt (or from SLEAP &lt;=1.4 best_model - only UNet backbone is supported)\n                and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json) are saved.\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mps\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n        Returns:\n            A subclass of `Predictor`.\n\n        See also: `SingleInstancePredictor`, `TopDownPredictor`, `BottomUpPredictor`,\n            `MoveNetPredictor`, `TopDownMultiClassPredictor`,\n            `BottomUpMultiClassPredictor`.\n        \"\"\"\n        model_configs = []\n        for model_path in model_paths:\n            path = Path(model_path)\n            if path / \"training_config.yaml\" in path.iterdir():\n                model_configs.append(\n                    OmegaConf.load((path / \"training_config.yaml\").as_posix())\n                )\n            elif path / \"training_config.json\" in path.iterdir():\n                model_configs.append(\n                    TrainingJobConfig.load_sleap_config(\n                        (path / \"training_config.json\").as_posix()\n                    )\n                )\n            else:\n                raise ValueError(\n                    f\"Could not find training_config.yaml or training_config.json in {model_path}\"\n                )\n\n        model_names = []\n        for config in model_configs:\n            model_names.append(get_model_type_from_cfg(config=config))\n\n        if \"single_instance\" in model_names:\n            confmap_ckpt_path = model_paths[model_names.index(\"single_instance\")]\n            predictor = SingleInstancePredictor.from_trained_models(\n                confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n            )\n\n        elif (\n            \"centroid\" in model_names\n            or \"centered_instance\" in model_names\n            or \"multi_class_topdown\" in model_names\n        ):\n            centroid_ckpt_path = None\n            confmap_ckpt_path = None\n            if \"centroid\" in model_names:\n                centroid_ckpt_path = model_paths[model_names.index(\"centroid\")]\n                predictor = TopDownPredictor.from_trained_models(\n                    centroid_ckpt_path=centroid_ckpt_path,\n                    confmap_ckpt_path=confmap_ckpt_path,\n                    backbone_ckpt_path=backbone_ckpt_path,\n                    head_ckpt_path=head_ckpt_path,\n                    peak_threshold=peak_threshold,\n                    integral_refinement=integral_refinement,\n                    integral_patch_size=integral_patch_size,\n                    batch_size=batch_size,\n                    max_instances=max_instances,\n                    return_confmaps=return_confmaps,\n                    device=device,\n                    preprocess_config=preprocess_config,\n                    anchor_part=anchor_part,\n                )\n            if \"centered_instance\" in model_names:\n                confmap_ckpt_path = model_paths[model_names.index(\"centered_instance\")]\n                # create an instance of the TopDown predictor class\n                predictor = TopDownPredictor.from_trained_models(\n                    centroid_ckpt_path=centroid_ckpt_path,\n                    confmap_ckpt_path=confmap_ckpt_path,\n                    backbone_ckpt_path=backbone_ckpt_path,\n                    head_ckpt_path=head_ckpt_path,\n                    peak_threshold=peak_threshold,\n                    integral_refinement=integral_refinement,\n                    integral_patch_size=integral_patch_size,\n                    batch_size=batch_size,\n                    max_instances=max_instances,\n                    return_confmaps=return_confmaps,\n                    device=device,\n                    preprocess_config=preprocess_config,\n                    anchor_part=anchor_part,\n                )\n            elif \"multi_class_topdown\" in model_names:\n                confmap_ckpt_path = model_paths[\n                    model_names.index(\"multi_class_topdown\")\n                ]\n                # create an instance of the TopDown predictor class\n                predictor = TopDownMultiClassPredictor.from_trained_models(\n                    centroid_ckpt_path=centroid_ckpt_path,\n                    confmap_ckpt_path=confmap_ckpt_path,\n                    backbone_ckpt_path=backbone_ckpt_path,\n                    head_ckpt_path=head_ckpt_path,\n                    peak_threshold=peak_threshold,\n                    integral_refinement=integral_refinement,\n                    integral_patch_size=integral_patch_size,\n                    batch_size=batch_size,\n                    max_instances=max_instances,\n                    return_confmaps=return_confmaps,\n                    device=device,\n                    preprocess_config=preprocess_config,\n                    anchor_part=anchor_part,\n                )\n\n        elif \"bottomup\" in model_names:\n            bottomup_ckpt_path = model_paths[model_names.index(\"bottomup\")]\n            predictor = BottomUpPredictor.from_trained_models(\n                bottomup_ckpt_path=bottomup_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n            )\n\n        elif \"multi_class_bottomup\" in model_names:\n            bottomup_ckpt_path = model_paths[model_names.index(\"multi_class_bottomup\")]\n            predictor = BottomUpMultiClassPredictor.from_trained_models(\n                bottomup_ckpt_path=bottomup_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n            )\n\n        else:\n            message = f\"Could not create predictor from model paths:\\n{model_paths}\"\n            logger.error(message)\n            raise ValueError(message)\n        return predictor\n\n    @classmethod\n    @abstractmethod\n    def from_trained_models(cls, *args, **kwargs):\n        \"\"\"Initialize the Predictor class for certain type of model.\"\"\"\n\n    @abstractmethod\n    def make_pipeline(\n        self,\n        data_path: str,\n        queue_maxsize: int = 32,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Create the data pipeline.\"\"\"\n\n    @abstractmethod\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the Inference model.\"\"\"\n\n    def _convert_tensors_to_numpy(self, output):\n        \"\"\"Convert tensors in output dictionary to numpy arrays.\"\"\"\n        for k, v in output.items():\n            if isinstance(v, torch.Tensor):\n                output[k] = output[k].cpu().numpy()\n            if isinstance(v, list) and isinstance(v[0], torch.Tensor):\n                for n in range(len(v)):\n                    v[n] = v[n].cpu().numpy()\n        return output\n\n    def _process_batch(self) -&gt; tuple:\n        \"\"\"Process a single batch of frames from the pipeline.\n\n        Returns:\n            Tuple of (imgs, fidxs, vidxs, org_szs, instances, eff_scales, done)\n            where done is True if the pipeline has finished.\n        \"\"\"\n        imgs = []\n        fidxs = []\n        vidxs = []\n        org_szs = []\n        instances = []\n        eff_scales = []\n        done = False\n\n        for _ in range(self.batch_size):\n            frame = self.pipeline.frame_buffer.get()\n            if frame[\"image\"] is None:\n                done = True\n                break\n            frame[\"image\"], eff_scale = apply_sizematcher(\n                frame[\"image\"],\n                self.preprocess_config[\"max_height\"],\n                self.preprocess_config[\"max_width\"],\n            )\n            if self.instances_key:\n                frame[\"instances\"] = frame[\"instances\"] * eff_scale\n            if self.preprocess_config[\"ensure_rgb\"] and frame[\"image\"].shape[-3] != 3:\n                frame[\"image\"] = frame[\"image\"].repeat(1, 3, 1, 1)\n            elif (\n                self.preprocess_config[\"ensure_grayscale\"]\n                and frame[\"image\"].shape[-3] != 1\n            ):\n                frame[\"image\"] = F.rgb_to_grayscale(\n                    frame[\"image\"], num_output_channels=1\n                )\n\n            eff_scales.append(torch.tensor(eff_scale))\n            imgs.append(frame[\"image\"].unsqueeze(dim=0))\n            fidxs.append(frame[\"frame_idx\"])\n            vidxs.append(frame[\"video_idx\"])\n            org_szs.append(frame[\"orig_size\"].unsqueeze(dim=0))\n            if self.instances_key:\n                instances.append(frame[\"instances\"].unsqueeze(dim=0))\n\n        return imgs, fidxs, vidxs, org_szs, instances, eff_scales, done\n\n    def _run_inference_on_batch(\n        self, imgs, fidxs, vidxs, org_szs, instances, eff_scales\n    ) -&gt; Iterator[Dict[str, np.ndarray]]:\n        \"\"\"Run inference on a prepared batch of frames.\n\n        Args:\n            imgs: List of image tensors.\n            fidxs: List of frame indices.\n            vidxs: List of video indices.\n            org_szs: List of original sizes.\n            instances: List of instance tensors.\n            eff_scales: List of effective scales.\n\n        Yields:\n            Dictionaries containing inference results for each frame.\n        \"\"\"\n        # TODO: all preprocessing should be moved into InferenceModels to be exportable.\n        imgs = torch.concatenate(imgs, dim=0)\n        fidxs = torch.tensor(fidxs, dtype=torch.int32)\n        vidxs = torch.tensor(vidxs, dtype=torch.int32)\n        org_szs = torch.concatenate(org_szs, dim=0)\n        eff_scales = torch.tensor(eff_scales, dtype=torch.float32)\n        if self.instances_key:\n            instances = torch.concatenate(instances, dim=0)\n        ex = {\n            \"image\": imgs,\n            \"frame_idx\": fidxs,\n            \"video_idx\": vidxs,\n            \"orig_size\": org_szs,\n            \"eff_scale\": eff_scales,\n        }\n        if self.instances_key:\n            ex[\"instances\"] = instances\n        if self.preprocess:\n            scale = self.preprocess_config[\"scale\"]\n            if scale != 1.0:\n                if self.instances_key:\n                    ex[\"image\"], ex[\"instances\"] = apply_resizer(\n                        ex[\"image\"], ex[\"instances\"]\n                    )\n                else:\n                    ex[\"image\"] = resize_image(ex[\"image\"], scale)\n            ex[\"image\"] = apply_pad_to_stride(ex[\"image\"], self.max_stride)\n        outputs_list = self.inference_model(ex)\n        if outputs_list is not None:\n            for output in outputs_list:\n                output = self._convert_tensors_to_numpy(output)\n                yield output\n\n    def _predict_generator(self) -&gt; Iterator[Dict[str, np.ndarray]]:\n        \"\"\"Create a generator that yields batches of inference results.\n\n        This method handles creating a pipeline object depending on the model type and\n        provider for loading the data, as well as looping over the batches and\n        running inference.\n\n        Returns:\n            A generator yielding batches predicted results as dictionaries of numpy\n            arrays.\n        \"\"\"\n        # Initialize inference model if needed.\n\n        if self.inference_model is None:\n            self._initialize_inference_model()\n\n        # Loop over data batches.\n        self.pipeline.start()\n        total_frames = self.pipeline.total_len()\n\n        try:\n            if self.gui:\n                # GUI mode: emit JSON progress lines\n                yield from self._predict_generator_gui(total_frames)\n            else:\n                # Normal mode: use Rich progress bar\n                yield from self._predict_generator_rich(total_frames)\n\n        except KeyboardInterrupt:\n            logger.info(\"Inference interrupted by user\")\n            raise KeyboardInterrupt\n\n        except Exception as e:\n            message = f\"Error in _predict_generator: {e}\"\n            logger.error(message)\n            raise Exception(message)\n\n        self.pipeline.join()\n\n    def _predict_generator_gui(\n        self, total_frames: int\n    ) -&gt; Iterator[Dict[str, np.ndarray]]:\n        \"\"\"Generator for GUI mode with JSON progress output.\n\n        Args:\n            total_frames: Total number of frames to process.\n\n        Yields:\n            Dictionaries containing inference results for each frame.\n        \"\"\"\n        start_time = time()\n        frames_processed = 0\n        last_report = time()\n        done = False\n\n        while not done:\n            imgs, fidxs, vidxs, org_szs, instances, eff_scales, done = (\n                self._process_batch()\n            )\n\n            if imgs:\n                yield from self._run_inference_on_batch(\n                    imgs, fidxs, vidxs, org_szs, instances, eff_scales\n                )\n\n                # Update progress\n                num_frames = len(fidxs)\n                frames_processed += num_frames\n\n                # Emit JSON progress (throttled to ~4Hz)\n                if time() - last_report &gt; 0.25:\n                    elapsed = time() - start_time\n                    rate = frames_processed / elapsed if elapsed &gt; 0 else 0\n                    remaining = total_frames - frames_processed\n                    eta = remaining / rate if rate &gt; 0 else 0\n\n                    progress_data = {\n                        \"n_processed\": frames_processed,\n                        \"n_total\": total_frames,\n                        \"rate\": round(rate, 1),\n                        \"eta\": round(eta, 1),\n                    }\n                    print(json.dumps(progress_data), flush=True)\n                    last_report = time()\n\n        # Final progress emit to ensure 100% is shown\n        elapsed = time() - start_time\n        progress_data = {\n            \"n_processed\": total_frames,\n            \"n_total\": total_frames,\n            \"rate\": round(frames_processed / elapsed, 1) if elapsed &gt; 0 else 0,\n            \"eta\": 0,\n        }\n        print(json.dumps(progress_data), flush=True)\n\n    def _predict_generator_rich(\n        self, total_frames: int\n    ) -&gt; Iterator[Dict[str, np.ndarray]]:\n        \"\"\"Generator for normal mode with Rich progress bar.\n\n        Args:\n            total_frames: Total number of frames to process.\n\n        Yields:\n            Dictionaries containing inference results for each frame.\n        \"\"\"\n        with Progress(\n            \"{task.description}\",\n            BarColumn(),\n            \"[progress.percentage]{task.percentage:&gt;3.0f}%\",\n            MofNCompleteColumn(),\n            \"ETA:\",\n            TimeRemainingColumn(),\n            \"Elapsed:\",\n            TimeElapsedColumn(),\n            RateColumn(),\n            auto_refresh=False,\n            refresh_per_second=4,\n            speed_estimate_period=5,\n        ) as progress:\n            task = progress.add_task(\"Predicting...\", total=total_frames)\n            last_report = time()\n            done = False\n\n            while not done:\n                imgs, fidxs, vidxs, org_szs, instances, eff_scales, done = (\n                    self._process_batch()\n                )\n\n                if imgs:\n                    yield from self._run_inference_on_batch(\n                        imgs, fidxs, vidxs, org_szs, instances, eff_scales\n                    )\n\n                    # Advance progress\n                    num_frames = len(fidxs)\n                    progress.update(task, advance=num_frames)\n\n                # Manually refresh progress bar\n                if time() - last_report &gt; 0.25:\n                    progress.refresh()\n                    last_report = time()\n\n        self.pipeline.join()\n\n    def predict(\n        self,\n        make_labels: bool = True,\n    ) -&gt; Union[List[Dict[str, np.ndarray]], sio.Labels]:\n        \"\"\"Run inference on a data source.\n\n        Args:\n            make_labels: If `True` (the default), returns a `sio.Labels` instance with\n                `sio.PredictedInstance`s. If `False`, just return a list of\n                dictionaries containing the raw arrays returned by the inference model.\n\n        Returns:\n            A `sio.Labels` with `sio.PredictedInstance`s if `make_labels` is `True`,\n            otherwise a list of dictionaries containing batches of numpy arrays with the\n            raw results.\n        \"\"\"\n        # Initialize inference loop generator.\n        generator = self._predict_generator()\n\n        if make_labels:\n            # Create SLEAP data structures from the predictions.\n            pred_labels = self._make_labeled_frames_from_generator(generator)\n            return pred_labels\n\n        else:\n            # Just return the raw results.\n            return list(generator)\n\n    @abstractmethod\n    def _make_labeled_frames_from_generator(self, generator) -&gt; sio.Labels:\n        \"\"\"Create `sio.Labels` object from the predictions.\"\"\"\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.from_model_paths","title":"<code>from_model_paths(model_paths, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, anchor_part=None)</code>  <code>classmethod</code>","text":"<p>Create the appropriate <code>Predictor</code> subclass from from the ckpt path.</p> <p>Parameters:</p> Name Type Description Default <code>model_paths</code> <code>List[Text]</code> <p>(List[str]) List of paths to the directory where the best.ckpt (or from SLEAP &lt;=1.4 best_model - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json) are saved.</p> required <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown centroid and centered-instance model, where the first element corresponds to centroid model peak finding threshold and the second element is for centered-instance model peak finding.</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Predictor</code> <p>A subclass of <code>Predictor</code>.</p> <code>SingleInstancePredictor</code>, <code>TopDownPredictor</code>, <code>BottomUpPredictor</code>, <p><code>MoveNetPredictor</code>, <code>TopDownMultiClassPredictor</code>, <code>BottomUpMultiClassPredictor</code>.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_model_paths(\n    cls,\n    model_paths: List[Text],\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: Union[float, List[float]] = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    anchor_part: Optional[str] = None,\n) -&gt; \"Predictor\":\n    \"\"\"Create the appropriate `Predictor` subclass from from the ckpt path.\n\n    Args:\n        model_paths: (List[str]) List of paths to the directory where the best.ckpt (or from SLEAP &lt;=1.4 best_model - only UNet backbone is supported)\n            and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json) are saved.\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n            centroid and centered-instance model, where the first element corresponds\n            to centroid model peak finding threshold and the second element is for\n            centered-instance model peak finding.\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n    Returns:\n        A subclass of `Predictor`.\n\n    See also: `SingleInstancePredictor`, `TopDownPredictor`, `BottomUpPredictor`,\n        `MoveNetPredictor`, `TopDownMultiClassPredictor`,\n        `BottomUpMultiClassPredictor`.\n    \"\"\"\n    model_configs = []\n    for model_path in model_paths:\n        path = Path(model_path)\n        if path / \"training_config.yaml\" in path.iterdir():\n            model_configs.append(\n                OmegaConf.load((path / \"training_config.yaml\").as_posix())\n            )\n        elif path / \"training_config.json\" in path.iterdir():\n            model_configs.append(\n                TrainingJobConfig.load_sleap_config(\n                    (path / \"training_config.json\").as_posix()\n                )\n            )\n        else:\n            raise ValueError(\n                f\"Could not find training_config.yaml or training_config.json in {model_path}\"\n            )\n\n    model_names = []\n    for config in model_configs:\n        model_names.append(get_model_type_from_cfg(config=config))\n\n    if \"single_instance\" in model_names:\n        confmap_ckpt_path = model_paths[model_names.index(\"single_instance\")]\n        predictor = SingleInstancePredictor.from_trained_models(\n            confmap_ckpt_path,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n        )\n\n    elif (\n        \"centroid\" in model_names\n        or \"centered_instance\" in model_names\n        or \"multi_class_topdown\" in model_names\n    ):\n        centroid_ckpt_path = None\n        confmap_ckpt_path = None\n        if \"centroid\" in model_names:\n            centroid_ckpt_path = model_paths[model_names.index(\"centroid\")]\n            predictor = TopDownPredictor.from_trained_models(\n                centroid_ckpt_path=centroid_ckpt_path,\n                confmap_ckpt_path=confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n                anchor_part=anchor_part,\n            )\n        if \"centered_instance\" in model_names:\n            confmap_ckpt_path = model_paths[model_names.index(\"centered_instance\")]\n            # create an instance of the TopDown predictor class\n            predictor = TopDownPredictor.from_trained_models(\n                centroid_ckpt_path=centroid_ckpt_path,\n                confmap_ckpt_path=confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n                anchor_part=anchor_part,\n            )\n        elif \"multi_class_topdown\" in model_names:\n            confmap_ckpt_path = model_paths[\n                model_names.index(\"multi_class_topdown\")\n            ]\n            # create an instance of the TopDown predictor class\n            predictor = TopDownMultiClassPredictor.from_trained_models(\n                centroid_ckpt_path=centroid_ckpt_path,\n                confmap_ckpt_path=confmap_ckpt_path,\n                backbone_ckpt_path=backbone_ckpt_path,\n                head_ckpt_path=head_ckpt_path,\n                peak_threshold=peak_threshold,\n                integral_refinement=integral_refinement,\n                integral_patch_size=integral_patch_size,\n                batch_size=batch_size,\n                max_instances=max_instances,\n                return_confmaps=return_confmaps,\n                device=device,\n                preprocess_config=preprocess_config,\n                anchor_part=anchor_part,\n            )\n\n    elif \"bottomup\" in model_names:\n        bottomup_ckpt_path = model_paths[model_names.index(\"bottomup\")]\n        predictor = BottomUpPredictor.from_trained_models(\n            bottomup_ckpt_path=bottomup_ckpt_path,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n        )\n\n    elif \"multi_class_bottomup\" in model_names:\n        bottomup_ckpt_path = model_paths[model_names.index(\"multi_class_bottomup\")]\n        predictor = BottomUpMultiClassPredictor.from_trained_models(\n            bottomup_ckpt_path=bottomup_ckpt_path,\n            backbone_ckpt_path=backbone_ckpt_path,\n            head_ckpt_path=head_ckpt_path,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n        )\n\n    else:\n        message = f\"Could not create predictor from model paths:\\n{model_paths}\"\n        logger.error(message)\n        raise ValueError(message)\n    return predictor\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.from_trained_models","title":"<code>from_trained_models(*args, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Initialize the Predictor class for certain type of model.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_trained_models(cls, *args, **kwargs):\n    \"\"\"Initialize the Predictor class for certain type of model.\"\"\"\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.make_pipeline","title":"<code>make_pipeline(data_path, queue_maxsize=32, frames=None, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>  <code>abstractmethod</code>","text":"<p>Create the data pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@abstractmethod\ndef make_pipeline(\n    self,\n    data_path: str,\n    queue_maxsize: int = 32,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Create the data pipeline.\"\"\"\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.Predictor.predict","title":"<code>predict(make_labels=True)</code>","text":"<p>Run inference on a data source.</p> <p>Parameters:</p> Name Type Description Default <code>make_labels</code> <code>bool</code> <p>If <code>True</code> (the default), returns a <code>sio.Labels</code> instance with <code>sio.PredictedInstance</code>s. If <code>False</code>, just return a list of dictionaries containing the raw arrays returned by the inference model.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[List[Dict[str, ndarray]], Labels]</code> <p>A <code>sio.Labels</code> with <code>sio.PredictedInstance</code>s if <code>make_labels</code> is <code>True</code>, otherwise a list of dictionaries containing batches of numpy arrays with the raw results.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def predict(\n    self,\n    make_labels: bool = True,\n) -&gt; Union[List[Dict[str, np.ndarray]], sio.Labels]:\n    \"\"\"Run inference on a data source.\n\n    Args:\n        make_labels: If `True` (the default), returns a `sio.Labels` instance with\n            `sio.PredictedInstance`s. If `False`, just return a list of\n            dictionaries containing the raw arrays returned by the inference model.\n\n    Returns:\n        A `sio.Labels` with `sio.PredictedInstance`s if `make_labels` is `True`,\n        otherwise a list of dictionaries containing batches of numpy arrays with the\n        raw results.\n    \"\"\"\n    # Initialize inference loop generator.\n    generator = self._predict_generator()\n\n    if make_labels:\n        # Create SLEAP data structures from the predictions.\n        pred_labels = self._make_labeled_frames_from_generator(generator)\n        return pred_labels\n\n    else:\n        # Just return the raw results.\n        return list(generator)\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.RateColumn","title":"<code>RateColumn</code>","text":"<p>               Bases: <code>ProgressColumn</code></p> <p>Renders the progress rate.</p> <p>Methods:</p> Name Description <code>render</code> <p>Show progress rate.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>class RateColumn(rich.progress.ProgressColumn):\n    \"\"\"Renders the progress rate.\"\"\"\n\n    def render(self, task: \"Task\") -&gt; rich.progress.Text:\n        \"\"\"Show progress rate.\"\"\"\n        speed = task.speed\n        if speed is None:\n            return rich.progress.Text(\"?\", style=\"progress.data.speed\")\n        return rich.progress.Text(f\"{speed:.1f} FPS\", style=\"progress.data.speed\")\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.RateColumn.render","title":"<code>render(task)</code>","text":"<p>Show progress rate.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def render(self, task: \"Task\") -&gt; rich.progress.Text:\n    \"\"\"Show progress rate.\"\"\"\n    speed = task.speed\n    if speed is None:\n        return rich.progress.Text(\"?\", style=\"progress.data.speed\")\n    return rich.progress.Text(f\"{speed:.1f} FPS\", style=\"progress.data.speed\")\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.SingleInstancePredictor","title":"<code>SingleInstancePredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>Single-Instance predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained single instance SLEAP-NN model.</p> <p>This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>confmap_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the             single-instance model.</p> <code>confmap_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights for            single-instance model.</p> <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters     in the <code>data_config.preprocessing</code> section.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass SingleInstancePredictor(Predictor):\n    \"\"\"Single-Instance predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained single instance SLEAP-NN model.\n\n    This should be initialized using the `from_trained_models()` constructor.\n\n    Attributes:\n        confmap_config: A Dictionary with the configs used for training the\n                        single-instance model.\n        confmap_model: A LightningModule instance created from the trained weights for\n                       single-instance model.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    confmap_config: Optional[OmegaConf] = attrs.field(default=None)\n    confmap_model: Optional[L.LightningModule] = attrs.field(default=None)\n    backbone_type: str = \"unet\"\n    videos: Optional[List[sio.Video]] = attrs.field(default=None)\n    skeletons: Optional[List[sio.Skeleton]] = attrs.field(default=None)\n    peak_threshold: float = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        self.inference_model = SingleInstanceInferenceModel(\n            torch_model=self.confmap_model,\n            peak_threshold=self.peak_threshold,\n            output_stride=self.confmap_config.model_config.head_configs.single_instance.confmaps.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            input_scale=self.confmap_config.data_config.preprocessing.scale,\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        confmap_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        max_stride: int = 16,\n    ) -&gt; \"SingleInstancePredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            confmap_ckpt_path: Path to a single instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mps\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `SingleInstancePredictor` with the loaded models.\n\n        \"\"\"\n        is_sleap_ckpt = False\n        if (\n            Path(confmap_ckpt_path) / \"training_config.yaml\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            confmap_config = OmegaConf.load(\n                (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(confmap_ckpt_path) / \"training_config.json\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            confmap_config = TrainingJobConfig.load_sleap_config(\n                (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in confmap_config.model_config.backbone_config.items():\n            if v is not None:\n                backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n            confmap_model = SingleInstanceLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"single_instance\",\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=confmap_config.model_config.init_weights,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                backbone_type=backbone_type,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n                weights_only=False,\n            )\n        else:\n            confmap_converted_model = load_legacy_model(\n                model_dir=f\"{confmap_ckpt_path}\"\n            )\n            confmap_model = SingleInstanceLightningModule(\n                backbone_type=backbone_type,\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=confmap_config.model_config.init_weights,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                model_type=\"single_instance\",\n            )\n            confmap_model.eval()\n            confmap_model.model = confmap_converted_model\n            confmap_model.to(device)\n\n        confmap_model.eval()\n\n        skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n        confmap_model.to(device)\n\n        for k, v in preprocess_config.items():\n            if v is None:\n                preprocess_config[k] = (\n                    confmap_config.data_config.preprocessing[k]\n                    if k in confmap_config.data_config.preprocessing\n                    else None\n                )\n\n        # create an instance of SingleInstancePredictor class\n        obj = cls(\n            confmap_config=confmap_config,\n            confmap_model=confmap_model,\n            backbone_type=backbone_type,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n            max_stride=confmap_config.model_config.backbone_config[f\"{backbone_type}\"][\n                \"max_stride\"\n            ],\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        inference_object: Union[str, Path, sio.Labels, sio.Video],\n        queue_maxsize: int = 32,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n            frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n            only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n\n        \"\"\"\n        if isinstance(inference_object, str) or isinstance(inference_object, Path):\n            inference_object = (\n                sio.load_slp(inference_object)\n                if inference_object.endswith(\".slp\")\n                else sio.load_video(\n                    inference_object,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n            )\n\n        self.preprocess = True\n        # LabelsReader provider\n        if isinstance(inference_object, sio.Labels) and video_index is None:\n            provider = LabelsReader\n\n            frame_buffer = Queue(maxsize=queue_maxsize)\n\n            self.pipeline = provider(\n                labels=inference_object,\n                frame_buffer=frame_buffer,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n                exclude_user_labeled=exclude_user_labeled,\n                only_predicted_frames=only_predicted_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n\n            if isinstance(inference_object, sio.Labels) and video_index is not None:\n                labels = inference_object\n                video = labels.videos[video_index]\n                # Filter out user-labeled frames if requested\n                filtered_frames = _filter_user_labeled_frames(\n                    labels, video, frames, exclude_user_labeled\n                )\n                self.pipeline = provider.from_video(\n                    video=video,\n                    queue_maxsize=queue_maxsize,\n                    frames=filtered_frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                frame_buffer = Queue(maxsize=queue_maxsize)\n                self.pipeline = provider(\n                    video=inference_object,\n                    frame_buffer=frame_buffer,\n                    frames=frames,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`.\n                This can be created using the `_predict_generator()` method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        # open video backend for tracking\n        for video in self.videos:\n            if not video.open_backend:\n                video.open()\n\n        predicted_frames = []\n\n        skeleton_idx = 0\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                pred_instances,\n                pred_values,\n                org_size,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"orig_size\"],\n            ):\n                if np.isnan(pred_instances).all():\n                    continue\n                inst = sio.PredictedInstance.from_numpy(\n                    points_data=pred_instances,\n                    skeleton=self.skeletons[skeleton_idx],\n                    score=np.nansum(pred_values),\n                    point_scores=pred_values,\n                )\n                predicted_frames.append(\n                    sio.LabeledFrame(\n                        video=self.videos[video_idx],\n                        frame_idx=frame_idx,\n                        instances=[inst],\n                    )\n                )\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.SingleInstancePredictor.from_trained_models","title":"<code>from_trained_models(confmap_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, return_confmaps=False, device='cpu', preprocess_config=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>confmap_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a single instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>SingleInstancePredictor</code> <p>An instance of <code>SingleInstancePredictor</code> with the loaded models.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    confmap_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    max_stride: int = 16,\n) -&gt; \"SingleInstancePredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        confmap_ckpt_path: Path to a single instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `SingleInstancePredictor` with the loaded models.\n\n    \"\"\"\n    is_sleap_ckpt = False\n    if (\n        Path(confmap_ckpt_path) / \"training_config.yaml\"\n        in Path(confmap_ckpt_path).iterdir()\n    ):\n        confmap_config = OmegaConf.load(\n            (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n        )\n    elif (\n        Path(confmap_ckpt_path) / \"training_config.json\"\n        in Path(confmap_ckpt_path).iterdir()\n    ):\n        is_sleap_ckpt = True\n        confmap_config = TrainingJobConfig.load_sleap_config(\n            (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n        )\n\n    # check which backbone architecture\n    for k, v in confmap_config.model_config.backbone_config.items():\n        if v is not None:\n            backbone_type = k\n            break\n\n    if not is_sleap_ckpt:\n        ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n        confmap_model = SingleInstanceLightningModule.load_from_checkpoint(\n            checkpoint_path=ckpt_path,\n            model_type=\"single_instance\",\n            backbone_config=confmap_config.model_config.backbone_config,\n            head_configs=confmap_config.model_config.head_configs,\n            pretrained_backbone_weights=None,\n            pretrained_head_weights=None,\n            init_weights=confmap_config.model_config.init_weights,\n            lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n            backbone_type=backbone_type,\n            online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=confmap_config.trainer_config.optimizer_name,\n            learning_rate=confmap_config.trainer_config.optimizer.lr,\n            amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            map_location=device,\n            weights_only=False,\n        )\n    else:\n        confmap_converted_model = load_legacy_model(\n            model_dir=f\"{confmap_ckpt_path}\"\n        )\n        confmap_model = SingleInstanceLightningModule(\n            backbone_type=backbone_type,\n            backbone_config=confmap_config.model_config.backbone_config,\n            head_configs=confmap_config.model_config.head_configs,\n            pretrained_backbone_weights=None,\n            pretrained_head_weights=None,\n            init_weights=confmap_config.model_config.init_weights,\n            lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n            online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=confmap_config.trainer_config.optimizer_name,\n            learning_rate=confmap_config.trainer_config.optimizer.lr,\n            amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            model_type=\"single_instance\",\n        )\n        confmap_model.eval()\n        confmap_model.model = confmap_converted_model\n        confmap_model.to(device)\n\n    confmap_model.eval()\n\n    skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n    if backbone_ckpt_path is not None and head_ckpt_path is not None:\n        logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".backbone\" in k\n        }\n        confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    elif backbone_ckpt_path is not None:\n        logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n        ckpt = torch.load(\n            backbone_ckpt_path, map_location=device, weights_only=False\n        )\n        confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    if head_ckpt_path is not None:\n        logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n        ckpt = torch.load(head_ckpt_path, map_location=device, weights_only=False)\n        ckpt[\"state_dict\"] = {\n            k: ckpt[\"state_dict\"][k]\n            for k in ckpt[\"state_dict\"].keys()\n            if \".head_layers\" in k\n        }\n        confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n    confmap_model.to(device)\n\n    for k, v in preprocess_config.items():\n        if v is None:\n            preprocess_config[k] = (\n                confmap_config.data_config.preprocessing[k]\n                if k in confmap_config.data_config.preprocessing\n                else None\n            )\n\n    # create an instance of SingleInstancePredictor class\n    obj = cls(\n        confmap_config=confmap_config,\n        confmap_model=confmap_model,\n        backbone_type=backbone_type,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        return_confmaps=return_confmaps,\n        device=device,\n        preprocess_config=preprocess_config,\n        max_stride=confmap_config.model_config.backbone_config[f\"{backbone_type}\"][\n            \"max_stride\"\n        ],\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.SingleInstancePredictor.make_pipeline","title":"<code>make_pipeline(inference_object, queue_maxsize=32, frames=None, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inference_object</code> <code>Union[str, Path, Labels, Video]</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> or sio.Labels or sio.Video to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 32.</p> <code>32</code> <code>frames</code> <code>Optional[list]</code> <p>List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>exclude_user_labeled</code> <code>bool</code> <p>(bool) <code>True</code> to skip frames that have user-labeled instances. Default: <code>False</code>.</p> <code>False</code> <code>only_predicted_frames</code> <code>bool</code> <p>(bool) <code>True</code> to run inference only on frames that already have predictions. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    inference_object: Union[str, Path, sio.Labels, sio.Video],\n    queue_maxsize: int = 32,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n        frames: List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n        only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n\n    \"\"\"\n    if isinstance(inference_object, str) or isinstance(inference_object, Path):\n        inference_object = (\n            sio.load_slp(inference_object)\n            if inference_object.endswith(\".slp\")\n            else sio.load_video(\n                inference_object,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n        )\n\n    self.preprocess = True\n    # LabelsReader provider\n    if isinstance(inference_object, sio.Labels) and video_index is None:\n        provider = LabelsReader\n\n        frame_buffer = Queue(maxsize=queue_maxsize)\n\n        self.pipeline = provider(\n            labels=inference_object,\n            frame_buffer=frame_buffer,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n            exclude_user_labeled=exclude_user_labeled,\n            only_predicted_frames=only_predicted_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n\n        if isinstance(inference_object, sio.Labels) and video_index is not None:\n            labels = inference_object\n            video = labels.videos[video_index]\n            # Filter out user-labeled frames if requested\n            filtered_frames = _filter_user_labeled_frames(\n                labels, video, frames, exclude_user_labeled\n            )\n            self.pipeline = provider.from_video(\n                video=video,\n                queue_maxsize=queue_maxsize,\n                frames=filtered_frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            frame_buffer = Queue(maxsize=queue_maxsize)\n            self.pipeline = provider(\n                video=inference_object,\n                frame_buffer=frame_buffer,\n                frames=frames,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownMultiClassPredictor","title":"<code>TopDownMultiClassPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>Top-down multi-class predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained TopDown SLEAP-NN model. This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>centroid_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the centroid model.</p> <code>confmap_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the             centered-instance model</p> <code>centroid_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights             for centroid model.</p> <code>confmap_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights            for centered-instance model.</p> <code>centroid_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>centered_instance_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below     this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown     centroid and centered-instance model, where the first element corresponds     to centroid model peak finding threshold and the second element is for     centered-instance model peak finding.</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass TopDownMultiClassPredictor(Predictor):\n    \"\"\"Top-down multi-class predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained TopDown SLEAP-NN model. This should be initialized using the\n    `from_trained_models()` constructor.\n\n    Attributes:\n        centroid_config: A Dictionary with the configs used for training the centroid model.\n        confmap_config: A Dictionary with the configs used for training the\n                        centered-instance model\n        centroid_model: A LightningModule instance created from the trained weights\n                        for centroid model.\n        confmap_model: A LightningModule instance created from the trained weights\n                       for centered-instance model.\n        centroid_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        centered_instance_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    centroid_config: Optional[OmegaConf] = None\n    confmap_config: Optional[OmegaConf] = None\n    centroid_model: Optional[L.LightningModule] = None\n    confmap_model: Optional[L.LightningModule] = None\n    centroid_backbone_type: Optional[str] = None\n    centered_instance_backbone_type: Optional[str] = None\n    videos: Optional[List[sio.Video]] = None\n    skeletons: Optional[List[sio.Skeleton]] = None\n    peak_threshold: Union[float, List[float]] = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    anchor_part: Optional[str] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # Create an instance of CentroidLayer if centroid_config is not None\n        return_crops = False\n        # if both centroid and centered-instance model are provided, set return crops to True\n        if self.confmap_model:\n            return_crops = True\n        if isinstance(self.peak_threshold, list):\n            centroid_peak_threshold = self.peak_threshold[0]\n            centered_instance_peak_threshold = self.peak_threshold[1]\n        else:\n            centroid_peak_threshold = self.peak_threshold\n            centered_instance_peak_threshold = self.peak_threshold\n\n        if self.anchor_part is not None:\n            anchor_ind = self.skeletons[0].node_names.index(self.anchor_part)\n        else:\n            anch_pt = None\n            if self.centroid_config is not None:\n                anch_pt = (\n                    self.centroid_config.model_config.head_configs.centroid.confmaps.anchor_part\n                )\n            if self.confmap_config is not None:\n                anch_pt = (\n                    self.confmap_config.model_config.head_configs.multi_class_topdown.confmaps.anchor_part\n                )\n            anchor_ind = (\n                self.skeletons[0].node_names.index(anch_pt)\n                if anch_pt is not None\n                else None\n            )\n\n        if self.centroid_config is None:\n            centroid_crop_layer = CentroidCrop(\n                use_gt_centroids=True,\n                crop_hw=(\n                    self.preprocess_config.crop_size,\n                    self.preprocess_config.crop_size,\n                ),\n                anchor_ind=anchor_ind,\n                return_crops=return_crops,\n            )\n\n        else:\n            max_stride = self.centroid_config.model_config.backbone_config[\n                f\"{self.centroid_backbone_type}\"\n            ][\"max_stride\"]\n            # initialize centroid crop layer\n            centroid_crop_layer = CentroidCrop(\n                torch_model=self.centroid_model,\n                peak_threshold=centroid_peak_threshold,\n                output_stride=self.centroid_config.model_config.head_configs.centroid.confmaps.output_stride,\n                refinement=self.integral_refinement,\n                integral_patch_size=self.integral_patch_size,\n                return_confmaps=self.return_confmaps,\n                return_crops=return_crops,\n                max_instances=self.max_instances,\n                max_stride=max_stride,\n                input_scale=self.centroid_config.data_config.preprocessing.scale,\n                crop_hw=(\n                    self.preprocess_config.crop_size,\n                    self.preprocess_config.crop_size,\n                ),\n                use_gt_centroids=False,\n            )\n\n        max_stride = self.confmap_config.model_config.backbone_config[\n            f\"{self.centered_instance_backbone_type}\"\n        ][\"max_stride\"]\n        instance_peaks_layer = TopDownMultiClassFindInstancePeaks(\n            torch_model=self.confmap_model,\n            peak_threshold=centered_instance_peak_threshold,\n            output_stride=self.confmap_config.model_config.head_configs.multi_class_topdown.confmaps.output_stride,\n            refinement=self.integral_refinement,\n            integral_patch_size=self.integral_patch_size,\n            return_confmaps=self.return_confmaps,\n            max_stride=max_stride,\n            input_scale=self.confmap_config.data_config.preprocessing.scale,\n        )\n\n        if self.centroid_config is None:\n            self.instances_key = (\n                True  # we need `instances` to get ground-truth centroids\n            )\n\n        # Initialize the inference model with centroid and instance peak layers\n        self.inference_model = TopDownInferenceModel(\n            centroid_crop=centroid_crop_layer, instance_peaks=instance_peaks_layer\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        centroid_ckpt_path: Optional[Text] = None,\n        confmap_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        anchor_part: Optional[str] = None,\n        max_stride: int = 16,\n    ) -&gt; \"TopDownPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n            confmap_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mps\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n            max_stride: The maximum stride of the backbone network, as specified in the model's\n                `backbone_config`. This determines the downsampling factor applied by the backbone,\n                and is used to ensure that input images are padded or resized to be compatible\n                with the model's architecture. Default: 16.\n\n        Returns:\n            An instance of `TopDownPredictor` with the loaded models.\n\n            One of the two models can be left as `None` to perform inference with ground\n            truth data. This will only work with `LabelsReader` as the provider.\n\n        \"\"\"\n        centered_instance_backbone_type = None\n        centroid_backbone_type = None\n        if centroid_ckpt_path is not None:\n            is_sleap_ckpt = False\n            if (\n                Path(centroid_ckpt_path) / \"training_config.yaml\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                centroid_config = OmegaConf.load(\n                    (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(centroid_ckpt_path) / \"training_config.json\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                centroid_config = TrainingJobConfig.load_sleap_config(\n                    (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            # Load centroid model.\n            skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n            # check which backbone architecture\n            for k, v in centroid_config.model_config.backbone_config.items():\n                if v is not None:\n                    centroid_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n\n                centroid_model = CentroidLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"centroid\",\n                    backbone_type=centroid_backbone_type,\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=centroid_config.model_config.init_weights,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                    map_location=device,\n                    weights_only=False,\n                )\n\n            else:\n                centroid_converted_model = load_legacy_model(\n                    model_dir=f\"{centroid_ckpt_path}\"\n                )\n                centroid_model = CentroidLightningModule(\n                    model_type=\"centroid\",\n                    backbone_type=centroid_backbone_type,\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=centroid_config.model_config.init_weights,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                )\n                centroid_model.eval()\n                centroid_model.model = centroid_converted_model\n                centroid_model.to(device)\n\n            centroid_model.eval()\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                centroid_model.load_state_dict(\n                    ckpt[\"state_dict\"],\n                    strict=False,\n                    weights_only=False,\n                )\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            centroid_model.to(device)\n\n        else:\n            centroid_config = None\n            centroid_model = None\n\n        if confmap_ckpt_path is not None:\n            # Load confmap model.\n            is_sleap_ckpt = False\n            if (\n                Path(confmap_ckpt_path) / \"training_config.yaml\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                confmap_config = OmegaConf.load(\n                    (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(confmap_ckpt_path) / \"training_config.json\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                confmap_config = TrainingJobConfig.load_sleap_config(\n                    (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            # check which backbone architecture\n            for k, v in confmap_config.model_config.backbone_config.items():\n                if v is not None:\n                    centered_instance_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n\n                confmap_model = TopDownCenteredInstanceMultiClassLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"multi_class_topdown\",\n                    backbone_type=centered_instance_backbone_type,\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=confmap_config.model_config.init_weights,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                    map_location=device,\n                    weights_only=False,\n                )\n            else:\n                confmap_converted_model = load_legacy_model(\n                    model_dir=f\"{confmap_ckpt_path}\"\n                )\n                confmap_model = TopDownCenteredInstanceMultiClassLightningModule(\n                    model_type=\"multi_class_topdown\",\n                    backbone_type=centered_instance_backbone_type,\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=confmap_config.model_config.init_weights,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                )\n                confmap_model.eval()\n                confmap_model.model = confmap_converted_model\n                confmap_model.to(device)\n\n            confmap_model.eval()\n            skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path,\n                    map_location=device,\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            confmap_model.to(device)\n\n        else:\n            confmap_config = None\n            confmap_model = None\n\n        if centroid_config is None and confmap_config is None:\n            message = (\n                \"Both a centroid and a confidence map model must be provided to \"\n                \"initialize a TopDownMultiClassPredictor.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        if centroid_config is not None:\n            preprocess_config[\"scale\"] = (\n                centroid_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                centroid_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                centroid_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                centroid_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                centroid_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        else:\n            preprocess_config[\"scale\"] = (\n                confmap_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                confmap_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                confmap_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                confmap_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                confmap_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        preprocess_config[\"crop_size\"] = (\n            confmap_config.data_config.preprocessing.crop_size\n            if preprocess_config[\"crop_size\"] is None and confmap_config is not None\n            else preprocess_config[\"crop_size\"]\n        )\n\n        # create an instance of TopDownPredictor class\n        obj = cls(\n            centroid_config=centroid_config,\n            centroid_model=centroid_model,\n            confmap_config=confmap_config,\n            confmap_model=confmap_model,\n            centroid_backbone_type=centroid_backbone_type,\n            centered_instance_backbone_type=centered_instance_backbone_type,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n            anchor_part=anchor_part,\n            max_stride=(\n                centroid_config.model_config.backbone_config[\n                    f\"{centroid_backbone_type}\"\n                ][\"max_stride\"]\n                if centroid_config is not None\n                else confmap_config.model_config.backbone_config[\n                    f\"{centered_instance_backbone_type}\"\n                ][\"max_stride\"]\n            ),\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        inference_object: Union[str, Path, sio.Labels, sio.Video],\n        queue_maxsize: int = 32,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n            frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n            only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        if isinstance(inference_object, str) or isinstance(inference_object, Path):\n            inference_object = (\n                sio.load_slp(inference_object)\n                if inference_object.endswith(\".slp\")\n                else sio.load_video(\n                    inference_object,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n            )\n\n        # LabelsReader provider\n        if isinstance(inference_object, sio.Labels) and video_index is None:\n            provider = LabelsReader\n\n            self.preprocess = False\n\n            frame_buffer = Queue(maxsize=queue_maxsize)\n\n            self.pipeline = provider(\n                labels=inference_object,\n                frame_buffer=frame_buffer,\n                instances_key=self.instances_key,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n                exclude_user_labeled=exclude_user_labeled,\n                only_predicted_frames=only_predicted_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n            if self.centroid_config is None:\n                message = (\n                    \"Ground truth data was not detected... \"\n                    \"Please load both models when predicting on non-ground-truth data.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n\n            self.preprocess = False\n\n            if isinstance(inference_object, sio.Labels) and video_index is not None:\n                labels = inference_object\n                video = labels.videos[video_index]\n                # Filter out user-labeled frames if requested\n                filtered_frames = _filter_user_labeled_frames(\n                    labels, video, frames, exclude_user_labeled\n                )\n                self.pipeline = provider.from_video(\n                    video=video,\n                    queue_maxsize=queue_maxsize,\n                    frames=filtered_frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                frame_buffer = Queue(maxsize=queue_maxsize)\n                self.pipeline = provider(\n                    video=inference_object,\n                    frame_buffer=frame_buffer,\n                    frames=frames,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        # open video backend for tracking\n        for video in self.videos:\n            if not video.open_backend:\n                video.open()\n\n        preds = defaultdict(list)\n        predicted_frames = []\n        skeleton_idx = 0\n\n        tracks = [\n            sio.Track(name=x)\n            for x in self.confmap_config.model_config.head_configs.multi_class_topdown.class_vectors.classes\n        ]\n\n        # Loop through each predicted instance.\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                bbox,\n                pred_instances,\n                pred_values,\n                centroid_val,\n                org_size,\n                class_ind,\n                instance_score,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"instance_bbox\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"centroid_val\"],\n                ex[\"orig_size\"],\n                ex[\"pred_class_inds\"],\n                ex[\"instance_scores\"],\n            ):\n                if np.isnan(pred_instances).all():\n                    continue\n                pred_instances = pred_instances + bbox.squeeze(axis=0)[0, :]\n\n                track = None\n                if tracks is not None:\n                    track = tracks[class_ind]\n\n                preds[(int(video_idx), int(frame_idx))].append(\n                    sio.PredictedInstance.from_numpy(\n                        points_data=pred_instances,\n                        skeleton=self.skeletons[skeleton_idx],\n                        point_scores=pred_values,\n                        score=centroid_val,\n                        track=track,\n                        tracking_score=instance_score,\n                    )\n                )\n        for key, inst in preds.items():\n            # Create list of LabeledFrames.\n            video_idx, frame_idx = key\n            lf = sio.LabeledFrame(\n                video=self.videos[video_idx],\n                frame_idx=frame_idx,\n                instances=inst,\n            )\n\n            predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownMultiClassPredictor.from_trained_models","title":"<code>from_trained_models(centroid_ckpt_path=None, confmap_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, anchor_part=None, max_stride=16)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).</p> <code>None</code> <code>confmap_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>None</code> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>TopDownPredictor</code> <p>An instance of <code>TopDownPredictor</code> with the loaded models.</p> <p>One of the two models can be left as <code>None</code> to perform inference with ground truth data. This will only work with <code>LabelsReader</code> as the provider.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    centroid_ckpt_path: Optional[Text] = None,\n    confmap_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    anchor_part: Optional[str] = None,\n    max_stride: int = 16,\n) -&gt; \"TopDownPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n        confmap_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    Returns:\n        An instance of `TopDownPredictor` with the loaded models.\n\n        One of the two models can be left as `None` to perform inference with ground\n        truth data. This will only work with `LabelsReader` as the provider.\n\n    \"\"\"\n    centered_instance_backbone_type = None\n    centroid_backbone_type = None\n    if centroid_ckpt_path is not None:\n        is_sleap_ckpt = False\n        if (\n            Path(centroid_ckpt_path) / \"training_config.yaml\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            centroid_config = OmegaConf.load(\n                (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(centroid_ckpt_path) / \"training_config.json\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            centroid_config = TrainingJobConfig.load_sleap_config(\n                (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # Load centroid model.\n        skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n        # check which backbone architecture\n        for k, v in centroid_config.model_config.backbone_config.items():\n            if v is not None:\n                centroid_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n\n            centroid_model = CentroidLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"centroid\",\n                backbone_type=centroid_backbone_type,\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=centroid_config.model_config.init_weights,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n                weights_only=False,\n            )\n\n        else:\n            centroid_converted_model = load_legacy_model(\n                model_dir=f\"{centroid_ckpt_path}\"\n            )\n            centroid_model = CentroidLightningModule(\n                model_type=\"centroid\",\n                backbone_type=centroid_backbone_type,\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=centroid_config.model_config.init_weights,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n            )\n            centroid_model.eval()\n            centroid_model.model = centroid_converted_model\n            centroid_model.to(device)\n\n        centroid_model.eval()\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            centroid_model.load_state_dict(\n                ckpt[\"state_dict\"],\n                strict=False,\n                weights_only=False,\n            )\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        centroid_model.to(device)\n\n    else:\n        centroid_config = None\n        centroid_model = None\n\n    if confmap_ckpt_path is not None:\n        # Load confmap model.\n        is_sleap_ckpt = False\n        if (\n            Path(confmap_ckpt_path) / \"training_config.yaml\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            confmap_config = OmegaConf.load(\n                (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(confmap_ckpt_path) / \"training_config.json\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            confmap_config = TrainingJobConfig.load_sleap_config(\n                (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        # check which backbone architecture\n        for k, v in confmap_config.model_config.backbone_config.items():\n            if v is not None:\n                centered_instance_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n\n            confmap_model = TopDownCenteredInstanceMultiClassLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"multi_class_topdown\",\n                backbone_type=centered_instance_backbone_type,\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=confmap_config.model_config.init_weights,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n                weights_only=False,\n            )\n        else:\n            confmap_converted_model = load_legacy_model(\n                model_dir=f\"{confmap_ckpt_path}\"\n            )\n            confmap_model = TopDownCenteredInstanceMultiClassLightningModule(\n                model_type=\"multi_class_topdown\",\n                backbone_type=centered_instance_backbone_type,\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=confmap_config.model_config.init_weights,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            )\n            confmap_model.eval()\n            confmap_model.model = confmap_converted_model\n            confmap_model.to(device)\n\n        confmap_model.eval()\n        skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path,\n                map_location=device,\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        confmap_model.to(device)\n\n    else:\n        confmap_config = None\n        confmap_model = None\n\n    if centroid_config is None and confmap_config is None:\n        message = (\n            \"Both a centroid and a confidence map model must be provided to \"\n            \"initialize a TopDownMultiClassPredictor.\"\n        )\n        logger.error(message)\n        raise ValueError(message)\n\n    if centroid_config is not None:\n        preprocess_config[\"scale\"] = (\n            centroid_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            centroid_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            centroid_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            centroid_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            centroid_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    else:\n        preprocess_config[\"scale\"] = (\n            confmap_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            confmap_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            confmap_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            confmap_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            confmap_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    preprocess_config[\"crop_size\"] = (\n        confmap_config.data_config.preprocessing.crop_size\n        if preprocess_config[\"crop_size\"] is None and confmap_config is not None\n        else preprocess_config[\"crop_size\"]\n    )\n\n    # create an instance of TopDownPredictor class\n    obj = cls(\n        centroid_config=centroid_config,\n        centroid_model=centroid_model,\n        confmap_config=confmap_config,\n        confmap_model=confmap_model,\n        centroid_backbone_type=centroid_backbone_type,\n        centered_instance_backbone_type=centered_instance_backbone_type,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        device=device,\n        preprocess_config=preprocess_config,\n        anchor_part=anchor_part,\n        max_stride=(\n            centroid_config.model_config.backbone_config[\n                f\"{centroid_backbone_type}\"\n            ][\"max_stride\"]\n            if centroid_config is not None\n            else confmap_config.model_config.backbone_config[\n                f\"{centered_instance_backbone_type}\"\n            ][\"max_stride\"]\n        ),\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownMultiClassPredictor.make_pipeline","title":"<code>make_pipeline(inference_object, queue_maxsize=32, frames=None, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inference_object</code> <code>Union[str, Path, Labels, Video]</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> or sio.Labels or sio.Video to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 32.</p> <code>32</code> <code>frames</code> <code>Optional[list]</code> <p>(list) List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>exclude_user_labeled</code> <code>bool</code> <p>(bool) <code>True</code> to skip frames that have user-labeled instances. Default: <code>False</code>.</p> <code>False</code> <code>only_predicted_frames</code> <code>bool</code> <p>(bool) <code>True</code> to run inference only on frames that already have predictions. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    inference_object: Union[str, Path, sio.Labels, sio.Video],\n    queue_maxsize: int = 32,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n        frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n        only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    if isinstance(inference_object, str) or isinstance(inference_object, Path):\n        inference_object = (\n            sio.load_slp(inference_object)\n            if inference_object.endswith(\".slp\")\n            else sio.load_video(\n                inference_object,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n        )\n\n    # LabelsReader provider\n    if isinstance(inference_object, sio.Labels) and video_index is None:\n        provider = LabelsReader\n\n        self.preprocess = False\n\n        frame_buffer = Queue(maxsize=queue_maxsize)\n\n        self.pipeline = provider(\n            labels=inference_object,\n            frame_buffer=frame_buffer,\n            instances_key=self.instances_key,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n            exclude_user_labeled=exclude_user_labeled,\n            only_predicted_frames=only_predicted_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n        if self.centroid_config is None:\n            message = (\n                \"Ground truth data was not detected... \"\n                \"Please load both models when predicting on non-ground-truth data.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        self.preprocess = False\n\n        if isinstance(inference_object, sio.Labels) and video_index is not None:\n            labels = inference_object\n            video = labels.videos[video_index]\n            # Filter out user-labeled frames if requested\n            filtered_frames = _filter_user_labeled_frames(\n                labels, video, frames, exclude_user_labeled\n            )\n            self.pipeline = provider.from_video(\n                video=video,\n                queue_maxsize=queue_maxsize,\n                frames=filtered_frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            frame_buffer = Queue(maxsize=queue_maxsize)\n            self.pipeline = provider(\n                video=inference_object,\n                frame_buffer=frame_buffer,\n                frames=frames,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownPredictor","title":"<code>TopDownPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>Top-down multi-instance predictor.</p> <p>This high-level class handles initialization, preprocessing and predicting using a trained TopDown SLEAP-NN model. This should be initialized using the <code>from_trained_models()</code> constructor.</p> <p>Attributes:</p> Name Type Description <code>centroid_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the centroid model.</p> <code>confmap_config</code> <code>Optional[OmegaConf]</code> <p>A Dictionary with the configs used for training the             centered-instance model</p> <code>centroid_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights             for centroid model.</p> <code>confmap_model</code> <code>Optional[LightningModule]</code> <p>A LightningModule instance created from the trained weights            for centered-instance model.</p> <code>centroid_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>centered_instance_backbone_type</code> <code>Optional[str]</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> <code>videos</code> <code>Optional[List[Video]]</code> <p>List of <code>sio.Video</code> objects for creating the <code>sio.Labels</code> object from             the output predictions.</p> <code>skeletons</code> <code>Optional[List[Skeleton]]</code> <p>List of <code>sio.Skeleton</code> objects for creating <code>sio.Labels</code> object from             the output predictions.</p> <code>peak_threshold</code> <code>Union[float, List[float]]</code> <p>(float) Minimum confidence threshold. Peaks with values below     this will be ignored. Default: 0.2. This can also be <code>List[float]</code> for topdown     centroid and centered-instance model, where the first element corresponds     to centroid model peak finding threshold and the second element is for     centered-instance model peak finding.</p> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>tracker</code> <code>Optional[Tracker]</code> <p>A <code>sleap_nn.tracking.Tracker</code> that will be called to associate detections over time. Predicted instances will not be assigned to tracks if if this is <code>None</code>.</p> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>max_stride</code> <code>int</code> <p>The maximum stride of the backbone network, as specified in the model's <code>backbone_config</code>. This determines the downsampling factor applied by the backbone, and is used to ensure that input images are padded or resized to be compatible with the model's architecture. Default: 16.</p> <p>Methods:</p> Name Description <code>from_trained_models</code> <p>Create predictor from saved models.</p> <code>make_pipeline</code> <p>Make a data loading pipeline.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@attrs.define\nclass TopDownPredictor(Predictor):\n    \"\"\"Top-down multi-instance predictor.\n\n    This high-level class handles initialization, preprocessing and predicting using a\n    trained TopDown SLEAP-NN model. This should be initialized using the\n    `from_trained_models()` constructor.\n\n    Attributes:\n        centroid_config: A Dictionary with the configs used for training the centroid model.\n        confmap_config: A Dictionary with the configs used for training the\n                        centered-instance model\n        centroid_model: A LightningModule instance created from the trained weights\n                        for centroid model.\n        confmap_model: A LightningModule instance created from the trained weights\n                       for centered-instance model.\n        centroid_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        centered_instance_backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        videos: List of `sio.Video` objects for creating the `sio.Labels` object from\n                        the output predictions.\n        skeletons: List of `sio.Skeleton` objects for creating `sio.Labels` object from\n                        the output predictions.\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2. This can also be `List[float]` for topdown\n                centroid and centered-instance model, where the first element corresponds\n                to centroid model peak finding threshold and the second element is for\n                centered-instance model peak finding.\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        tracker: A `sleap_nn.tracking.Tracker` that will be called to associate\n            detections over time. Predicted instances will not be assigned to tracks if\n            if this is `None`.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n        max_stride: The maximum stride of the backbone network, as specified in the model's\n            `backbone_config`. This determines the downsampling factor applied by the backbone,\n            and is used to ensure that input images are padded or resized to be compatible\n            with the model's architecture. Default: 16.\n\n    \"\"\"\n\n    centroid_config: Optional[OmegaConf] = None\n    confmap_config: Optional[OmegaConf] = None\n    centroid_model: Optional[L.LightningModule] = None\n    confmap_model: Optional[L.LightningModule] = None\n    centroid_backbone_type: Optional[str] = None\n    centered_instance_backbone_type: Optional[str] = None\n    videos: Optional[List[sio.Video]] = None\n    skeletons: Optional[List[sio.Skeleton]] = None\n    peak_threshold: Union[float, List[float]] = 0.2\n    integral_refinement: str = \"integral\"\n    integral_patch_size: int = 5\n    batch_size: int = 4\n    max_instances: Optional[int] = None\n    return_confmaps: bool = False\n    device: str = \"cpu\"\n    preprocess_config: Optional[OmegaConf] = None\n    tracker: Optional[Tracker] = None\n    anchor_part: Optional[str] = None\n    max_stride: int = 16\n\n    def _initialize_inference_model(self):\n        \"\"\"Initialize the inference model from the trained models and configuration.\"\"\"\n        # Create an instance of CentroidLayer if centroid_config is not None\n        return_crops = False\n        # if both centroid and centered-instance model are provided, set return crops to True\n        if self.confmap_model:\n            return_crops = True\n        if isinstance(self.peak_threshold, list):\n            centroid_peak_threshold = self.peak_threshold[0]\n            centered_instance_peak_threshold = self.peak_threshold[1]\n        else:\n            centroid_peak_threshold = self.peak_threshold\n            centered_instance_peak_threshold = self.peak_threshold\n\n        if self.anchor_part is not None:\n            anchor_ind = self.skeletons[0].node_names.index(self.anchor_part)\n        else:\n            anch_pt = None\n            if self.centroid_config is not None:\n                anch_pt = (\n                    self.centroid_config.model_config.head_configs.centroid.confmaps.anchor_part\n                )\n            if self.confmap_config is not None:\n                anch_pt = (\n                    self.confmap_config.model_config.head_configs.centered_instance.confmaps.anchor_part\n                )\n            anchor_ind = (\n                self.skeletons[0].node_names.index(anch_pt)\n                if anch_pt is not None\n                else None\n            )\n\n        if self.centroid_config is None:\n            centroid_crop_layer = CentroidCrop(\n                use_gt_centroids=True,\n                crop_hw=(\n                    self.preprocess_config.crop_size,\n                    self.preprocess_config.crop_size,\n                ),\n                anchor_ind=anchor_ind,\n                return_crops=return_crops,\n            )\n\n        else:\n            max_stride = self.centroid_config.model_config.backbone_config[\n                f\"{self.centroid_backbone_type}\"\n            ][\"max_stride\"]\n            # initialize centroid crop layer\n            centroid_crop_layer = CentroidCrop(\n                torch_model=self.centroid_model,\n                peak_threshold=centroid_peak_threshold,\n                output_stride=self.centroid_config.model_config.head_configs.centroid.confmaps.output_stride,\n                refinement=self.integral_refinement,\n                integral_patch_size=self.integral_patch_size,\n                return_confmaps=self.return_confmaps,\n                return_crops=return_crops,\n                max_instances=self.max_instances,\n                max_stride=max_stride,\n                input_scale=self.centroid_config.data_config.preprocessing.scale,\n                crop_hw=(\n                    self.preprocess_config.crop_size,\n                    self.preprocess_config.crop_size,\n                ),\n                use_gt_centroids=False,\n            )\n\n        # Create an instance of FindInstancePeaks layer if confmap_config is not None\n        if self.confmap_config is None:\n            instance_peaks_layer = FindInstancePeaksGroundTruth()\n            self.instances_key = True\n        else:\n            max_stride = self.confmap_config.model_config.backbone_config[\n                f\"{self.centered_instance_backbone_type}\"\n            ][\"max_stride\"]\n            instance_peaks_layer = FindInstancePeaks(\n                torch_model=self.confmap_model,\n                peak_threshold=centered_instance_peak_threshold,\n                output_stride=self.confmap_config.model_config.head_configs.centered_instance.confmaps.output_stride,\n                refinement=self.integral_refinement,\n                integral_patch_size=self.integral_patch_size,\n                return_confmaps=self.return_confmaps,\n                max_stride=max_stride,\n                input_scale=self.confmap_config.data_config.preprocessing.scale,\n            )\n\n        if self.centroid_config is None and self.confmap_config is not None:\n            self.instances_key = (\n                True  # we need `instances` to get ground-truth centroids\n            )\n\n        # Initialize the inference model with centroid and instance peak layers\n        self.inference_model = TopDownInferenceModel(\n            centroid_crop=centroid_crop_layer, instance_peaks=instance_peaks_layer\n        )\n\n    @classmethod\n    def from_trained_models(\n        cls,\n        centroid_ckpt_path: Optional[Text] = None,\n        confmap_ckpt_path: Optional[Text] = None,\n        backbone_ckpt_path: Optional[str] = None,\n        head_ckpt_path: Optional[str] = None,\n        peak_threshold: float = 0.2,\n        integral_refinement: str = \"integral\",\n        integral_patch_size: int = 5,\n        batch_size: int = 4,\n        max_instances: Optional[int] = None,\n        return_confmaps: bool = False,\n        device: str = \"cpu\",\n        preprocess_config: Optional[OmegaConf] = None,\n        anchor_part: Optional[str] = None,\n    ) -&gt; \"TopDownPredictor\":\n        \"\"\"Create predictor from saved models.\n\n        Args:\n            centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n            confmap_ckpt_path: Path to a centered-instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n            backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n                from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n            head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n                are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n                from `backbone_ckpt_path` if provided.)\n            peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n                this will be ignored. Default: 0.2\n            integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n                If `\"integral\"`, peaks will be refined with integral regression.\n                Default: \"integral\".\n            integral_patch_size: (int) Size of patches to crop around each rough peak as an\n                integer scalar. Default: 5.\n            batch_size: (int) Number of samples per batch. Default: 4.\n            max_instances: (int) Max number of instances to consider from the predictions.\n            return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n                along with the predicted peak values and points. Default: False.\n            device: (str) Device on which torch.Tensor will be allocated. One of the\n                (\"cpu\", \"cuda\", \"mps\").\n                Default: \"cpu\"\n            preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n                in the `data_config.preprocessing` section.\n            anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n                provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n        Returns:\n            An instance of `TopDownPredictor` with the loaded models.\n\n            One of the two models can be left as `None` to perform inference with ground\n            truth data. This will only work with `LabelsReader` as the provider.\n\n        \"\"\"\n        centered_instance_backbone_type = None\n        centroid_backbone_type = None\n        if centroid_ckpt_path is not None:\n            is_sleap_ckpt = False\n            # Load centroid model.\n            if (\n                Path(centroid_ckpt_path) / \"training_config.yaml\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                centroid_config = OmegaConf.load(\n                    (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(centroid_ckpt_path) / \"training_config.json\"\n                in Path(centroid_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                centroid_config = TrainingJobConfig.load_sleap_config(\n                    (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n            # check which backbone architecture\n            for k, v in centroid_config.model_config.backbone_config.items():\n                if v is not None:\n                    centroid_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n                centroid_model = CentroidLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"centroid\",\n                    backbone_type=centroid_backbone_type,\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=centroid_config.model_config.init_weights,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                    map_location=device,\n                    weights_only=False,\n                )\n            else:\n                # Load the converted model\n                centroid_converted_model = load_legacy_model(\n                    model_dir=f\"{centroid_ckpt_path}\"\n                )\n                centroid_model = CentroidLightningModule(\n                    backbone_type=centroid_backbone_type,\n                    model_type=\"centroid\",\n                    backbone_config=centroid_config.model_config.backbone_config,\n                    head_configs=centroid_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=centroid_config.model_config.init_weights,\n                    lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                    online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=centroid_config.trainer_config.optimizer_name,\n                    learning_rate=centroid_config.trainer_config.optimizer.lr,\n                    amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                )\n\n                centroid_model.eval()\n                centroid_model.model = centroid_converted_model\n                centroid_model.to(device)\n\n            centroid_model.eval()\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            centroid_model.to(device)\n\n        else:\n            centroid_config = None\n            centroid_model = None\n\n        if confmap_ckpt_path is not None:\n            is_sleap_ckpt = False\n            # Load confmap model.\n            if (\n                Path(confmap_ckpt_path) / \"training_config.yaml\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                confmap_config = OmegaConf.load(\n                    (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n                )\n            elif (\n                Path(confmap_ckpt_path) / \"training_config.json\"\n                in Path(confmap_ckpt_path).iterdir()\n            ):\n                is_sleap_ckpt = True\n                confmap_config = TrainingJobConfig.load_sleap_config(\n                    (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n                )\n\n            skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n            # check which backbone architecture\n            for k, v in confmap_config.model_config.backbone_config.items():\n                if v is not None:\n                    centered_instance_backbone_type = k\n                    break\n\n            if not is_sleap_ckpt:\n                ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n                confmap_model = TopDownCenteredInstanceLightningModule.load_from_checkpoint(\n                    checkpoint_path=ckpt_path,\n                    model_type=\"centered_instance\",\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=confmap_config.model_config.init_weights,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                    backbone_type=centered_instance_backbone_type,\n                    map_location=device,\n                    weights_only=False,\n                )\n            else:\n                # Load the converted model\n                confmap_converted_model = load_legacy_model(\n                    model_dir=f\"{confmap_ckpt_path}\"\n                )\n\n                # Create a new LightningModule with the converted model\n                confmap_model = TopDownCenteredInstanceLightningModule(\n                    backbone_type=centered_instance_backbone_type,\n                    model_type=\"centered_instance\",\n                    backbone_config=confmap_config.model_config.backbone_config,\n                    head_configs=confmap_config.model_config.head_configs,\n                    pretrained_backbone_weights=None,\n                    pretrained_head_weights=None,\n                    init_weights=confmap_config.model_config.init_weights,\n                    lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                    online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                    hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                    min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                    max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                    loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                    optimizer=confmap_config.trainer_config.optimizer_name,\n                    learning_rate=confmap_config.trainer_config.optimizer.lr,\n                    amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                )\n\n                confmap_model.eval()\n                confmap_model.model = confmap_converted_model\n                confmap_model.to(device)\n\n            confmap_model.eval()\n\n            if backbone_ckpt_path is not None and head_ckpt_path is not None:\n                logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif backbone_ckpt_path is not None:\n                logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    backbone_ckpt_path, map_location=device, weights_only=False\n                )\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            if head_ckpt_path is not None:\n                logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n                ckpt = torch.load(\n                    head_ckpt_path, map_location=device, weights_only=False\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            confmap_model.to(device)\n\n        else:\n            confmap_config = None\n            confmap_model = None\n\n        if centroid_config is not None:\n            preprocess_config[\"scale\"] = (\n                centroid_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                centroid_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                centroid_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                centroid_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                centroid_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        else:\n            preprocess_config[\"scale\"] = (\n                confmap_config.data_config.preprocessing.scale\n                if preprocess_config[\"scale\"] is None\n                else preprocess_config[\"scale\"]\n            )\n            preprocess_config[\"ensure_rgb\"] = (\n                confmap_config.data_config.preprocessing.ensure_rgb\n                if preprocess_config[\"ensure_rgb\"] is None\n                else preprocess_config[\"ensure_rgb\"]\n            )\n            preprocess_config[\"ensure_grayscale\"] = (\n                confmap_config.data_config.preprocessing.ensure_grayscale\n                if preprocess_config[\"ensure_grayscale\"] is None\n                else preprocess_config[\"ensure_grayscale\"]\n            )\n            preprocess_config[\"max_height\"] = (\n                confmap_config.data_config.preprocessing.max_height\n                if preprocess_config[\"max_height\"] is None\n                else preprocess_config[\"max_height\"]\n            )\n            preprocess_config[\"max_width\"] = (\n                confmap_config.data_config.preprocessing.max_width\n                if preprocess_config[\"max_width\"] is None\n                else preprocess_config[\"max_width\"]\n            )\n\n        preprocess_config[\"crop_size\"] = (\n            confmap_config.data_config.preprocessing.crop_size\n            if preprocess_config[\"crop_size\"] is None and confmap_config is not None\n            else preprocess_config[\"crop_size\"]\n        )\n\n        # create an instance of TopDownPredictor class\n        obj = cls(\n            centroid_config=centroid_config,\n            centroid_model=centroid_model,\n            confmap_config=confmap_config,\n            confmap_model=confmap_model,\n            centroid_backbone_type=centroid_backbone_type,\n            centered_instance_backbone_type=centered_instance_backbone_type,\n            skeletons=skeletons,\n            peak_threshold=peak_threshold,\n            integral_refinement=integral_refinement,\n            integral_patch_size=integral_patch_size,\n            batch_size=batch_size,\n            max_instances=max_instances,\n            return_confmaps=return_confmaps,\n            device=device,\n            preprocess_config=preprocess_config,\n            anchor_part=anchor_part,\n            max_stride=(\n                centroid_config.model_config.backbone_config[\n                    f\"{centroid_backbone_type}\"\n                ][\"max_stride\"]\n                if centroid_config is not None\n                else confmap_config.model_config.backbone_config[\n                    f\"{centered_instance_backbone_type}\"\n                ][\"max_stride\"]\n            ),\n        )\n\n        obj._initialize_inference_model()\n        return obj\n\n    def make_pipeline(\n        self,\n        inference_object: Union[str, Path, sio.Labels, sio.Video],\n        queue_maxsize: int = 32,\n        frames: Optional[list] = None,\n        only_labeled_frames: bool = False,\n        only_suggested_frames: bool = False,\n        exclude_user_labeled: bool = False,\n        only_predicted_frames: bool = False,\n        video_index: Optional[int] = None,\n        video_dataset: Optional[str] = None,\n        video_input_format: str = \"channels_last\",\n    ):\n        \"\"\"Make a data loading pipeline.\n\n        Args:\n            inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n            queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n            frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n            only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n            only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n            exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n            only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n            video_index: (int) Integer index of video in .slp file to predict on. To be used\n                with an .slp path as an alternative to specifying the video path.\n            video_dataset: (str) The dataset for HDF5 videos.\n            video_input_format: (str) The input_format for HDF5 videos.\n\n        Returns:\n            This method initiates the reader class (doesn't return a pipeline) and the\n            Thread is started in Predictor._predict_generator() method.\n        \"\"\"\n        if isinstance(inference_object, str) or isinstance(inference_object, Path):\n            inference_object = (\n                sio.load_slp(inference_object)\n                if inference_object.endswith(\".slp\")\n                else sio.load_video(\n                    inference_object,\n                    dataset=video_dataset,\n                    input_format=video_input_format,\n                )\n            )\n\n        # LabelsReader provider\n        if isinstance(inference_object, sio.Labels) and video_index is None:\n            provider = LabelsReader\n\n            self.preprocess = False\n\n            frame_buffer = Queue(maxsize=queue_maxsize)\n\n            self.pipeline = provider(\n                labels=inference_object,\n                frame_buffer=frame_buffer,\n                instances_key=self.instances_key,\n                only_labeled_frames=only_labeled_frames,\n                only_suggested_frames=only_suggested_frames,\n                exclude_user_labeled=exclude_user_labeled,\n                only_predicted_frames=only_predicted_frames,\n            )\n            self.videos = self.pipeline.labels.videos\n\n        else:\n            provider = VideoReader\n            if self.centroid_config is None:\n                message = (\n                    \"Ground truth data was not detected... \"\n                    \"Please load both models when predicting on non-ground-truth data.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n\n            self.preprocess = False\n\n            if isinstance(inference_object, sio.Labels) and video_index is not None:\n                labels = inference_object\n                video = labels.videos[video_index]\n                # Filter out user-labeled frames if requested\n                filtered_frames = _filter_user_labeled_frames(\n                    labels, video, frames, exclude_user_labeled\n                )\n                self.pipeline = provider.from_video(\n                    video=video,\n                    queue_maxsize=queue_maxsize,\n                    frames=filtered_frames,\n                )\n\n            else:  # for mp4 or hdf5 videos\n                frame_buffer = Queue(maxsize=queue_maxsize)\n                self.pipeline = provider(\n                    video=inference_object,\n                    frame_buffer=frame_buffer,\n                    frames=frames,\n                )\n\n            self.videos = [self.pipeline.video]\n\n    def _make_labeled_frames_from_generator(\n        self,\n        generator: Iterator[Dict[str, np.ndarray]],\n    ) -&gt; sio.Labels:\n        \"\"\"Create labeled frames from a generator that yields inference results.\n\n        This method converts pure arrays into SLEAP-specific data structures and assigns\n        tracks to the predicted instances if tracker is specified.\n\n        Args:\n            generator: A generator that returns dictionaries with inference results.\n                This should return dictionaries with keys `\"instance_image\"`, `\"video_idx\"`,\n                `\"frame_idx\"`, `\"pred_instance_peaks\"`, `\"pred_peak_values\"`, and\n                `\"centroid_val\"`. This can be created using the `_predict_generator()`\n                method.\n\n        Returns:\n            A `sio.Labels` object with `sio.PredictedInstance`s created from\n            arrays returned from the inference result generator.\n        \"\"\"\n        # open video backend for tracking\n        for video in self.videos:\n            if not video.open_backend:\n                video.open()\n\n        preds = defaultdict(list)\n        predicted_frames = []\n        skeleton_idx = 0\n        # Loop through each predicted instance.\n        for ex in generator:\n            # loop through each sample in a batch\n            for (\n                video_idx,\n                frame_idx,\n                bbox,\n                pred_instances,\n                pred_values,\n                instance_score,\n                org_size,\n            ) in zip(\n                ex[\"video_idx\"],\n                ex[\"frame_idx\"],\n                ex[\"instance_bbox\"],\n                ex[\"pred_instance_peaks\"],\n                ex[\"pred_peak_values\"],\n                ex[\"centroid_val\"],\n                ex[\"orig_size\"],\n            ):\n                if np.isnan(pred_instances).all():\n                    continue\n                pred_instances = pred_instances + bbox.squeeze(axis=0)[0, :]\n                preds[(int(video_idx), int(frame_idx))].append(\n                    sio.PredictedInstance.from_numpy(\n                        points_data=pred_instances,\n                        skeleton=self.skeletons[skeleton_idx],\n                        point_scores=pred_values,\n                        score=instance_score,\n                    )\n                )\n        for key, inst in preds.items():\n            # Create list of LabeledFrames.\n            video_idx, frame_idx = key\n            lf = sio.LabeledFrame(\n                video=self.videos[video_idx],\n                frame_idx=frame_idx,\n                instances=inst,\n            )\n\n            if self.tracker:\n                lf.instances = self.tracker.track(\n                    untracked_instances=inst, frame_idx=frame_idx, image=lf.image\n                )\n\n            predicted_frames.append(lf)\n\n        pred_labels = sio.Labels(\n            videos=self.videos,\n            skeletons=self.skeletons,\n            labeled_frames=predicted_frames,\n        )\n        return pred_labels\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownPredictor.from_trained_models","title":"<code>from_trained_models(centroid_ckpt_path=None, confmap_ckpt_path=None, backbone_ckpt_path=None, head_ckpt_path=None, peak_threshold=0.2, integral_refinement='integral', integral_patch_size=5, batch_size=4, max_instances=None, return_confmaps=False, device='cpu', preprocess_config=None, anchor_part=None)</code>  <code>classmethod</code>","text":"<p>Create predictor from saved models.</p> <p>Parameters:</p> Name Type Description Default <code>centroid_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).</p> <code>None</code> <code>confmap_ckpt_path</code> <code>Optional[Text]</code> <p>Path to a centered-instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).</p> <code>None</code> <code>backbone_ckpt_path</code> <code>Optional[str]</code> <p>(str) To run inference on any <code>.ckpt</code> other than <code>best.ckpt</code> from the <code>model_paths</code> dir, the path to the <code>.ckpt</code> file should be passed here.</p> <code>None</code> <code>head_ckpt_path</code> <code>Optional[str]</code> <p>(str) Path to <code>.ckpt</code> file if a different set of head layer weights are to be used. If <code>None</code>, the <code>best.ckpt</code> from <code>model_paths</code> dir is used (or the ckpt from <code>backbone_ckpt_path</code> if provided.)</p> <code>None</code> <code>peak_threshold</code> <code>float</code> <p>(float) Minimum confidence threshold. Peaks with values below this will be ignored. Default: 0.2</p> <code>0.2</code> <code>integral_refinement</code> <code>str</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. Default: \"integral\".</p> <code>'integral'</code> <code>integral_patch_size</code> <code>int</code> <p>(int) Size of patches to crop around each rough peak as an integer scalar. Default: 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>(int) Number of samples per batch. Default: 4.</p> <code>4</code> <code>max_instances</code> <code>Optional[int]</code> <p>(int) Max number of instances to consider from the predictions.</p> <code>None</code> <code>return_confmaps</code> <code>bool</code> <p>(bool) If <code>True</code>, predicted confidence maps will be returned along with the predicted peak values and points. Default: False.</p> <code>False</code> <code>device</code> <code>str</code> <p>(str) Device on which torch.Tensor will be allocated. One of the (\"cpu\", \"cuda\", \"mps\"). Default: \"cpu\"</p> <code>'cpu'</code> <code>preprocess_config</code> <code>Optional[OmegaConf]</code> <p>(OmegaConf) OmegaConf object with keys as the parameters in the <code>data_config.preprocessing</code> section.</p> <code>None</code> <code>anchor_part</code> <code>Optional[str]</code> <p>(str) The name of the node to use as the anchor for the centroid. If not provided, the anchor part in the <code>training_config.yaml</code> is used instead. Default: None.</p> <code>None</code> <p>Returns:</p> Type Description <code>TopDownPredictor</code> <p>An instance of <code>TopDownPredictor</code> with the loaded models.</p> <p>One of the two models can be left as <code>None</code> to perform inference with ground truth data. This will only work with <code>LabelsReader</code> as the provider.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>@classmethod\ndef from_trained_models(\n    cls,\n    centroid_ckpt_path: Optional[Text] = None,\n    confmap_ckpt_path: Optional[Text] = None,\n    backbone_ckpt_path: Optional[str] = None,\n    head_ckpt_path: Optional[str] = None,\n    peak_threshold: float = 0.2,\n    integral_refinement: str = \"integral\",\n    integral_patch_size: int = 5,\n    batch_size: int = 4,\n    max_instances: Optional[int] = None,\n    return_confmaps: bool = False,\n    device: str = \"cpu\",\n    preprocess_config: Optional[OmegaConf] = None,\n    anchor_part: Optional[str] = None,\n) -&gt; \"TopDownPredictor\":\n    \"\"\"Create predictor from saved models.\n\n    Args:\n        centroid_ckpt_path: Path to a centroid ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n        confmap_ckpt_path: Path to a centered-instance ckpt dir with best.ckpt (or from SLEAP &lt;=1.4 best_model.h5 - only UNet backbone is supported) and training_config.yaml (or from SLEAP &lt;=1.4 training_config.json - only UNet backbone is supported).\n        backbone_ckpt_path: (str) To run inference on any `.ckpt` other than `best.ckpt`\n            from the `model_paths` dir, the path to the `.ckpt` file should be passed here.\n        head_ckpt_path: (str) Path to `.ckpt` file if a different set of head layer weights\n            are to be used. If `None`, the `best.ckpt` from `model_paths` dir is used (or the ckpt\n            from `backbone_ckpt_path` if provided.)\n        peak_threshold: (float) Minimum confidence threshold. Peaks with values below\n            this will be ignored. Default: 0.2\n        integral_refinement: If `None`, returns the grid-aligned peaks with no refinement.\n            If `\"integral\"`, peaks will be refined with integral regression.\n            Default: \"integral\".\n        integral_patch_size: (int) Size of patches to crop around each rough peak as an\n            integer scalar. Default: 5.\n        batch_size: (int) Number of samples per batch. Default: 4.\n        max_instances: (int) Max number of instances to consider from the predictions.\n        return_confmaps: (bool) If `True`, predicted confidence maps will be returned\n            along with the predicted peak values and points. Default: False.\n        device: (str) Device on which torch.Tensor will be allocated. One of the\n            (\"cpu\", \"cuda\", \"mps\").\n            Default: \"cpu\"\n        preprocess_config: (OmegaConf) OmegaConf object with keys as the parameters\n            in the `data_config.preprocessing` section.\n        anchor_part: (str) The name of the node to use as the anchor for the centroid. If not\n            provided, the anchor part in the `training_config.yaml` is used instead. Default: None.\n\n    Returns:\n        An instance of `TopDownPredictor` with the loaded models.\n\n        One of the two models can be left as `None` to perform inference with ground\n        truth data. This will only work with `LabelsReader` as the provider.\n\n    \"\"\"\n    centered_instance_backbone_type = None\n    centroid_backbone_type = None\n    if centroid_ckpt_path is not None:\n        is_sleap_ckpt = False\n        # Load centroid model.\n        if (\n            Path(centroid_ckpt_path) / \"training_config.yaml\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            centroid_config = OmegaConf.load(\n                (Path(centroid_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(centroid_ckpt_path) / \"training_config.json\"\n            in Path(centroid_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            centroid_config = TrainingJobConfig.load_sleap_config(\n                (Path(centroid_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        skeletons = get_skeleton_from_config(centroid_config.data_config.skeletons)\n\n        # check which backbone architecture\n        for k, v in centroid_config.model_config.backbone_config.items():\n            if v is not None:\n                centroid_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(centroid_ckpt_path) / \"best.ckpt\").as_posix()\n            centroid_model = CentroidLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"centroid\",\n                backbone_type=centroid_backbone_type,\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=centroid_config.model_config.init_weights,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n                map_location=device,\n                weights_only=False,\n            )\n        else:\n            # Load the converted model\n            centroid_converted_model = load_legacy_model(\n                model_dir=f\"{centroid_ckpt_path}\"\n            )\n            centroid_model = CentroidLightningModule(\n                backbone_type=centroid_backbone_type,\n                model_type=\"centroid\",\n                backbone_config=centroid_config.model_config.backbone_config,\n                head_configs=centroid_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=centroid_config.model_config.init_weights,\n                lr_scheduler=centroid_config.trainer_config.lr_scheduler,\n                online_mining=centroid_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=centroid_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=centroid_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=centroid_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=centroid_config.trainer_config.optimizer_name,\n                learning_rate=centroid_config.trainer_config.optimizer.lr,\n                amsgrad=centroid_config.trainer_config.optimizer.amsgrad,\n            )\n\n            centroid_model.eval()\n            centroid_model.model = centroid_converted_model\n            centroid_model.to(device)\n\n        centroid_model.eval()\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            centroid_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        centroid_model.to(device)\n\n    else:\n        centroid_config = None\n        centroid_model = None\n\n    if confmap_ckpt_path is not None:\n        is_sleap_ckpt = False\n        # Load confmap model.\n        if (\n            Path(confmap_ckpt_path) / \"training_config.yaml\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            confmap_config = OmegaConf.load(\n                (Path(confmap_ckpt_path) / \"training_config.yaml\").as_posix()\n            )\n        elif (\n            Path(confmap_ckpt_path) / \"training_config.json\"\n            in Path(confmap_ckpt_path).iterdir()\n        ):\n            is_sleap_ckpt = True\n            confmap_config = TrainingJobConfig.load_sleap_config(\n                (Path(confmap_ckpt_path) / \"training_config.json\").as_posix()\n            )\n\n        skeletons = get_skeleton_from_config(confmap_config.data_config.skeletons)\n\n        # check which backbone architecture\n        for k, v in confmap_config.model_config.backbone_config.items():\n            if v is not None:\n                centered_instance_backbone_type = k\n                break\n\n        if not is_sleap_ckpt:\n            ckpt_path = (Path(confmap_ckpt_path) / \"best.ckpt\").as_posix()\n            confmap_model = TopDownCenteredInstanceLightningModule.load_from_checkpoint(\n                checkpoint_path=ckpt_path,\n                model_type=\"centered_instance\",\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=confmap_config.model_config.init_weights,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n                backbone_type=centered_instance_backbone_type,\n                map_location=device,\n                weights_only=False,\n            )\n        else:\n            # Load the converted model\n            confmap_converted_model = load_legacy_model(\n                model_dir=f\"{confmap_ckpt_path}\"\n            )\n\n            # Create a new LightningModule with the converted model\n            confmap_model = TopDownCenteredInstanceLightningModule(\n                backbone_type=centered_instance_backbone_type,\n                model_type=\"centered_instance\",\n                backbone_config=confmap_config.model_config.backbone_config,\n                head_configs=confmap_config.model_config.head_configs,\n                pretrained_backbone_weights=None,\n                pretrained_head_weights=None,\n                init_weights=confmap_config.model_config.init_weights,\n                lr_scheduler=confmap_config.trainer_config.lr_scheduler,\n                online_mining=confmap_config.trainer_config.online_hard_keypoint_mining.online_mining,\n                hard_to_easy_ratio=confmap_config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n                min_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n                max_hard_keypoints=confmap_config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n                loss_scale=confmap_config.trainer_config.online_hard_keypoint_mining.loss_scale,\n                optimizer=confmap_config.trainer_config.optimizer_name,\n                learning_rate=confmap_config.trainer_config.optimizer.lr,\n                amsgrad=confmap_config.trainer_config.optimizer.amsgrad,\n            )\n\n            confmap_model.eval()\n            confmap_model.model = confmap_converted_model\n            confmap_model.to(device)\n\n        confmap_model.eval()\n\n        if backbone_ckpt_path is not None and head_ckpt_path is not None:\n            logger.info(f\"Loading backbone weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif backbone_ckpt_path is not None:\n            logger.info(f\"Loading weights from `{backbone_ckpt_path}` ...\")\n            ckpt = torch.load(\n                backbone_ckpt_path, map_location=device, weights_only=False\n            )\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        if head_ckpt_path is not None:\n            logger.info(f\"Loading head weights from `{head_ckpt_path}` ...\")\n            ckpt = torch.load(\n                head_ckpt_path, map_location=device, weights_only=False\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            confmap_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        confmap_model.to(device)\n\n    else:\n        confmap_config = None\n        confmap_model = None\n\n    if centroid_config is not None:\n        preprocess_config[\"scale\"] = (\n            centroid_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            centroid_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            centroid_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            centroid_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            centroid_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    else:\n        preprocess_config[\"scale\"] = (\n            confmap_config.data_config.preprocessing.scale\n            if preprocess_config[\"scale\"] is None\n            else preprocess_config[\"scale\"]\n        )\n        preprocess_config[\"ensure_rgb\"] = (\n            confmap_config.data_config.preprocessing.ensure_rgb\n            if preprocess_config[\"ensure_rgb\"] is None\n            else preprocess_config[\"ensure_rgb\"]\n        )\n        preprocess_config[\"ensure_grayscale\"] = (\n            confmap_config.data_config.preprocessing.ensure_grayscale\n            if preprocess_config[\"ensure_grayscale\"] is None\n            else preprocess_config[\"ensure_grayscale\"]\n        )\n        preprocess_config[\"max_height\"] = (\n            confmap_config.data_config.preprocessing.max_height\n            if preprocess_config[\"max_height\"] is None\n            else preprocess_config[\"max_height\"]\n        )\n        preprocess_config[\"max_width\"] = (\n            confmap_config.data_config.preprocessing.max_width\n            if preprocess_config[\"max_width\"] is None\n            else preprocess_config[\"max_width\"]\n        )\n\n    preprocess_config[\"crop_size\"] = (\n        confmap_config.data_config.preprocessing.crop_size\n        if preprocess_config[\"crop_size\"] is None and confmap_config is not None\n        else preprocess_config[\"crop_size\"]\n    )\n\n    # create an instance of TopDownPredictor class\n    obj = cls(\n        centroid_config=centroid_config,\n        centroid_model=centroid_model,\n        confmap_config=confmap_config,\n        confmap_model=confmap_model,\n        centroid_backbone_type=centroid_backbone_type,\n        centered_instance_backbone_type=centered_instance_backbone_type,\n        skeletons=skeletons,\n        peak_threshold=peak_threshold,\n        integral_refinement=integral_refinement,\n        integral_patch_size=integral_patch_size,\n        batch_size=batch_size,\n        max_instances=max_instances,\n        return_confmaps=return_confmaps,\n        device=device,\n        preprocess_config=preprocess_config,\n        anchor_part=anchor_part,\n        max_stride=(\n            centroid_config.model_config.backbone_config[\n                f\"{centroid_backbone_type}\"\n            ][\"max_stride\"]\n            if centroid_config is not None\n            else confmap_config.model_config.backbone_config[\n                f\"{centered_instance_backbone_type}\"\n            ][\"max_stride\"]\n        ),\n    )\n\n    obj._initialize_inference_model()\n    return obj\n</code></pre>"},{"location":"api/inference/predictors/#sleap_nn.inference.predictors.TopDownPredictor.make_pipeline","title":"<code>make_pipeline(inference_object, queue_maxsize=32, frames=None, only_labeled_frames=False, only_suggested_frames=False, exclude_user_labeled=False, only_predicted_frames=False, video_index=None, video_dataset=None, video_input_format='channels_last')</code>","text":"<p>Make a data loading pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>inference_object</code> <code>Union[str, Path, Labels, Video]</code> <p>(str) Path to <code>.slp</code> file or <code>.mp4</code> or sio.Labels or sio.Video to run inference on.</p> required <code>queue_maxsize</code> <code>int</code> <p>(int) Maximum size of the frame buffer queue. Default: 32.</p> <code>32</code> <code>frames</code> <code>Optional[list]</code> <p>(list) List of frames indices. If <code>None</code>, all frames in the video are used. Default: None.</p> <code>None</code> <code>only_labeled_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on user-labeled frames. Default: <code>False</code>.</p> <code>False</code> <code>only_suggested_frames</code> <code>bool</code> <p>(bool) <code>True</code> if inference should be run only on unlabeled suggested frames. Default: <code>False</code>.</p> <code>False</code> <code>exclude_user_labeled</code> <code>bool</code> <p>(bool) <code>True</code> to skip frames that have user-labeled instances. Default: <code>False</code>.</p> <code>False</code> <code>only_predicted_frames</code> <code>bool</code> <p>(bool) <code>True</code> to run inference only on frames that already have predictions. Default: <code>False</code>.</p> <code>False</code> <code>video_index</code> <code>Optional[int]</code> <p>(int) Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video path.</p> <code>None</code> <code>video_dataset</code> <code>Optional[str]</code> <p>(str) The dataset for HDF5 videos.</p> <code>None</code> <code>video_input_format</code> <code>str</code> <p>(str) The input_format for HDF5 videos.</p> <code>'channels_last'</code> <p>Returns:</p> Type Description <p>This method initiates the reader class (doesn't return a pipeline) and the Thread is started in Predictor._predict_generator() method.</p> Source code in <code>sleap_nn/inference/predictors.py</code> <pre><code>def make_pipeline(\n    self,\n    inference_object: Union[str, Path, sio.Labels, sio.Video],\n    queue_maxsize: int = 32,\n    frames: Optional[list] = None,\n    only_labeled_frames: bool = False,\n    only_suggested_frames: bool = False,\n    exclude_user_labeled: bool = False,\n    only_predicted_frames: bool = False,\n    video_index: Optional[int] = None,\n    video_dataset: Optional[str] = None,\n    video_input_format: str = \"channels_last\",\n):\n    \"\"\"Make a data loading pipeline.\n\n    Args:\n        inference_object: (str) Path to `.slp` file or `.mp4` or sio.Labels or sio.Video to run inference on.\n        queue_maxsize: (int) Maximum size of the frame buffer queue. Default: 32.\n        frames: (list) List of frames indices. If `None`, all frames in the video are used. Default: None.\n        only_labeled_frames: (bool) `True` if inference should be run only on user-labeled frames. Default: `False`.\n        only_suggested_frames: (bool) `True` if inference should be run only on unlabeled suggested frames. Default: `False`.\n        exclude_user_labeled: (bool) `True` to skip frames that have user-labeled instances. Default: `False`.\n        only_predicted_frames: (bool) `True` to run inference only on frames that already have predictions. Default: `False`.\n        video_index: (int) Integer index of video in .slp file to predict on. To be used\n            with an .slp path as an alternative to specifying the video path.\n        video_dataset: (str) The dataset for HDF5 videos.\n        video_input_format: (str) The input_format for HDF5 videos.\n\n    Returns:\n        This method initiates the reader class (doesn't return a pipeline) and the\n        Thread is started in Predictor._predict_generator() method.\n    \"\"\"\n    if isinstance(inference_object, str) or isinstance(inference_object, Path):\n        inference_object = (\n            sio.load_slp(inference_object)\n            if inference_object.endswith(\".slp\")\n            else sio.load_video(\n                inference_object,\n                dataset=video_dataset,\n                input_format=video_input_format,\n            )\n        )\n\n    # LabelsReader provider\n    if isinstance(inference_object, sio.Labels) and video_index is None:\n        provider = LabelsReader\n\n        self.preprocess = False\n\n        frame_buffer = Queue(maxsize=queue_maxsize)\n\n        self.pipeline = provider(\n            labels=inference_object,\n            frame_buffer=frame_buffer,\n            instances_key=self.instances_key,\n            only_labeled_frames=only_labeled_frames,\n            only_suggested_frames=only_suggested_frames,\n            exclude_user_labeled=exclude_user_labeled,\n            only_predicted_frames=only_predicted_frames,\n        )\n        self.videos = self.pipeline.labels.videos\n\n    else:\n        provider = VideoReader\n        if self.centroid_config is None:\n            message = (\n                \"Ground truth data was not detected... \"\n                \"Please load both models when predicting on non-ground-truth data.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n\n        self.preprocess = False\n\n        if isinstance(inference_object, sio.Labels) and video_index is not None:\n            labels = inference_object\n            video = labels.videos[video_index]\n            # Filter out user-labeled frames if requested\n            filtered_frames = _filter_user_labeled_frames(\n                labels, video, frames, exclude_user_labeled\n            )\n            self.pipeline = provider.from_video(\n                video=video,\n                queue_maxsize=queue_maxsize,\n                frames=filtered_frames,\n            )\n\n        else:  # for mp4 or hdf5 videos\n            frame_buffer = Queue(maxsize=queue_maxsize)\n            self.pipeline = provider(\n                video=inference_object,\n                frame_buffer=frame_buffer,\n                frames=frames,\n            )\n\n        self.videos = [self.pipeline.video]\n</code></pre>"},{"location":"api/inference/provenance/","title":"provenance","text":""},{"location":"api/inference/provenance/#sleap_nn.inference.provenance","title":"<code>sleap_nn.inference.provenance</code>","text":"<p>Provenance metadata utilities for inference outputs.</p> <p>This module provides utilities for building and managing provenance metadata that is stored in SLP files produced during inference. Provenance metadata helps track where predictions came from and how they were generated.</p> <p>Functions:</p> Name Description <code>build_inference_provenance</code> <p>Build provenance metadata dictionary for inference output.</p> <code>build_tracking_only_provenance</code> <p>Build provenance metadata for tracking-only pipeline.</p> <code>merge_provenance</code> <p>Merge additional provenance fields into base provenance.</p>"},{"location":"api/inference/provenance/#sleap_nn.inference.provenance.build_inference_provenance","title":"<code>build_inference_provenance(model_paths=None, model_type=None, start_time=None, end_time=None, input_labels=None, input_path=None, frames_processed=None, frames_total=None, frame_selection_method=None, inference_params=None, tracking_params=None, device=None, cli_args=None, include_system_info=True)</code>","text":"<p>Build provenance metadata dictionary for inference output.</p> <p>This function creates a comprehensive provenance dictionary that captures all relevant metadata about an inference run, enabling reproducibility and tracking of prediction origins.</p> <p>Parameters:</p> Name Type Description Default <code>model_paths</code> <code>Optional[list[str]]</code> <p>List of paths to model checkpoints used for inference.</p> <code>None</code> <code>model_type</code> <code>Optional[str]</code> <p>Type of model used (e.g., \"top_down\", \"bottom_up\", \"single_instance\").</p> <code>None</code> <code>start_time</code> <code>Optional[datetime]</code> <p>Datetime when inference started.</p> <code>None</code> <code>end_time</code> <code>Optional[datetime]</code> <p>Datetime when inference finished.</p> <code>None</code> <code>input_labels</code> <code>Optional[Labels]</code> <p>Input Labels object if inference was run on an SLP file. The provenance from this object will be preserved.</p> <code>None</code> <code>input_path</code> <code>Optional[Union[str, Path]]</code> <p>Path to input file (SLP or video).</p> <code>None</code> <code>frames_processed</code> <code>Optional[int]</code> <p>Number of frames that were processed.</p> <code>None</code> <code>frames_total</code> <code>Optional[int]</code> <p>Total number of frames in the input.</p> <code>None</code> <code>frame_selection_method</code> <code>Optional[str]</code> <p>Method used to select frames (e.g., \"all\", \"labeled\", \"suggested\", \"range\").</p> <code>None</code> <code>inference_params</code> <code>Optional[dict[str, Any]]</code> <p>Dictionary of inference parameters (peak_threshold, integral_refinement, batch_size, etc.).</p> <code>None</code> <code>tracking_params</code> <code>Optional[dict[str, Any]]</code> <p>Dictionary of tracking parameters if tracking was run.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device used for inference (e.g., \"cuda:0\", \"cpu\", \"mps\").</p> <code>None</code> <code>cli_args</code> <code>Optional[dict[str, Any]]</code> <p>Command-line arguments if available.</p> <code>None</code> <code>include_system_info</code> <code>bool</code> <p>If True, include detailed system information. Set to False for lighter-weight provenance.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing provenance metadata suitable for storing in Labels.provenance.</p> Example <p>from datetime import datetime provenance = build_inference_provenance( ...     model_paths=[\"/path/to/model.ckpt\"], ...     model_type=\"top_down\", ...     start_time=datetime.now(), ...     end_time=datetime.now(), ...     device=\"cuda:0\", ... ) labels.provenance = provenance labels.save(\"predictions.slp\")</p> Source code in <code>sleap_nn/inference/provenance.py</code> <pre><code>def build_inference_provenance(\n    model_paths: Optional[list[str]] = None,\n    model_type: Optional[str] = None,\n    start_time: Optional[datetime] = None,\n    end_time: Optional[datetime] = None,\n    input_labels: Optional[sio.Labels] = None,\n    input_path: Optional[Union[str, Path]] = None,\n    frames_processed: Optional[int] = None,\n    frames_total: Optional[int] = None,\n    frame_selection_method: Optional[str] = None,\n    inference_params: Optional[dict[str, Any]] = None,\n    tracking_params: Optional[dict[str, Any]] = None,\n    device: Optional[str] = None,\n    cli_args: Optional[dict[str, Any]] = None,\n    include_system_info: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"Build provenance metadata dictionary for inference output.\n\n    This function creates a comprehensive provenance dictionary that captures\n    all relevant metadata about an inference run, enabling reproducibility\n    and tracking of prediction origins.\n\n    Args:\n        model_paths: List of paths to model checkpoints used for inference.\n        model_type: Type of model used (e.g., \"top_down\", \"bottom_up\",\n            \"single_instance\").\n        start_time: Datetime when inference started.\n        end_time: Datetime when inference finished.\n        input_labels: Input Labels object if inference was run on an SLP file.\n            The provenance from this object will be preserved.\n        input_path: Path to input file (SLP or video).\n        frames_processed: Number of frames that were processed.\n        frames_total: Total number of frames in the input.\n        frame_selection_method: Method used to select frames (e.g., \"all\",\n            \"labeled\", \"suggested\", \"range\").\n        inference_params: Dictionary of inference parameters (peak_threshold,\n            integral_refinement, batch_size, etc.).\n        tracking_params: Dictionary of tracking parameters if tracking was run.\n        device: Device used for inference (e.g., \"cuda:0\", \"cpu\", \"mps\").\n        cli_args: Command-line arguments if available.\n        include_system_info: If True, include detailed system information.\n            Set to False for lighter-weight provenance.\n\n    Returns:\n        Dictionary containing provenance metadata suitable for storing in\n        Labels.provenance.\n\n    Example:\n        &gt;&gt;&gt; from datetime import datetime\n        &gt;&gt;&gt; provenance = build_inference_provenance(\n        ...     model_paths=[\"/path/to/model.ckpt\"],\n        ...     model_type=\"top_down\",\n        ...     start_time=datetime.now(),\n        ...     end_time=datetime.now(),\n        ...     device=\"cuda:0\",\n        ... )\n        &gt;&gt;&gt; labels.provenance = provenance\n        &gt;&gt;&gt; labels.save(\"predictions.slp\")\n    \"\"\"\n    provenance: dict[str, Any] = {}\n\n    # Timestamps\n    if start_time is not None:\n        provenance[\"inference_start_timestamp\"] = start_time.isoformat()\n    if end_time is not None:\n        provenance[\"inference_end_timestamp\"] = end_time.isoformat()\n    if start_time is not None and end_time is not None:\n        runtime_seconds = (end_time - start_time).total_seconds()\n        provenance[\"inference_runtime_seconds\"] = runtime_seconds\n\n    # Version information\n    provenance[\"sleap_nn_version\"] = sleap_nn.__version__\n    provenance[\"sleap_io_version\"] = sio.__version__\n\n    # Model information\n    if model_paths is not None:\n        # Store as absolute POSIX paths for cross-platform compatibility\n        provenance[\"model_paths\"] = [\n            Path(p).resolve().as_posix() if isinstance(p, (str, Path)) else str(p)\n            for p in model_paths\n        ]\n    if model_type is not None:\n        provenance[\"model_type\"] = model_type\n\n    # Input data lineage\n    if input_path is not None:\n        provenance[\"source_file\"] = (\n            Path(input_path).resolve().as_posix()\n            if isinstance(input_path, (str, Path))\n            else str(input_path)\n        )\n\n    # Preserve input provenance if available\n    if input_labels is not None and hasattr(input_labels, \"provenance\"):\n        input_prov = dict(input_labels.provenance)\n        if input_prov:\n            provenance[\"input_provenance\"] = input_prov\n            # Also set source_labels for compatibility with sleap-io conventions\n            if \"filename\" in input_prov:\n                provenance[\"source_labels\"] = input_prov[\"filename\"]\n\n    # Frame selection information\n    if frames_processed is not None or frames_total is not None:\n        frame_info: dict[str, Any] = {}\n        if frame_selection_method is not None:\n            frame_info[\"method\"] = frame_selection_method\n        if frames_processed is not None:\n            frame_info[\"frames_processed\"] = frames_processed\n        if frames_total is not None:\n            frame_info[\"frames_total\"] = frames_total\n        if frame_info:\n            provenance[\"frame_selection\"] = frame_info\n\n    # Inference parameters\n    if inference_params is not None:\n        # Filter out None values and convert paths\n        clean_params = {}\n        for key, value in inference_params.items():\n            if value is not None:\n                if isinstance(value, Path):\n                    clean_params[key] = value.as_posix()\n                else:\n                    clean_params[key] = value\n        if clean_params:\n            provenance[\"inference_config\"] = clean_params\n\n    # Tracking parameters\n    if tracking_params is not None:\n        clean_tracking = {k: v for k, v in tracking_params.items() if v is not None}\n        if clean_tracking:\n            provenance[\"tracking_config\"] = clean_tracking\n\n    # Device information\n    if device is not None:\n        provenance[\"device\"] = device\n\n    # CLI arguments\n    if cli_args is not None:\n        # Filter out None values\n        clean_cli = {k: v for k, v in cli_args.items() if v is not None}\n        if clean_cli:\n            provenance[\"cli_args\"] = clean_cli\n\n    # System information (can be disabled for lighter provenance)\n    if include_system_info:\n        try:\n            system_info = get_system_info_dict()\n            # Extract key fields for provenance (avoid excessive nesting)\n            provenance[\"system_info\"] = {\n                \"python_version\": system_info.get(\"python_version\"),\n                \"platform\": system_info.get(\"platform\"),\n                \"pytorch_version\": system_info.get(\"pytorch_version\"),\n                \"cuda_version\": system_info.get(\"cuda_version\"),\n                \"accelerator\": system_info.get(\"accelerator\"),\n                \"gpu_count\": system_info.get(\"gpu_count\"),\n            }\n            # Include GPU names if available\n            if system_info.get(\"gpus\"):\n                provenance[\"system_info\"][\"gpus\"] = [\n                    gpu.get(\"name\") for gpu in system_info[\"gpus\"]\n                ]\n        except Exception:\n            # Don't fail inference if system info collection fails\n            pass\n\n    return provenance\n</code></pre>"},{"location":"api/inference/provenance/#sleap_nn.inference.provenance.build_tracking_only_provenance","title":"<code>build_tracking_only_provenance(input_labels=None, input_path=None, start_time=None, end_time=None, tracking_params=None, frames_processed=None, include_system_info=True)</code>","text":"<p>Build provenance metadata for tracking-only pipeline.</p> <p>This is a simplified version of build_inference_provenance for when only tracking is run without model inference.</p> <p>Parameters:</p> Name Type Description Default <code>input_labels</code> <code>Optional[Labels]</code> <p>Input Labels object with existing predictions.</p> <code>None</code> <code>input_path</code> <code>Optional[Union[str, Path]]</code> <p>Path to input SLP file.</p> <code>None</code> <code>start_time</code> <code>Optional[datetime]</code> <p>Datetime when tracking started.</p> <code>None</code> <code>end_time</code> <code>Optional[datetime]</code> <p>Datetime when tracking finished.</p> <code>None</code> <code>tracking_params</code> <code>Optional[dict[str, Any]]</code> <p>Dictionary of tracking parameters.</p> <code>None</code> <code>frames_processed</code> <code>Optional[int]</code> <p>Number of frames that were tracked.</p> <code>None</code> <code>include_system_info</code> <code>bool</code> <p>If True, include system information.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing provenance metadata.</p> Source code in <code>sleap_nn/inference/provenance.py</code> <pre><code>def build_tracking_only_provenance(\n    input_labels: Optional[sio.Labels] = None,\n    input_path: Optional[Union[str, Path]] = None,\n    start_time: Optional[datetime] = None,\n    end_time: Optional[datetime] = None,\n    tracking_params: Optional[dict[str, Any]] = None,\n    frames_processed: Optional[int] = None,\n    include_system_info: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"Build provenance metadata for tracking-only pipeline.\n\n    This is a simplified version of build_inference_provenance for when\n    only tracking is run without model inference.\n\n    Args:\n        input_labels: Input Labels object with existing predictions.\n        input_path: Path to input SLP file.\n        start_time: Datetime when tracking started.\n        end_time: Datetime when tracking finished.\n        tracking_params: Dictionary of tracking parameters.\n        frames_processed: Number of frames that were tracked.\n        include_system_info: If True, include system information.\n\n    Returns:\n        Dictionary containing provenance metadata.\n    \"\"\"\n    provenance: dict[str, Any] = {}\n\n    # Timestamps\n    if start_time is not None:\n        provenance[\"tracking_start_timestamp\"] = start_time.isoformat()\n    if end_time is not None:\n        provenance[\"tracking_end_timestamp\"] = end_time.isoformat()\n    if start_time is not None and end_time is not None:\n        runtime_seconds = (end_time - start_time).total_seconds()\n        provenance[\"tracking_runtime_seconds\"] = runtime_seconds\n\n    # Version information\n    provenance[\"sleap_nn_version\"] = sleap_nn.__version__\n    provenance[\"sleap_io_version\"] = sio.__version__\n\n    # Note that this is tracking-only\n    provenance[\"pipeline_type\"] = \"tracking_only\"\n\n    # Input data lineage\n    if input_path is not None:\n        provenance[\"source_file\"] = (\n            Path(input_path).resolve().as_posix()\n            if isinstance(input_path, (str, Path))\n            else str(input_path)\n        )\n\n    # Preserve input provenance if available\n    if input_labels is not None and hasattr(input_labels, \"provenance\"):\n        input_prov = dict(input_labels.provenance)\n        if input_prov:\n            provenance[\"input_provenance\"] = input_prov\n            if \"filename\" in input_prov:\n                provenance[\"source_labels\"] = input_prov[\"filename\"]\n\n    # Frame information\n    if frames_processed is not None:\n        provenance[\"frames_processed\"] = frames_processed\n\n    # Tracking parameters\n    if tracking_params is not None:\n        clean_tracking = {k: v for k, v in tracking_params.items() if v is not None}\n        if clean_tracking:\n            provenance[\"tracking_config\"] = clean_tracking\n\n    # System information\n    if include_system_info:\n        try:\n            system_info = get_system_info_dict()\n            provenance[\"system_info\"] = {\n                \"python_version\": system_info.get(\"python_version\"),\n                \"platform\": system_info.get(\"platform\"),\n                \"pytorch_version\": system_info.get(\"pytorch_version\"),\n                \"accelerator\": system_info.get(\"accelerator\"),\n            }\n        except Exception:\n            pass\n\n    return provenance\n</code></pre>"},{"location":"api/inference/provenance/#sleap_nn.inference.provenance.merge_provenance","title":"<code>merge_provenance(base_provenance, additional, overwrite=True)</code>","text":"<p>Merge additional provenance fields into base provenance.</p> <p>Parameters:</p> Name Type Description Default <code>base_provenance</code> <code>dict[str, Any]</code> <p>Base provenance dictionary.</p> required <code>additional</code> <code>dict[str, Any]</code> <p>Additional fields to merge.</p> required <code>overwrite</code> <code>bool</code> <p>If True, additional fields overwrite base fields. If False, base fields take precedence.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Merged provenance dictionary.</p> Source code in <code>sleap_nn/inference/provenance.py</code> <pre><code>def merge_provenance(\n    base_provenance: dict[str, Any],\n    additional: dict[str, Any],\n    overwrite: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"Merge additional provenance fields into base provenance.\n\n    Args:\n        base_provenance: Base provenance dictionary.\n        additional: Additional fields to merge.\n        overwrite: If True, additional fields overwrite base fields.\n            If False, base fields take precedence.\n\n    Returns:\n        Merged provenance dictionary.\n    \"\"\"\n    result = dict(base_provenance)\n    for key, value in additional.items():\n        if key not in result or overwrite:\n            result[key] = value\n    return result\n</code></pre>"},{"location":"api/inference/single_instance/","title":"single_instance","text":""},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance","title":"<code>sleap_nn.inference.single_instance</code>","text":"<p>Inference modules for SingleInstance models.</p> <p>Classes:</p> Name Description <code>SingleInstanceInferenceModel</code> <p>Single instance prediction model.</p>"},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance.SingleInstanceInferenceModel","title":"<code>SingleInstanceInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Single instance prediction model.</p> <p>This model encapsulates the basic single instance approach where it is assumed that there is only one instance in the frame. The images are passed to a peak detector which is trained to detect all body parts for the instance assuming a single peak per body part.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/single_instance.py</code> <pre><code>class SingleInstanceInferenceModel(L.LightningModule):\n    \"\"\"Single instance prediction model.\n\n    This model encapsulates the basic single instance approach where it is assumed that\n    there is only one instance in the frame. The images are passed to a peak detector\n    which is trained to detect all body parts for the instance assuming a single peak\n    per body part.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = None,\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        input_scale: float = 1.0,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__()\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.input_scale = input_scale\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        Args:\n            inputs: Dictionary with \"image\" as one of the keys.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n                as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n        \"\"\"\n        # Network forward pass.\n        cms = self.torch_model(inputs[\"image\"])\n\n        peak_points, peak_vals = find_global_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n\n        # Adjust for stride and scale.\n        peak_points = peak_points * self.output_stride\n        if self.input_scale != 1.0:\n            peak_points = peak_points / self.input_scale\n        peak_points = peak_points / (\n            inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2)\n        ).to(peak_points.device)\n\n        # Build outputs.\n        outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n        if self.return_confmaps:\n            outputs[\"pred_confmaps\"] = cms.detach()\n        inputs.update(outputs)\n        return [inputs]\n</code></pre>"},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance.SingleInstanceInferenceModel.__init__","title":"<code>__init__(torch_model, output_stride=None, peak_threshold=0.0, refinement=None, integral_patch_size=5, return_confmaps=False, input_scale=1.0)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/single_instance.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    input_scale: float = 1.0,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__()\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.input_scale = input_scale\n</code></pre>"},{"location":"api/inference/single_instance/#sleap_nn.inference.single_instance.SingleInstanceInferenceModel.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with \"image\" as one of the keys.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch     as a <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> Source code in <code>sleap_nn/inference/single_instance.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    Args:\n        inputs: Dictionary with \"image\" as one of the keys.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch\n            as a `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n    \"\"\"\n    # Network forward pass.\n    cms = self.torch_model(inputs[\"image\"])\n\n    peak_points, peak_vals = find_global_peaks(\n        cms.detach(),\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n\n    # Adjust for stride and scale.\n    peak_points = peak_points * self.output_stride\n    if self.input_scale != 1.0:\n        peak_points = peak_points / self.input_scale\n    peak_points = peak_points / (\n        inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2)\n    ).to(peak_points.device)\n\n    # Build outputs.\n    outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n    if self.return_confmaps:\n        outputs[\"pred_confmaps\"] = cms.detach()\n    inputs.update(outputs)\n    return [inputs]\n</code></pre>"},{"location":"api/inference/topdown/","title":"topdown","text":""},{"location":"api/inference/topdown/#sleap_nn.inference.topdown","title":"<code>sleap_nn.inference.topdown</code>","text":"<p>Inference modules for TopDown centroid and centered-instance models.</p> <p>Classes:</p> Name Description <code>CentroidCrop</code> <p>Lightning Module for running inference for a centroid model.</p> <code>FindInstancePeaks</code> <p>Lightning Module that predicts instance peaks from images using a trained model.</p> <code>FindInstancePeaksGroundTruth</code> <p>LightningModule that simulates a centered instance peaks model.</p> <code>TopDownInferenceModel</code> <p>Top-down instance prediction model.</p> <code>TopDownMultiClassFindInstancePeaks</code> <p>Lightning Module that predicts instance peaks from images using a trained model.</p>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.CentroidCrop","title":"<code>CentroidCrop</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning Module for running inference for a centroid model.</p> <p>This layer encapsulates all of the inference operations requires for generating predictions from a centroid confidence map model. This includes model forward pass, generating crops for cenetered instance model, peak finding, coordinate adjustment and cropping.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>max_instances</code> <p>Max number of instances to consider during centroid predictions.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>return_crops</code> <p>If <code>True</code>, the output dictionary will also contain <code>instance_image</code> which is the cropped image of size (batch, 1, chn, crop_height, crop_width).</p> <code>crop_hw</code> <p>Tuple (height, width) representing the crop size.</p> <code>input_scale</code> <p>Float indicating if the images should be resized before being passed to the model.</p> <code>max_stride</code> <p>Maximum stride in a model that the images must be divisible by. If &gt; 1, this will pad the bottom and right of the images to ensure they meet this divisibility criteria. Padding is applied after the scaling specified in the <code>scale</code> attribute.</p> <code>use_gt_centroids</code> <p>If <code>True</code>, then the crops are generated using ground-truth centroids. If <code>False</code>, then centroids are predicted using a trained centroid model.</p> <code>anchor_ind</code> <p>The index of the node to use as the anchor for the centroid. If not provided or if not present in the instance, the midpoint of the bounding box is used instead.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict centroid confidence maps and crop around peaks.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class CentroidCrop(L.LightningModule):\n    \"\"\"Lightning Module for running inference for a centroid model.\n\n    This layer encapsulates all of the inference operations requires for generating\n    predictions from a centroid confidence map model. This includes model forward pass,\n    generating crops for cenetered instance model, peak finding, coordinate adjustment\n    and cropping.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        max_instances: Max number of instances to consider during centroid predictions.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        return_crops: If `True`, the output dictionary will also contain `instance_image`\n            which is the cropped image of size (batch, 1, chn, crop_height, crop_width).\n        crop_hw: Tuple (height, width) representing the crop size.\n        input_scale: Float indicating if the images should be resized before being\n            passed to the model.\n        max_stride: Maximum stride in a model that the images must be divisible by.\n            If &gt; 1, this will pad the bottom and right of the images to ensure they meet\n            this divisibility criteria. Padding is applied after the scaling specified\n            in the `scale` attribute.\n        use_gt_centroids: If `True`, then the crops are generated using ground-truth centroids.\n            If `False`, then centroids are predicted using a trained centroid model.\n        anchor_ind: The index of the node to use as the anchor for the centroid. If not\n            provided or if not present in the instance, the midpoint of the bounding box\n            is used instead.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: Optional[L.LightningModule] = None,\n        output_stride: int = 1,\n        peak_threshold: float = 0.0,\n        max_instances: Optional[int] = None,\n        refinement: Optional[str] = None,\n        integral_patch_size: int = 5,\n        return_confmaps: bool = False,\n        return_crops: bool = False,\n        crop_hw: Optional[List[int]] = None,\n        input_scale: float = 1.0,\n        max_stride: int = 1,\n        use_gt_centroids: bool = False,\n        anchor_ind: Optional[int] = None,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.max_instances = max_instances\n        self.return_crops = return_crops\n        self.crop_hw = crop_hw\n        self.input_scale = input_scale\n        self.max_stride = max_stride\n        self.use_gt_centroids = use_gt_centroids\n        self.anchor_ind = anchor_ind\n\n    def _generate_crops(self, inputs, cms: Optional[torch.Tensor] = None):\n        \"\"\"Generate Crops from the predicted centroids.\"\"\"\n        crops_dict = []\n        if cms is not None:\n            cms = cms.detach()\n        for idx, (centroid, centroid_val, image, fidx, vidx, sz, eff_sc) in enumerate(\n            zip(\n                self.refined_peaks_batched,\n                self.peak_vals_batched,\n                inputs[\"image\"],\n                inputs[\"frame_idx\"],\n                inputs[\"video_idx\"],\n                inputs[\"orig_size\"],\n                inputs[\"eff_scale\"],\n            )\n        ):\n            if torch.any(torch.isnan(centroid)):\n                if torch.all(torch.isnan(centroid)):\n                    continue\n                else:\n                    non_nans = ~torch.any(centroid.isnan(), dim=-1)\n                    centroid = centroid[non_nans]\n                    if len(centroid.shape) == 1:\n                        centroid = centroid.unsqueeze(dim=0)\n                    centroid_val = centroid_val[non_nans]\n            n = centroid.shape[0]\n            box_size = (\n                self.crop_hw[0],\n                self.crop_hw[1],\n            )\n            instance_bbox = torch.unsqueeze(\n                make_centered_bboxes(centroid, box_size[0], box_size[1]), 0\n            )  # (1, n, 4, 2)\n\n            # Generate cropped image of shape (n, C, crop_H, crop_W)\n            instance_image = crop_bboxes(\n                image,\n                bboxes=instance_bbox.squeeze(dim=0),\n                sample_inds=[0] * n,\n            )\n\n            # Access top left point (x,y) of bounding box and subtract this offset from\n            # position of nodes.\n            point = instance_bbox[0, :, 0]\n            centered_centroid = centroid - point\n\n            ex = {}\n            ex[\"image\"] = torch.cat([image] * n)\n            ex[\"centroid\"] = centered_centroid\n            ex[\"centroid_val\"] = centroid_val\n            ex[\"frame_idx\"] = torch.Tensor([fidx] * n)\n            ex[\"video_idx\"] = torch.Tensor([vidx] * n)\n            ex[\"instance_bbox\"] = instance_bbox.squeeze(dim=0).unsqueeze(dim=1)\n            ex[\"instance_image\"] = instance_image.unsqueeze(dim=1)\n            ex[\"orig_size\"] = torch.cat([torch.Tensor(sz)] * n)\n            ex[\"eff_scale\"] = torch.Tensor([eff_sc] * n)\n            ex[\"pred_centroids\"] = centroid\n            if self.return_confmaps:\n                ex[\"pred_centroid_confmaps\"] = torch.cat(\n                    [cms[idx].unsqueeze(dim=0)] * n\n                )\n            crops_dict.append(ex)\n\n        return crops_dict\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict centroid confidence maps and crop around peaks.\n\n        This layer can be chained with a `FindInstancePeaks` layer to create a top-down\n        inference function from full images.\n\n        Args:\n            inputs: Dictionary with key `\"image\"`. Other keys will be passed down the pipeline.\n\n        Returns:\n            A list of dictionaries (size = batch size) where each dictionary has cropped\n            images with key `instance_image` and `centroid_val` batched based on the\n            number of centroids predicted for each image in the original batch if\n            return_crops is True.\n            If return_crops is not True, this module returns the dictionary with\n            `centroids` and `centroid_val` keys with shapes (batch, 1, max_instances, 2)\n            and (batch, max_instances) respectively which could then to passed to\n            FindInstancePeaksGroundTruth class.\n        \"\"\"\n        if self.use_gt_centroids:\n            batch = inputs[\"video_idx\"].shape[0]\n            centroids = generate_centroids(\n                inputs[\"instances\"], anchor_ind=self.anchor_ind\n            )\n            centroid_vals = torch.ones(centroids.shape)[..., 0]\n            self.refined_peaks_batched = [x[0] for x in centroids]\n            self.peak_vals_batched = [x[0] for x in centroid_vals]\n\n            max_instances = (\n                self.max_instances\n                if self.max_instances is not None\n                else inputs[\"instances\"].shape[-3]\n            )\n\n            refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n            peak_vals_with_nans = torch.zeros((batch, max_instances))\n            for ind, (r, p) in enumerate(\n                zip(self.refined_peaks_batched, self.peak_vals_batched)\n            ):\n                refined_peaks_with_nans[ind] = r\n                peak_vals_with_nans[ind] = p\n\n            inputs.update(\n                {\n                    \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                    \"centroid_vals\": peak_vals_with_nans,\n                }\n            )\n\n            if self.return_crops:\n                crops_dict = self._generate_crops(inputs)\n                return crops_dict\n            else:\n                return inputs\n\n        # Network forward pass.\n        orig_image = inputs[\"image\"]\n        scaled_image = resize_image(orig_image, self.input_scale)\n        if self.max_stride != 1:\n            scaled_image = apply_pad_to_stride(scaled_image, self.max_stride)\n\n        cms = self.torch_model(scaled_image)\n\n        refined_peaks, peak_vals, peak_sample_inds, _ = find_local_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n        # Adjust for stride and scale.\n        refined_peaks = refined_peaks * self.output_stride  # (n_centroids, 2)\n        refined_peaks = refined_peaks / self.input_scale\n\n        batch = cms.shape[0]\n\n        # if max instances is not provided, find the max_instances for this batch\n        num_instances = defaultdict(int)\n        for p in peak_sample_inds:\n            num_instances[int(p)] += 1\n\n        if num_instances:\n            max_instances = max(num_instances.values()) if num_instances else None\n            if self.max_instances is not None:\n                max_instances = self.max_instances\n\n            self.refined_peaks_batched = []\n            self.peak_vals_batched = []\n\n            for b in range(batch):\n                indices = (peak_sample_inds == b).nonzero()\n                # list for predicted centroids and corresponding peak values for current batch.\n                current_peaks = refined_peaks[indices].squeeze(dim=-2)\n                current_peak_vals = peak_vals[indices].squeeze(dim=-1)\n                # Choose top k centroids if max_instances is provided.\n                if len(current_peaks) &gt; max_instances:\n                    current_peak_vals, indices = torch.topk(\n                        current_peak_vals, max_instances\n                    )\n                    current_peaks = current_peaks[indices]\n                    num_nans = 0\n                else:\n                    num_nans = max_instances - len(current_peaks)\n                nans = torch.full((num_nans, 2), torch.nan)\n                current_peaks = torch.cat(\n                    [current_peaks, nans.to(current_peaks.device)], dim=0\n                )\n                nans = torch.full((num_nans,), torch.nan)\n                current_peak_vals = torch.cat(\n                    [current_peak_vals, nans.to(current_peak_vals.device)], dim=0\n                )\n                self.refined_peaks_batched.append(current_peaks)\n                self.peak_vals_batched.append(current_peak_vals)\n\n            # Generate crops if return_crops=True to pass the crops to CenteredInstance model.\n            if self.return_crops:\n                inputs.update(\n                    {\n                        \"centroids\": self.refined_peaks_batched,\n                        \"centroid_vals\": self.peak_vals_batched,\n                    }\n                )\n                crops_dict = self._generate_crops(inputs, cms)\n                return crops_dict\n            else:\n                # batch the peaks to pass it to FindInstancePeaksGroundTruth class.\n                refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n                peak_vals_with_nans = torch.zeros((batch, max_instances))\n                for ind, (r, p) in enumerate(\n                    zip(self.refined_peaks_batched, self.peak_vals_batched)\n                ):\n                    refined_peaks_with_nans[ind] = r\n                    peak_vals_with_nans[ind] = p\n                refined_peaks_with_nans = refined_peaks_with_nans / (\n                    inputs[\"eff_scale\"]\n                    .unsqueeze(dim=1)\n                    .unsqueeze(dim=2)\n                    .to(refined_peaks_with_nans.device)\n                )\n                inputs.update(\n                    {\n                        \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                        \"centroid_vals\": peak_vals_with_nans,\n                    }\n                )\n                if self.return_confmaps:\n                    inputs.update(\n                        {\n                            \"pred_centroid_confmaps\": cms.detach(),\n                        }\n                    )\n\n                return inputs\n\n        else:\n            # if there are no peak detections\n            max_instances = 1 if self.max_instances is None else self.max_instances\n            if self.return_crops:\n                return None\n            refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n            peak_vals_with_nans = torch.zeros((batch, max_instances))\n            for b in range(batch):\n                refined_peaks_with_nans[b] = torch.full((1, 2), torch.nan)\n                peak_vals_with_nans[b] = torch.nan\n\n            inputs.update(\n                {\n                    \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                    \"centroid_vals\": peak_vals_with_nans,\n                }\n            )\n            if self.return_confmaps:\n                inputs.update(\n                    {\n                        \"pred_centroid_confmaps\": cms.detach(),\n                    }\n                )\n            return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.CentroidCrop.__init__","title":"<code>__init__(torch_model=None, output_stride=1, peak_threshold=0.0, max_instances=None, refinement=None, integral_patch_size=5, return_confmaps=False, return_crops=False, crop_hw=None, input_scale=1.0, max_stride=1, use_gt_centroids=False, anchor_ind=None, **kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    torch_model: Optional[L.LightningModule] = None,\n    output_stride: int = 1,\n    peak_threshold: float = 0.0,\n    max_instances: Optional[int] = None,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n    return_confmaps: bool = False,\n    return_crops: bool = False,\n    crop_hw: Optional[List[int]] = None,\n    input_scale: float = 1.0,\n    max_stride: int = 1,\n    use_gt_centroids: bool = False,\n    anchor_ind: Optional[int] = None,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.max_instances = max_instances\n    self.return_crops = return_crops\n    self.crop_hw = crop_hw\n    self.input_scale = input_scale\n    self.max_stride = max_stride\n    self.use_gt_centroids = use_gt_centroids\n    self.anchor_ind = anchor_ind\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.CentroidCrop.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict centroid confidence maps and crop around peaks.</p> <p>This layer can be chained with a <code>FindInstancePeaks</code> layer to create a top-down inference function from full images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with key <code>\"image\"</code>. Other keys will be passed down the pipeline.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A list of dictionaries (size = batch size) where each dictionary has cropped images with key <code>instance_image</code> and <code>centroid_val</code> batched based on the number of centroids predicted for each image in the original batch if return_crops is True. If return_crops is not True, this module returns the dictionary with <code>centroids</code> and <code>centroid_val</code> keys with shapes (batch, 1, max_instances, 2) and (batch, max_instances) respectively which could then to passed to FindInstancePeaksGroundTruth class.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(self, inputs: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict centroid confidence maps and crop around peaks.\n\n    This layer can be chained with a `FindInstancePeaks` layer to create a top-down\n    inference function from full images.\n\n    Args:\n        inputs: Dictionary with key `\"image\"`. Other keys will be passed down the pipeline.\n\n    Returns:\n        A list of dictionaries (size = batch size) where each dictionary has cropped\n        images with key `instance_image` and `centroid_val` batched based on the\n        number of centroids predicted for each image in the original batch if\n        return_crops is True.\n        If return_crops is not True, this module returns the dictionary with\n        `centroids` and `centroid_val` keys with shapes (batch, 1, max_instances, 2)\n        and (batch, max_instances) respectively which could then to passed to\n        FindInstancePeaksGroundTruth class.\n    \"\"\"\n    if self.use_gt_centroids:\n        batch = inputs[\"video_idx\"].shape[0]\n        centroids = generate_centroids(\n            inputs[\"instances\"], anchor_ind=self.anchor_ind\n        )\n        centroid_vals = torch.ones(centroids.shape)[..., 0]\n        self.refined_peaks_batched = [x[0] for x in centroids]\n        self.peak_vals_batched = [x[0] for x in centroid_vals]\n\n        max_instances = (\n            self.max_instances\n            if self.max_instances is not None\n            else inputs[\"instances\"].shape[-3]\n        )\n\n        refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n        peak_vals_with_nans = torch.zeros((batch, max_instances))\n        for ind, (r, p) in enumerate(\n            zip(self.refined_peaks_batched, self.peak_vals_batched)\n        ):\n            refined_peaks_with_nans[ind] = r\n            peak_vals_with_nans[ind] = p\n\n        inputs.update(\n            {\n                \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                \"centroid_vals\": peak_vals_with_nans,\n            }\n        )\n\n        if self.return_crops:\n            crops_dict = self._generate_crops(inputs)\n            return crops_dict\n        else:\n            return inputs\n\n    # Network forward pass.\n    orig_image = inputs[\"image\"]\n    scaled_image = resize_image(orig_image, self.input_scale)\n    if self.max_stride != 1:\n        scaled_image = apply_pad_to_stride(scaled_image, self.max_stride)\n\n    cms = self.torch_model(scaled_image)\n\n    refined_peaks, peak_vals, peak_sample_inds, _ = find_local_peaks(\n        cms.detach(),\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n    # Adjust for stride and scale.\n    refined_peaks = refined_peaks * self.output_stride  # (n_centroids, 2)\n    refined_peaks = refined_peaks / self.input_scale\n\n    batch = cms.shape[0]\n\n    # if max instances is not provided, find the max_instances for this batch\n    num_instances = defaultdict(int)\n    for p in peak_sample_inds:\n        num_instances[int(p)] += 1\n\n    if num_instances:\n        max_instances = max(num_instances.values()) if num_instances else None\n        if self.max_instances is not None:\n            max_instances = self.max_instances\n\n        self.refined_peaks_batched = []\n        self.peak_vals_batched = []\n\n        for b in range(batch):\n            indices = (peak_sample_inds == b).nonzero()\n            # list for predicted centroids and corresponding peak values for current batch.\n            current_peaks = refined_peaks[indices].squeeze(dim=-2)\n            current_peak_vals = peak_vals[indices].squeeze(dim=-1)\n            # Choose top k centroids if max_instances is provided.\n            if len(current_peaks) &gt; max_instances:\n                current_peak_vals, indices = torch.topk(\n                    current_peak_vals, max_instances\n                )\n                current_peaks = current_peaks[indices]\n                num_nans = 0\n            else:\n                num_nans = max_instances - len(current_peaks)\n            nans = torch.full((num_nans, 2), torch.nan)\n            current_peaks = torch.cat(\n                [current_peaks, nans.to(current_peaks.device)], dim=0\n            )\n            nans = torch.full((num_nans,), torch.nan)\n            current_peak_vals = torch.cat(\n                [current_peak_vals, nans.to(current_peak_vals.device)], dim=0\n            )\n            self.refined_peaks_batched.append(current_peaks)\n            self.peak_vals_batched.append(current_peak_vals)\n\n        # Generate crops if return_crops=True to pass the crops to CenteredInstance model.\n        if self.return_crops:\n            inputs.update(\n                {\n                    \"centroids\": self.refined_peaks_batched,\n                    \"centroid_vals\": self.peak_vals_batched,\n                }\n            )\n            crops_dict = self._generate_crops(inputs, cms)\n            return crops_dict\n        else:\n            # batch the peaks to pass it to FindInstancePeaksGroundTruth class.\n            refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n            peak_vals_with_nans = torch.zeros((batch, max_instances))\n            for ind, (r, p) in enumerate(\n                zip(self.refined_peaks_batched, self.peak_vals_batched)\n            ):\n                refined_peaks_with_nans[ind] = r\n                peak_vals_with_nans[ind] = p\n            refined_peaks_with_nans = refined_peaks_with_nans / (\n                inputs[\"eff_scale\"]\n                .unsqueeze(dim=1)\n                .unsqueeze(dim=2)\n                .to(refined_peaks_with_nans.device)\n            )\n            inputs.update(\n                {\n                    \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                    \"centroid_vals\": peak_vals_with_nans,\n                }\n            )\n            if self.return_confmaps:\n                inputs.update(\n                    {\n                        \"pred_centroid_confmaps\": cms.detach(),\n                    }\n                )\n\n            return inputs\n\n    else:\n        # if there are no peak detections\n        max_instances = 1 if self.max_instances is None else self.max_instances\n        if self.return_crops:\n            return None\n        refined_peaks_with_nans = torch.zeros((batch, max_instances, 2))\n        peak_vals_with_nans = torch.zeros((batch, max_instances))\n        for b in range(batch):\n            refined_peaks_with_nans[b] = torch.full((1, 2), torch.nan)\n            peak_vals_with_nans[b] = torch.nan\n\n        inputs.update(\n            {\n                \"centroids\": refined_peaks_with_nans.unsqueeze(dim=1),\n                \"centroid_vals\": peak_vals_with_nans,\n            }\n        )\n        if self.return_confmaps:\n            inputs.update(\n                {\n                    \"pred_centroid_confmaps\": cms.detach(),\n                }\n            )\n        return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaks","title":"<code>FindInstancePeaks</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning Module that predicts instance peaks from images using a trained model.</p> <p>This layer encapsulates all of the inference operations required for generating predictions from a centered instance confidence map model. This includes model forward pass, peak finding and coordinate adjustment.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>input_scale</code> <p>Float indicating the scale with which the images were scaled before cropping.</p> <code>max_stride</code> <p>Maximum stride in a model that the images must be divisible by. If &gt; 1, this will pad the bottom and right of the images to ensure they meet this divisibility criteria. Padding is applied after the scaling specified in the <code>scale</code> attribute.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class FindInstancePeaks(L.LightningModule):\n    \"\"\"Lightning Module that predicts instance peaks from images using a trained model.\n\n    This layer encapsulates all of the inference operations required for generating\n    predictions from a centered instance confidence map model. This includes\n    model forward pass, peak finding and coordinate adjustment.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        input_scale: Float indicating the scale with which the images were scaled before\n            cropping.\n        max_stride: Maximum stride in a model that the images must be divisible by.\n            If &gt; 1, this will pad the bottom and right of the images to ensure they meet\n            this divisibility criteria. Padding is applied after the scaling specified\n            in the `scale` attribute.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = None,\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        input_scale: float = 1.0,\n        max_stride: int = 1,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.input_scale = input_scale\n        self.max_stride = max_stride\n\n    def forward(\n        self,\n        inputs: Dict[str, torch.Tensor],\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        This layer can be chained with a `CentroidCrop` layer to create a top-down\n        inference function from full images.\n\n        Args:\n            inputs: Dictionary with keys:\n                `\"instance_image\"`: Cropped images.\n                Other keys will be passed down the pipeline.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n                `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n            If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n            generated the crops will also be included in the keys `\"centroid\"` and\n            `\"centroid_val\"`.\n\n        \"\"\"\n        # Network forward pass.\n        # resize and pad the input image\n        input_image = inputs[\"instance_image\"]\n        # resize the crop image\n        input_image = resize_image(input_image, self.input_scale)\n        if self.max_stride != 1:\n            input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n        cms = self.torch_model(input_image)\n\n        peak_points, peak_vals = find_global_peaks(\n            cms.detach(),\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n\n        # Adjust for stride and scale.\n        peak_points = peak_points * self.output_stride\n        if self.input_scale != 1.0:\n            peak_points = peak_points / self.input_scale\n\n        peak_points = peak_points / (\n            inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n        )\n\n        inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n            inputs[\"eff_scale\"]\n            .unsqueeze(dim=1)\n            .unsqueeze(dim=2)\n            .unsqueeze(dim=3)\n            .to(inputs[\"instance_bbox\"].device)\n        )\n\n        # Build outputs.\n        outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n        if self.return_confmaps:\n            outputs[\"pred_confmaps\"] = cms.detach()\n        inputs.update(outputs)\n        return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaks.__init__","title":"<code>__init__(torch_model, output_stride=None, peak_threshold=0.0, refinement=None, integral_patch_size=5, return_confmaps=False, input_scale=1.0, max_stride=1, **kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = None,\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    input_scale: float = 1.0,\n    max_stride: int = 1,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.input_scale = input_scale\n    self.max_stride = max_stride\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaks.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>This layer can be chained with a <code>CentroidCrop</code> layer to create a top-down inference function from full images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with keys: <code>\"instance_image\"</code>: Cropped images. Other keys will be passed down the pipeline.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch as a     <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> <p>If provided (e.g., from an input <code>CentroidCrop</code> layer), the centroids that generated the crops will also be included in the keys <code>\"centroid\"</code> and <code>\"centroid_val\"</code>.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(\n    self,\n    inputs: Dict[str, torch.Tensor],\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    This layer can be chained with a `CentroidCrop` layer to create a top-down\n    inference function from full images.\n\n    Args:\n        inputs: Dictionary with keys:\n            `\"instance_image\"`: Cropped images.\n            Other keys will be passed down the pipeline.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n            `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n        If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n        generated the crops will also be included in the keys `\"centroid\"` and\n        `\"centroid_val\"`.\n\n    \"\"\"\n    # Network forward pass.\n    # resize and pad the input image\n    input_image = inputs[\"instance_image\"]\n    # resize the crop image\n    input_image = resize_image(input_image, self.input_scale)\n    if self.max_stride != 1:\n        input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n    cms = self.torch_model(input_image)\n\n    peak_points, peak_vals = find_global_peaks(\n        cms.detach(),\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n\n    # Adjust for stride and scale.\n    peak_points = peak_points * self.output_stride\n    if self.input_scale != 1.0:\n        peak_points = peak_points / self.input_scale\n\n    peak_points = peak_points / (\n        inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n    )\n\n    inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n        inputs[\"eff_scale\"]\n        .unsqueeze(dim=1)\n        .unsqueeze(dim=2)\n        .unsqueeze(dim=3)\n        .to(inputs[\"instance_bbox\"].device)\n    )\n\n    # Build outputs.\n    outputs = {\"pred_instance_peaks\": peak_points, \"pred_peak_values\": peak_vals}\n    if self.return_confmaps:\n        outputs[\"pred_confmaps\"] = cms.detach()\n    inputs.update(outputs)\n    return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaksGroundTruth","title":"<code>FindInstancePeaksGroundTruth</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>LightningModule that simulates a centered instance peaks model.</p> <p>This layer is useful for testing and evaluating centroid models.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Return the ground truth instance peaks given a set of crops.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class FindInstancePeaksGroundTruth(L.LightningModule):\n    \"\"\"LightningModule that simulates a centered instance peaks model.\n\n    This layer is useful for testing and evaluating centroid models.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n\n    def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, np.array]:\n        \"\"\"Return the ground truth instance peaks given a set of crops.\"\"\"\n        b, _, _, nodes, _ = batch[\"instances\"].shape\n        # Use number of centroids as max_inst to ensure consistent output shape\n        # This handles the case where max_instances limits centroids but instances\n        # tensor has a different (global) max_instances from the labels file\n        num_centroids = batch[\"centroids\"].shape[2]\n        inst = (\n            batch[\"instances\"].unsqueeze(dim=-4).float()\n        )  # (batch, 1, 1, n_inst, nodes, 2)\n        cent = (\n            batch[\"centroids\"].unsqueeze(dim=-2).unsqueeze(dim=-3).float()\n        )  # (batch, 1, n_centroids, 1, 1, 2)\n        dists = torch.sum(\n            (inst - cent) ** 2, dim=-1\n        )  # (batch, 1, n_centroids, n_inst, nodes)\n        dists = torch.sqrt(dists)\n\n        dists = torch.where(torch.isnan(dists), torch.inf, dists)\n        dists = torch.min(dists, dim=-1).values  # (batch, 1, n_centroids, n_inst)\n\n        # find nearest gt instance\n        matches = torch.argmin(dists, dim=-1)  # (batch, 1, n_centroids)\n\n        # filter matches without NaNs (nans have been converted to inf)\n        subs = torch.argwhere(\n            ~torch.all(dists == torch.inf, dim=-1)\n        )  # each element represents an index with (batch, 1, n_centroids)\n        valid_matches = matches[subs[:, 0], 0, subs[:, 2]]\n        matched_batch_inds = subs[:, 0]\n\n        counts = torch.bincount(matched_batch_inds.detach())\n        peaks_list = batch[\"instances\"][matched_batch_inds, 0, valid_matches, :, :]\n        parsed = 0\n        for i in range(b):\n            if i not in matched_batch_inds:\n                batch_peaks = torch.full((num_centroids, nodes, 2), torch.nan)\n                vals = torch.full((num_centroids, nodes), torch.nan)\n            else:\n                c = counts[i]\n                batch_peaks = peaks_list[parsed : parsed + c]\n                num_inst = len(batch_peaks)\n                vals = torch.ones((num_inst, nodes))\n                if c &lt; num_centroids:\n                    batch_peaks = torch.cat(\n                        [\n                            batch_peaks,\n                            torch.full((num_centroids - num_inst, nodes, 2), torch.nan),\n                        ]\n                    )\n                    vals = torch.cat(\n                        [vals, torch.full((num_centroids - num_inst, nodes), torch.nan)]\n                    )\n                else:\n                    batch_peaks = batch_peaks[:num_centroids]\n                    vals = vals[:num_centroids]\n                parsed += c\n\n            batch_peaks = batch_peaks.unsqueeze(dim=0)\n\n            if i != 0:\n                peaks = torch.cat([peaks, batch_peaks])\n                peaks_vals = torch.cat([peaks_vals, vals])\n            else:\n                peaks = batch_peaks\n                peaks_vals = vals\n\n        peaks_output = batch\n        if peaks.size(0) != 0:\n            peaks = peaks / (\n                batch[\"eff_scale\"]\n                .unsqueeze(dim=1)\n                .unsqueeze(dim=2)\n                .unsqueeze(dim=3)\n                .to(peaks.device)\n            )\n        peaks_output[\"pred_instance_peaks\"] = peaks\n        peaks_output[\"pred_peak_values\"] = peaks_vals\n\n        batch_size = batch[\"centroids\"].shape[0]\n        output_dict = {}\n        output_dict[\"centroid\"] = batch[\"centroids\"].squeeze(dim=1).reshape(-1, 1, 2)\n        output_dict[\"centroid_val\"] = batch[\"centroid_vals\"].reshape(-1)\n        output_dict[\"pred_instance_peaks\"] = peaks_output[\n            \"pred_instance_peaks\"\n        ].reshape(-1, nodes, 2)\n        output_dict[\"pred_peak_values\"] = peaks_output[\"pred_peak_values\"].reshape(\n            -1, nodes\n        )\n        output_dict[\"instance_bbox\"] = torch.zeros(\n            (batch_size * num_centroids, 1, 4, 2)\n        )\n        frame_inds = []\n        video_inds = []\n        orig_szs = []\n        images = []\n        centroid_confmaps = []\n        for b_idx in range(b):\n            curr_batch_size = len(batch[\"centroids\"][b_idx][0])\n            frame_inds.extend([batch[\"frame_idx\"][b_idx]] * curr_batch_size)\n            video_inds.extend([batch[\"video_idx\"][b_idx]] * curr_batch_size)\n            orig_szs.append(torch.cat([batch[\"orig_size\"][b_idx]] * curr_batch_size))\n            images.append(\n                batch[\"image\"][b_idx].unsqueeze(0).repeat(curr_batch_size, 1, 1, 1, 1)\n            )\n            if \"pred_centroid_confmaps\" in batch:\n                centroid_confmaps.append(\n                    batch[\"pred_centroid_confmaps\"][b_idx]\n                    .unsqueeze(0)\n                    .repeat(curr_batch_size, 1, 1, 1)\n                )\n\n        output_dict[\"frame_idx\"] = torch.tensor(frame_inds)\n        output_dict[\"video_idx\"] = torch.tensor(video_inds)\n        output_dict[\"orig_size\"] = torch.concatenate(orig_szs, dim=0)\n        output_dict[\"image\"] = torch.cat(images, dim=0)\n        if centroid_confmaps:\n            output_dict[\"pred_centroid_confmaps\"] = torch.cat(centroid_confmaps, dim=0)\n        return output_dict\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaksGroundTruth.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.FindInstancePeaksGroundTruth.forward","title":"<code>forward(batch)</code>","text":"<p>Return the ground truth instance peaks given a set of crops.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, np.array]:\n    \"\"\"Return the ground truth instance peaks given a set of crops.\"\"\"\n    b, _, _, nodes, _ = batch[\"instances\"].shape\n    # Use number of centroids as max_inst to ensure consistent output shape\n    # This handles the case where max_instances limits centroids but instances\n    # tensor has a different (global) max_instances from the labels file\n    num_centroids = batch[\"centroids\"].shape[2]\n    inst = (\n        batch[\"instances\"].unsqueeze(dim=-4).float()\n    )  # (batch, 1, 1, n_inst, nodes, 2)\n    cent = (\n        batch[\"centroids\"].unsqueeze(dim=-2).unsqueeze(dim=-3).float()\n    )  # (batch, 1, n_centroids, 1, 1, 2)\n    dists = torch.sum(\n        (inst - cent) ** 2, dim=-1\n    )  # (batch, 1, n_centroids, n_inst, nodes)\n    dists = torch.sqrt(dists)\n\n    dists = torch.where(torch.isnan(dists), torch.inf, dists)\n    dists = torch.min(dists, dim=-1).values  # (batch, 1, n_centroids, n_inst)\n\n    # find nearest gt instance\n    matches = torch.argmin(dists, dim=-1)  # (batch, 1, n_centroids)\n\n    # filter matches without NaNs (nans have been converted to inf)\n    subs = torch.argwhere(\n        ~torch.all(dists == torch.inf, dim=-1)\n    )  # each element represents an index with (batch, 1, n_centroids)\n    valid_matches = matches[subs[:, 0], 0, subs[:, 2]]\n    matched_batch_inds = subs[:, 0]\n\n    counts = torch.bincount(matched_batch_inds.detach())\n    peaks_list = batch[\"instances\"][matched_batch_inds, 0, valid_matches, :, :]\n    parsed = 0\n    for i in range(b):\n        if i not in matched_batch_inds:\n            batch_peaks = torch.full((num_centroids, nodes, 2), torch.nan)\n            vals = torch.full((num_centroids, nodes), torch.nan)\n        else:\n            c = counts[i]\n            batch_peaks = peaks_list[parsed : parsed + c]\n            num_inst = len(batch_peaks)\n            vals = torch.ones((num_inst, nodes))\n            if c &lt; num_centroids:\n                batch_peaks = torch.cat(\n                    [\n                        batch_peaks,\n                        torch.full((num_centroids - num_inst, nodes, 2), torch.nan),\n                    ]\n                )\n                vals = torch.cat(\n                    [vals, torch.full((num_centroids - num_inst, nodes), torch.nan)]\n                )\n            else:\n                batch_peaks = batch_peaks[:num_centroids]\n                vals = vals[:num_centroids]\n            parsed += c\n\n        batch_peaks = batch_peaks.unsqueeze(dim=0)\n\n        if i != 0:\n            peaks = torch.cat([peaks, batch_peaks])\n            peaks_vals = torch.cat([peaks_vals, vals])\n        else:\n            peaks = batch_peaks\n            peaks_vals = vals\n\n    peaks_output = batch\n    if peaks.size(0) != 0:\n        peaks = peaks / (\n            batch[\"eff_scale\"]\n            .unsqueeze(dim=1)\n            .unsqueeze(dim=2)\n            .unsqueeze(dim=3)\n            .to(peaks.device)\n        )\n    peaks_output[\"pred_instance_peaks\"] = peaks\n    peaks_output[\"pred_peak_values\"] = peaks_vals\n\n    batch_size = batch[\"centroids\"].shape[0]\n    output_dict = {}\n    output_dict[\"centroid\"] = batch[\"centroids\"].squeeze(dim=1).reshape(-1, 1, 2)\n    output_dict[\"centroid_val\"] = batch[\"centroid_vals\"].reshape(-1)\n    output_dict[\"pred_instance_peaks\"] = peaks_output[\n        \"pred_instance_peaks\"\n    ].reshape(-1, nodes, 2)\n    output_dict[\"pred_peak_values\"] = peaks_output[\"pred_peak_values\"].reshape(\n        -1, nodes\n    )\n    output_dict[\"instance_bbox\"] = torch.zeros(\n        (batch_size * num_centroids, 1, 4, 2)\n    )\n    frame_inds = []\n    video_inds = []\n    orig_szs = []\n    images = []\n    centroid_confmaps = []\n    for b_idx in range(b):\n        curr_batch_size = len(batch[\"centroids\"][b_idx][0])\n        frame_inds.extend([batch[\"frame_idx\"][b_idx]] * curr_batch_size)\n        video_inds.extend([batch[\"video_idx\"][b_idx]] * curr_batch_size)\n        orig_szs.append(torch.cat([batch[\"orig_size\"][b_idx]] * curr_batch_size))\n        images.append(\n            batch[\"image\"][b_idx].unsqueeze(0).repeat(curr_batch_size, 1, 1, 1, 1)\n        )\n        if \"pred_centroid_confmaps\" in batch:\n            centroid_confmaps.append(\n                batch[\"pred_centroid_confmaps\"][b_idx]\n                .unsqueeze(0)\n                .repeat(curr_batch_size, 1, 1, 1)\n            )\n\n    output_dict[\"frame_idx\"] = torch.tensor(frame_inds)\n    output_dict[\"video_idx\"] = torch.tensor(video_inds)\n    output_dict[\"orig_size\"] = torch.concatenate(orig_szs, dim=0)\n    output_dict[\"image\"] = torch.cat(images, dim=0)\n    if centroid_confmaps:\n        output_dict[\"pred_centroid_confmaps\"] = torch.cat(centroid_confmaps, dim=0)\n    return output_dict\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownInferenceModel","title":"<code>TopDownInferenceModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Top-down instance prediction model.</p> <p>This model encapsulates the top-down approach where instances are first detected by local peak detection of an anchor point and then cropped. These instance-centered crops are then passed to an instance peak detector which is trained to detect all remaining body parts for the instance that is centered within the crop.</p> <p>Attributes:</p> Name Type Description <code>centroid_crop</code> <p>A centroid cropping layer. This can be either <code>CentroidCrop</code> or <code>None</code>. This layer takes the full image as input and outputs a set of centroids and cropped boxes. If <code>None</code>, the centroids are calculated with the provided anchor index using InstanceCentroid module and the centroid vals are set as 1.</p> <code>instance_peaks</code> <p>A instance peak detection layer. This can be either <code>FindInstancePeaks</code> or <code>FindInstancePeaksGroundTruth</code> or <code>TopDownMultiClassFindInstancePeaks</code>. This layer takes as input the output of the centroid cropper (if CentroidCrop not None else the image is cropped with the InstanceCropper module) and outputs the detected peaks for the instances within each crop.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the class with Inference models.</p> <code>forward</code> <p>Predict instances for one batch of images.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class TopDownInferenceModel(L.LightningModule):\n    \"\"\"Top-down instance prediction model.\n\n    This model encapsulates the top-down approach where instances are first detected by\n    local peak detection of an anchor point and then cropped. These instance-centered\n    crops are then passed to an instance peak detector which is trained to detect all\n    remaining body parts for the instance that is centered within the crop.\n\n    Attributes:\n        centroid_crop: A centroid cropping layer. This can be either `CentroidCrop` or\n            `None`. This layer takes the full image as input and outputs a set of centroids\n            and cropped boxes. If `None`, the centroids are calculated with the provided anchor index\n            using InstanceCentroid module and the centroid vals are set as 1.\n        instance_peaks: A instance peak detection layer. This can be either `FindInstancePeaks`\n            or `FindInstancePeaksGroundTruth` or `TopDownMultiClassFindInstancePeaks`. This layer takes as input the output of the centroid cropper\n            (if CentroidCrop not None else the image is cropped with the InstanceCropper module)\n            and outputs the detected peaks for the instances within each crop.\n    \"\"\"\n\n    def __init__(\n        self,\n        centroid_crop: Union[CentroidCrop, None],\n        instance_peaks: Union[\n            FindInstancePeaks,\n            FindInstancePeaksGroundTruth,\n            TopDownMultiClassFindInstancePeaks,\n        ],\n    ):\n        \"\"\"Initialize the class with Inference models.\"\"\"\n        super().__init__()\n        self.centroid_crop = centroid_crop\n        self.instance_peaks = instance_peaks\n\n    def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict instances for one batch of images.\n\n        Args:\n            batch: This is a dictionary containing the image batch in the `image` key.\n                   If centroid model is not provided, the dictionary should have\n                   `instance_image` key.\n\n        Returns:\n            The predicted instances as a list of dictionaries of tensors with the\n            entries in example along with the below keys:\n\n            `\"centroids\": (batch_size, 1, 2)`: Instance centroids.\n            `\"centroid_val\": (batch_size, 1)`: Instance centroid confidence\n                values.\n            `\"pred_instance_peaks\": (batch_size, n_nodes, 2)`: Instance skeleton\n                points.\n            `\"pred_peak_vals\": (batch_size, n_nodes)`: Confidence\n                values for the instance skeleton points.\n        \"\"\"\n        if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n            if \"instances\" not in batch:\n                message = (\n                    \"Ground truth data was not detected... \"\n                    \"Please load both models when predicting on non-ground-truth data.\"\n                )\n                logger.error(message)\n                raise ValueError(message)\n        self.centroid_crop.eval()\n        peaks_output = []\n        batch = self.centroid_crop(batch)\n\n        if batch is not None:\n            if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n                peaks_output.append(self.instance_peaks(batch))\n            else:\n                for i in batch:\n                    self.instance_peaks.eval()\n                    peaks_output.append(\n                        self.instance_peaks(\n                            i,\n                        )\n                    )\n            return peaks_output\n        return batch\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownInferenceModel.__init__","title":"<code>__init__(centroid_crop, instance_peaks)</code>","text":"<p>Initialize the class with Inference models.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    centroid_crop: Union[CentroidCrop, None],\n    instance_peaks: Union[\n        FindInstancePeaks,\n        FindInstancePeaksGroundTruth,\n        TopDownMultiClassFindInstancePeaks,\n    ],\n):\n    \"\"\"Initialize the class with Inference models.\"\"\"\n    super().__init__()\n    self.centroid_crop = centroid_crop\n    self.instance_peaks = instance_peaks\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownInferenceModel.forward","title":"<code>forward(batch)</code>","text":"<p>Predict instances for one batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>This is a dictionary containing the image batch in the <code>image</code> key.    If centroid model is not provided, the dictionary should have    <code>instance_image</code> key.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>The predicted instances as a list of dictionaries of tensors with the entries in example along with the below keys:</p> <p><code>\"centroids\": (batch_size, 1, 2)</code>: Instance centroids. <code>\"centroid_val\": (batch_size, 1)</code>: Instance centroid confidence     values. <code>\"pred_instance_peaks\": (batch_size, n_nodes, 2)</code>: Instance skeleton     points. <code>\"pred_peak_vals\": (batch_size, n_nodes)</code>: Confidence     values for the instance skeleton points.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(self, batch: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict instances for one batch of images.\n\n    Args:\n        batch: This is a dictionary containing the image batch in the `image` key.\n               If centroid model is not provided, the dictionary should have\n               `instance_image` key.\n\n    Returns:\n        The predicted instances as a list of dictionaries of tensors with the\n        entries in example along with the below keys:\n\n        `\"centroids\": (batch_size, 1, 2)`: Instance centroids.\n        `\"centroid_val\": (batch_size, 1)`: Instance centroid confidence\n            values.\n        `\"pred_instance_peaks\": (batch_size, n_nodes, 2)`: Instance skeleton\n            points.\n        `\"pred_peak_vals\": (batch_size, n_nodes)`: Confidence\n            values for the instance skeleton points.\n    \"\"\"\n    if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n        if \"instances\" not in batch:\n            message = (\n                \"Ground truth data was not detected... \"\n                \"Please load both models when predicting on non-ground-truth data.\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n    self.centroid_crop.eval()\n    peaks_output = []\n    batch = self.centroid_crop(batch)\n\n    if batch is not None:\n        if isinstance(self.instance_peaks, FindInstancePeaksGroundTruth):\n            peaks_output.append(self.instance_peaks(batch))\n        else:\n            for i in batch:\n                self.instance_peaks.eval()\n                peaks_output.append(\n                    self.instance_peaks(\n                        i,\n                    )\n                )\n        return peaks_output\n    return batch\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownMultiClassFindInstancePeaks","title":"<code>TopDownMultiClassFindInstancePeaks</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning Module that predicts instance peaks from images using a trained model.</p> <p>This layer encapsulates all of the inference operations required for generating predictions from a centered instance confidence map model. This includes model forward pass, peak finding and coordinate adjustment.</p> <p>Attributes:</p> Name Type Description <code>torch_model</code> <p>A <code>nn.Module</code> that accepts rank-5 images as input and predicts rank-4 confidence maps as output. This should be a model that is trained on centered instance confidence maps.</p> <code>output_stride</code> <p>Output stride of the model, denoting the scale of the output confidence maps relative to the images (after input scaling). This is used for adjusting the peak coordinates to the image grid.</p> <code>peak_threshold</code> <p>Minimum confidence map value to consider a global peak as valid.</p> <code>refinement</code> <p>If <code>None</code>, returns the grid-aligned peaks with no refinement. If <code>\"integral\"</code>, peaks will be refined with integral regression. If <code>\"local\"</code>, peaks will be refined with quarter pixel local gradient offset. This has no effect if the model has an offset regression head.</p> <code>integral_patch_size</code> <p>Size of patches to crop around each rough peak for integral refinement as an integer scalar.</p> <code>return_confmaps</code> <p>If <code>True</code>, the confidence maps will be returned together with the predicted peaks.</p> <code>return_class_vectors</code> <p>If <code>True</code>, the classification probabilities will be returned together with the predicted peaks. This will not line up with the grouped instances, for which the associtated class probabilities will always be returned in <code>\"instance_scores\"</code>.</p> <code>input_scale</code> <p>Float indicating the scale with which the images were scaled before cropping.</p> <code>max_stride</code> <p>Maximum stride in a model that the images must be divisible by. If &gt; 1, this will pad the bottom and right of the images to ensure they meet this divisibility criteria. Padding is applied after the scaling specified in the <code>scale</code> attribute.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the model attributes.</p> <code>forward</code> <p>Predict confidence maps and infer peak coordinates.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>class TopDownMultiClassFindInstancePeaks(L.LightningModule):\n    \"\"\"Lightning Module that predicts instance peaks from images using a trained model.\n\n    This layer encapsulates all of the inference operations required for generating\n    predictions from a centered instance confidence map model. This includes\n    model forward pass, peak finding and coordinate adjustment.\n\n    Attributes:\n        torch_model: A `nn.Module` that accepts rank-5 images as input and predicts\n            rank-4 confidence maps as output. This should be a model that is trained on\n            centered instance confidence maps.\n        output_stride: Output stride of the model, denoting the scale of the output\n            confidence maps relative to the images (after input scaling). This is used\n            for adjusting the peak coordinates to the image grid.\n        peak_threshold: Minimum confidence map value to consider a global peak as valid.\n        refinement: If `None`, returns the grid-aligned peaks with no refinement. If\n            `\"integral\"`, peaks will be refined with integral regression. If `\"local\"`,\n            peaks will be refined with quarter pixel local gradient offset. This has no\n            effect if the model has an offset regression head.\n        integral_patch_size: Size of patches to crop around each rough peak for integral\n            refinement as an integer scalar.\n        return_confmaps: If `True`, the confidence maps will be returned together with\n            the predicted peaks.\n        return_class_vectors: If `True`, the classification probabilities will be\n            returned together with the predicted peaks. This will not line up with the\n            grouped instances, for which the associtated class probabilities will always\n            be returned in `\"instance_scores\"`.\n        input_scale: Float indicating the scale with which the images were scaled before\n            cropping.\n        max_stride: Maximum stride in a model that the images must be divisible by.\n            If &gt; 1, this will pad the bottom and right of the images to ensure they meet\n            this divisibility criteria. Padding is applied after the scaling specified\n            in the `scale` attribute.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        torch_model: L.LightningModule,\n        output_stride: Optional[int] = None,\n        peak_threshold: float = 0.0,\n        refinement: Optional[str] = \"integral\",\n        integral_patch_size: int = 5,\n        return_confmaps: Optional[bool] = False,\n        return_class_vectors: bool = False,\n        input_scale: float = 1.0,\n        max_stride: int = 1,\n        **kwargs,\n    ):\n        \"\"\"Initialise the model attributes.\"\"\"\n        super().__init__(**kwargs)\n        self.torch_model = torch_model\n        self.peak_threshold = peak_threshold\n        self.refinement = refinement\n        self.integral_patch_size = integral_patch_size\n        self.output_stride = output_stride\n        self.return_confmaps = return_confmaps\n        self.return_class_vectors = return_class_vectors\n        self.input_scale = input_scale\n        self.max_stride = max_stride\n\n    def forward(\n        self,\n        inputs: Dict[str, torch.Tensor],\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predict confidence maps and infer peak coordinates.\n\n        This layer can be chained with a `CentroidCrop` layer to create a top-down\n        inference function from full images.\n\n        Args:\n            inputs: Dictionary with keys:\n                `\"instance_image\"`: Cropped images.\n                Other keys will be passed down the pipeline.\n\n        Returns:\n            A dictionary of outputs with keys:\n\n            `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n                `torch.Tensor` of shape `(samples, nodes, 2)`.\n            `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n                peaks for each instance in the batch as a `torch.Tensor` of shape\n                `(samples, nodes)`.\n\n            If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n            generated the crops will also be included in the keys `\"centroid\"` and\n            `\"centroid_val\"`.\n\n        \"\"\"\n        # Network forward pass.\n        # resize and pad the input image\n        input_image = inputs[\"instance_image\"]\n        # resize the crop image\n        input_image = resize_image(input_image, self.input_scale)\n        if self.max_stride != 1:\n            input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n        out = self.torch_model(input_image)\n        cms = out[\"CenteredInstanceConfmapsHead\"].detach()\n        peak_class_probs = out[\"ClassVectorsHead\"].detach()\n\n        peak_points, peak_vals = find_global_peaks(\n            cms,\n            threshold=self.peak_threshold,\n            refinement=self.refinement,\n            integral_patch_size=self.integral_patch_size,\n        )\n\n        # Adjust for stride and scale.\n        peak_points = peak_points * self.output_stride\n        if self.input_scale != 1.0:\n            peak_points = peak_points / self.input_scale\n\n        peak_points = peak_points / (\n            inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n        )\n\n        inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n            inputs[\"eff_scale\"]\n            .unsqueeze(dim=1)\n            .unsqueeze(dim=2)\n            .unsqueeze(dim=3)\n            .to(inputs[\"instance_bbox\"].device)\n        )\n\n        (\n            class_inds,\n            class_probs,\n        ) = get_class_inds_from_vectors(peak_class_probs)\n\n        # Build outputs.\n        outputs = {\n            \"pred_instance_peaks\": peak_points,\n            \"pred_peak_values\": peak_vals,\n            \"instance_scores\": class_probs,\n            \"pred_class_inds\": class_inds,\n        }\n\n        if self.return_confmaps:\n            outputs[\"pred_confmaps\"] = cms\n        if self.return_class_vectors:\n            outputs[\"pred_class_vectors\"] = peak_class_probs\n        inputs.update(outputs)\n        return inputs\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownMultiClassFindInstancePeaks.__init__","title":"<code>__init__(torch_model, output_stride=None, peak_threshold=0.0, refinement='integral', integral_patch_size=5, return_confmaps=False, return_class_vectors=False, input_scale=1.0, max_stride=1, **kwargs)</code>","text":"<p>Initialise the model attributes.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def __init__(\n    self,\n    torch_model: L.LightningModule,\n    output_stride: Optional[int] = None,\n    peak_threshold: float = 0.0,\n    refinement: Optional[str] = \"integral\",\n    integral_patch_size: int = 5,\n    return_confmaps: Optional[bool] = False,\n    return_class_vectors: bool = False,\n    input_scale: float = 1.0,\n    max_stride: int = 1,\n    **kwargs,\n):\n    \"\"\"Initialise the model attributes.\"\"\"\n    super().__init__(**kwargs)\n    self.torch_model = torch_model\n    self.peak_threshold = peak_threshold\n    self.refinement = refinement\n    self.integral_patch_size = integral_patch_size\n    self.output_stride = output_stride\n    self.return_confmaps = return_confmaps\n    self.return_class_vectors = return_class_vectors\n    self.input_scale = input_scale\n    self.max_stride = max_stride\n</code></pre>"},{"location":"api/inference/topdown/#sleap_nn.inference.topdown.TopDownMultiClassFindInstancePeaks.forward","title":"<code>forward(inputs)</code>","text":"<p>Predict confidence maps and infer peak coordinates.</p> <p>This layer can be chained with a <code>CentroidCrop</code> layer to create a top-down inference function from full images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, Tensor]</code> <p>Dictionary with keys: <code>\"instance_image\"</code>: Cropped images. Other keys will be passed down the pipeline.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary of outputs with keys:</p> <p><code>\"pred_instance_peaks\"</code>: The predicted peaks for each instance in the batch as a     <code>torch.Tensor</code> of shape <code>(samples, nodes, 2)</code>. <code>\"pred_peak_vals\"</code>: The value of the confidence maps at the predicted     peaks for each instance in the batch as a <code>torch.Tensor</code> of shape     <code>(samples, nodes)</code>.</p> <p>If provided (e.g., from an input <code>CentroidCrop</code> layer), the centroids that generated the crops will also be included in the keys <code>\"centroid\"</code> and <code>\"centroid_val\"</code>.</p> Source code in <code>sleap_nn/inference/topdown.py</code> <pre><code>def forward(\n    self,\n    inputs: Dict[str, torch.Tensor],\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predict confidence maps and infer peak coordinates.\n\n    This layer can be chained with a `CentroidCrop` layer to create a top-down\n    inference function from full images.\n\n    Args:\n        inputs: Dictionary with keys:\n            `\"instance_image\"`: Cropped images.\n            Other keys will be passed down the pipeline.\n\n    Returns:\n        A dictionary of outputs with keys:\n\n        `\"pred_instance_peaks\"`: The predicted peaks for each instance in the batch as a\n            `torch.Tensor` of shape `(samples, nodes, 2)`.\n        `\"pred_peak_vals\"`: The value of the confidence maps at the predicted\n            peaks for each instance in the batch as a `torch.Tensor` of shape\n            `(samples, nodes)`.\n\n        If provided (e.g., from an input `CentroidCrop` layer), the centroids that\n        generated the crops will also be included in the keys `\"centroid\"` and\n        `\"centroid_val\"`.\n\n    \"\"\"\n    # Network forward pass.\n    # resize and pad the input image\n    input_image = inputs[\"instance_image\"]\n    # resize the crop image\n    input_image = resize_image(input_image, self.input_scale)\n    if self.max_stride != 1:\n        input_image = apply_pad_to_stride(input_image, self.max_stride)\n\n    out = self.torch_model(input_image)\n    cms = out[\"CenteredInstanceConfmapsHead\"].detach()\n    peak_class_probs = out[\"ClassVectorsHead\"].detach()\n\n    peak_points, peak_vals = find_global_peaks(\n        cms,\n        threshold=self.peak_threshold,\n        refinement=self.refinement,\n        integral_patch_size=self.integral_patch_size,\n    )\n\n    # Adjust for stride and scale.\n    peak_points = peak_points * self.output_stride\n    if self.input_scale != 1.0:\n        peak_points = peak_points / self.input_scale\n\n    peak_points = peak_points / (\n        inputs[\"eff_scale\"].unsqueeze(dim=1).unsqueeze(dim=2).to(peak_points.device)\n    )\n\n    inputs[\"instance_bbox\"] = inputs[\"instance_bbox\"] / (\n        inputs[\"eff_scale\"]\n        .unsqueeze(dim=1)\n        .unsqueeze(dim=2)\n        .unsqueeze(dim=3)\n        .to(inputs[\"instance_bbox\"].device)\n    )\n\n    (\n        class_inds,\n        class_probs,\n    ) = get_class_inds_from_vectors(peak_class_probs)\n\n    # Build outputs.\n    outputs = {\n        \"pred_instance_peaks\": peak_points,\n        \"pred_peak_values\": peak_vals,\n        \"instance_scores\": class_probs,\n        \"pred_class_inds\": class_inds,\n    }\n\n    if self.return_confmaps:\n        outputs[\"pred_confmaps\"] = cms\n    if self.return_class_vectors:\n        outputs[\"pred_class_vectors\"] = peak_class_probs\n    inputs.update(outputs)\n    return inputs\n</code></pre>"},{"location":"api/inference/utils/","title":"utils","text":""},{"location":"api/inference/utils/#sleap_nn.inference.utils","title":"<code>sleap_nn.inference.utils</code>","text":"<p>Miscellaneous utility functions for Inference modules.</p> <p>Functions:</p> Name Description <code>get_skeleton_from_config</code> <p>Create Sleap-io Skeleton objects from config.</p> <code>interp1d</code> <p>Linear 1-D interpolation.</p>"},{"location":"api/inference/utils/#sleap_nn.inference.utils.get_skeleton_from_config","title":"<code>get_skeleton_from_config(skeleton_config)</code>","text":"<p>Create Sleap-io Skeleton objects from config.</p> <p>Parameters:</p> Name Type Description Default <code>skeleton_config</code> <code>OmegaConf</code> <p>OmegaConf object containing the skeleton config.</p> required <p>Returns:</p> Type Description <p>Returns a list of <code>sio.Skeleton</code> objects created from the skeleton config stored in the <code>training_config.yaml</code>.</p> Source code in <code>sleap_nn/inference/utils.py</code> <pre><code>def get_skeleton_from_config(skeleton_config: OmegaConf):\n    \"\"\"Create Sleap-io Skeleton objects from config.\n\n    Args:\n        skeleton_config: OmegaConf object containing the skeleton config.\n\n    Returns:\n        Returns a list of `sio.Skeleton` objects created from the skeleton config\n        stored in the `training_config.yaml`.\n\n    \"\"\"\n    skeletons = []\n    for skel_cfg in skeleton_config:\n        skel = SkeletonYAMLDecoder().decode(dict(skel_cfg))\n        skel.name = skel_cfg.name\n        skeletons.append(skel)\n\n    return skeletons\n</code></pre>"},{"location":"api/inference/utils/#sleap_nn.inference.utils.interp1d","title":"<code>interp1d(x, y, xnew)</code>","text":"<p>Linear 1-D interpolation.</p> <p>Src: https://github.com/aliutkus/torchinterp1d/blob/master/torchinterp1d/interp1d.py</p> <p>Parameters:</p> Name Type Description Default <code>x </code> <p>(N, ) or (D, N) Tensor.</p> required <code>y </code> <p>(N,) or (D, N) float Tensor. The length of <code>y</code> along its last dimension must be the same as that of <code>x</code></p> required <code>xnew </code> <p>(P,) or (D, P) Tensor. <code>xnew</code> can only be 1-D if both <code>x</code> and <code>y</code> are 1-D. Otherwise, its length along the first dimension must be the same as that of whichever <code>x</code> and <code>y</code> is 2-D.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>(P, ) or (D, P) Tensor.</p> Source code in <code>sleap_nn/inference/utils.py</code> <pre><code>def interp1d(x: torch.Tensor, y: torch.Tensor, xnew: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Linear 1-D interpolation.\n\n    Src: https://github.com/aliutkus/torchinterp1d/blob/master/torchinterp1d/interp1d.py\n\n    Args:\n        x : (N, ) or (D, N) Tensor.\n        y : (N,) or (D, N) float Tensor. The length of `y` along its\n            last dimension must be the same as that of `x`\n        xnew : (P,) or (D, P) Tensor. `xnew` can only be 1-D if\n            _both_ `x` and `y` are 1-D. Otherwise, its length along the first\n            dimension must be the same as that of whichever `x` and `y` is 2-D.\n\n    Returns:\n        (P, ) or (D, P) Tensor.\n    \"\"\"\n    # making the vectors at least 2D\n    is_flat = {}\n    v = {}\n    eps = torch.finfo(y.dtype).eps\n    for name, vec in {\"x\": x, \"y\": y, \"xnew\": xnew}.items():\n        assert len(vec.shape) &lt;= 2, \"interp1d: all inputs must be at most 2-D.\"\n        if len(vec.shape) == 1:\n            v[name] = vec[None, :]\n        else:\n            v[name] = vec\n        is_flat[name] = v[name].shape[0] == 1\n    device = y.device\n\n    # Checking for the dimensions\n    assert v[\"x\"].shape[1] == v[\"y\"].shape[1] and (\n        v[\"x\"].shape[0] == v[\"y\"].shape[0]\n        or v[\"x\"].shape[0] == 1\n        or v[\"y\"].shape[0] == 1\n    ), (\n        \"x and y must have the same number of columns, and either \"\n        \"the same number of row or one of them having only one \"\n        \"row.\"\n    )\n\n    if (v[\"x\"].shape[0] == 1) and (v[\"y\"].shape[0] == 1) and (v[\"xnew\"].shape[0] &gt; 1):\n        # if there is only one row for both x and y, there is no need to\n        # loop over the rows of xnew because they will all have to face the\n        # same interpolation problem. We should just stack them together to\n        # call interp1d and put them back in place afterwards.\n        v[\"xnew\"] = v[\"xnew\"].contiguous().view(1, -1)\n\n    # identify the dimensions of output\n    D = max(v[\"x\"].shape[0], v[\"xnew\"].shape[0])\n    shape_ynew = (D, v[\"xnew\"].shape[-1])\n    ynew = torch.zeros(*shape_ynew, device=device)\n\n    # moving everything to the desired device in case it was not there\n    # already (not handling the case things do not fit entirely, user will\n    # do it if required.)\n    for name in v:\n        v[name] = v[name].to(device)\n\n    # calling searchsorted on the x values.\n    ind = ynew.long()\n\n    # expanding xnew to match the number of rows of x in case only one xnew is\n    # provided\n    if v[\"xnew\"].shape[0] == 1:\n        v[\"xnew\"] = v[\"xnew\"].expand(v[\"x\"].shape[0], -1)\n\n    # the squeeze is because torch.searchsorted does accept either an n-d tensor with\n    # matching shapes for x and xnew or a 1d vector for x. Here we would\n    # have (1,len) for x sometimes\n    torch.searchsorted(v[\"x\"].contiguous().squeeze(), v[\"xnew\"].contiguous(), out=ind)\n\n    # the `-1` is because searchsorted looks for the index where the values\n    # must be inserted to preserve order. And we want the index of the\n    # preceding value.\n    ind -= 1\n    # we clamp the index, because the number of intervals is x.shape-1,\n    # and the left neighbour should hence be at most number of intervals\n    # -1, i.e. number of columns in x -2\n    ind = torch.clamp(ind, 0, v[\"x\"].shape[1] - 1 - 1)\n\n    # helper function to select stuff according to the found indices.\n    def sel(name):\n        if is_flat[name]:\n            return v[name].contiguous().view(-1)[ind]\n        return torch.gather(v[name], 1, ind)\n\n    # assuming x are sorted in the dimension 1, computing the slopes for\n    # the segments\n    is_flat[\"slopes\"] = is_flat[\"x\"]\n    # now we have found the indices of the neighbors, we start building the\n    # output.\n    v[\"slopes\"] = (v[\"y\"][:, 1:] - v[\"y\"][:, :-1]) / (\n        eps + (v[\"x\"][:, 1:] - v[\"x\"][:, :-1])\n    )\n\n    # now build the linear interpolation\n    ynew = sel(\"y\") + sel(\"slopes\") * (v[\"xnew\"] - sel(\"x\"))\n\n    if len(y.shape) == 1:\n        ynew = ynew.view(-1)\n\n    return ynew\n</code></pre>"},{"location":"api/tracking/","title":"tracking","text":""},{"location":"api/tracking/#sleap_nn.tracking","title":"<code>sleap_nn.tracking</code>","text":"<p>Tracker related modules.</p> <p>Modules:</p> Name Description <code>candidates</code> <p>Candidate generation modules for tracking.</p> <code>track_instance</code> <p>TrackInstance Data structure for Tracker queue.</p> <code>tracker</code> <p>Module for tracking.</p> <code>utils</code> <p>Helper functions for Tracker module.</p>"},{"location":"api/tracking/track_instance/","title":"track_instance","text":""},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance","title":"<code>sleap_nn.tracking.track_instance</code>","text":"<p>TrackInstance Data structure for Tracker queue.</p> <p>Classes:</p> Name Description <code>TrackInstanceLocalQueue</code> <p>Data structure for instances in tracker queue for Local Queue method.</p> <code>TrackInstances</code> <p>Data structure for instances in tracker queue for fixed window method.</p> <code>TrackedInstanceFeature</code> <p>Data structure for tracked instances.</p>"},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance.TrackInstanceLocalQueue","title":"<code>TrackInstanceLocalQueue</code>","text":"<p>Data structure for instances in tracker queue for Local Queue method.</p> Source code in <code>sleap_nn/tracking/track_instance.py</code> <pre><code>@attrs.define\nclass TrackInstanceLocalQueue:\n    \"\"\"Data structure for instances in tracker queue for Local Queue method.\"\"\"\n\n    src_instance: sio.PredictedInstance\n    src_instance_idx: int\n    feature: np.array\n    track_id: Optional[int] = None\n    tracking_score: Optional[float] = None\n    frame_idx: Optional[float] = None\n    image: Optional[np.array] = None\n</code></pre>"},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance.TrackInstances","title":"<code>TrackInstances</code>","text":"<p>Data structure for instances in tracker queue for fixed window method.</p> Source code in <code>sleap_nn/tracking/track_instance.py</code> <pre><code>@attrs.define\nclass TrackInstances:\n    \"\"\"Data structure for instances in tracker queue for fixed window method.\"\"\"\n\n    src_instances: List[sio.PredictedInstance]\n    features: List[np.array]\n    track_ids: Optional[List[int]] = None\n    tracking_scores: Optional[List[float]] = None\n    frame_idx: Optional[float] = None\n    image: Optional[np.array] = None\n</code></pre>"},{"location":"api/tracking/track_instance/#sleap_nn.tracking.track_instance.TrackedInstanceFeature","title":"<code>TrackedInstanceFeature</code>","text":"<p>Data structure for tracked instances.</p> <p>This data structure is used for updating the previous tracked instances and get the features of the tracked instances. <code>shifted_keypoints</code> is used only for the <code>FlowShiftTracker</code> to store the optical flow shifted instances.</p> Source code in <code>sleap_nn/tracking/track_instance.py</code> <pre><code>@attrs.define\nclass TrackedInstanceFeature:\n    \"\"\"Data structure for tracked instances.\n\n    This data structure is used for updating the previous tracked instances and get the\n    features of the tracked instances. `shifted_keypoints` is used only for the `FlowShiftTracker`\n    to store the optical flow shifted instances.\n    \"\"\"\n\n    feature: np.ndarray\n    src_predicted_instance: sio.PredictedInstance\n    frame_idx: int\n    tracking_score: float\n    shifted_keypoints: np.ndarray = None\n</code></pre>"},{"location":"api/tracking/tracker/","title":"tracker","text":""},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker","title":"<code>sleap_nn.tracking.tracker</code>","text":"<p>Module for tracking.</p> <p>Classes:</p> Name Description <code>FlowShiftTracker</code> <p>Module for tracking using optical flow shift matching.</p> <code>RateColumn</code> <p>Renders the progress rate.</p> <code>Tracker</code> <p>Simple Pose Tracker.</p> <p>Functions:</p> Name Description <code>connect_single_breaks</code> <p>Merge single-frame breaks in tracks by connecting single lost track with single new track.</p> <code>run_tracker</code> <p>Run tracking on a given set of frames.</p>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.FlowShiftTracker","title":"<code>FlowShiftTracker</code>","text":"<p>               Bases: <code>Tracker</code></p> <p>Module for tracking using optical flow shift matching.</p> <p>This module handles tracking instances across frames by creating new track IDs (or) assigning track IDs to each instance when the <code>.track()</code> is called using optical flow based track matching. This is a sub-class of the <code>Tracker</code> module, which configures the <code>update_candidates()</code> method specific to optical flow shift matching. This class is initialized in the <code>Tracker.from_config()</code> method.</p> <p>Attributes:</p> Name Type Description <code>candidates</code> <p>Either <code>FixedWindowCandidates</code> or <code>LocalQueueCandidates</code> object.</p> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>features</code> <code>str</code> <p>One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1     (excluded), use a robust quantile similarity score for the     track. If the value is 1, use the max similarity (non-robust).     For selecting a robust score, 0.95 is a good value.</p> <code>track_matching_method</code> <code>str</code> <p>track matching algorithm. One of <code>hungarian</code>, <code>greedy.     Default:</code>hungarian`.</p> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using optical flow. Default: <code>False</code>.</p> <code>is_local_queue</code> <code>bool</code> <p><code>True</code> if <code>LocalQueueCandidates</code> is used else <code>False</code>.</p> <code>img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0.</p> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21.</p> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling. Default: 3</p> <code>tracking_target_instance_count</code> <code>Optional[int]</code> <p>Target number of instances to track per frame. (default: None)</p> <code>tracking_pre_cull_to_target</code> <code>int</code> <p>If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame before tracking. (default: 0)</p> <code>tracking_pre_cull_iou_threshold</code> <code>float</code> <p>If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count before tracking. (default: 0)</p> <p>Methods:</p> Name Description <code>get_shifted_instances_from_prv_frames</code> <p>Generate shifted instances onto the new frame by applying optical flow.</p> <code>update_candidates</code> <p>Return dictionary with the features of tracked instances.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>@attrs.define\nclass FlowShiftTracker(Tracker):\n    \"\"\"Module for tracking using optical flow shift matching.\n\n    This module handles tracking instances across frames by creating new track IDs (or)\n    assigning track IDs to each instance when the `.track()` is called using optical flow\n    based track matching. This is a sub-class of the `Tracker` module, which configures\n    the `update_candidates()` method specific to optical flow shift matching. This class is\n    initialized in the `Tracker.from_config()` method.\n\n    Attributes:\n        candidates: Either `FixedWindowCandidates` or `LocalQueueCandidates` object.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: One of [`keypoints`, `centroids`, `bboxes`, `image`].\n            Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n                (excluded), use a robust quantile similarity score for the\n                track. If the value is 1, use the max similarity (non-robust).\n                For selecting a robust score, 0.95 is a good value.\n        track_matching_method: track matching algorithm. One of `hungarian`, `greedy.\n                Default: `hungarian`.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n            optical flow. Default: `False`.\n        is_local_queue: `True` if `LocalQueueCandidates` is used else `False`.\n        img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0.\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21.\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n            Default: 3\n        tracking_target_instance_count: Target number of instances to track per frame. (default: None)\n        tracking_pre_cull_to_target: If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame *before* tracking. (default: 0)\n        tracking_pre_cull_iou_threshold: If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count *before* tracking. (default: 0)\n\n    \"\"\"\n\n    img_scale: float = 1.0\n    of_window_size: int = 21\n    of_max_levels: int = 3\n\n    def _compute_optical_flow(\n        self, ref_pts: np.ndarray, ref_img: np.ndarray, new_img: np.ndarray\n    ):\n        \"\"\"Compute instances on new frame using optical flow displacements.\"\"\"\n        ref_img, new_img = self._preprocess_imgs(ref_img, new_img)\n        shifted_pts, status, errs = cv2.calcOpticalFlowPyrLK(\n            ref_img,\n            new_img,\n            (np.concatenate(ref_pts, axis=0)).astype(\"float32\") * self.img_scale,\n            None,\n            winSize=(self.of_window_size, self.of_window_size),\n            maxLevel=self.of_max_levels,\n            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01),\n        )\n        shifted_pts /= self.img_scale\n        return shifted_pts, status, errs\n\n    def _preprocess_imgs(self, ref_img: np.ndarray, new_img: np.ndarray):\n        \"\"\"Pre-process images for optical flow.\"\"\"\n        # Convert to uint8 for cv2.calcOpticalFlowPyrLK\n        if np.issubdtype(ref_img.dtype, np.floating):\n            ref_img = ref_img.astype(\"uint8\")\n        if np.issubdtype(new_img.dtype, np.floating):\n            new_img = new_img.astype(\"uint8\")\n\n        # Ensure images are rank 2 in case there is a singleton channel dimension.\n        if ref_img.ndim &gt; 3:\n            ref_img = np.squeeze(ref_img)\n            new_img = np.squeeze(new_img)\n\n        # Convert RGB to grayscale.\n        if ref_img.ndim &gt; 2 and ref_img.shape[0] == 3:\n            ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)\n            new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n\n        # Input image scaling.\n        if self.img_scale != 1:\n            ref_img = cv2.resize(ref_img, None, None, self.img_scale, self.img_scale)\n            new_img = cv2.resize(new_img, None, None, self.img_scale, self.img_scale)\n\n        return ref_img, new_img\n\n    def get_shifted_instances_from_prv_frames(\n        self,\n        candidates_list: Union[Deque, DefaultDict[int, Deque]],\n        new_img: np.ndarray,\n        feature_method,\n    ) -&gt; Dict[int, List[TrackedInstanceFeature]]:\n        \"\"\"Generate shifted instances onto the new frame by applying optical flow.\"\"\"\n        shifted_instances_prv_frames = defaultdict(list)\n\n        if self.is_local_queue:\n            # for local queue\n            ref_candidates = self.candidate.get_instances_groupby_frame_idx(\n                candidates_list\n            )\n            for fidx, ref_candidate_list in ref_candidates.items():\n                ref_pts = [x.src_instance.numpy() for x in ref_candidate_list]\n                shifted_pts, status, errs = self._compute_optical_flow(\n                    ref_pts=ref_pts,\n                    ref_img=ref_candidate_list[0].image,\n                    new_img=new_img,\n                )\n\n                sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n                shifted_pts = np.split(shifted_pts, sections, axis=0)\n                status = np.split(status, sections, axis=0)\n                errs = np.split(errs, sections, axis=0)\n\n                # Create shifted instances.\n                for idx, (ref_candidate, pts, found) in enumerate(\n                    zip(ref_candidate_list, shifted_pts, status)\n                ):\n                    # Exclude points that weren't found by optical flow.\n                    found = found.squeeze().astype(bool)\n                    pts[~found] = np.nan\n\n                    # Create a shifted instance.\n                    shifted_instances_prv_frames[ref_candidate.track_id].append(\n                        TrackedInstanceFeature(\n                            feature=feature_method(pts),\n                            src_predicted_instance=ref_candidate.src_instance,\n                            frame_idx=fidx,\n                            tracking_score=ref_candidate.tracking_score,\n                            shifted_keypoints=pts,\n                        )\n                    )\n\n        else:\n            # for fixed window\n            candidates_list = (\n                candidates_list\n                if candidates_list is not None\n                else self.candidate.tracker_queue\n            )\n            for ref_candidate in candidates_list:\n                ref_pts = [x.numpy() for x in ref_candidate.src_instances]\n                shifted_pts, status, errs = self._compute_optical_flow(\n                    ref_pts=ref_pts, ref_img=ref_candidate.image, new_img=new_img\n                )\n\n                sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n                shifted_pts = np.split(shifted_pts, sections, axis=0)\n                status = np.split(status, sections, axis=0)\n                errs = np.split(errs, sections, axis=0)\n\n                # Create shifted instances.\n                for idx, (pts, found) in enumerate(zip(shifted_pts, status)):\n                    # Exclude points that weren't found by optical flow.\n                    found = found.squeeze().astype(bool)\n                    pts[~found] = np.nan\n\n                    # Create a shifted instance.\n                    shifted_instances_prv_frames[ref_candidate.track_ids[idx]].append(\n                        TrackedInstanceFeature(\n                            feature=feature_method(pts),\n                            src_predicted_instance=ref_candidate.src_instances[idx],\n                            frame_idx=ref_candidate.frame_idx,\n                            tracking_score=ref_candidate.tracking_scores[idx],\n                            shifted_keypoints=pts,\n                        )\n                    )\n\n        return shifted_instances_prv_frames\n\n    def update_candidates(\n        self,\n        candidates_list: Union[Deque, DefaultDict[int, Deque]],\n        image: np.ndarray,\n    ) -&gt; Dict[int, TrackedInstanceFeature]:\n        \"\"\"Return dictionary with the features of tracked instances.\n\n        In this method, the tracked instances in the tracker queue are shifted on to the\n        current frame using optical flow. The features are then computed from the shifted\n        instances.\n\n        Args:\n            candidates_list: Tracker queue from the candidate class.\n            image: Image of the current untracked frame. (used for flow shift tracker)\n\n        Returns:\n            Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n        \"\"\"\n        # get feature method for the shifted instances\n        if self.features not in self._feature_methods:\n            message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n            logger.error(message)\n            raise ValueError(message)\n        feature_method = self._feature_methods[self.features]\n\n        # get shifted instances from optical flow\n        shifted_instances_prv_frames = self.get_shifted_instances_from_prv_frames(\n            candidates_list=candidates_list,\n            new_img=image,\n            feature_method=feature_method,\n        )\n\n        return shifted_instances_prv_frames\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.FlowShiftTracker.get_shifted_instances_from_prv_frames","title":"<code>get_shifted_instances_from_prv_frames(candidates_list, new_img, feature_method)</code>","text":"<p>Generate shifted instances onto the new frame by applying optical flow.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def get_shifted_instances_from_prv_frames(\n    self,\n    candidates_list: Union[Deque, DefaultDict[int, Deque]],\n    new_img: np.ndarray,\n    feature_method,\n) -&gt; Dict[int, List[TrackedInstanceFeature]]:\n    \"\"\"Generate shifted instances onto the new frame by applying optical flow.\"\"\"\n    shifted_instances_prv_frames = defaultdict(list)\n\n    if self.is_local_queue:\n        # for local queue\n        ref_candidates = self.candidate.get_instances_groupby_frame_idx(\n            candidates_list\n        )\n        for fidx, ref_candidate_list in ref_candidates.items():\n            ref_pts = [x.src_instance.numpy() for x in ref_candidate_list]\n            shifted_pts, status, errs = self._compute_optical_flow(\n                ref_pts=ref_pts,\n                ref_img=ref_candidate_list[0].image,\n                new_img=new_img,\n            )\n\n            sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n            shifted_pts = np.split(shifted_pts, sections, axis=0)\n            status = np.split(status, sections, axis=0)\n            errs = np.split(errs, sections, axis=0)\n\n            # Create shifted instances.\n            for idx, (ref_candidate, pts, found) in enumerate(\n                zip(ref_candidate_list, shifted_pts, status)\n            ):\n                # Exclude points that weren't found by optical flow.\n                found = found.squeeze().astype(bool)\n                pts[~found] = np.nan\n\n                # Create a shifted instance.\n                shifted_instances_prv_frames[ref_candidate.track_id].append(\n                    TrackedInstanceFeature(\n                        feature=feature_method(pts),\n                        src_predicted_instance=ref_candidate.src_instance,\n                        frame_idx=fidx,\n                        tracking_score=ref_candidate.tracking_score,\n                        shifted_keypoints=pts,\n                    )\n                )\n\n    else:\n        # for fixed window\n        candidates_list = (\n            candidates_list\n            if candidates_list is not None\n            else self.candidate.tracker_queue\n        )\n        for ref_candidate in candidates_list:\n            ref_pts = [x.numpy() for x in ref_candidate.src_instances]\n            shifted_pts, status, errs = self._compute_optical_flow(\n                ref_pts=ref_pts, ref_img=ref_candidate.image, new_img=new_img\n            )\n\n            sections = np.cumsum([len(x) for x in ref_pts])[:-1]\n            shifted_pts = np.split(shifted_pts, sections, axis=0)\n            status = np.split(status, sections, axis=0)\n            errs = np.split(errs, sections, axis=0)\n\n            # Create shifted instances.\n            for idx, (pts, found) in enumerate(zip(shifted_pts, status)):\n                # Exclude points that weren't found by optical flow.\n                found = found.squeeze().astype(bool)\n                pts[~found] = np.nan\n\n                # Create a shifted instance.\n                shifted_instances_prv_frames[ref_candidate.track_ids[idx]].append(\n                    TrackedInstanceFeature(\n                        feature=feature_method(pts),\n                        src_predicted_instance=ref_candidate.src_instances[idx],\n                        frame_idx=ref_candidate.frame_idx,\n                        tracking_score=ref_candidate.tracking_scores[idx],\n                        shifted_keypoints=pts,\n                    )\n                )\n\n    return shifted_instances_prv_frames\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.FlowShiftTracker.update_candidates","title":"<code>update_candidates(candidates_list, image)</code>","text":"<p>Return dictionary with the features of tracked instances.</p> <p>In this method, the tracked instances in the tracker queue are shifted on to the current frame using optical flow. The features are then computed from the shifted instances.</p> <p>Parameters:</p> Name Type Description Default <code>candidates_list</code> <code>Union[Deque, DefaultDict[int, Deque]]</code> <p>Tracker queue from the candidate class.</p> required <code>image</code> <code>ndarray</code> <p>Image of the current untracked frame. (used for flow shift tracker)</p> required <p>Returns:</p> Type Description <code>Dict[int, TrackedInstanceFeature]</code> <p>Dictionary with keys as track IDs and values as the list of <code>TrackedInstanceFeature</code>.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def update_candidates(\n    self,\n    candidates_list: Union[Deque, DefaultDict[int, Deque]],\n    image: np.ndarray,\n) -&gt; Dict[int, TrackedInstanceFeature]:\n    \"\"\"Return dictionary with the features of tracked instances.\n\n    In this method, the tracked instances in the tracker queue are shifted on to the\n    current frame using optical flow. The features are then computed from the shifted\n    instances.\n\n    Args:\n        candidates_list: Tracker queue from the candidate class.\n        image: Image of the current untracked frame. (used for flow shift tracker)\n\n    Returns:\n        Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n    \"\"\"\n    # get feature method for the shifted instances\n    if self.features not in self._feature_methods:\n        message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n        logger.error(message)\n        raise ValueError(message)\n    feature_method = self._feature_methods[self.features]\n\n    # get shifted instances from optical flow\n    shifted_instances_prv_frames = self.get_shifted_instances_from_prv_frames(\n        candidates_list=candidates_list,\n        new_img=image,\n        feature_method=feature_method,\n    )\n\n    return shifted_instances_prv_frames\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.RateColumn","title":"<code>RateColumn</code>","text":"<p>               Bases: <code>ProgressColumn</code></p> <p>Renders the progress rate.</p> <p>Methods:</p> Name Description <code>render</code> <p>Show progress rate.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>class RateColumn(rich.progress.ProgressColumn):\n    \"\"\"Renders the progress rate.\"\"\"\n\n    def render(self, task: \"Task\") -&gt; rich.progress.Text:\n        \"\"\"Show progress rate.\"\"\"\n        speed = task.speed\n        if speed is None:\n            return rich.progress.Text(\"?\", style=\"progress.data.speed\")\n        return rich.progress.Text(f\"{speed:.1f} frames/s\", style=\"progress.data.speed\")\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.RateColumn.render","title":"<code>render(task)</code>","text":"<p>Show progress rate.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def render(self, task: \"Task\") -&gt; rich.progress.Text:\n    \"\"\"Show progress rate.\"\"\"\n    speed = task.speed\n    if speed is None:\n        return rich.progress.Text(\"?\", style=\"progress.data.speed\")\n    return rich.progress.Text(f\"{speed:.1f} frames/s\", style=\"progress.data.speed\")\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker","title":"<code>Tracker</code>","text":"<p>Simple Pose Tracker.</p> <p>This is the base class for all Trackers. This module handles tracking instances across frames by creating new track IDs (or) assigning track IDs to each predicted instance when the <code>.track()</code> is called. This class is initialized in the <code>Predictor</code> classes.</p> <p>Attributes:</p> Name Type Description <code>candidate</code> <code>Union[FixedWindowCandidates, LocalQueueCandidates]</code> <p>Instance of either <code>FixedWindowCandidates</code> or <code>LocalQueueCandidates</code>.</p> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using optical flow shifts. Default: <code>False</code>.</p> <code>is_local_queue</code> <code>bool</code> <p><code>True</code> if <code>LocalQueueCandidates</code> is used else <code>False</code>.</p> <p>Methods:</p> Name Description <code>assign_tracks</code> <p>Assign track IDs using Hungarian method.</p> <code>from_config</code> <p>Create <code>Tracker</code> from config.</p> <code>generate_candidates</code> <p>Get the tracked instances from tracker queue.</p> <code>get_features</code> <p>Get features for the current untracked instances.</p> <code>get_scores</code> <p>Compute association score between untracked and tracked instances.</p> <code>scores_to_cost_matrix</code> <p>Converts <code>scores</code> matrix to cost matrix for track assignments.</p> <code>track</code> <p>Assign track IDs to the untracked list of <code>sio.PredictedInstance</code> objects.</p> <code>update_candidates</code> <p>Return dictionary with the features of tracked instances.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>@attrs.define\nclass Tracker:\n    \"\"\"Simple Pose Tracker.\n\n    This is the base class for all Trackers. This module handles tracking instances\n    across frames by creating new track IDs (or) assigning track IDs to each predicted\n    instance when the `.track()` is called. This class is initialized in the `Predictor`\n    classes.\n\n    Attributes:\n        candidate: Instance of either `FixedWindowCandidates` or `LocalQueueCandidates`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n            optical flow shifts. Default: `False`.\n        is_local_queue: `True` if `LocalQueueCandidates` is used else `False`.\n\n    \"\"\"\n\n    candidate: Union[FixedWindowCandidates, LocalQueueCandidates] = (\n        FixedWindowCandidates()\n    )\n    min_match_points: int = 0\n    features: str = \"keypoints\"\n    scoring_method: str = \"oks\"\n    scoring_reduction: str = \"mean\"\n    track_matching_method: str = \"hungarian\"\n    robust_best_instance: float = 1.0\n    use_flow: bool = False\n    is_local_queue: bool = False\n    tracking_target_instance_count: Optional[int] = None\n    tracking_pre_cull_to_target: int = 0\n    tracking_pre_cull_iou_threshold: float = 0\n    _scoring_functions: Dict[str, Any] = {\n        \"oks\": compute_oks,\n        \"iou\": compute_iou,\n        \"cosine_sim\": compute_cosine_sim,\n        \"euclidean_dist\": compute_euclidean_distance,\n    }\n    _quantile_method = functools.partial(np.quantile, q=robust_best_instance)\n    _scoring_reduction_methods: Dict[str, Any] = {\n        \"mean\": np.nanmean,\n        \"max\": np.nanmax,\n        \"robust_quantile\": _quantile_method,\n    }\n    _feature_methods: Dict[str, Any] = {\n        \"keypoints\": get_keypoints,\n        \"centroids\": get_centroid,\n        \"bboxes\": get_bbox,\n    }\n    _track_matching_methods: Dict[str, Any] = {\n        \"hungarian\": hungarian_matching,\n        \"greedy\": greedy_matching,\n    }\n    _track_objects: Dict[int, sio.Track] = {}\n\n    @classmethod\n    def from_config(\n        cls,\n        window_size: int = 5,\n        min_new_track_points: int = 0,\n        candidates_method: str = \"fixed_window\",\n        min_match_points: int = 0,\n        features: str = \"keypoints\",\n        scoring_method: str = \"oks\",\n        scoring_reduction: str = \"mean\",\n        robust_best_instance: float = 1.0,\n        track_matching_method: str = \"hungarian\",\n        max_tracks: Optional[int] = None,\n        use_flow: bool = False,\n        of_img_scale: float = 1.0,\n        of_window_size: int = 21,\n        of_max_levels: int = 3,\n        tracking_target_instance_count: Optional[int] = None,\n        tracking_pre_cull_to_target: int = 0,\n        tracking_pre_cull_iou_threshold: float = 0,\n    ):\n        \"\"\"Create `Tracker` from config.\n\n        Args:\n            window_size: Number of frames to look for in the candidate instances to match\n                with the current detections. Default: 5.\n            min_new_track_points: We won't spawn a new track for an instance with\n                fewer than this many non-nan points. Default: 0.\n            candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n                method, candidates from the last `window_size` frames. In local queues,\n                last `window_size` instances for each track ID is considered for matching\n                against the current detection. Default: `fixed_window`.\n            min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n            features: Feature representation for the candidates to update current detections.\n                One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n            scoring_method: Method to compute association score between features from the\n                current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n                `euclidean_dist`]. Default: `oks`.\n            scoring_reduction: Method to aggregate and reduce multiple scores if there are\n                several detections associated with the same track. One of [`mean`, `max`,\n                `robust_quantile`]. Default: `mean`.\n            robust_best_instance: If the value is between 0 and 1\n                (excluded), use a robust quantile similarity score for the\n                track. If the value is 1, use the max similarity (non-robust).\n                For selecting a robust score, 0.95 is a good value.\n            track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n                Default: `hungarian`.\n            max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n                (only for local queues candidate) Default: None.\n            use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n            optical flow shifts. Default: `False`.\n            of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n                this to increase performance at the cost of finer accuracy. Sometimes\n                decreasing the image scale can improve performance with fast movements.\n                Default: 1.0. (only if `use_flow` is True)\n            of_window_size: Optical flow window size to consider at each pyramid scale\n                level. Default: 21. (only if `use_flow` is True)\n            of_max_levels: Number of pyramid scale levels to consider. This is different\n                from the scale parameter, which determines the initial image scaling.\n                Default: 3. (only if `use_flow` is True)\n            tracking_target_instance_count: Target number of instances to track per frame. (default: None)\n            tracking_pre_cull_to_target: If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame *before* tracking. (default: 0)\n            tracking_pre_cull_iou_threshold: If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count *before* tracking. (default: 0)\n\n        \"\"\"\n        if candidates_method == \"fixed_window\":\n            candidate = FixedWindowCandidates(\n                window_size=window_size,\n                min_new_track_points=min_new_track_points,\n            )\n            is_local_queue = False\n\n        elif candidates_method == \"local_queues\":\n            candidate = LocalQueueCandidates(\n                window_size=window_size,\n                max_tracks=max_tracks,\n                min_new_track_points=min_new_track_points,\n            )\n            is_local_queue = True\n\n        else:\n            message = f\"{candidates_method} is not a valid method. Please choose one of [`fixed_window`, `local_queues`]\"\n            logger.error(message)\n            raise ValueError(message)\n\n        if use_flow:\n            return FlowShiftTracker(\n                candidate=candidate,\n                min_match_points=min_match_points,\n                features=features,\n                scoring_method=scoring_method,\n                scoring_reduction=scoring_reduction,\n                robust_best_instance=robust_best_instance,\n                track_matching_method=track_matching_method,\n                img_scale=of_img_scale,\n                of_window_size=of_window_size,\n                of_max_levels=of_max_levels,\n                is_local_queue=is_local_queue,\n                tracking_target_instance_count=tracking_target_instance_count,\n                tracking_pre_cull_to_target=tracking_pre_cull_to_target,\n                tracking_pre_cull_iou_threshold=tracking_pre_cull_iou_threshold,\n            )\n\n        tracker = cls(\n            candidate=candidate,\n            min_match_points=min_match_points,\n            features=features,\n            scoring_method=scoring_method,\n            scoring_reduction=scoring_reduction,\n            robust_best_instance=robust_best_instance,\n            track_matching_method=track_matching_method,\n            use_flow=use_flow,\n            is_local_queue=is_local_queue,\n            tracking_target_instance_count=tracking_target_instance_count,\n            tracking_pre_cull_to_target=tracking_pre_cull_to_target,\n            tracking_pre_cull_iou_threshold=tracking_pre_cull_iou_threshold,\n        )\n        return tracker\n\n    def track(\n        self,\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.ndarray = None,\n    ) -&gt; List[sio.PredictedInstance]:\n        \"\"\"Assign track IDs to the untracked list of `sio.PredictedInstance` objects.\n\n        Args:\n            untracked_instances: List of untracked `sio.PredictedInstance` objects.\n            frame_idx: Frame index of the predicted instances.\n            image: Source image if visual features are to be used (also when using flow).\n\n        Returns:\n            List of `sio.PredictedInstance` objects, each having an assigned track.\n        \"\"\"\n        if (\n            self.tracking_target_instance_count is not None\n            and self.tracking_target_instance_count\n            and self.tracking_pre_cull_to_target\n        ):\n            untracked_instances = cull_frame_instances(\n                untracked_instances,\n                self.tracking_target_instance_count,\n                self.tracking_pre_cull_iou_threshold,\n            )\n        # get features for the untracked instances.\n        current_instances = self.get_features(untracked_instances, frame_idx, image)\n\n        candidates_list = (\n            self.generate_candidates()\n        )  # either Deque/ DefaultDict for FixedWindow/ LocalQueue candidate.\n\n        if candidates_list:\n            # if track queue is not empty\n\n            # update candidates if needed and get the features from previous tracked instances.\n            candidates_feature_dict = self.update_candidates(candidates_list, image)\n\n            # scoring function\n            scores = self.get_scores(current_instances, candidates_feature_dict)\n            cost_matrix = self.scores_to_cost_matrix(scores)\n\n            # track assignment\n            current_tracked_instances = self.assign_tracks(\n                current_instances, cost_matrix\n            )\n\n        else:\n            # Initialize the tracker queue if empty.\n            current_tracked_instances = self.candidate.add_new_tracks(current_instances)\n\n        # convert the `current_instances` back to `List[sio.PredictedInstance]` objects.\n        if self.is_local_queue:\n            new_pred_instances = []\n            for instance in current_tracked_instances:\n                if instance.track_id is not None:\n                    if instance.track_id not in self._track_objects:\n                        self._track_objects[instance.track_id] = sio.Track(\n                            f\"track_{instance.track_id}\"\n                        )\n                    instance.src_instance.track = self._track_objects[instance.track_id]\n                    instance.src_instance.tracking_score = instance.tracking_score\n                new_pred_instances.append(instance.src_instance)\n\n        else:\n            new_pred_instances = []\n            for idx, inst in enumerate(current_tracked_instances.src_instances):\n                track_id = current_tracked_instances.track_ids[idx]\n                if track_id is not None:\n                    if track_id not in self._track_objects:\n                        self._track_objects[track_id] = sio.Track(f\"track_{track_id}\")\n                    inst.track = self._track_objects[track_id]\n                    inst.tracking_score = current_tracked_instances.tracking_scores[idx]\n                    new_pred_instances.append(inst)\n\n        return new_pred_instances\n\n    def get_features(\n        self,\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.ndarray = None,\n    ) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n        \"\"\"Get features for the current untracked instances.\n\n        The feature can either be an embedding of cropped image around each instance (visual feature),\n        the bounding box coordinates, or centroids, or the poses as a feature.\n\n        Args:\n            untracked_instances: List of untracked `sio.PredictedInstance` objects.\n            frame_idx: Frame index of the current untracked instances.\n            image: Image of the current frame if visual features are to be used.\n\n        Returns:\n            `TrackInstances` object or `List[TrackInstanceLocalQueue]` with the features\n            assigned for the untracked instances and track_id set as `None`.\n        \"\"\"\n        if self.features not in self._feature_methods:\n            message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n            logger.error(message)\n            raise ValueError(message)\n\n        feature_method = self._feature_methods[self.features]\n        feature_list = []\n        for pred_instance in untracked_instances:\n            feature_list.append(feature_method(pred_instance))\n\n        current_instances = self.candidate.get_track_instances(\n            feature_list, untracked_instances, frame_idx=frame_idx, image=image\n        )\n\n        return current_instances\n\n    def generate_candidates(self):\n        \"\"\"Get the tracked instances from tracker queue.\"\"\"\n        return self.candidate.tracker_queue\n\n    def update_candidates(\n        self, candidates_list: Union[Deque, DefaultDict[int, Deque]], image: np.ndarray\n    ) -&gt; Dict[int, TrackedInstanceFeature]:\n        \"\"\"Return dictionary with the features of tracked instances.\n\n        Args:\n            candidates_list: List of tracked instances from tracker queue to consider.\n            image: Image of the current untracked frame. (used for flow shift tracker)\n\n        Returns:\n            Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n        \"\"\"\n        candidates_feature_dict = defaultdict(list)\n        for track_id in self.candidate.current_tracks:\n            candidates_feature_dict[track_id].extend(\n                self.candidate.get_features_from_track_id(track_id, candidates_list)\n            )\n        return candidates_feature_dict\n\n    def get_scores(\n        self,\n        current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n        candidates_feature_dict: Dict[int, TrackedInstanceFeature],\n    ):\n        \"\"\"Compute association score between untracked and tracked instances.\n\n        For visual feature vectors, this can be `cosine_sim`, for bounding boxes\n        it could be `iou`, for centroids it could be `euclidean_dist`, and for poses it\n        could be `oks`.\n\n        Args:\n            current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n                with features and unassigned tracks.\n            candidates_feature_dict: Dictionary with keys as track IDs and values as the\n                list of `TrackedInstanceFeature`.\n\n        Returns:\n            scores: Score matrix of shape (num_new_instances, num_existing_tracks)\n        \"\"\"\n        if self.scoring_method not in self._scoring_functions:\n            message = \"Invalid `scoring_method` argument. Please provide one of `oks`, `cosine_sim`, `iou`, and `euclidean_dist`.\"\n            logger.error(message)\n            raise ValueError(message)\n\n        if self.scoring_reduction not in self._scoring_reduction_methods:\n            message = \"Invalid `scoring_reduction` argument. Please provide one of `mean`, `max`, and `robust_quantile`.\"\n            logger.error(message)\n            raise ValueError(message)\n\n        scoring_method = self._scoring_functions[self.scoring_method]\n        scoring_reduction = self._scoring_reduction_methods[self.scoring_reduction]\n\n        # Get list of features for the `current_instances`.\n        if self.is_local_queue:\n            current_instances_features = [x.feature for x in current_instances]\n        else:\n            current_instances_features = [x for x in current_instances.features]\n\n        scores = np.zeros(\n            (len(current_instances_features), len(self.candidate.current_tracks))\n        )\n\n        for f_idx, f in enumerate(current_instances_features):\n            for t_idx, track_id in enumerate(self.candidate.current_tracks):\n                scores_trackid = [\n                    scoring_method(f, x.feature)\n                    for x in candidates_feature_dict[track_id]\n                    if (~np.isnan(x.src_predicted_instance.numpy()).any(axis=1)).sum()\n                    &gt; self.min_match_points  # only if the candidates have min non-nan points\n                ]\n                score_trackid = scoring_reduction(scores_trackid)  # scoring reduction\n                scores[f_idx][t_idx] = score_trackid\n\n        return scores\n\n    def scores_to_cost_matrix(self, scores: np.ndarray):\n        \"\"\"Converts `scores` matrix to cost matrix for track assignments.\"\"\"\n        cost_matrix = -scores\n        cost_matrix[np.isnan(cost_matrix)] = np.inf\n        return cost_matrix\n\n    def assign_tracks(\n        self,\n        current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n        cost_matrix: np.ndarray,\n    ) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n        \"\"\"Assign track IDs using Hungarian method.\n\n        Args:\n            current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n                with features and unassigned tracks.\n            cost_matrix: Cost matrix of shape (num_new_instances, num_existing_tracks).\n\n        Returns:\n            `TrackInstances` object or `List[TrackInstanceLocalQueue]`objects with\n                track IDs assigned.\n        \"\"\"\n        if self.track_matching_method not in self._track_matching_methods:\n            message = \"Invalid `track_matching_method` argument. Please provide one of `hungarian`, and `greedy`.\"\n            logger.error(message)\n            raise ValueError(message)\n\n        matching_method = self._track_matching_methods[self.track_matching_method]\n\n        row_inds, col_inds = matching_method(cost_matrix)\n        tracking_scores = [\n            -cost_matrix[row, col] for row, col in zip(row_inds, col_inds)\n        ]\n\n        # update the candidates tracker queue with the newly tracked instances and assign\n        # track IDs to `current_instances`.\n        current_tracked_instances = self.candidate.update_tracks(\n            current_instances, row_inds, col_inds, tracking_scores\n        )\n\n        return current_tracked_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.assign_tracks","title":"<code>assign_tracks(current_instances, cost_matrix)</code>","text":"<p>Assign track IDs using Hungarian method.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code> with features and unassigned tracks.</p> required <code>cost_matrix</code> <code>ndarray</code> <p>Cost matrix of shape (num_new_instances, num_existing_tracks).</p> required <p>Returns:</p> Type Description <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code>objects with     track IDs assigned.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def assign_tracks(\n    self,\n    current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n    cost_matrix: np.ndarray,\n) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n    \"\"\"Assign track IDs using Hungarian method.\n\n    Args:\n        current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n            with features and unassigned tracks.\n        cost_matrix: Cost matrix of shape (num_new_instances, num_existing_tracks).\n\n    Returns:\n        `TrackInstances` object or `List[TrackInstanceLocalQueue]`objects with\n            track IDs assigned.\n    \"\"\"\n    if self.track_matching_method not in self._track_matching_methods:\n        message = \"Invalid `track_matching_method` argument. Please provide one of `hungarian`, and `greedy`.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    matching_method = self._track_matching_methods[self.track_matching_method]\n\n    row_inds, col_inds = matching_method(cost_matrix)\n    tracking_scores = [\n        -cost_matrix[row, col] for row, col in zip(row_inds, col_inds)\n    ]\n\n    # update the candidates tracker queue with the newly tracked instances and assign\n    # track IDs to `current_instances`.\n    current_tracked_instances = self.candidate.update_tracks(\n        current_instances, row_inds, col_inds, tracking_scores\n    )\n\n    return current_tracked_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.from_config","title":"<code>from_config(window_size=5, min_new_track_points=0, candidates_method='fixed_window', min_match_points=0, features='keypoints', scoring_method='oks', scoring_reduction='mean', robust_best_instance=1.0, track_matching_method='hungarian', max_tracks=None, use_flow=False, of_img_scale=1.0, of_window_size=21, of_max_levels=3, tracking_target_instance_count=None, tracking_pre_cull_to_target=0, tracking_pre_cull_iou_threshold=0)</code>  <code>classmethod</code>","text":"<p>Create <code>Tracker</code> from config.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Number of frames to look for in the candidate instances to match with the current detections. Default: 5.</p> <code>5</code> <code>min_new_track_points</code> <code>int</code> <p>We won't spawn a new track for an instance with fewer than this many non-nan points. Default: 0.</p> <code>0</code> <code>candidates_method</code> <code>str</code> <p>Either of <code>fixed_window</code> or <code>local_queues</code>. In fixed window method, candidates from the last <code>window_size</code> frames. In local queues, last <code>window_size</code> instances for each track ID is considered for matching against the current detection. Default: <code>fixed_window</code>.</p> <code>'fixed_window'</code> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>0</code> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>'keypoints'</code> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>'oks'</code> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>'mean'</code> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>1.0</code> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>'hungarian'</code> <code>max_tracks</code> <code>Optional[int]</code> <p>Meaximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate) Default: None.</p> <code>None</code> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using</p> <code>False</code> <code>optical flow shifts. Default</code> <p><code>False</code>.</p> required <code>of_img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0. (only if <code>use_flow</code> is True)</p> <code>1.0</code> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21. (only if <code>use_flow</code> is True)</p> <code>21</code> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling. Default: 3. (only if <code>use_flow</code> is True)</p> <code>3</code> <code>tracking_target_instance_count</code> <code>Optional[int]</code> <p>Target number of instances to track per frame. (default: None)</p> <code>None</code> <code>tracking_pre_cull_to_target</code> <code>int</code> <p>If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame before tracking. (default: 0)</p> <code>0</code> <code>tracking_pre_cull_iou_threshold</code> <code>float</code> <p>If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count before tracking. (default: 0)</p> <code>0</code> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    window_size: int = 5,\n    min_new_track_points: int = 0,\n    candidates_method: str = \"fixed_window\",\n    min_match_points: int = 0,\n    features: str = \"keypoints\",\n    scoring_method: str = \"oks\",\n    scoring_reduction: str = \"mean\",\n    robust_best_instance: float = 1.0,\n    track_matching_method: str = \"hungarian\",\n    max_tracks: Optional[int] = None,\n    use_flow: bool = False,\n    of_img_scale: float = 1.0,\n    of_window_size: int = 21,\n    of_max_levels: int = 3,\n    tracking_target_instance_count: Optional[int] = None,\n    tracking_pre_cull_to_target: int = 0,\n    tracking_pre_cull_iou_threshold: float = 0,\n):\n    \"\"\"Create `Tracker` from config.\n\n    Args:\n        window_size: Number of frames to look for in the candidate instances to match\n            with the current detections. Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many non-nan points. Default: 0.\n        candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n            method, candidates from the last `window_size` frames. In local queues,\n            last `window_size` instances for each track ID is considered for matching\n            against the current detection. Default: `fixed_window`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n            (only for local queues candidate) Default: None.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n        optical flow shifts. Default: `False`.\n        of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0. (only if `use_flow` is True)\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21. (only if `use_flow` is True)\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n            Default: 3. (only if `use_flow` is True)\n        tracking_target_instance_count: Target number of instances to track per frame. (default: None)\n        tracking_pre_cull_to_target: If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame *before* tracking. (default: 0)\n        tracking_pre_cull_iou_threshold: If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count *before* tracking. (default: 0)\n\n    \"\"\"\n    if candidates_method == \"fixed_window\":\n        candidate = FixedWindowCandidates(\n            window_size=window_size,\n            min_new_track_points=min_new_track_points,\n        )\n        is_local_queue = False\n\n    elif candidates_method == \"local_queues\":\n        candidate = LocalQueueCandidates(\n            window_size=window_size,\n            max_tracks=max_tracks,\n            min_new_track_points=min_new_track_points,\n        )\n        is_local_queue = True\n\n    else:\n        message = f\"{candidates_method} is not a valid method. Please choose one of [`fixed_window`, `local_queues`]\"\n        logger.error(message)\n        raise ValueError(message)\n\n    if use_flow:\n        return FlowShiftTracker(\n            candidate=candidate,\n            min_match_points=min_match_points,\n            features=features,\n            scoring_method=scoring_method,\n            scoring_reduction=scoring_reduction,\n            robust_best_instance=robust_best_instance,\n            track_matching_method=track_matching_method,\n            img_scale=of_img_scale,\n            of_window_size=of_window_size,\n            of_max_levels=of_max_levels,\n            is_local_queue=is_local_queue,\n            tracking_target_instance_count=tracking_target_instance_count,\n            tracking_pre_cull_to_target=tracking_pre_cull_to_target,\n            tracking_pre_cull_iou_threshold=tracking_pre_cull_iou_threshold,\n        )\n\n    tracker = cls(\n        candidate=candidate,\n        min_match_points=min_match_points,\n        features=features,\n        scoring_method=scoring_method,\n        scoring_reduction=scoring_reduction,\n        robust_best_instance=robust_best_instance,\n        track_matching_method=track_matching_method,\n        use_flow=use_flow,\n        is_local_queue=is_local_queue,\n        tracking_target_instance_count=tracking_target_instance_count,\n        tracking_pre_cull_to_target=tracking_pre_cull_to_target,\n        tracking_pre_cull_iou_threshold=tracking_pre_cull_iou_threshold,\n    )\n    return tracker\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.generate_candidates","title":"<code>generate_candidates()</code>","text":"<p>Get the tracked instances from tracker queue.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def generate_candidates(self):\n    \"\"\"Get the tracked instances from tracker queue.\"\"\"\n    return self.candidate.tracker_queue\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.get_features","title":"<code>get_features(untracked_instances, frame_idx, image=None)</code>","text":"<p>Get features for the current untracked instances.</p> <p>The feature can either be an embedding of cropped image around each instance (visual feature), the bounding box coordinates, or centroids, or the poses as a feature.</p> <p>Parameters:</p> Name Type Description Default <code>untracked_instances</code> <code>List[PredictedInstance]</code> <p>List of untracked <code>sio.PredictedInstance</code> objects.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index of the current untracked instances.</p> required <code>image</code> <code>ndarray</code> <p>Image of the current frame if visual features are to be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code> with the features assigned for the untracked instances and track_id set as <code>None</code>.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def get_features(\n    self,\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.ndarray = None,\n) -&gt; Union[TrackInstances, List[TrackInstanceLocalQueue]]:\n    \"\"\"Get features for the current untracked instances.\n\n    The feature can either be an embedding of cropped image around each instance (visual feature),\n    the bounding box coordinates, or centroids, or the poses as a feature.\n\n    Args:\n        untracked_instances: List of untracked `sio.PredictedInstance` objects.\n        frame_idx: Frame index of the current untracked instances.\n        image: Image of the current frame if visual features are to be used.\n\n    Returns:\n        `TrackInstances` object or `List[TrackInstanceLocalQueue]` with the features\n        assigned for the untracked instances and track_id set as `None`.\n    \"\"\"\n    if self.features not in self._feature_methods:\n        message = \"Invalid `features` argument. Please provide one of `keypoints`, `centroids`, `bboxes` and `image`\"\n        logger.error(message)\n        raise ValueError(message)\n\n    feature_method = self._feature_methods[self.features]\n    feature_list = []\n    for pred_instance in untracked_instances:\n        feature_list.append(feature_method(pred_instance))\n\n    current_instances = self.candidate.get_track_instances(\n        feature_list, untracked_instances, frame_idx=frame_idx, image=image\n    )\n\n    return current_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.get_scores","title":"<code>get_scores(current_instances, candidates_feature_dict)</code>","text":"<p>Compute association score between untracked and tracked instances.</p> <p>For visual feature vectors, this can be <code>cosine_sim</code>, for bounding boxes it could be <code>iou</code>, for centroids it could be <code>euclidean_dist</code>, and for poses it could be <code>oks</code>.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>Union[TrackInstances, List[TrackInstanceLocalQueue]]</code> <p><code>TrackInstances</code> object or <code>List[TrackInstanceLocalQueue]</code> with features and unassigned tracks.</p> required <code>candidates_feature_dict</code> <code>Dict[int, TrackedInstanceFeature]</code> <p>Dictionary with keys as track IDs and values as the list of <code>TrackedInstanceFeature</code>.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <p>Score matrix of shape (num_new_instances, num_existing_tracks)</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def get_scores(\n    self,\n    current_instances: Union[TrackInstances, List[TrackInstanceLocalQueue]],\n    candidates_feature_dict: Dict[int, TrackedInstanceFeature],\n):\n    \"\"\"Compute association score between untracked and tracked instances.\n\n    For visual feature vectors, this can be `cosine_sim`, for bounding boxes\n    it could be `iou`, for centroids it could be `euclidean_dist`, and for poses it\n    could be `oks`.\n\n    Args:\n        current_instances: `TrackInstances` object or `List[TrackInstanceLocalQueue]`\n            with features and unassigned tracks.\n        candidates_feature_dict: Dictionary with keys as track IDs and values as the\n            list of `TrackedInstanceFeature`.\n\n    Returns:\n        scores: Score matrix of shape (num_new_instances, num_existing_tracks)\n    \"\"\"\n    if self.scoring_method not in self._scoring_functions:\n        message = \"Invalid `scoring_method` argument. Please provide one of `oks`, `cosine_sim`, `iou`, and `euclidean_dist`.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    if self.scoring_reduction not in self._scoring_reduction_methods:\n        message = \"Invalid `scoring_reduction` argument. Please provide one of `mean`, `max`, and `robust_quantile`.\"\n        logger.error(message)\n        raise ValueError(message)\n\n    scoring_method = self._scoring_functions[self.scoring_method]\n    scoring_reduction = self._scoring_reduction_methods[self.scoring_reduction]\n\n    # Get list of features for the `current_instances`.\n    if self.is_local_queue:\n        current_instances_features = [x.feature for x in current_instances]\n    else:\n        current_instances_features = [x for x in current_instances.features]\n\n    scores = np.zeros(\n        (len(current_instances_features), len(self.candidate.current_tracks))\n    )\n\n    for f_idx, f in enumerate(current_instances_features):\n        for t_idx, track_id in enumerate(self.candidate.current_tracks):\n            scores_trackid = [\n                scoring_method(f, x.feature)\n                for x in candidates_feature_dict[track_id]\n                if (~np.isnan(x.src_predicted_instance.numpy()).any(axis=1)).sum()\n                &gt; self.min_match_points  # only if the candidates have min non-nan points\n            ]\n            score_trackid = scoring_reduction(scores_trackid)  # scoring reduction\n            scores[f_idx][t_idx] = score_trackid\n\n    return scores\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.scores_to_cost_matrix","title":"<code>scores_to_cost_matrix(scores)</code>","text":"<p>Converts <code>scores</code> matrix to cost matrix for track assignments.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def scores_to_cost_matrix(self, scores: np.ndarray):\n    \"\"\"Converts `scores` matrix to cost matrix for track assignments.\"\"\"\n    cost_matrix = -scores\n    cost_matrix[np.isnan(cost_matrix)] = np.inf\n    return cost_matrix\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.track","title":"<code>track(untracked_instances, frame_idx, image=None)</code>","text":"<p>Assign track IDs to the untracked list of <code>sio.PredictedInstance</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>untracked_instances</code> <code>List[PredictedInstance]</code> <p>List of untracked <code>sio.PredictedInstance</code> objects.</p> required <code>frame_idx</code> <code>int</code> <p>Frame index of the predicted instances.</p> required <code>image</code> <code>ndarray</code> <p>Source image if visual features are to be used (also when using flow).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[PredictedInstance]</code> <p>List of <code>sio.PredictedInstance</code> objects, each having an assigned track.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def track(\n    self,\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.ndarray = None,\n) -&gt; List[sio.PredictedInstance]:\n    \"\"\"Assign track IDs to the untracked list of `sio.PredictedInstance` objects.\n\n    Args:\n        untracked_instances: List of untracked `sio.PredictedInstance` objects.\n        frame_idx: Frame index of the predicted instances.\n        image: Source image if visual features are to be used (also when using flow).\n\n    Returns:\n        List of `sio.PredictedInstance` objects, each having an assigned track.\n    \"\"\"\n    if (\n        self.tracking_target_instance_count is not None\n        and self.tracking_target_instance_count\n        and self.tracking_pre_cull_to_target\n    ):\n        untracked_instances = cull_frame_instances(\n            untracked_instances,\n            self.tracking_target_instance_count,\n            self.tracking_pre_cull_iou_threshold,\n        )\n    # get features for the untracked instances.\n    current_instances = self.get_features(untracked_instances, frame_idx, image)\n\n    candidates_list = (\n        self.generate_candidates()\n    )  # either Deque/ DefaultDict for FixedWindow/ LocalQueue candidate.\n\n    if candidates_list:\n        # if track queue is not empty\n\n        # update candidates if needed and get the features from previous tracked instances.\n        candidates_feature_dict = self.update_candidates(candidates_list, image)\n\n        # scoring function\n        scores = self.get_scores(current_instances, candidates_feature_dict)\n        cost_matrix = self.scores_to_cost_matrix(scores)\n\n        # track assignment\n        current_tracked_instances = self.assign_tracks(\n            current_instances, cost_matrix\n        )\n\n    else:\n        # Initialize the tracker queue if empty.\n        current_tracked_instances = self.candidate.add_new_tracks(current_instances)\n\n    # convert the `current_instances` back to `List[sio.PredictedInstance]` objects.\n    if self.is_local_queue:\n        new_pred_instances = []\n        for instance in current_tracked_instances:\n            if instance.track_id is not None:\n                if instance.track_id not in self._track_objects:\n                    self._track_objects[instance.track_id] = sio.Track(\n                        f\"track_{instance.track_id}\"\n                    )\n                instance.src_instance.track = self._track_objects[instance.track_id]\n                instance.src_instance.tracking_score = instance.tracking_score\n            new_pred_instances.append(instance.src_instance)\n\n    else:\n        new_pred_instances = []\n        for idx, inst in enumerate(current_tracked_instances.src_instances):\n            track_id = current_tracked_instances.track_ids[idx]\n            if track_id is not None:\n                if track_id not in self._track_objects:\n                    self._track_objects[track_id] = sio.Track(f\"track_{track_id}\")\n                inst.track = self._track_objects[track_id]\n                inst.tracking_score = current_tracked_instances.tracking_scores[idx]\n                new_pred_instances.append(inst)\n\n    return new_pred_instances\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.Tracker.update_candidates","title":"<code>update_candidates(candidates_list, image)</code>","text":"<p>Return dictionary with the features of tracked instances.</p> <p>Parameters:</p> Name Type Description Default <code>candidates_list</code> <code>Union[Deque, DefaultDict[int, Deque]]</code> <p>List of tracked instances from tracker queue to consider.</p> required <code>image</code> <code>ndarray</code> <p>Image of the current untracked frame. (used for flow shift tracker)</p> required <p>Returns:</p> Type Description <code>Dict[int, TrackedInstanceFeature]</code> <p>Dictionary with keys as track IDs and values as the list of <code>TrackedInstanceFeature</code>.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def update_candidates(\n    self, candidates_list: Union[Deque, DefaultDict[int, Deque]], image: np.ndarray\n) -&gt; Dict[int, TrackedInstanceFeature]:\n    \"\"\"Return dictionary with the features of tracked instances.\n\n    Args:\n        candidates_list: List of tracked instances from tracker queue to consider.\n        image: Image of the current untracked frame. (used for flow shift tracker)\n\n    Returns:\n        Dictionary with keys as track IDs and values as the list of `TrackedInstanceFeature`.\n    \"\"\"\n    candidates_feature_dict = defaultdict(list)\n    for track_id in self.candidate.current_tracks:\n        candidates_feature_dict[track_id].extend(\n            self.candidate.get_features_from_track_id(track_id, candidates_list)\n        )\n    return candidates_feature_dict\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.connect_single_breaks","title":"<code>connect_single_breaks(lfs, max_instances)</code>","text":"<p>Merge single-frame breaks in tracks by connecting single lost track with single new track.</p> <p>Parameters:</p> Name Type Description Default <code>lfs</code> <code>List[LabeledFrame]</code> <p>List of <code>LabeledFrame</code> objects with predicted instances.</p> required <code>max_instances</code> <code>int</code> <p>The maximum number of instances we want per frame.</p> required <p>Returns:</p> Type Description <code>List[LabeledFrame]</code> <p>Updated list of labeled frames with modified track IDs.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def connect_single_breaks(\n    lfs: List[sio.LabeledFrame], max_instances: int\n) -&gt; List[sio.LabeledFrame]:\n    \"\"\"Merge single-frame breaks in tracks by connecting single lost track with single new track.\n\n    Args:\n        lfs: List of `LabeledFrame` objects with predicted instances.\n        max_instances: The maximum number of instances we want per frame.\n\n    Returns:\n        Updated list of labeled frames with modified track IDs.\n    \"\"\"\n    if not lfs:\n        return lfs\n\n    # Move instances in new tracks into tracks that disappeared on previous frame\n    fix_track_map = dict()\n    last_good_frame_tracks = {inst.track for inst in lfs[0].instances}\n    for lf in lfs:\n        frame_tracks = {inst.track for inst in lf.instances}\n\n        tracks_fixed_before = frame_tracks.intersection(set(fix_track_map.keys()))\n        if tracks_fixed_before:\n            for inst in lf.instances:\n                if (\n                    inst.track in fix_track_map\n                    and fix_track_map[inst.track] not in frame_tracks\n                ):\n                    inst.track = fix_track_map[inst.track]\n                    frame_tracks = {inst.track for inst in lf.instances}\n\n        extra_tracks = frame_tracks - last_good_frame_tracks\n        missing_tracks = last_good_frame_tracks - frame_tracks\n\n        if len(extra_tracks) == 1 and len(missing_tracks) == 1:\n            for inst in lf.instances:\n                if inst.track in extra_tracks:\n                    old_track = inst.track\n                    new_track = missing_tracks.pop()\n                    fix_track_map[old_track] = new_track\n                    inst.track = new_track\n\n                    break\n        else:\n            if len(frame_tracks) == max_instances:\n                last_good_frame_tracks = frame_tracks\n\n    return lfs\n</code></pre>"},{"location":"api/tracking/tracker/#sleap_nn.tracking.tracker.run_tracker","title":"<code>run_tracker(untracked_frames, window_size=5, min_new_track_points=0, candidates_method='fixed_window', min_match_points=0, features='keypoints', scoring_method='oks', scoring_reduction='mean', robust_best_instance=1.0, track_matching_method='hungarian', max_tracks=None, use_flow=False, of_img_scale=1.0, of_window_size=21, of_max_levels=3, post_connect_single_breaks=False, tracking_target_instance_count=None, tracking_pre_cull_to_target=0, tracking_pre_cull_iou_threshold=0, tracking_clean_instance_count=0, tracking_clean_iou_threshold=0)</code>","text":"<p>Run tracking on a given set of frames.</p> <p>Parameters:</p> Name Type Description Default <code>untracked_frames</code> <code>List[LabeledFrame]</code> <p>List of labeled frames with predicted instances to be tracked.</p> required <code>window_size</code> <code>int</code> <p>Number of frames to look for in the candidate instances to match     with the current detections. Default: 5.</p> <code>5</code> <code>min_new_track_points</code> <code>int</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>0</code> <code>candidates_method</code> <code>str</code> <p>Either of <code>fixed_window</code> or <code>local_queues</code>. In fixed window method, candidates from the last <code>window_size</code> frames. In local queues, last <code>window_size</code> instances for each track ID is considered for matching against the current detection. Default: <code>fixed_window</code>.</p> <code>'fixed_window'</code> <code>min_match_points</code> <code>int</code> <p>Minimum non-NaN points for match candidates. Default: 0.</p> <code>0</code> <code>features</code> <code>str</code> <p>Feature representation for the candidates to update current detections. One of [<code>keypoints</code>, <code>centroids</code>, <code>bboxes</code>, <code>image</code>]. Default: <code>keypoints</code>.</p> <code>'keypoints'</code> <code>scoring_method</code> <code>str</code> <p>Method to compute association score between features from the current frame and the previous tracks. One of [<code>oks</code>, <code>cosine_sim</code>, <code>iou</code>, <code>euclidean_dist</code>]. Default: <code>oks</code>.</p> <code>'oks'</code> <code>scoring_reduction</code> <code>str</code> <p>Method to aggregate and reduce multiple scores if there are several detections associated with the same track. One of [<code>mean</code>, <code>max</code>, <code>robust_quantile</code>]. Default: <code>mean</code>.</p> <code>'mean'</code> <code>robust_best_instance</code> <code>float</code> <p>If the value is between 0 and 1 (excluded), use a robust quantile similarity score for the track. If the value is 1, use the max similarity (non-robust). For selecting a robust score, 0.95 is a good value.</p> <code>1.0</code> <code>track_matching_method</code> <code>str</code> <p>Track matching algorithm. One of <code>hungarian</code>, <code>greedy. Default:</code>hungarian`.</p> <code>'hungarian'</code> <code>max_tracks</code> <code>Optional[int]</code> <p>Meaximum number of new tracks to be created to avoid redundant tracks. (only for local queues candidate) Default: None.</p> <code>None</code> <code>use_flow</code> <code>bool</code> <p>If True, <code>FlowShiftTracker</code> is used, where the poses are matched using</p> <code>False</code> <code>optical flow shifts. Default</code> <p><code>False</code>.</p> required <code>of_img_scale</code> <code>float</code> <p>Factor to scale the images by when computing optical flow. Decrease this to increase performance at the cost of finer accuracy. Sometimes decreasing the image scale can improve performance with fast movements. Default: 1.0. (only if <code>use_flow</code> is True)</p> <code>1.0</code> <code>of_window_size</code> <code>int</code> <p>Optical flow window size to consider at each pyramid scale level. Default: 21. (only if <code>use_flow</code> is True)</p> <code>21</code> <code>of_max_levels</code> <code>int</code> <p>Number of pyramid scale levels to consider. This is different from the scale parameter, which determines the initial image scaling.     Default: 3. (only if <code>use_flow</code> is True).</p> <code>3</code> <code>post_connect_single_breaks</code> <code>bool</code> <p>If True and <code>max_tracks</code> is not None with local queues candidate method, connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.</p> <code>False</code> <code>tracking_target_instance_count</code> <code>Optional[int]</code> <p>Target number of instances to track per frame. (default: None)</p> <code>None</code> <code>tracking_pre_cull_to_target</code> <code>int</code> <p>If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame before tracking. (default: 0)</p> <code>0</code> <code>tracking_pre_cull_iou_threshold</code> <code>float</code> <p>If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count before tracking. (default: 0)</p> <code>0</code> <code>tracking_clean_instance_count</code> <code>int</code> <p>Target number of instances to clean after tracking. (default: 0)</p> <code>0</code> <code>tracking_clean_iou_threshold</code> <code>float</code> <p>IOU to use when culling instances after tracking. (default: 0)</p> <code>0</code> <p>Returns:</p> Type Description <code>List[LabeledFrame]</code> <p><code>sio.Labels</code> object with tracked instances.</p> Source code in <code>sleap_nn/tracking/tracker.py</code> <pre><code>def run_tracker(\n    untracked_frames: List[sio.LabeledFrame],\n    window_size: int = 5,\n    min_new_track_points: int = 0,\n    candidates_method: str = \"fixed_window\",\n    min_match_points: int = 0,\n    features: str = \"keypoints\",\n    scoring_method: str = \"oks\",\n    scoring_reduction: str = \"mean\",\n    robust_best_instance: float = 1.0,\n    track_matching_method: str = \"hungarian\",\n    max_tracks: Optional[int] = None,\n    use_flow: bool = False,\n    of_img_scale: float = 1.0,\n    of_window_size: int = 21,\n    of_max_levels: int = 3,\n    post_connect_single_breaks: bool = False,\n    tracking_target_instance_count: Optional[int] = None,\n    tracking_pre_cull_to_target: int = 0,\n    tracking_pre_cull_iou_threshold: float = 0,\n    tracking_clean_instance_count: int = 0,\n    tracking_clean_iou_threshold: float = 0,\n) -&gt; List[sio.LabeledFrame]:\n    \"\"\"Run tracking on a given set of frames.\n\n    Args:\n        untracked_frames: List of labeled frames with predicted instances to be tracked.\n        window_size: Number of frames to look for in the candidate instances to match\n                with the current detections. Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        candidates_method: Either of `fixed_window` or `local_queues`. In fixed window\n            method, candidates from the last `window_size` frames. In local queues,\n            last `window_size` instances for each track ID is considered for matching\n            against the current detection. Default: `fixed_window`.\n        min_match_points: Minimum non-NaN points for match candidates. Default: 0.\n        features: Feature representation for the candidates to update current detections.\n            One of [`keypoints`, `centroids`, `bboxes`, `image`]. Default: `keypoints`.\n        scoring_method: Method to compute association score between features from the\n            current frame and the previous tracks. One of [`oks`, `cosine_sim`, `iou`,\n            `euclidean_dist`]. Default: `oks`.\n        scoring_reduction: Method to aggregate and reduce multiple scores if there are\n            several detections associated with the same track. One of [`mean`, `max`,\n            `robust_quantile`]. Default: `mean`.\n        robust_best_instance: If the value is between 0 and 1\n            (excluded), use a robust quantile similarity score for the\n            track. If the value is 1, use the max similarity (non-robust).\n            For selecting a robust score, 0.95 is a good value.\n        track_matching_method: Track matching algorithm. One of `hungarian`, `greedy.\n            Default: `hungarian`.\n        max_tracks: Meaximum number of new tracks to be created to avoid redundant tracks.\n            (only for local queues candidate) Default: None.\n        use_flow: If True, `FlowShiftTracker` is used, where the poses are matched using\n        optical flow shifts. Default: `False`.\n        of_img_scale: Factor to scale the images by when computing optical flow. Decrease\n            this to increase performance at the cost of finer accuracy. Sometimes\n            decreasing the image scale can improve performance with fast movements.\n            Default: 1.0. (only if `use_flow` is True)\n        of_window_size: Optical flow window size to consider at each pyramid scale\n            level. Default: 21. (only if `use_flow` is True)\n        of_max_levels: Number of pyramid scale levels to consider. This is different\n            from the scale parameter, which determines the initial image scaling.\n                Default: 3. (only if `use_flow` is True).\n        post_connect_single_breaks: If True and `max_tracks` is not None with local queues candidate method,\n            connects track breaks when exactly one track is lost and exactly one new track is spawned in the frame.\n        tracking_target_instance_count: Target number of instances to track per frame. (default: None)\n        tracking_pre_cull_to_target: If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame *before* tracking. (default: 0)\n        tracking_pre_cull_iou_threshold: If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count *before* tracking. (default: 0)\n        tracking_clean_instance_count: Target number of instances to clean *after* tracking. (default: 0)\n        tracking_clean_iou_threshold: IOU to use when culling instances *after* tracking. (default: 0)\n\n    Returns:\n        `sio.Labels` object with tracked instances.\n\n    \"\"\"\n    tracker = Tracker.from_config(\n        window_size=window_size,\n        min_new_track_points=min_new_track_points,\n        candidates_method=candidates_method,\n        min_match_points=min_match_points,\n        features=features,\n        scoring_method=scoring_method,\n        scoring_reduction=scoring_reduction,\n        robust_best_instance=robust_best_instance,\n        track_matching_method=track_matching_method,\n        max_tracks=max_tracks,\n        use_flow=use_flow,\n        of_img_scale=of_img_scale,\n        of_window_size=of_window_size,\n        of_max_levels=of_max_levels,\n        tracking_target_instance_count=tracking_target_instance_count,\n        tracking_pre_cull_to_target=tracking_pre_cull_to_target,\n        tracking_pre_cull_iou_threshold=tracking_pre_cull_iou_threshold,\n    )\n\n    try:\n        with Progress(\n            \"{task.description}\",\n            BarColumn(),\n            \"[progress.percentage]{task.percentage:&gt;3.0f}%\",\n            MofNCompleteColumn(),\n            \"ETA:\",\n            TimeRemainingColumn(),\n            \"Elapsed:\",\n            TimeElapsedColumn(),\n            RateColumn(),\n            auto_refresh=False,\n            refresh_per_second=4,\n            speed_estimate_period=5,\n        ) as progress:\n            task = progress.add_task(\"Tracking...\", total=len(untracked_frames))\n            last_report = time()\n\n            tracked_lfs = []\n            for lf in untracked_frames:\n                # prefer user instances over predicted instance\n                instances = []\n                if lf.has_user_instances:\n                    instances_to_track = lf.user_instances\n                    if lf.has_predicted_instances:\n                        instances = lf.predicted_instances\n                else:\n                    instances_to_track = lf.predicted_instances\n\n                instances.extend(\n                    tracker.track(\n                        untracked_instances=instances_to_track,\n                        frame_idx=lf.frame_idx,\n                        image=lf.image,\n                    )\n                )\n                tracked_lfs.append(\n                    sio.LabeledFrame(\n                        video=lf.video, frame_idx=lf.frame_idx, instances=instances\n                    )\n                )\n\n                progress.update(task, advance=1)\n\n                if time() - last_report &gt; 0.25:\n                    progress.refresh()\n                    last_report = time()\n\n    except KeyboardInterrupt:\n        logger.info(\"Tracking interrupted by user\")\n        raise KeyboardInterrupt\n\n    if tracking_clean_instance_count &gt; 0:\n        logger.info(\"Post-processing: Culling instances...\")\n        tracked_lfs = cull_instances(\n            tracked_lfs, tracking_clean_instance_count, tracking_clean_iou_threshold\n        )\n        if not post_connect_single_breaks:\n            logger.info(\"Post-processing: Connecting single breaks...\")\n            tracked_lfs = connect_single_breaks(\n                tracked_lfs, tracking_clean_instance_count\n            )\n\n    if post_connect_single_breaks:\n        if (\n            tracking_target_instance_count is None\n            or tracking_target_instance_count == 0\n        ):\n            if max_tracks is not None:\n                suggestion = f\"Add --tracking_target_instance_count {max_tracks} to your command (using your --max_tracks value).\"\n            else:\n                suggestion = \"Add --tracking_target_instance_count N where N is the expected number of instances per frame.\"\n            message = (\n                f\"--post_connect_single_breaks requires --tracking_target_instance_count to be set. \"\n                f\"{suggestion}\"\n            )\n            logger.error(message)\n            raise ValueError(message)\n        start_final_pass_time = time()\n        start_fp_timestamp = str(datetime.now())\n        logger.info(\n            f\"Started final-pass (connecting single breaks) at: {start_fp_timestamp}\"\n        )\n        tracked_lfs = connect_single_breaks(\n            tracked_lfs, max_instances=tracking_target_instance_count\n        )\n        finish_fp_timestamp = str(datetime.now())\n        total_fp_elapsed = time() - start_final_pass_time\n        logger.info(\n            f\"Finished final-pass (connecting single breaks) at: {finish_fp_timestamp}\"\n        )\n        logger.info(f\"Total runtime: {total_fp_elapsed} secs\")\n\n    return tracked_lfs\n</code></pre>"},{"location":"api/tracking/utils/","title":"utils","text":""},{"location":"api/tracking/utils/#sleap_nn.tracking.utils","title":"<code>sleap_nn.tracking.utils</code>","text":"<p>Helper functions for Tracker module.</p> <p>Functions:</p> Name Description <code>compute_cosine_sim</code> <p>Return cosine simalirity between a and b vectors.</p> <code>compute_euclidean_distance</code> <p>Return the negative euclidean distance between a and b points.</p> <code>compute_iou</code> <p>Return the intersection over union for given a and b bounding boxes [xmin, ymin, xmax, ymax].</p> <code>cull_frame_instances</code> <p>Removes instances (for single frame) over instance per frame threshold.</p> <code>cull_instances</code> <p>Removes instances from frames over instance per frame threshold.</p> <code>get_bbox</code> <p>Return the bounding box coordinates for the <code>PredictedInstance</code> object.</p> <code>get_centroid</code> <p>Return the centroid of the <code>PredictedInstance</code> object.</p> <code>get_keypoints</code> <p>Return keypoints as np.array from the <code>PredictedInstance</code> object.</p> <code>greedy_matching</code> <p>Match new instances to existing tracks using greedy bipartite matching.</p> <code>hungarian_matching</code> <p>Match new instances to existing tracks using Hungarian matching.</p> <code>nms_fast</code> <p>From: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/.</p> <code>nms_instances</code> <p>NMS for instances.</p>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.compute_cosine_sim","title":"<code>compute_cosine_sim(a, b)</code>","text":"<p>Return cosine simalirity between a and b vectors.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def compute_cosine_sim(a, b):\n    \"\"\"Return cosine simalirity between a and b vectors.\"\"\"\n    number = np.dot(a, b)\n    denom = np.linalg.norm(a) * np.linalg.norm(b)\n    cosine_sim = number / denom\n    return cosine_sim\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.compute_euclidean_distance","title":"<code>compute_euclidean_distance(a, b)</code>","text":"<p>Return the negative euclidean distance between a and b points.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def compute_euclidean_distance(a, b):\n    \"\"\"Return the negative euclidean distance between a and b points.\"\"\"\n    return -np.linalg.norm(a - b)\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.compute_iou","title":"<code>compute_iou(a, b)</code>","text":"<p>Return the intersection over union for given a and b bounding boxes [xmin, ymin, xmax, ymax].</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def compute_iou(a, b):\n    \"\"\"Return the intersection over union for given a and b bounding boxes [xmin, ymin, xmax, ymax].\"\"\"\n    (xmin1, ymin1, xmax1, ymax1), (xmin2, ymin2, xmax2, ymax2) = a, b\n\n    xmin_intersection = max(xmin1, xmin2)\n    ymin_intersection = max(ymin1, ymin2)\n    xmax_intersection = min(xmax1, xmax2)\n    ymax_intersection = min(ymax1, ymax2)\n\n    intersection_area = max(0, xmax_intersection - xmin_intersection + 1) * max(\n        0, ymax_intersection - ymin_intersection + 1\n    )\n    bbox1_area = (xmax1 - xmin1 + 1) * (ymax1 - ymin1 + 1)\n    bbox2_area = (xmax2 - xmin2 + 1) * (ymax2 - ymin2 + 1)\n    union_area = bbox1_area + bbox2_area - intersection_area\n\n    iou = intersection_area / union_area\n    return iou\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.cull_frame_instances","title":"<code>cull_frame_instances(instances_list, instance_count, iou_threshold=None)</code>","text":"<p>Removes instances (for single frame) over instance per frame threshold.</p> <p>Parameters:</p> Name Type Description Default <code>instances_list</code> <code>List[PredictedInstance]</code> <p>The list of instances for a single frame.</p> required <code>instance_count</code> <code>int</code> <p>The maximum number of instances we want per frame.</p> required <code>iou_threshold</code> <code>Optional[float]</code> <p>Intersection over Union (IOU) threshold to use when removing overlapping instances over target count; if None, then only use score to determine which instances to remove.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[PredictedInstance]</code> <p>Updated list of frames, also modifies frames in place.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def cull_frame_instances(\n    instances_list: List[sio.PredictedInstance],\n    instance_count: int,\n    iou_threshold: Optional[float] = None,\n) -&gt; List[sio.PredictedInstance]:\n    \"\"\"Removes instances (for single frame) over instance per frame threshold.\n\n    Args:\n        instances_list: The list of instances for a single frame.\n        instance_count: The maximum number of instances we want per frame.\n        iou_threshold: Intersection over Union (IOU) threshold to use when\n            removing overlapping instances over target count; if None, then\n            only use score to determine which instances to remove.\n\n    Returns:\n        Updated list of frames, also modifies frames in place.\n    \"\"\"\n    if not instances_list:\n        return\n\n    if len(instances_list) &gt; instance_count:\n        # List of instances which we'll pare down\n        keep_instances = instances_list\n\n        # Use NMS to remove overlapping instances over target count\n        if iou_threshold:\n            keep_instances, extra_instances = nms_instances(\n                keep_instances,\n                iou_threshold=iou_threshold,\n                target_count=instance_count,\n            )\n            updated_instances_list = []\n            # Remove the extra instances\n            for inst in extra_instances:\n                for instance in instances_list:\n                    if not instance.same_pose_as(inst):\n                        updated_instances_list.append(instance)\n            instances_list = updated_instances_list\n\n        # Use lower score to remove instances over target count\n        if len(keep_instances) &gt; instance_count:\n            # Sort by ascending score, get target number of instances\n            # from the end of list (i.e., with highest score)\n            extra_instances = sorted(keep_instances, key=operator.attrgetter(\"score\"))[\n                :-instance_count\n            ]\n\n            # Remove the extra instances\n            updated_instances_list = []\n            for inst in extra_instances:\n                for instance in instances_list:\n                    if instance.same_pose_as(inst):\n                        updated_instances_list.append(instance)\n            instances_list = updated_instances_list\n\n    return instances_list\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.cull_instances","title":"<code>cull_instances(frames, instance_count, iou_threshold=None)</code>","text":"<p>Removes instances from frames over instance per frame threshold.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>List[LabeledFrame]</code> <p>The list of <code>LabeledFrame</code> objects with predictions.</p> required <code>instance_count</code> <code>int</code> <p>The maximum number of instances we want per frame.</p> required <code>iou_threshold</code> <code>Optional[float]</code> <p>Intersection over Union (IOU) threshold to use when removing overlapping instances over target count; if None, then only use score to determine which instances to remove.</p> <code>None</code> <p>Returns:</p> Type Description <p>None; modifies frames in place.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def cull_instances(\n    frames: List[sio.LabeledFrame],\n    instance_count: int,\n    iou_threshold: Optional[float] = None,\n):\n    \"\"\"Removes instances from frames over instance per frame threshold.\n\n    Args:\n        frames: The list of `LabeledFrame` objects with predictions.\n        instance_count: The maximum number of instances we want per frame.\n        iou_threshold: Intersection over Union (IOU) threshold to use when\n            removing overlapping instances over target count; if None, then\n            only use score to determine which instances to remove.\n\n    Returns:\n        None; modifies frames in place.\n    \"\"\"\n    if not frames:\n        return\n\n    frames.sort(key=lambda lf: lf.frame_idx)\n\n    lf_inst_list = []\n    # Find all frames with more instances than the desired threshold\n    for lf in frames:\n        if len(lf.predicted_instances) &gt; instance_count:\n            # List of instances which we'll pare down\n            keep_instances = lf.predicted_instances\n\n            # Use NMS to remove overlapping instances over target count\n            if iou_threshold:\n                keep_instances, extra_instances = nms_instances(\n                    keep_instances,\n                    iou_threshold=iou_threshold,\n                    target_count=instance_count,\n                )\n                # Mark for removal\n                lf_inst_list.extend([(lf, inst) for inst in extra_instances])\n\n            # Use lower score to remove instances over target count\n            if len(keep_instances) &gt; instance_count:\n                # Sort by ascending score, get target number of instances\n                # from the end of list (i.e., with highest score)\n                extra_instances = sorted(\n                    keep_instances, key=operator.attrgetter(\"score\")\n                )[:-instance_count]\n\n                # Mark for removal\n                lf_inst_list.extend([(lf, inst) for inst in extra_instances])\n\n    # Remove instances over per frame threshold\n    for lf, inst in lf_inst_list:\n        filtered_instances = []\n        for instance in lf.instances:\n            if not instance.same_pose_as(inst):\n                filtered_instances.append(instance)\n        lf.instances = filtered_instances\n\n    return frames\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.get_bbox","title":"<code>get_bbox(pred_instance)</code>","text":"<p>Return the bounding box coordinates for the <code>PredictedInstance</code> object.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def get_bbox(pred_instance: Union[sio.PredictedInstance, np.ndarray]):\n    \"\"\"Return the bounding box coordinates for the `PredictedInstance` object.\"\"\"\n    points = (\n        pred_instance.numpy()\n        if not isinstance(pred_instance, np.ndarray)\n        else pred_instance\n    )\n    bbox = np.concatenate(\n        [\n            np.nanmin(points, axis=0),\n            np.nanmax(points, axis=0),\n        ]  # [xmin, ymin, xmax, ymax]\n    )\n    return bbox\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.get_centroid","title":"<code>get_centroid(pred_instance)</code>","text":"<p>Return the centroid of the <code>PredictedInstance</code> object.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def get_centroid(pred_instance: Union[sio.PredictedInstance, np.ndarray]):\n    \"\"\"Return the centroid of the `PredictedInstance` object.\"\"\"\n    pts = pred_instance\n    if not isinstance(pred_instance, np.ndarray):\n        pts = pred_instance.numpy()\n    centroid = np.nanmedian(pts, axis=0)\n    return centroid\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.get_keypoints","title":"<code>get_keypoints(pred_instance)</code>","text":"<p>Return keypoints as np.array from the <code>PredictedInstance</code> object.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def get_keypoints(pred_instance: Union[sio.PredictedInstance, np.ndarray]):\n    \"\"\"Return keypoints as np.array from the `PredictedInstance` object.\"\"\"\n    if isinstance(pred_instance, np.ndarray):\n        return pred_instance\n    return pred_instance.numpy()\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.greedy_matching","title":"<code>greedy_matching(cost_matrix)</code>","text":"<p>Match new instances to existing tracks using greedy bipartite matching.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def greedy_matching(cost_matrix: np.ndarray) -&gt; List[Tuple[int, int]]:\n    \"\"\"Match new instances to existing tracks using greedy bipartite matching.\"\"\"\n    # Sort edges by ascending cost.\n    rows, cols = np.unravel_index(np.argsort(cost_matrix, axis=None), cost_matrix.shape)\n    unassigned_edges = list(zip(rows, cols))\n\n    # Greedily assign edges.\n    row_inds, col_inds = [], []\n    while len(unassigned_edges) &gt; 0:\n        # Assign the lowest cost edge.\n        row_ind, col_ind = unassigned_edges.pop(0)\n        row_inds.append(row_ind)\n        col_inds.append(col_ind)\n\n        # Remove all other edges that contain either node (in reverse order).\n        for i in range(len(unassigned_edges) - 1, -1, -1):\n            if unassigned_edges[i][0] == row_ind or unassigned_edges[i][1] == col_ind:\n                del unassigned_edges[i]\n\n    return row_inds, col_inds\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.hungarian_matching","title":"<code>hungarian_matching(cost_matrix)</code>","text":"<p>Match new instances to existing tracks using Hungarian matching.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def hungarian_matching(cost_matrix: np.ndarray) -&gt; List[Tuple[int, int]]:\n    \"\"\"Match new instances to existing tracks using Hungarian matching.\"\"\"\n    row_ids, col_ids = linear_sum_assignment(cost_matrix)\n    return row_ids, col_ids\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.nms_fast","title":"<code>nms_fast(boxes, scores, iou_threshold, target_count=None)</code>","text":"<p>From: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def nms_fast(boxes, scores, iou_threshold, target_count=None) -&gt; List[int]:\n    \"\"\"From: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/.\"\"\"\n    # if there are no boxes, return an empty list\n    if len(boxes) == 0:\n        return []\n\n    # if we already have fewer boxes than the target count, return all boxes\n    if target_count and len(boxes) &lt; target_count:\n        return list(range(len(boxes)))\n\n    # if the bounding boxes coordinates are integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n\n    # initialize the list of picked indexes\n    picked_idxs = []\n\n    # init list of boxes removed by nms\n    nms_idxs = []\n    # grab the coordinates of the bounding boxes\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by their scores\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    idxs = np.argsort(scores)\n\n    # keep looping while some indexes still remain in the indexes list\n    while len(idxs) &gt; 0:\n        # we want to add the best box which is the last box in sorted list\n        picked_box_idx = idxs[-1]\n\n        # last = len(idxs) - 1\n        # i = idxs[last]\n        picked_idxs.append(picked_box_idx)\n\n        # find the largest (x, y) coordinates for the start of\n        # the bounding box and the smallest (x, y) coordinates\n        # for the end of the bounding box\n        xx1 = np.maximum(x1[picked_box_idx], x1[idxs[:-1]])\n        yy1 = np.maximum(y1[picked_box_idx], y1[idxs[:-1]])\n        xx2 = np.minimum(x2[picked_box_idx], x2[idxs[:-1]])\n        yy2 = np.minimum(y2[picked_box_idx], y2[idxs[:-1]])\n\n        # compute the width and height of the bounding box\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n\n        # compute the ratio of overlap\n        overlap = (w * h) / area[idxs[:-1]]\n\n        # find boxes with iou over threshold\n        nms_for_new_box = np.where(overlap &gt; iou_threshold)[0]\n        nms_idxs.extend(list(idxs[nms_for_new_box]))\n\n        # delete new box (last in list) plus nms boxes\n        idxs = np.delete(idxs, nms_for_new_box)[:-1]\n\n    # if we're below the target number of boxes, add some back\n    if target_count and nms_idxs and len(picked_idxs) &lt; target_count:\n        # sort by descending score\n        nms_idxs.sort(key=lambda idx: -scores[idx])\n\n        add_back_count = min(len(nms_idxs), len(picked_idxs) - target_count)\n        picked_idxs.extend(nms_idxs[:add_back_count])\n\n    # return the list of picked boxes\n    return picked_idxs\n</code></pre>"},{"location":"api/tracking/utils/#sleap_nn.tracking.utils.nms_instances","title":"<code>nms_instances(instances, iou_threshold, target_count=None)</code>","text":"<p>NMS for instances.</p> Source code in <code>sleap_nn/tracking/utils.py</code> <pre><code>def nms_instances(\n    instances, iou_threshold, target_count=None\n) -&gt; Tuple[List[sio.PredictedInstance], List[sio.PredictedInstance]]:\n    \"\"\"NMS for instances.\"\"\"\n    # get_bbox: # [xmin, ymin, xmax, ymax]\n    boxes = np.array([get_bbox(inst) for inst in instances])\n    scores = np.array([inst.score for inst in instances])\n    picks = nms_fast(boxes, scores, iou_threshold, target_count)\n\n    to_keep = [inst for i, inst in enumerate(instances) if i in picks]\n    to_remove = [inst for i, inst in enumerate(instances) if i not in picks]\n\n    return to_keep, to_remove\n</code></pre>"},{"location":"api/tracking/candidates/","title":"candidates","text":""},{"location":"api/tracking/candidates/#sleap_nn.tracking.candidates","title":"<code>sleap_nn.tracking.candidates</code>","text":"<p>Candidate generation modules for tracking.</p> <p>Modules:</p> Name Description <code>fixed_window</code> <p>Module to generate Fixed window candidates.</p> <code>local_queues</code> <p>Module to generate Tracking local queue candidates.</p>"},{"location":"api/tracking/candidates/fixed_window/","title":"fixed_window","text":""},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window","title":"<code>sleap_nn.tracking.candidates.fixed_window</code>","text":"<p>Module to generate Fixed window candidates.</p> <p>Classes:</p> Name Description <code>FixedWindowCandidates</code> <p>Fixed-window method for candidate generation.</p>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates","title":"<code>FixedWindowCandidates</code>","text":"<p>Fixed-window method for candidate generation.</p> <p>This module handles <code>tracker_queue</code> using the fixed window method, where track assignments are determined based on the last <code>window_size</code> frames.</p> <p>Attributes:</p> Name Type Description <code>window_size</code> <p>Number of previous frames to compare the current predicted instance with. Default: 5.</p> <code>min_new_track_points</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>tracker_queue</code> <p>Deque object that stores the past <code>window_size</code> tracked instances.</p> <code>all_tracks</code> <p>List of track IDs that are created.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize class variables.</p> <code>add_new_tracks</code> <p>Add new track IDs to the <code>TrackInstances</code> object and to the tracker queue.</p> <code>get_features_from_track_id</code> <p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> <code>get_new_track_id</code> <p>Return a new track_id.</p> <code>get_track_instances</code> <p>Return an instance of <code>TrackInstances</code> object for the <code>untracked_instances</code>.</p> <code>update_tracks</code> <p>Assign tracks to <code>TrackInstances</code> based on the output of track matching algorithm.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>class FixedWindowCandidates:\n    \"\"\"Fixed-window method for candidate generation.\n\n    This module handles `tracker_queue` using the fixed window method, where track assignments\n    are determined based on the last `window_size` frames.\n\n    Attributes:\n        window_size: Number of previous frames to compare the current predicted instance with.\n            Default: 5.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        tracker_queue: Deque object that stores the past `window_size` tracked instances.\n        all_tracks: List of track IDs that are created.\n    \"\"\"\n\n    def __init__(self, window_size: int = 5, min_new_track_points: int = 0):\n        \"\"\"Initialize class variables.\"\"\"\n        self.window_size = window_size\n        self.min_new_track_points = min_new_track_points\n        self.tracker_queue = deque(maxlen=self.window_size)\n        self.all_tracks = []\n\n    @property\n    def current_tracks(self):\n        \"\"\"Get track IDs of items currently in tracker queue.\"\"\"\n        if not len(self.tracker_queue):\n            return []\n        else:\n            curr_tracks = set()\n            for item in self.tracker_queue:\n                curr_tracks.update(item.track_ids)\n            return list(curr_tracks)\n\n    def get_track_instances(\n        self,\n        feature_list: List[Union[np.array]],\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.array,\n    ) -&gt; TrackInstances:\n        \"\"\"Return an instance of `TrackInstances` object for the `untracked_instances`.\"\"\"\n        track_instance = TrackInstances(\n            src_instances=untracked_instances,\n            track_ids=[None] * len(untracked_instances),\n            tracking_scores=[None] * len(untracked_instances),\n            features=feature_list,\n            frame_idx=frame_idx,\n            image=image,\n        )\n        return track_instance\n\n    def get_features_from_track_id(\n        self, track_id: int, candidates_list: Optional[Deque] = None\n    ) -&gt; List[TrackedInstanceFeature]:\n        \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n        Note: If `candidates_list` is `None`, then features of all the instances in the\n            tracker queue are returned by default. Else, only the features from the given\n            candidates_list are returned.\n        \"\"\"\n        output = []\n        tracked_candidates = (\n            candidates_list if candidates_list is not None else self.tracker_queue\n        )\n        for t in tracked_candidates:\n            if track_id in t.track_ids:\n                track_idx = t.track_ids.index(track_id)\n                tracked_instance_feature = TrackedInstanceFeature(\n                    feature=t.features[track_idx],\n                    src_predicted_instance=t.src_instances[track_idx],\n                    frame_idx=t.frame_idx,\n                    tracking_score=t.tracking_scores[track_idx],\n                    shifted_keypoints=None,\n                )\n                output.append(tracked_instance_feature)\n        return output\n\n    def get_new_track_id(self) -&gt; int:\n        \"\"\"Return a new track_id.\"\"\"\n        if not self.all_tracks:\n            new_track_id = 0\n        else:\n            new_track_id = max(self.all_tracks) + 1\n        return new_track_id\n\n    def add_new_tracks(\n        self, current_instances: TrackInstances, add_to_queue: bool = True\n    ) -&gt; TrackInstances:\n        \"\"\"Add new track IDs to the `TrackInstances` object and to the tracker queue.\"\"\"\n        is_new_track = False\n        for i, src_instance in enumerate(current_instances.src_instances):\n            # Spawning a new track only if num visbile points is more than the threshold\n            num_visible_keypoints = (~np.isnan(src_instance.numpy()).any(axis=1)).sum()\n            if (\n                num_visible_keypoints &gt; self.min_new_track_points\n                and current_instances.track_ids[i] is None\n            ):\n                is_new_track = True\n                new_tracks_id = self.get_new_track_id()\n                current_instances.track_ids[i] = new_tracks_id\n                current_instances.tracking_scores[i] = 1.0\n                self.all_tracks.append(new_tracks_id)\n\n        if add_to_queue and is_new_track:\n            self.tracker_queue.append(current_instances)\n\n        return current_instances\n\n    def update_tracks(\n        self,\n        current_instances: TrackInstances,\n        row_inds: np.array,\n        col_inds: np.array,\n        tracking_scores: List[float],\n    ) -&gt; TrackInstances:\n        \"\"\"Assign tracks to `TrackInstances` based on the output of track matching algorithm.\n\n        Args:\n            current_instances: `TrackInstances` instance with features and unassigned tracks.\n            row_inds: List of indices for the  `current_instances` object that has an assigned\n                track.\n            col_inds: List of track IDs that have been assigned a new instance.\n            tracking_scores: List of tracking scores from the cost matrix.\n\n        \"\"\"\n        add_to_queue = True\n        if row_inds is not None and col_inds is not None:\n            for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n                current_instances.track_ids[row] = self.current_tracks[col]\n                current_instances.tracking_scores[row] = tracking_scores[idx]\n\n            # update tracks to queue\n            self.tracker_queue.append(current_instances)\n            add_to_queue = False\n\n            # Create new tracks for instances with unassigned tracks from track matching\n            new_current_instances_inds = [\n                x for x in range(len(current_instances.features)) if x not in row_inds\n            ]\n            if new_current_instances_inds:\n                current_instances = self.add_new_tracks(\n                    current_instances, add_to_queue=add_to_queue\n                )\n        return current_instances\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.current_tracks","title":"<code>current_tracks</code>  <code>property</code>","text":"<p>Get track IDs of items currently in tracker queue.</p>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.__init__","title":"<code>__init__(window_size=5, min_new_track_points=0)</code>","text":"<p>Initialize class variables.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def __init__(self, window_size: int = 5, min_new_track_points: int = 0):\n    \"\"\"Initialize class variables.\"\"\"\n    self.window_size = window_size\n    self.min_new_track_points = min_new_track_points\n    self.tracker_queue = deque(maxlen=self.window_size)\n    self.all_tracks = []\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.add_new_tracks","title":"<code>add_new_tracks(current_instances, add_to_queue=True)</code>","text":"<p>Add new track IDs to the <code>TrackInstances</code> object and to the tracker queue.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def add_new_tracks(\n    self, current_instances: TrackInstances, add_to_queue: bool = True\n) -&gt; TrackInstances:\n    \"\"\"Add new track IDs to the `TrackInstances` object and to the tracker queue.\"\"\"\n    is_new_track = False\n    for i, src_instance in enumerate(current_instances.src_instances):\n        # Spawning a new track only if num visbile points is more than the threshold\n        num_visible_keypoints = (~np.isnan(src_instance.numpy()).any(axis=1)).sum()\n        if (\n            num_visible_keypoints &gt; self.min_new_track_points\n            and current_instances.track_ids[i] is None\n        ):\n            is_new_track = True\n            new_tracks_id = self.get_new_track_id()\n            current_instances.track_ids[i] = new_tracks_id\n            current_instances.tracking_scores[i] = 1.0\n            self.all_tracks.append(new_tracks_id)\n\n    if add_to_queue and is_new_track:\n        self.tracker_queue.append(current_instances)\n\n    return current_instances\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.get_features_from_track_id","title":"<code>get_features_from_track_id(track_id, candidates_list=None)</code>","text":"<p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> If <code>candidates_list</code> is <code>None</code>, then features of all the instances in the <p>tracker queue are returned by default. Else, only the features from the given candidates_list are returned.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def get_features_from_track_id(\n    self, track_id: int, candidates_list: Optional[Deque] = None\n) -&gt; List[TrackedInstanceFeature]:\n    \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n    Note: If `candidates_list` is `None`, then features of all the instances in the\n        tracker queue are returned by default. Else, only the features from the given\n        candidates_list are returned.\n    \"\"\"\n    output = []\n    tracked_candidates = (\n        candidates_list if candidates_list is not None else self.tracker_queue\n    )\n    for t in tracked_candidates:\n        if track_id in t.track_ids:\n            track_idx = t.track_ids.index(track_id)\n            tracked_instance_feature = TrackedInstanceFeature(\n                feature=t.features[track_idx],\n                src_predicted_instance=t.src_instances[track_idx],\n                frame_idx=t.frame_idx,\n                tracking_score=t.tracking_scores[track_idx],\n                shifted_keypoints=None,\n            )\n            output.append(tracked_instance_feature)\n    return output\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.get_new_track_id","title":"<code>get_new_track_id()</code>","text":"<p>Return a new track_id.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def get_new_track_id(self) -&gt; int:\n    \"\"\"Return a new track_id.\"\"\"\n    if not self.all_tracks:\n        new_track_id = 0\n    else:\n        new_track_id = max(self.all_tracks) + 1\n    return new_track_id\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.get_track_instances","title":"<code>get_track_instances(feature_list, untracked_instances, frame_idx, image)</code>","text":"<p>Return an instance of <code>TrackInstances</code> object for the <code>untracked_instances</code>.</p> Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def get_track_instances(\n    self,\n    feature_list: List[Union[np.array]],\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.array,\n) -&gt; TrackInstances:\n    \"\"\"Return an instance of `TrackInstances` object for the `untracked_instances`.\"\"\"\n    track_instance = TrackInstances(\n        src_instances=untracked_instances,\n        track_ids=[None] * len(untracked_instances),\n        tracking_scores=[None] * len(untracked_instances),\n        features=feature_list,\n        frame_idx=frame_idx,\n        image=image,\n    )\n    return track_instance\n</code></pre>"},{"location":"api/tracking/candidates/fixed_window/#sleap_nn.tracking.candidates.fixed_window.FixedWindowCandidates.update_tracks","title":"<code>update_tracks(current_instances, row_inds, col_inds, tracking_scores)</code>","text":"<p>Assign tracks to <code>TrackInstances</code> based on the output of track matching algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>TrackInstances</code> <p><code>TrackInstances</code> instance with features and unassigned tracks.</p> required <code>row_inds</code> <code>array</code> <p>List of indices for the  <code>current_instances</code> object that has an assigned track.</p> required <code>col_inds</code> <code>array</code> <p>List of track IDs that have been assigned a new instance.</p> required <code>tracking_scores</code> <code>List[float]</code> <p>List of tracking scores from the cost matrix.</p> required Source code in <code>sleap_nn/tracking/candidates/fixed_window.py</code> <pre><code>def update_tracks(\n    self,\n    current_instances: TrackInstances,\n    row_inds: np.array,\n    col_inds: np.array,\n    tracking_scores: List[float],\n) -&gt; TrackInstances:\n    \"\"\"Assign tracks to `TrackInstances` based on the output of track matching algorithm.\n\n    Args:\n        current_instances: `TrackInstances` instance with features and unassigned tracks.\n        row_inds: List of indices for the  `current_instances` object that has an assigned\n            track.\n        col_inds: List of track IDs that have been assigned a new instance.\n        tracking_scores: List of tracking scores from the cost matrix.\n\n    \"\"\"\n    add_to_queue = True\n    if row_inds is not None and col_inds is not None:\n        for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n            current_instances.track_ids[row] = self.current_tracks[col]\n            current_instances.tracking_scores[row] = tracking_scores[idx]\n\n        # update tracks to queue\n        self.tracker_queue.append(current_instances)\n        add_to_queue = False\n\n        # Create new tracks for instances with unassigned tracks from track matching\n        new_current_instances_inds = [\n            x for x in range(len(current_instances.features)) if x not in row_inds\n        ]\n        if new_current_instances_inds:\n            current_instances = self.add_new_tracks(\n                current_instances, add_to_queue=add_to_queue\n            )\n    return current_instances\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/","title":"local_queues","text":""},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues","title":"<code>sleap_nn.tracking.candidates.local_queues</code>","text":"<p>Module to generate Tracking local queue candidates.</p> <p>Classes:</p> Name Description <code>LocalQueueCandidates</code> <p>Track local queues method for candidate generation.</p>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates","title":"<code>LocalQueueCandidates</code>","text":"<p>Track local queues method for candidate generation.</p> <p>This module handles <code>tracker_queue</code> using the local queues method, where track assignments are determined based on the last <code>window_size</code> instances for each track.</p> <p>Attributes:</p> Name Type Description <code>window_size</code> <p>Number of previous frames to compare the current predicted instance with. Default: 5.</p> <code>max_tracks</code> <p>Maximum number of new tracks that can be created. Default: None.</p> <code>min_new_track_points</code> <p>We won't spawn a new track for an instance with fewer than this many points. Default: 0.</p> <code>tracker_queue</code> <p>Dictionary that stores the past frames of all the tracks identified so far as <code>deque</code>.</p> <code>current_tracks</code> <p>List of track IDs that are being tracked.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize class variables.</p> <code>add_new_tracks</code> <p>Add new track IDs to the <code>TrackInstanceLocalQueue</code> objects and to the tracker queue.</p> <code>get_features_from_track_id</code> <p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> <code>get_instances_groupby_frame_idx</code> <p>Return dictionary with list of <code>TrackInstanceLocalQueue</code> objects grouped by frame index.</p> <code>get_new_track_id</code> <p>Return a new track_id.</p> <code>get_track_instances</code> <p>Return a list of <code>TrackInstanceLocalQueue</code> instances for the <code>untracked_instances</code>.</p> <code>update_tracks</code> <p>Assign tracks to <code>TrackInstanceLocalQueue</code> objects based on the output of track matching algorithm.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>class LocalQueueCandidates:\n    \"\"\"Track local queues method for candidate generation.\n\n    This module handles `tracker_queue` using the local queues method, where track assignments\n    are determined based on the last `window_size` instances for each track.\n\n    Attributes:\n        window_size: Number of previous frames to compare the current predicted instance with.\n            Default: 5.\n        max_tracks: Maximum number of new tracks that can be created. Default: None.\n        min_new_track_points: We won't spawn a new track for an instance with\n            fewer than this many points. Default: 0.\n        tracker_queue: Dictionary that stores the past frames of all the tracks identified\n            so far as `deque`.\n        current_tracks: List of track IDs that are being tracked.\n    \"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 5,\n        max_tracks: Optional[int] = None,\n        min_new_track_points: int = 0,\n    ):\n        \"\"\"Initialize class variables.\"\"\"\n        self.window_size = window_size\n        self.max_tracks = max_tracks\n        self.min_new_track_points = min_new_track_points\n        self.tracker_queue = defaultdict(Deque)\n        self.current_tracks = []\n\n    def get_track_instances(\n        self,\n        feature_list: List[Union[np.array]],\n        untracked_instances: List[sio.PredictedInstance],\n        frame_idx: int,\n        image: np.array,\n    ) -&gt; List[TrackInstanceLocalQueue]:\n        \"\"\"Return a list of `TrackInstanceLocalQueue` instances for the `untracked_instances`.\"\"\"\n        track_instances = []\n        for ind, (feat, instance) in enumerate(zip(feature_list, untracked_instances)):\n            track_instance = TrackInstanceLocalQueue(\n                src_instance=instance,\n                src_instance_idx=ind,\n                track_id=None,\n                feature=feat,\n                frame_idx=frame_idx,\n                image=image,\n            )\n            track_instances.append(track_instance)\n        return track_instances\n\n    def get_features_from_track_id(\n        self, track_id: int, candidates_list: Optional[DefaultDict[int, Deque]] = None\n    ) -&gt; List[TrackedInstanceFeature]:\n        \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n        Note: If `candidates_list` is `None`, then features of all the instances in the\n            tracker queue are returned by default. Else, only the features from the given\n            candidates_list are returned.\n        \"\"\"\n        tracked_instances = (\n            candidates_list if candidates_list is not None else self.tracker_queue\n        )\n        output = []\n        for t in tracked_instances[track_id]:\n            tracked_instance_feature = TrackedInstanceFeature(\n                feature=t.feature,\n                src_predicted_instance=t.src_instance,\n                frame_idx=t.frame_idx,\n                tracking_score=t.tracking_score,\n                shifted_keypoints=None,\n            )\n            output.append(tracked_instance_feature)\n        return output\n\n    def get_new_track_id(self) -&gt; int:\n        \"\"\"Return a new track_id.\"\"\"\n        if not self.current_tracks:\n            new_track_id = 0\n        else:\n            new_track_id = max(self.current_tracks) + 1\n            if self.max_tracks is not None and new_track_id &gt;= self.max_tracks:\n                return None\n        self.tracker_queue[new_track_id] = deque(maxlen=self.window_size)\n        return new_track_id\n\n    def add_new_tracks(\n        self, current_instances: List[TrackInstanceLocalQueue]\n    ) -&gt; List[TrackInstanceLocalQueue]:\n        \"\"\"Add new track IDs to the `TrackInstanceLocalQueue` objects and to the tracker queue.\"\"\"\n        track_instances = []\n        for t in current_instances:\n            # Spawning a new track only if num visbile points is more than the threshold\n            num_visible_keypoints = (\n                ~np.isnan(t.src_instance.numpy()).any(axis=1)\n            ).sum()\n            if num_visible_keypoints &gt; self.min_new_track_points:\n                new_track_id = self.get_new_track_id()\n                if new_track_id is not None:\n                    t.track_id = new_track_id\n                    t.tracking_score = 1.0\n                    self.current_tracks.append(new_track_id)\n                    self.tracker_queue[new_track_id].append(t)\n                else:\n                    continue\n                # if new_track_id = `None`, max tracks is reached and we skip this instance\n            track_instances.append(t)\n\n        return track_instances\n\n    def update_tracks(\n        self,\n        current_instances: List[TrackInstanceLocalQueue],\n        row_inds: np.array,\n        col_inds: np.array,\n        tracking_scores: List[float],\n    ) -&gt; List[TrackInstanceLocalQueue]:\n        \"\"\"Assign tracks to `TrackInstanceLocalQueue` objects based on the output of track matching algorithm.\n\n        Args:\n            current_instances: List of TrackInstanceLocalQueue objects with features and unassigned tracks.\n            row_inds: List of indices for the  `current_instances` object that has an assigned\n                track.\n            col_inds: List of track IDs that have been assigned a new instance.\n            tracking_scores: List of tracking scores from the cost matrix.\n\n        \"\"\"\n        res = []\n        if row_inds is not None and col_inds is not None:\n            for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n                current_instances[row].track_id = col\n                current_instances[row].tracking_score = tracking_scores[idx]\n                res.append(current_instances[row])\n\n            for track_instance in current_instances:\n                if track_instance.track_id is not None:\n                    self.tracker_queue[track_instance.track_id].append(track_instance)\n\n            # Create new tracks for instances with unassigned tracks from track matching\n            new_current_instances_inds = [\n                x for x in range(len(current_instances)) if x not in row_inds\n            ]\n            if new_current_instances_inds:\n                for ind in new_current_instances_inds:\n                    res.extend(self.add_new_tracks([current_instances[ind]]))\n\n        return [x for x in res if x.track_id is not None]\n\n    def get_instances_groupby_frame_idx(\n        self, candidates_list: Optional[DefaultDict[int, Deque]]\n    ) -&gt; Dict[int, List[TrackInstanceLocalQueue]]:\n        \"\"\"Return dictionary with list of `TrackInstanceLocalQueue` objects grouped by frame index.\"\"\"\n        instances_dict = defaultdict(list)\n        tracked_instances = (\n            candidates_list if candidates_list is not None else self.tracker_queue\n        )\n        for _, instances in tracked_instances.items():\n            for instance in instances:\n                instances_dict[instance.frame_idx].append(instance)\n        return instances_dict\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.__init__","title":"<code>__init__(window_size=5, max_tracks=None, min_new_track_points=0)</code>","text":"<p>Initialize class variables.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def __init__(\n    self,\n    window_size: int = 5,\n    max_tracks: Optional[int] = None,\n    min_new_track_points: int = 0,\n):\n    \"\"\"Initialize class variables.\"\"\"\n    self.window_size = window_size\n    self.max_tracks = max_tracks\n    self.min_new_track_points = min_new_track_points\n    self.tracker_queue = defaultdict(Deque)\n    self.current_tracks = []\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.add_new_tracks","title":"<code>add_new_tracks(current_instances)</code>","text":"<p>Add new track IDs to the <code>TrackInstanceLocalQueue</code> objects and to the tracker queue.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def add_new_tracks(\n    self, current_instances: List[TrackInstanceLocalQueue]\n) -&gt; List[TrackInstanceLocalQueue]:\n    \"\"\"Add new track IDs to the `TrackInstanceLocalQueue` objects and to the tracker queue.\"\"\"\n    track_instances = []\n    for t in current_instances:\n        # Spawning a new track only if num visbile points is more than the threshold\n        num_visible_keypoints = (\n            ~np.isnan(t.src_instance.numpy()).any(axis=1)\n        ).sum()\n        if num_visible_keypoints &gt; self.min_new_track_points:\n            new_track_id = self.get_new_track_id()\n            if new_track_id is not None:\n                t.track_id = new_track_id\n                t.tracking_score = 1.0\n                self.current_tracks.append(new_track_id)\n                self.tracker_queue[new_track_id].append(t)\n            else:\n                continue\n            # if new_track_id = `None`, max tracks is reached and we skip this instance\n        track_instances.append(t)\n\n    return track_instances\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_features_from_track_id","title":"<code>get_features_from_track_id(track_id, candidates_list=None)</code>","text":"<p>Return list of <code>TrackedInstanceFeature</code> objects for instances in tracker queue with the given <code>track_id</code>.</p> If <code>candidates_list</code> is <code>None</code>, then features of all the instances in the <p>tracker queue are returned by default. Else, only the features from the given candidates_list are returned.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_features_from_track_id(\n    self, track_id: int, candidates_list: Optional[DefaultDict[int, Deque]] = None\n) -&gt; List[TrackedInstanceFeature]:\n    \"\"\"Return list of `TrackedInstanceFeature` objects for instances in tracker queue with the given `track_id`.\n\n    Note: If `candidates_list` is `None`, then features of all the instances in the\n        tracker queue are returned by default. Else, only the features from the given\n        candidates_list are returned.\n    \"\"\"\n    tracked_instances = (\n        candidates_list if candidates_list is not None else self.tracker_queue\n    )\n    output = []\n    for t in tracked_instances[track_id]:\n        tracked_instance_feature = TrackedInstanceFeature(\n            feature=t.feature,\n            src_predicted_instance=t.src_instance,\n            frame_idx=t.frame_idx,\n            tracking_score=t.tracking_score,\n            shifted_keypoints=None,\n        )\n        output.append(tracked_instance_feature)\n    return output\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_instances_groupby_frame_idx","title":"<code>get_instances_groupby_frame_idx(candidates_list)</code>","text":"<p>Return dictionary with list of <code>TrackInstanceLocalQueue</code> objects grouped by frame index.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_instances_groupby_frame_idx(\n    self, candidates_list: Optional[DefaultDict[int, Deque]]\n) -&gt; Dict[int, List[TrackInstanceLocalQueue]]:\n    \"\"\"Return dictionary with list of `TrackInstanceLocalQueue` objects grouped by frame index.\"\"\"\n    instances_dict = defaultdict(list)\n    tracked_instances = (\n        candidates_list if candidates_list is not None else self.tracker_queue\n    )\n    for _, instances in tracked_instances.items():\n        for instance in instances:\n            instances_dict[instance.frame_idx].append(instance)\n    return instances_dict\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_new_track_id","title":"<code>get_new_track_id()</code>","text":"<p>Return a new track_id.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_new_track_id(self) -&gt; int:\n    \"\"\"Return a new track_id.\"\"\"\n    if not self.current_tracks:\n        new_track_id = 0\n    else:\n        new_track_id = max(self.current_tracks) + 1\n        if self.max_tracks is not None and new_track_id &gt;= self.max_tracks:\n            return None\n    self.tracker_queue[new_track_id] = deque(maxlen=self.window_size)\n    return new_track_id\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.get_track_instances","title":"<code>get_track_instances(feature_list, untracked_instances, frame_idx, image)</code>","text":"<p>Return a list of <code>TrackInstanceLocalQueue</code> instances for the <code>untracked_instances</code>.</p> Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def get_track_instances(\n    self,\n    feature_list: List[Union[np.array]],\n    untracked_instances: List[sio.PredictedInstance],\n    frame_idx: int,\n    image: np.array,\n) -&gt; List[TrackInstanceLocalQueue]:\n    \"\"\"Return a list of `TrackInstanceLocalQueue` instances for the `untracked_instances`.\"\"\"\n    track_instances = []\n    for ind, (feat, instance) in enumerate(zip(feature_list, untracked_instances)):\n        track_instance = TrackInstanceLocalQueue(\n            src_instance=instance,\n            src_instance_idx=ind,\n            track_id=None,\n            feature=feat,\n            frame_idx=frame_idx,\n            image=image,\n        )\n        track_instances.append(track_instance)\n    return track_instances\n</code></pre>"},{"location":"api/tracking/candidates/local_queues/#sleap_nn.tracking.candidates.local_queues.LocalQueueCandidates.update_tracks","title":"<code>update_tracks(current_instances, row_inds, col_inds, tracking_scores)</code>","text":"<p>Assign tracks to <code>TrackInstanceLocalQueue</code> objects based on the output of track matching algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>current_instances</code> <code>List[TrackInstanceLocalQueue]</code> <p>List of TrackInstanceLocalQueue objects with features and unassigned tracks.</p> required <code>row_inds</code> <code>array</code> <p>List of indices for the  <code>current_instances</code> object that has an assigned track.</p> required <code>col_inds</code> <code>array</code> <p>List of track IDs that have been assigned a new instance.</p> required <code>tracking_scores</code> <code>List[float]</code> <p>List of tracking scores from the cost matrix.</p> required Source code in <code>sleap_nn/tracking/candidates/local_queues.py</code> <pre><code>def update_tracks(\n    self,\n    current_instances: List[TrackInstanceLocalQueue],\n    row_inds: np.array,\n    col_inds: np.array,\n    tracking_scores: List[float],\n) -&gt; List[TrackInstanceLocalQueue]:\n    \"\"\"Assign tracks to `TrackInstanceLocalQueue` objects based on the output of track matching algorithm.\n\n    Args:\n        current_instances: List of TrackInstanceLocalQueue objects with features and unassigned tracks.\n        row_inds: List of indices for the  `current_instances` object that has an assigned\n            track.\n        col_inds: List of track IDs that have been assigned a new instance.\n        tracking_scores: List of tracking scores from the cost matrix.\n\n    \"\"\"\n    res = []\n    if row_inds is not None and col_inds is not None:\n        for idx, (row, col) in enumerate(zip(row_inds, col_inds)):\n            current_instances[row].track_id = col\n            current_instances[row].tracking_score = tracking_scores[idx]\n            res.append(current_instances[row])\n\n        for track_instance in current_instances:\n            if track_instance.track_id is not None:\n                self.tracker_queue[track_instance.track_id].append(track_instance)\n\n        # Create new tracks for instances with unassigned tracks from track matching\n        new_current_instances_inds = [\n            x for x in range(len(current_instances)) if x not in row_inds\n        ]\n        if new_current_instances_inds:\n            for ind in new_current_instances_inds:\n                res.extend(self.add_new_tracks([current_instances[ind]]))\n\n    return [x for x in res if x.track_id is not None]\n</code></pre>"},{"location":"api/training/","title":"training","text":""},{"location":"api/training/#sleap_nn.training","title":"<code>sleap_nn.training</code>","text":"<p>Training-related modules.</p> <p>Modules:</p> Name Description <code>callbacks</code> <p>Custom Callback modules for Lightning Trainer.</p> <code>lightning_modules</code> <p>This module has the LightningModule classes for all model types.</p> <code>losses</code> <p>Custom loss functions.</p> <code>model_trainer</code> <p>This module is to train a sleap-nn model using Lightning.</p> <code>schedulers</code> <p>Custom learning rate schedulers for sleap-nn training.</p> <code>utils</code> <p>Miscellaneous utility functions for training.</p>"},{"location":"api/training/callbacks/","title":"callbacks","text":""},{"location":"api/training/callbacks/#sleap_nn.training.callbacks","title":"<code>sleap_nn.training.callbacks</code>","text":"<p>Custom Callback modules for Lightning Trainer.</p> <p>Classes:</p> Name Description <code>CSVLoggerCallback</code> <p>Callback for logging metrics to csv.</p> <code>CentroidEvaluationCallback</code> <p>Callback to run centroid-specific evaluation metrics at end of validation epochs.</p> <code>EpochEndEvaluationCallback</code> <p>Callback to run full evaluation metrics at end of validation epochs.</p> <code>MatplotlibSaver</code> <p>Callback for saving images rendered with matplotlib during training.</p> <code>ProgressReporterZMQ</code> <p>Callback to publish training progress events to a ZMQ PUB socket.</p> <code>SleapProgressBar</code> <p>Custom progress bar with better formatting for small metric values.</p> <code>TrainingControllerZMQ</code> <p>Lightning callback to receive control commands during training via ZMQ.</p> <code>UnifiedVizCallback</code> <p>Unified callback for all visualization outputs during training.</p> <code>WandBPredImageLogger</code> <p>Callback for writing image predictions to wandb as a Table.</p> <code>WandBVizCallback</code> <p>Callback for logging visualization images directly to wandb with slider support.</p> <code>WandBVizCallbackWithPAFs</code> <p>Extended WandBVizCallback that also logs PAF visualizations for bottom-up models.</p> <p>Functions:</p> Name Description <code>match_centroids</code> <p>Match predicted centroids to ground truth using Hungarian algorithm.</p>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CSVLoggerCallback","title":"<code>CSVLoggerCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for logging metrics to csv.</p> <p>Attributes:</p> Name Type Description <code>filepath</code> <p>Path to save the csv file.</p> <code>keys</code> <p>List of field names to be logged in the csv.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attributes.</p> <code>on_validation_epoch_end</code> <p>Log metrics to csv at the end of validation epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class CSVLoggerCallback(Callback):\n    \"\"\"Callback for logging metrics to csv.\n\n    Attributes:\n        filepath: Path to save the csv file.\n        keys: List of field names to be logged in the csv.\n    \"\"\"\n\n    def __init__(\n        self,\n        filepath: Path,\n        keys: list = [\"epoch\", \"train_loss\", \"val_loss\", \"learning_rate\"],\n    ):\n        \"\"\"Initialize attributes.\"\"\"\n        super().__init__()\n        self.filepath = filepath\n        self.keys = keys\n        self.initialized = False\n\n    def _init_file(self):\n        \"\"\"Create the .csv file.\"\"\"\n        if RANK in [0, -1]:  # Global rank 0 or -1 (non-distributed)\n            self.filepath.parent.mkdir(parents=True, exist_ok=True)\n            with open(self.filepath, \"w\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=self.keys)\n                writer.writeheader()\n        self.initialized = True\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Log metrics to csv at the end of validation epoch.\"\"\"\n        if trainer.is_global_zero:\n            if not self.initialized:\n                self._init_file()\n\n            metrics = trainer.callback_metrics\n            log_data = {}\n            for key in self.keys:\n                if key == \"epoch\":\n                    log_data[\"epoch\"] = trainer.current_epoch\n                elif key == \"learning_rate\":\n                    # Handle multiple formats:\n                    # 1. Direct \"learning_rate\" key\n                    # 2. \"train/lr\" key (current format from lightning modules)\n                    # 3. \"lr-*\" keys from LearningRateMonitor (legacy)\n                    value = metrics.get(key, None)\n                    if value is None:\n                        value = metrics.get(\"train/lr\", None)\n                    if value is None:\n                        # Look for lr-* keys from LearningRateMonitor (legacy)\n                        for metric_key in metrics.keys():\n                            if metric_key.startswith(\"lr-\"):\n                                value = metrics[metric_key]\n                                break\n                    log_data[key] = value.item() if value is not None else None\n                else:\n                    value = metrics.get(key, None)\n                    log_data[key] = value.item() if value is not None else None\n\n            with open(self.filepath, \"a\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=self.keys)\n                writer.writerow(log_data)\n\n        # Sync all processes after file I/O\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CSVLoggerCallback.__init__","title":"<code>__init__(filepath, keys=['epoch', 'train_loss', 'val_loss', 'learning_rate'])</code>","text":"<p>Initialize attributes.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    filepath: Path,\n    keys: list = [\"epoch\", \"train_loss\", \"val_loss\", \"learning_rate\"],\n):\n    \"\"\"Initialize attributes.\"\"\"\n    super().__init__()\n    self.filepath = filepath\n    self.keys = keys\n    self.initialized = False\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CSVLoggerCallback.on_validation_epoch_end","title":"<code>on_validation_epoch_end(trainer, pl_module)</code>","text":"<p>Log metrics to csv at the end of validation epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n    \"\"\"Log metrics to csv at the end of validation epoch.\"\"\"\n    if trainer.is_global_zero:\n        if not self.initialized:\n            self._init_file()\n\n        metrics = trainer.callback_metrics\n        log_data = {}\n        for key in self.keys:\n            if key == \"epoch\":\n                log_data[\"epoch\"] = trainer.current_epoch\n            elif key == \"learning_rate\":\n                # Handle multiple formats:\n                # 1. Direct \"learning_rate\" key\n                # 2. \"train/lr\" key (current format from lightning modules)\n                # 3. \"lr-*\" keys from LearningRateMonitor (legacy)\n                value = metrics.get(key, None)\n                if value is None:\n                    value = metrics.get(\"train/lr\", None)\n                if value is None:\n                    # Look for lr-* keys from LearningRateMonitor (legacy)\n                    for metric_key in metrics.keys():\n                        if metric_key.startswith(\"lr-\"):\n                            value = metrics[metric_key]\n                            break\n                log_data[key] = value.item() if value is not None else None\n            else:\n                value = metrics.get(key, None)\n                log_data[key] = value.item() if value is not None else None\n\n        with open(self.filepath, \"a\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=self.keys)\n            writer.writerow(log_data)\n\n    # Sync all processes after file I/O\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CentroidEvaluationCallback","title":"<code>CentroidEvaluationCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to run centroid-specific evaluation metrics at end of validation epochs.</p> <p>This callback is designed specifically for centroid models, which predict a single point (centroid) per instance rather than full pose skeletons. It computes distance-based metrics and detection metrics that are more appropriate for point detection tasks than OKS/PCK metrics.</p> Metrics computed <ul> <li>Distance metrics: mean, median, p90, p95, max Euclidean distance</li> <li>Detection metrics: precision, recall, F1 score</li> <li>Counts: true positives, false positives, false negatives</li> </ul> <p>Attributes:</p> Name Type Description <code>videos</code> <p>List of sio.Video objects.</p> <code>eval_frequency</code> <p>Run evaluation every N epochs (default: 1).</p> <code>match_threshold</code> <p>Maximum distance (pixels) for matching pred to GT (default: 50.0).</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the callback.</p> <code>on_validation_epoch_end</code> <p>Run centroid evaluation and log metrics at end of validation epoch.</p> <code>on_validation_epoch_start</code> <p>Enable prediction collection at the start of validation.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class CentroidEvaluationCallback(Callback):\n    \"\"\"Callback to run centroid-specific evaluation metrics at end of validation epochs.\n\n    This callback is designed specifically for centroid models, which predict a single\n    point (centroid) per instance rather than full pose skeletons. It computes\n    distance-based metrics and detection metrics that are more appropriate for\n    point detection tasks than OKS/PCK metrics.\n\n    Metrics computed:\n        - Distance metrics: mean, median, p90, p95, max Euclidean distance\n        - Detection metrics: precision, recall, F1 score\n        - Counts: true positives, false positives, false negatives\n\n    Attributes:\n        videos: List of sio.Video objects.\n        eval_frequency: Run evaluation every N epochs (default: 1).\n        match_threshold: Maximum distance (pixels) for matching pred to GT (default: 50.0).\n    \"\"\"\n\n    def __init__(\n        self,\n        videos: list,\n        eval_frequency: int = 1,\n        match_threshold: float = 50.0,\n    ):\n        \"\"\"Initialize the callback.\n\n        Args:\n            videos: List of sio.Video objects.\n            eval_frequency: Run evaluation every N epochs (default: 1).\n            match_threshold: Maximum distance in pixels for a prediction to be\n                considered a match to a ground truth centroid (default: 50.0).\n        \"\"\"\n        super().__init__()\n        self.videos = videos\n        self.eval_frequency = eval_frequency\n        self.match_threshold = match_threshold\n\n    def on_validation_epoch_start(self, trainer, pl_module):\n        \"\"\"Enable prediction collection at the start of validation.\n\n        Skip during sanity check to avoid inference issues.\n        \"\"\"\n        if trainer.sanity_checking:\n            return\n        pl_module._collect_val_predictions = True\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Run centroid evaluation and log metrics at end of validation epoch.\"\"\"\n        import numpy as np\n        from lightning.pytorch.loggers import WandbLogger\n\n        # Determine if we should run evaluation this epoch (only on rank 0)\n        should_evaluate = (\n            trainer.current_epoch + 1\n        ) % self.eval_frequency == 0 and trainer.is_global_zero\n\n        if should_evaluate:\n            # Check if we have predictions\n            if not pl_module.val_predictions or not pl_module.val_ground_truth:\n                logger.warning(\n                    \"No predictions collected for centroid epoch-end evaluation\"\n                )\n            else:\n                try:\n                    metrics = self._compute_metrics(\n                        pl_module.val_predictions, pl_module.val_ground_truth, np\n                    )\n\n                    # Log to WandB\n                    self._log_metrics(trainer, metrics, trainer.current_epoch)\n\n                    logger.info(\n                        f\"Epoch {trainer.current_epoch} centroid evaluation: \"\n                        f\"precision={metrics['precision']:.4f}, \"\n                        f\"recall={metrics['recall']:.4f}, \"\n                        f\"dist_avg={metrics['dist_avg']:.2f}px\"\n                    )\n\n                except Exception as e:\n                    logger.warning(f\"Centroid epoch-end evaluation failed: {e}\")\n\n        # Cleanup - all ranks reset the flag, rank 0 clears the lists\n        pl_module._collect_val_predictions = False\n        if trainer.is_global_zero:\n            pl_module.val_predictions = []\n            pl_module.val_ground_truth = []\n\n        # Sync all processes - barrier must be reached by ALL ranks\n        trainer.strategy.barrier()\n\n    def _compute_metrics(self, predictions: list, ground_truth: list, np) -&gt; dict:\n        \"\"\"Compute centroid-specific metrics.\n\n        Args:\n            predictions: List of prediction dicts with \"pred_peaks\" key.\n            ground_truth: List of ground truth dicts with \"gt_instances\" key.\n            np: NumPy module.\n\n        Returns:\n            Dictionary of computed metrics.\n        \"\"\"\n        all_distances = []\n        total_tp = 0\n        total_fp = 0\n        total_fn = 0\n\n        # Group predictions and GT by frame\n        pred_by_frame = {}\n        for pred in predictions:\n            key = (pred[\"video_idx\"], pred[\"frame_idx\"])\n            if key not in pred_by_frame:\n                pred_by_frame[key] = []\n            # pred_peaks shape: (n_inst, 1, 2) -&gt; extract centroids as (n_inst, 2)\n            centroids = pred[\"pred_peaks\"].reshape(-1, 2)\n            # Filter out NaN centroids\n            valid_mask = ~np.isnan(centroids).any(axis=1)\n            pred_by_frame[key].append(centroids[valid_mask])\n\n        gt_by_frame = {}\n        for gt in ground_truth:\n            key = (gt[\"video_idx\"], gt[\"frame_idx\"])\n            if key not in gt_by_frame:\n                gt_by_frame[key] = []\n            # gt_instances shape: (n_inst, 1, 2) -&gt; extract centroids as (n_inst, 2)\n            centroids = gt[\"gt_instances\"].reshape(-1, 2)\n            # Filter out NaN centroids\n            valid_mask = ~np.isnan(centroids).any(axis=1)\n            gt_by_frame[key].append(centroids[valid_mask])\n\n        # Process each frame\n        all_frames = set(pred_by_frame.keys()) | set(gt_by_frame.keys())\n        for frame_key in all_frames:\n            # Concatenate all predictions for this frame\n            if frame_key in pred_by_frame:\n                frame_preds = np.concatenate(pred_by_frame[frame_key], axis=0)\n            else:\n                frame_preds = np.zeros((0, 2))\n\n            # Concatenate all GT for this frame\n            if frame_key in gt_by_frame:\n                frame_gt = np.concatenate(gt_by_frame[frame_key], axis=0)\n            else:\n                frame_gt = np.zeros((0, 2))\n\n            # Match predictions to ground truth\n            matched_pred, matched_gt, unmatched_pred, unmatched_gt = match_centroids(\n                frame_preds, frame_gt, max_distance=self.match_threshold\n            )\n\n            # Compute distances for matched pairs\n            if len(matched_pred) &gt; 0:\n                matched_pred_points = frame_preds[matched_pred]\n                matched_gt_points = frame_gt[matched_gt]\n                distances = np.linalg.norm(\n                    matched_pred_points - matched_gt_points, axis=1\n                )\n                all_distances.extend(distances.tolist())\n\n            # Update counts\n            total_tp += len(matched_pred)\n            total_fp += len(unmatched_pred)\n            total_fn += len(unmatched_gt)\n\n        # Compute aggregate metrics\n        all_distances = np.array(all_distances)\n\n        # Distance metrics (only if we have matches)\n        if len(all_distances) &gt; 0:\n            dist_avg = float(np.mean(all_distances))\n            dist_median = float(np.median(all_distances))\n            dist_p90 = float(np.percentile(all_distances, 90))\n            dist_p95 = float(np.percentile(all_distances, 95))\n            dist_max = float(np.max(all_distances))\n        else:\n            dist_avg = dist_median = dist_p90 = dist_p95 = dist_max = float(\"nan\")\n\n        # Detection metrics\n        precision = (\n            total_tp / (total_tp + total_fp) if (total_tp + total_fp) &gt; 0 else 0.0\n        )\n        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) &gt; 0 else 0.0\n        f1 = (\n            2 * precision * recall / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0.0\n        )\n\n        return {\n            \"dist_avg\": dist_avg,\n            \"dist_median\": dist_median,\n            \"dist_p90\": dist_p90,\n            \"dist_p95\": dist_p95,\n            \"dist_max\": dist_max,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1\": f1,\n            \"n_true_positives\": total_tp,\n            \"n_false_positives\": total_fp,\n            \"n_false_negatives\": total_fn,\n            \"n_total_predictions\": total_tp + total_fp,\n            \"n_total_ground_truth\": total_tp + total_fn,\n        }\n\n    def _log_metrics(self, trainer, metrics: dict, epoch: int):\n        \"\"\"Log centroid evaluation metrics to WandB.\"\"\"\n        import numpy as np\n        from lightning.pytorch.loggers import WandbLogger\n\n        # Get WandB logger\n        wandb_logger = None\n        for log in trainer.loggers:\n            if isinstance(log, WandbLogger):\n                wandb_logger = log\n                break\n\n        if wandb_logger is None:\n            return\n\n        log_dict = {\"epoch\": epoch}\n\n        # Distance metrics (with NaN handling)\n        if not np.isnan(metrics[\"dist_avg\"]):\n            log_dict[\"eval/val/centroid_dist_avg\"] = metrics[\"dist_avg\"]\n        if not np.isnan(metrics[\"dist_median\"]):\n            log_dict[\"eval/val/centroid_dist_median\"] = metrics[\"dist_median\"]\n        if not np.isnan(metrics[\"dist_p90\"]):\n            log_dict[\"eval/val/centroid_dist_p90\"] = metrics[\"dist_p90\"]\n        if not np.isnan(metrics[\"dist_p95\"]):\n            log_dict[\"eval/val/centroid_dist_p95\"] = metrics[\"dist_p95\"]\n        if not np.isnan(metrics[\"dist_max\"]):\n            log_dict[\"eval/val/centroid_dist_max\"] = metrics[\"dist_max\"]\n\n        # Detection metrics\n        log_dict[\"eval/val/centroid_precision\"] = metrics[\"precision\"]\n        log_dict[\"eval/val/centroid_recall\"] = metrics[\"recall\"]\n        log_dict[\"eval/val/centroid_f1\"] = metrics[\"f1\"]\n\n        # Counts\n        log_dict[\"eval/val/centroid_n_tp\"] = metrics[\"n_true_positives\"]\n        log_dict[\"eval/val/centroid_n_fp\"] = metrics[\"n_false_positives\"]\n        log_dict[\"eval/val/centroid_n_fn\"] = metrics[\"n_false_negatives\"]\n\n        wandb_logger.experiment.log(log_dict, commit=False)\n\n        # Update best metrics in summary\n        for key, value in log_dict.items():\n            if key == \"epoch\":\n                continue\n            summary_key = f\"best/{key}\"\n            current_best = wandb_logger.experiment.summary.get(summary_key)\n            # For distance metrics, lower is better; for others, higher is better\n            is_distance = \"dist\" in key\n            if current_best is None:\n                wandb_logger.experiment.summary[summary_key] = value\n            elif is_distance and value &lt; current_best:\n                wandb_logger.experiment.summary[summary_key] = value\n            elif not is_distance and value &gt; current_best:\n                wandb_logger.experiment.summary[summary_key] = value\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CentroidEvaluationCallback.__init__","title":"<code>__init__(videos, eval_frequency=1, match_threshold=50.0)</code>","text":"<p>Initialize the callback.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <code>list</code> <p>List of sio.Video objects.</p> required <code>eval_frequency</code> <code>int</code> <p>Run evaluation every N epochs (default: 1).</p> <code>1</code> <code>match_threshold</code> <code>float</code> <p>Maximum distance in pixels for a prediction to be considered a match to a ground truth centroid (default: 50.0).</p> <code>50.0</code> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    videos: list,\n    eval_frequency: int = 1,\n    match_threshold: float = 50.0,\n):\n    \"\"\"Initialize the callback.\n\n    Args:\n        videos: List of sio.Video objects.\n        eval_frequency: Run evaluation every N epochs (default: 1).\n        match_threshold: Maximum distance in pixels for a prediction to be\n            considered a match to a ground truth centroid (default: 50.0).\n    \"\"\"\n    super().__init__()\n    self.videos = videos\n    self.eval_frequency = eval_frequency\n    self.match_threshold = match_threshold\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CentroidEvaluationCallback.on_validation_epoch_end","title":"<code>on_validation_epoch_end(trainer, pl_module)</code>","text":"<p>Run centroid evaluation and log metrics at end of validation epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n    \"\"\"Run centroid evaluation and log metrics at end of validation epoch.\"\"\"\n    import numpy as np\n    from lightning.pytorch.loggers import WandbLogger\n\n    # Determine if we should run evaluation this epoch (only on rank 0)\n    should_evaluate = (\n        trainer.current_epoch + 1\n    ) % self.eval_frequency == 0 and trainer.is_global_zero\n\n    if should_evaluate:\n        # Check if we have predictions\n        if not pl_module.val_predictions or not pl_module.val_ground_truth:\n            logger.warning(\n                \"No predictions collected for centroid epoch-end evaluation\"\n            )\n        else:\n            try:\n                metrics = self._compute_metrics(\n                    pl_module.val_predictions, pl_module.val_ground_truth, np\n                )\n\n                # Log to WandB\n                self._log_metrics(trainer, metrics, trainer.current_epoch)\n\n                logger.info(\n                    f\"Epoch {trainer.current_epoch} centroid evaluation: \"\n                    f\"precision={metrics['precision']:.4f}, \"\n                    f\"recall={metrics['recall']:.4f}, \"\n                    f\"dist_avg={metrics['dist_avg']:.2f}px\"\n                )\n\n            except Exception as e:\n                logger.warning(f\"Centroid epoch-end evaluation failed: {e}\")\n\n    # Cleanup - all ranks reset the flag, rank 0 clears the lists\n    pl_module._collect_val_predictions = False\n    if trainer.is_global_zero:\n        pl_module.val_predictions = []\n        pl_module.val_ground_truth = []\n\n    # Sync all processes - barrier must be reached by ALL ranks\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.CentroidEvaluationCallback.on_validation_epoch_start","title":"<code>on_validation_epoch_start(trainer, pl_module)</code>","text":"<p>Enable prediction collection at the start of validation.</p> <p>Skip during sanity check to avoid inference issues.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_validation_epoch_start(self, trainer, pl_module):\n    \"\"\"Enable prediction collection at the start of validation.\n\n    Skip during sanity check to avoid inference issues.\n    \"\"\"\n    if trainer.sanity_checking:\n        return\n    pl_module._collect_val_predictions = True\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.EpochEndEvaluationCallback","title":"<code>EpochEndEvaluationCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to run full evaluation metrics at end of validation epochs.</p> <p>This callback collects predictions and ground truth during validation, then runs the full evaluation pipeline (OKS, mAP, PCK, etc.) and logs metrics to WandB.</p> <p>Attributes:</p> Name Type Description <code>skeleton</code> <p>sio.Skeleton for creating instances.</p> <code>videos</code> <p>List of sio.Video objects.</p> <code>eval_frequency</code> <p>Run evaluation every N epochs (default: 1).</p> <code>oks_stddev</code> <p>OKS standard deviation (default: 0.025).</p> <code>oks_scale</code> <p>Optional OKS scale override.</p> <code>metrics_to_log</code> <p>List of metric keys to log.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the callback.</p> <code>on_validation_epoch_end</code> <p>Run evaluation and log metrics at end of validation epoch.</p> <code>on_validation_epoch_start</code> <p>Enable prediction collection at the start of validation.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class EpochEndEvaluationCallback(Callback):\n    \"\"\"Callback to run full evaluation metrics at end of validation epochs.\n\n    This callback collects predictions and ground truth during validation,\n    then runs the full evaluation pipeline (OKS, mAP, PCK, etc.) and logs\n    metrics to WandB.\n\n    Attributes:\n        skeleton: sio.Skeleton for creating instances.\n        videos: List of sio.Video objects.\n        eval_frequency: Run evaluation every N epochs (default: 1).\n        oks_stddev: OKS standard deviation (default: 0.025).\n        oks_scale: Optional OKS scale override.\n        metrics_to_log: List of metric keys to log.\n    \"\"\"\n\n    def __init__(\n        self,\n        skeleton: \"sio.Skeleton\",\n        videos: list,\n        eval_frequency: int = 1,\n        oks_stddev: float = 0.025,\n        oks_scale: Optional[float] = None,\n        metrics_to_log: Optional[list] = None,\n    ):\n        \"\"\"Initialize the callback.\n\n        Args:\n            skeleton: sio.Skeleton for creating instances.\n            videos: List of sio.Video objects.\n            eval_frequency: Run evaluation every N epochs (default: 1).\n            oks_stddev: OKS standard deviation (default: 0.025).\n            oks_scale: Optional OKS scale override.\n            metrics_to_log: List of metric keys to log. If None, logs all available.\n        \"\"\"\n        super().__init__()\n        self.skeleton = skeleton\n        self.videos = videos\n        self.eval_frequency = eval_frequency\n        self.oks_stddev = oks_stddev\n        self.oks_scale = oks_scale\n        self.metrics_to_log = metrics_to_log or [\n            \"mOKS\",\n            \"oks_voc.mAP\",\n            \"oks_voc.mAR\",\n            \"distance/avg\",\n            \"distance/p50\",\n            \"distance/p95\",\n            \"distance/p99\",\n            \"mPCK\",\n            \"PCK@5\",\n            \"PCK@10\",\n            \"visibility_precision\",\n            \"visibility_recall\",\n        ]\n\n    def on_validation_epoch_start(self, trainer, pl_module):\n        \"\"\"Enable prediction collection at the start of validation.\n\n        Skip during sanity check to avoid inference issues.\n        \"\"\"\n        if trainer.sanity_checking:\n            return\n        pl_module._collect_val_predictions = True\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Run evaluation and log metrics at end of validation epoch.\"\"\"\n        import sleap_io as sio\n        import numpy as np\n        from lightning.pytorch.loggers import WandbLogger\n        from sleap_nn.evaluation import Evaluator\n\n        # Determine if we should run evaluation this epoch (only on rank 0)\n        should_evaluate = (\n            trainer.current_epoch + 1\n        ) % self.eval_frequency == 0 and trainer.is_global_zero\n\n        if should_evaluate:\n            # Check if we have predictions\n            if not pl_module.val_predictions or not pl_module.val_ground_truth:\n                logger.warning(\"No predictions collected for epoch-end evaluation\")\n            else:\n                try:\n                    # Build sio.Labels from accumulated predictions and ground truth\n                    pred_labels = self._build_pred_labels(\n                        pl_module.val_predictions, sio, np\n                    )\n                    gt_labels = self._build_gt_labels(\n                        pl_module.val_ground_truth, sio, np\n                    )\n\n                    # Check if we have valid frames to evaluate\n                    if len(pred_labels) == 0:\n                        logger.warning(\n                            \"No valid predictions for epoch-end evaluation \"\n                            \"(all predictions may be empty or NaN)\"\n                        )\n                    else:\n                        # Run evaluation\n                        evaluator = Evaluator(\n                            ground_truth_instances=gt_labels,\n                            predicted_instances=pred_labels,\n                            oks_stddev=self.oks_stddev,\n                            oks_scale=self.oks_scale,\n                            user_labels_only=False,  # All validation frames are \"user\" frames\n                        )\n                        metrics = evaluator.evaluate()\n\n                        # Log to WandB\n                        self._log_metrics(trainer, metrics, trainer.current_epoch)\n\n                        logger.info(\n                            f\"Epoch {trainer.current_epoch} evaluation: \"\n                            f\"PCK@5={metrics['pck_metrics']['PCK@5']:.4f}, \"\n                            f\"mOKS={metrics['mOKS']['mOKS']:.4f}, \"\n                            f\"mAP={metrics['voc_metrics']['oks_voc.mAP']:.4f}\"\n                        )\n\n                except Exception as e:\n                    logger.warning(f\"Epoch-end evaluation failed: {e}\")\n\n        # Cleanup - all ranks reset the flag, rank 0 clears the lists\n        pl_module._collect_val_predictions = False\n        if trainer.is_global_zero:\n            pl_module.val_predictions = []\n            pl_module.val_ground_truth = []\n\n        # Sync all processes - barrier must be reached by ALL ranks\n        trainer.strategy.barrier()\n\n    def _build_pred_labels(self, predictions: list, sio, np) -&gt; \"sio.Labels\":\n        \"\"\"Convert prediction dicts to sio.Labels.\"\"\"\n        labeled_frames = []\n        for pred in predictions:\n            pred_peaks = pred[\"pred_peaks\"]\n            pred_scores = pred[\"pred_scores\"]\n\n            # Handle NaN/missing predictions\n            if pred_peaks is None or (\n                isinstance(pred_peaks, np.ndarray) and np.isnan(pred_peaks).all()\n            ):\n                continue\n\n            # Handle multi-instance predictions (bottomup)\n            if len(pred_peaks.shape) == 2:\n                # Single instance: (n_nodes, 2) -&gt; (1, n_nodes, 2)\n                pred_peaks = pred_peaks.reshape(1, -1, 2)\n                pred_scores = pred_scores.reshape(1, -1)\n\n            instances = []\n            for inst_idx in range(len(pred_peaks)):\n                inst_points = pred_peaks[inst_idx]\n                inst_scores = pred_scores[inst_idx] if pred_scores is not None else None\n\n                # Skip if all NaN\n                if np.isnan(inst_points).all():\n                    continue\n\n                inst = sio.PredictedInstance.from_numpy(\n                    points_data=inst_points,\n                    skeleton=self.skeleton,\n                    point_scores=(\n                        inst_scores\n                        if inst_scores is not None\n                        else np.ones(len(inst_points))\n                    ),\n                    score=(\n                        float(np.nanmean(inst_scores))\n                        if inst_scores is not None\n                        else 1.0\n                    ),\n                )\n                instances.append(inst)\n\n            if instances:\n                lf = sio.LabeledFrame(\n                    video=self.videos[pred[\"video_idx\"]],\n                    frame_idx=pred[\"frame_idx\"],\n                    instances=instances,\n                )\n                labeled_frames.append(lf)\n\n        return sio.Labels(\n            videos=self.videos,\n            skeletons=[self.skeleton],\n            labeled_frames=labeled_frames,\n        )\n\n    def _build_gt_labels(self, ground_truth: list, sio, np) -&gt; \"sio.Labels\":\n        \"\"\"Convert ground truth dicts to sio.Labels.\"\"\"\n        labeled_frames = []\n        for gt in ground_truth:\n            instances = []\n            gt_instances = gt[\"gt_instances\"]\n\n            # Handle shape variations\n            if len(gt_instances.shape) == 2:\n                # (n_nodes, 2) -&gt; (1, n_nodes, 2)\n                gt_instances = gt_instances.reshape(1, -1, 2)\n\n            for i in range(min(gt[\"num_instances\"], len(gt_instances))):\n                inst_data = gt_instances[i]\n                if np.isnan(inst_data).all():\n                    continue\n                inst = sio.Instance.from_numpy(\n                    points_data=inst_data,\n                    skeleton=self.skeleton,\n                )\n                instances.append(inst)\n\n            if instances:\n                lf = sio.LabeledFrame(\n                    video=self.videos[gt[\"video_idx\"]],\n                    frame_idx=gt[\"frame_idx\"],\n                    instances=instances,\n                )\n                labeled_frames.append(lf)\n\n        return sio.Labels(\n            videos=self.videos,\n            skeletons=[self.skeleton],\n            labeled_frames=labeled_frames,\n        )\n\n    def _log_metrics(self, trainer, metrics: dict, epoch: int):\n        \"\"\"Log evaluation metrics to WandB.\"\"\"\n        import numpy as np\n        from lightning.pytorch.loggers import WandbLogger\n\n        # Get WandB logger\n        wandb_logger = None\n        for log in trainer.loggers:\n            if isinstance(log, WandbLogger):\n                wandb_logger = log\n                break\n\n        if wandb_logger is None:\n            return\n\n        log_dict = {\"epoch\": epoch}\n\n        # Extract key metrics with consistent naming\n        # All eval metrics use eval/val/ prefix since they're computed on validation data\n        if \"mOKS\" in self.metrics_to_log:\n            log_dict[\"eval/val/mOKS\"] = metrics[\"mOKS\"][\"mOKS\"]\n\n        if \"oks_voc.mAP\" in self.metrics_to_log:\n            log_dict[\"eval/val/oks_voc_mAP\"] = metrics[\"voc_metrics\"][\"oks_voc.mAP\"]\n\n        if \"oks_voc.mAR\" in self.metrics_to_log:\n            log_dict[\"eval/val/oks_voc_mAR\"] = metrics[\"voc_metrics\"][\"oks_voc.mAR\"]\n\n        # Distance metrics grouped under eval/val/distance/\n        if \"distance/avg\" in self.metrics_to_log:\n            val = metrics[\"distance_metrics\"][\"avg\"]\n            if not np.isnan(val):\n                log_dict[\"eval/val/distance/avg\"] = val\n\n        if \"distance/p50\" in self.metrics_to_log:\n            val = metrics[\"distance_metrics\"][\"p50\"]\n            if not np.isnan(val):\n                log_dict[\"eval/val/distance/p50\"] = val\n\n        if \"distance/p95\" in self.metrics_to_log:\n            val = metrics[\"distance_metrics\"][\"p95\"]\n            if not np.isnan(val):\n                log_dict[\"eval/val/distance/p95\"] = val\n\n        if \"distance/p99\" in self.metrics_to_log:\n            val = metrics[\"distance_metrics\"][\"p99\"]\n            if not np.isnan(val):\n                log_dict[\"eval/val/distance/p99\"] = val\n\n        # PCK metrics\n        if \"mPCK\" in self.metrics_to_log:\n            log_dict[\"eval/val/mPCK\"] = metrics[\"pck_metrics\"][\"mPCK\"]\n\n        # PCK at specific thresholds (precomputed in evaluation.py)\n        if \"PCK@5\" in self.metrics_to_log:\n            log_dict[\"eval/val/PCK_5\"] = metrics[\"pck_metrics\"][\"PCK@5\"]\n\n        if \"PCK@10\" in self.metrics_to_log:\n            log_dict[\"eval/val/PCK_10\"] = metrics[\"pck_metrics\"][\"PCK@10\"]\n\n        # Visibility metrics\n        if \"visibility_precision\" in self.metrics_to_log:\n            val = metrics[\"visibility_metrics\"][\"precision\"]\n            if not np.isnan(val):\n                log_dict[\"eval/val/visibility_precision\"] = val\n\n        if \"visibility_recall\" in self.metrics_to_log:\n            val = metrics[\"visibility_metrics\"][\"recall\"]\n            if not np.isnan(val):\n                log_dict[\"eval/val/visibility_recall\"] = val\n\n        wandb_logger.experiment.log(log_dict, commit=False)\n\n        # Update best metrics in summary (excluding epoch)\n        for key, value in log_dict.items():\n            if key == \"epoch\":\n                continue\n            # Create summary key like \"best/eval/val/mOKS\"\n            summary_key = f\"best/{key}\"\n            current_best = wandb_logger.experiment.summary.get(summary_key)\n            # For distance metrics, lower is better; for others, higher is better\n            is_distance = \"distance\" in key\n            if current_best is None:\n                wandb_logger.experiment.summary[summary_key] = value\n            elif is_distance and value &lt; current_best:\n                wandb_logger.experiment.summary[summary_key] = value\n            elif not is_distance and value &gt; current_best:\n                wandb_logger.experiment.summary[summary_key] = value\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.EpochEndEvaluationCallback.__init__","title":"<code>__init__(skeleton, videos, eval_frequency=1, oks_stddev=0.025, oks_scale=None, metrics_to_log=None)</code>","text":"<p>Initialize the callback.</p> <p>Parameters:</p> Name Type Description Default <code>skeleton</code> <code>Skeleton</code> <p>sio.Skeleton for creating instances.</p> required <code>videos</code> <code>list</code> <p>List of sio.Video objects.</p> required <code>eval_frequency</code> <code>int</code> <p>Run evaluation every N epochs (default: 1).</p> <code>1</code> <code>oks_stddev</code> <code>float</code> <p>OKS standard deviation (default: 0.025).</p> <code>0.025</code> <code>oks_scale</code> <code>Optional[float]</code> <p>Optional OKS scale override.</p> <code>None</code> <code>metrics_to_log</code> <code>Optional[list]</code> <p>List of metric keys to log. If None, logs all available.</p> <code>None</code> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    skeleton: \"sio.Skeleton\",\n    videos: list,\n    eval_frequency: int = 1,\n    oks_stddev: float = 0.025,\n    oks_scale: Optional[float] = None,\n    metrics_to_log: Optional[list] = None,\n):\n    \"\"\"Initialize the callback.\n\n    Args:\n        skeleton: sio.Skeleton for creating instances.\n        videos: List of sio.Video objects.\n        eval_frequency: Run evaluation every N epochs (default: 1).\n        oks_stddev: OKS standard deviation (default: 0.025).\n        oks_scale: Optional OKS scale override.\n        metrics_to_log: List of metric keys to log. If None, logs all available.\n    \"\"\"\n    super().__init__()\n    self.skeleton = skeleton\n    self.videos = videos\n    self.eval_frequency = eval_frequency\n    self.oks_stddev = oks_stddev\n    self.oks_scale = oks_scale\n    self.metrics_to_log = metrics_to_log or [\n        \"mOKS\",\n        \"oks_voc.mAP\",\n        \"oks_voc.mAR\",\n        \"distance/avg\",\n        \"distance/p50\",\n        \"distance/p95\",\n        \"distance/p99\",\n        \"mPCK\",\n        \"PCK@5\",\n        \"PCK@10\",\n        \"visibility_precision\",\n        \"visibility_recall\",\n    ]\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.EpochEndEvaluationCallback.on_validation_epoch_end","title":"<code>on_validation_epoch_end(trainer, pl_module)</code>","text":"<p>Run evaluation and log metrics at end of validation epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n    \"\"\"Run evaluation and log metrics at end of validation epoch.\"\"\"\n    import sleap_io as sio\n    import numpy as np\n    from lightning.pytorch.loggers import WandbLogger\n    from sleap_nn.evaluation import Evaluator\n\n    # Determine if we should run evaluation this epoch (only on rank 0)\n    should_evaluate = (\n        trainer.current_epoch + 1\n    ) % self.eval_frequency == 0 and trainer.is_global_zero\n\n    if should_evaluate:\n        # Check if we have predictions\n        if not pl_module.val_predictions or not pl_module.val_ground_truth:\n            logger.warning(\"No predictions collected for epoch-end evaluation\")\n        else:\n            try:\n                # Build sio.Labels from accumulated predictions and ground truth\n                pred_labels = self._build_pred_labels(\n                    pl_module.val_predictions, sio, np\n                )\n                gt_labels = self._build_gt_labels(\n                    pl_module.val_ground_truth, sio, np\n                )\n\n                # Check if we have valid frames to evaluate\n                if len(pred_labels) == 0:\n                    logger.warning(\n                        \"No valid predictions for epoch-end evaluation \"\n                        \"(all predictions may be empty or NaN)\"\n                    )\n                else:\n                    # Run evaluation\n                    evaluator = Evaluator(\n                        ground_truth_instances=gt_labels,\n                        predicted_instances=pred_labels,\n                        oks_stddev=self.oks_stddev,\n                        oks_scale=self.oks_scale,\n                        user_labels_only=False,  # All validation frames are \"user\" frames\n                    )\n                    metrics = evaluator.evaluate()\n\n                    # Log to WandB\n                    self._log_metrics(trainer, metrics, trainer.current_epoch)\n\n                    logger.info(\n                        f\"Epoch {trainer.current_epoch} evaluation: \"\n                        f\"PCK@5={metrics['pck_metrics']['PCK@5']:.4f}, \"\n                        f\"mOKS={metrics['mOKS']['mOKS']:.4f}, \"\n                        f\"mAP={metrics['voc_metrics']['oks_voc.mAP']:.4f}\"\n                    )\n\n            except Exception as e:\n                logger.warning(f\"Epoch-end evaluation failed: {e}\")\n\n    # Cleanup - all ranks reset the flag, rank 0 clears the lists\n    pl_module._collect_val_predictions = False\n    if trainer.is_global_zero:\n        pl_module.val_predictions = []\n        pl_module.val_ground_truth = []\n\n    # Sync all processes - barrier must be reached by ALL ranks\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.EpochEndEvaluationCallback.on_validation_epoch_start","title":"<code>on_validation_epoch_start(trainer, pl_module)</code>","text":"<p>Enable prediction collection at the start of validation.</p> <p>Skip during sanity check to avoid inference issues.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_validation_epoch_start(self, trainer, pl_module):\n    \"\"\"Enable prediction collection at the start of validation.\n\n    Skip during sanity check to avoid inference issues.\n    \"\"\"\n    if trainer.sanity_checking:\n        return\n    pl_module._collect_val_predictions = True\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.MatplotlibSaver","title":"<code>MatplotlibSaver</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for saving images rendered with matplotlib during training.</p> <p>This is useful for saving visualizations of the training to disk. It will be called at the end of each epoch.</p> <p>Attributes:</p> Name Type Description <code>plot_fn</code> <p>Function with no arguments that returns a matplotlib figure handle.</p> <code>save_folder</code> <p>Path to a directory to save images to.</p> <code>prefix</code> <p>String that will be prepended to the filenames. This is useful for indicating which dataset the visualization was sampled from.</p> Notes <p>This will save images with the naming pattern:     \"{save_folder}/{prefix}.{epoch}.png\" or:     \"{save_folder}/{epoch}.png\" if a prefix is not specified.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize callback.</p> <code>on_train_epoch_end</code> <p>Save figure at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class MatplotlibSaver(Callback):\n    \"\"\"Callback for saving images rendered with matplotlib during training.\n\n    This is useful for saving visualizations of the training to disk. It will be called\n    at the end of each epoch.\n\n    Attributes:\n        plot_fn: Function with no arguments that returns a matplotlib figure handle.\n        save_folder: Path to a directory to save images to.\n        prefix: String that will be prepended to the filenames. This is useful for\n            indicating which dataset the visualization was sampled from.\n\n    Notes:\n        This will save images with the naming pattern:\n            \"{save_folder}/{prefix}.{epoch}.png\"\n        or:\n            \"{save_folder}/{epoch}.png\"\n        if a prefix is not specified.\n    \"\"\"\n\n    def __init__(\n        self,\n        save_folder: str,\n        plot_fn: Callable[[], matplotlib.figure.Figure],\n        prefix: Optional[str] = None,\n    ):\n        \"\"\"Initialize callback.\"\"\"\n        self.save_folder = save_folder\n        self.plot_fn = plot_fn\n        self.prefix = prefix\n        super().__init__()\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Save figure at the end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            # Call plotting function.\n            figure = self.plot_fn()\n\n            # Build filename.\n            prefix = \"\"\n            if self.prefix is not None:\n                prefix = self.prefix + \".\"\n            figure_path = (\n                Path(self.save_folder) / f\"{prefix}{trainer.current_epoch:04d}.png\"\n            ).as_posix()\n\n            # Save rendered figure.\n            figure.savefig(figure_path, format=\"png\")\n            plt.close(figure)\n\n        # Sync all processes after file I/O\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.MatplotlibSaver.__init__","title":"<code>__init__(save_folder, plot_fn, prefix=None)</code>","text":"<p>Initialize callback.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    save_folder: str,\n    plot_fn: Callable[[], matplotlib.figure.Figure],\n    prefix: Optional[str] = None,\n):\n    \"\"\"Initialize callback.\"\"\"\n    self.save_folder = save_folder\n    self.plot_fn = plot_fn\n    self.prefix = prefix\n    super().__init__()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.MatplotlibSaver.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Save figure at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Save figure at the end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        # Call plotting function.\n        figure = self.plot_fn()\n\n        # Build filename.\n        prefix = \"\"\n        if self.prefix is not None:\n            prefix = self.prefix + \".\"\n        figure_path = (\n            Path(self.save_folder) / f\"{prefix}{trainer.current_epoch:04d}.png\"\n        ).as_posix()\n\n        # Save rendered figure.\n        figure.savefig(figure_path, format=\"png\")\n        plt.close(figure)\n\n    # Sync all processes after file I/O\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ","title":"<code>ProgressReporterZMQ</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to publish training progress events to a ZMQ PUB socket.</p> <p>This is used to publish training metrics to the given socket.</p> <p>Attributes:</p> Name Type Description <code>address</code> <p>The ZMQ address to publish to, e.g., \"tcp://127.0.0.1:9001\".</p> <code>what</code> <p>Identifier tag for the type of training job (e.g., model name or job type).</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Close zmq socket and context when callback is destroyed.</p> <code>__init__</code> <p>Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.</p> <code>on_train_batch_end</code> <p>Called at the end of each training batch.</p> <code>on_train_batch_start</code> <p>Called at the beginning of each training batch.</p> <code>on_train_end</code> <p>Called at the end of training process.</p> <code>on_train_epoch_end</code> <p>Called at the end of each epoch.</p> <code>on_train_epoch_start</code> <p>Called at the beginning of each epoch.</p> <code>on_train_start</code> <p>Called at the beginning of training process.</p> <code>send</code> <p>Send a message over ZMQ.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class ProgressReporterZMQ(Callback):\n    \"\"\"Callback to publish training progress events to a ZMQ PUB socket.\n\n    This is used to publish training metrics to the given socket.\n\n    Attributes:\n        address: The ZMQ address to publish to, e.g., \"tcp://127.0.0.1:9001\".\n        what: Identifier tag for the type of training job (e.g., model name or job type).\n    \"\"\"\n\n    def __init__(self, address=\"tcp://127.0.0.1:9001\", what=\"\"):\n        \"\"\"Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.\"\"\"\n        super().__init__()\n        self.address = address\n        self.what = what\n\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.PUB)\n        self.socket.connect(self.address)\n\n        logger.info(\n            f\"ProgressReporterZMQ publishing to {self.address} for '{self.what}'\"\n        )\n\n    def __del__(self):\n        \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n        logger.info(f\"Closing ZMQ reporter.\")\n        self.socket.setsockopt(zmq.LINGER, 0)\n        self.socket.close()\n        self.context.term()\n\n    def send(self, event: str, logs=None, **kwargs):\n        \"\"\"Send a message over ZMQ.\"\"\"\n        msg = dict(what=self.what, event=event, logs=logs, **kwargs)\n        self.socket.send_string(jsonpickle.encode(msg))\n\n    def on_train_start(self, trainer, pl_module):\n        \"\"\"Called at the beginning of training process.\"\"\"\n        if trainer.is_global_zero:\n            # Include WandB URL if available\n            wandb_url = None\n            if wandb.run is not None:\n                wandb_url = wandb.run.url\n            self.send(\"train_begin\", wandb_url=wandb_url)\n        trainer.strategy.barrier()\n\n    def on_train_end(self, trainer, pl_module):\n        \"\"\"Called at the end of training process.\"\"\"\n        if trainer.is_global_zero:\n            self.send(\"train_end\")\n        trainer.strategy.barrier()\n\n    def on_train_epoch_start(self, trainer, pl_module):\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            self.send(\"epoch_begin\", epoch=trainer.current_epoch)\n        trainer.strategy.barrier()\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Called at the end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            logs = trainer.callback_metrics\n            self.send(\n                \"epoch_end\", epoch=trainer.current_epoch, logs=self._sanitize_logs(logs)\n            )\n        trainer.strategy.barrier()\n\n    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n        \"\"\"Called at the beginning of each training batch.\"\"\"\n        if trainer.is_global_zero:\n            self.send(\"batch_start\", batch=batch_idx)\n        trainer.strategy.barrier()\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Called at the end of each training batch.\"\"\"\n        if trainer.is_global_zero:\n            logs = trainer.callback_metrics\n            self.send(\n                \"batch_end\",\n                epoch=trainer.current_epoch,\n                batch=batch_idx,\n                logs=self._sanitize_logs(logs),\n            )\n        trainer.strategy.barrier()\n\n    def _sanitize_logs(self, logs):\n        \"\"\"Convert any torch tensors to Python floats for serialization.\"\"\"\n        return {\n            k: float(v.item()) if hasattr(v, \"item\") else v for k, v in logs.items()\n        }\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.__del__","title":"<code>__del__()</code>","text":"<p>Close zmq socket and context when callback is destroyed.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __del__(self):\n    \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n    logger.info(f\"Closing ZMQ reporter.\")\n    self.socket.setsockopt(zmq.LINGER, 0)\n    self.socket.close()\n    self.context.term()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.__init__","title":"<code>__init__(address='tcp://127.0.0.1:9001', what='')</code>","text":"<p>Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(self, address=\"tcp://127.0.0.1:9001\", what=\"\"):\n    \"\"\"Initialize the progress reporter callback by connecting to the specified ZMQ PUB socket.\"\"\"\n    super().__init__()\n    self.address = address\n    self.what = what\n\n    self.context = zmq.Context()\n    self.socket = self.context.socket(zmq.PUB)\n    self.socket.connect(self.address)\n\n    logger.info(\n        f\"ProgressReporterZMQ publishing to {self.address} for '{self.what}'\"\n    )\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_batch_end","title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Called at the end of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n    \"\"\"Called at the end of each training batch.\"\"\"\n    if trainer.is_global_zero:\n        logs = trainer.callback_metrics\n        self.send(\n            \"batch_end\",\n            epoch=trainer.current_epoch,\n            batch=batch_idx,\n            logs=self._sanitize_logs(logs),\n        )\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_batch_start","title":"<code>on_train_batch_start(trainer, pl_module, batch, batch_idx)</code>","text":"<p>Called at the beginning of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n    \"\"\"Called at the beginning of each training batch.\"\"\"\n    if trainer.is_global_zero:\n        self.send(\"batch_start\", batch=batch_idx)\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_end","title":"<code>on_train_end(trainer, pl_module)</code>","text":"<p>Called at the end of training process.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_end(self, trainer, pl_module):\n    \"\"\"Called at the end of training process.\"\"\"\n    if trainer.is_global_zero:\n        self.send(\"train_end\")\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Called at the end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        logs = trainer.callback_metrics\n        self.send(\n            \"epoch_end\", epoch=trainer.current_epoch, logs=self._sanitize_logs(logs)\n        )\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_epoch_start","title":"<code>on_train_epoch_start(trainer, pl_module)</code>","text":"<p>Called at the beginning of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_start(self, trainer, pl_module):\n    \"\"\"Called at the beginning of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        self.send(\"epoch_begin\", epoch=trainer.current_epoch)\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.on_train_start","title":"<code>on_train_start(trainer, pl_module)</code>","text":"<p>Called at the beginning of training process.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_start(self, trainer, pl_module):\n    \"\"\"Called at the beginning of training process.\"\"\"\n    if trainer.is_global_zero:\n        # Include WandB URL if available\n        wandb_url = None\n        if wandb.run is not None:\n            wandb_url = wandb.run.url\n        self.send(\"train_begin\", wandb_url=wandb_url)\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.ProgressReporterZMQ.send","title":"<code>send(event, logs=None, **kwargs)</code>","text":"<p>Send a message over ZMQ.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def send(self, event: str, logs=None, **kwargs):\n    \"\"\"Send a message over ZMQ.\"\"\"\n    msg = dict(what=self.what, event=event, logs=logs, **kwargs)\n    self.socket.send_string(jsonpickle.encode(msg))\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.SleapProgressBar","title":"<code>SleapProgressBar</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p> <p>Custom progress bar with better formatting for small metric values.</p> <p>The default TQDMProgressBar truncates small floats like 1e-5 to \"0.000\". This subclass formats metrics using scientific notation when appropriate.</p> <p>Methods:</p> Name Description <code>get_metrics</code> <p>Override to format metrics with scientific notation for small values.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class SleapProgressBar(TQDMProgressBar):\n    \"\"\"Custom progress bar with better formatting for small metric values.\n\n    The default TQDMProgressBar truncates small floats like 1e-5 to \"0.000\".\n    This subclass formats metrics using scientific notation when appropriate.\n    \"\"\"\n\n    def get_metrics(\n        self, trainer, pl_module\n    ) -&gt; dict[str, Union[int, str, float, dict[str, float]]]:\n        \"\"\"Override to format metrics with scientific notation for small values.\"\"\"\n        items = super().get_metrics(trainer, pl_module)\n        formatted = {}\n        for k, v in items.items():\n            if isinstance(v, float):\n                # Use scientific notation for very small values\n                if v != 0 and abs(v) &lt; 0.001:\n                    formatted[k] = f\"{v:.2e}\"\n                else:\n                    # Use 4 decimal places for normal values\n                    formatted[k] = f\"{v:.4f}\"\n            else:\n                formatted[k] = v\n        return formatted\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.SleapProgressBar.get_metrics","title":"<code>get_metrics(trainer, pl_module)</code>","text":"<p>Override to format metrics with scientific notation for small values.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def get_metrics(\n    self, trainer, pl_module\n) -&gt; dict[str, Union[int, str, float, dict[str, float]]]:\n    \"\"\"Override to format metrics with scientific notation for small values.\"\"\"\n    items = super().get_metrics(trainer, pl_module)\n    formatted = {}\n    for k, v in items.items():\n        if isinstance(v, float):\n            # Use scientific notation for very small values\n            if v != 0 and abs(v) &lt; 0.001:\n                formatted[k] = f\"{v:.2e}\"\n            else:\n                # Use 4 decimal places for normal values\n                formatted[k] = f\"{v:.4f}\"\n        else:\n            formatted[k] = v\n    return formatted\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ","title":"<code>TrainingControllerZMQ</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Lightning callback to receive control commands during training via ZMQ.</p> <p>This is typically used to allow SLEAP GUI interface (SLEAP LossViewer) to dynamically control the training process (stopping early) by publishing commands to a ZMQ socket.</p> <p>Attributes:</p> Name Type Description <code>address</code> <p>ZMQ socket address to subscribe to.</p> <code>topic</code> <p>Topic filter for messages.</p> <code>timeout</code> <p>Poll timeout in milliseconds when checking for new messages.</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Close zmq socket and context when callback is destroyed.</p> <code>__init__</code> <p>Initialize the controller callback by connecting to the specified ZMQ PUB socket.</p> <code>on_train_batch_end</code> <p>Called at the end of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class TrainingControllerZMQ(Callback):\n    \"\"\"Lightning callback to receive control commands during training via ZMQ.\n\n    This is typically used to allow SLEAP GUI interface (SLEAP LossViewer)\n    to dynamically control the training process (stopping early) by publishing commands to a ZMQ socket.\n\n    Attributes:\n        address: ZMQ socket address to subscribe to.\n        topic: Topic filter for messages.\n        timeout: Poll timeout in milliseconds when checking for new messages.\n    \"\"\"\n\n    def __init__(self, address=\"tcp://127.0.0.1:9000\", topic=\"\", poll_timeout=10):\n        \"\"\"Initialize the controller callback by connecting to the specified ZMQ PUB socket.\"\"\"\n        super().__init__()\n        self.address = address\n        self.topic = topic\n        self.timeout = poll_timeout\n\n        # Initialize ZMQ\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.SUB)\n        self.socket.subscribe(self.topic)\n        self.socket.connect(self.address)\n        logger.info(\n            f\"Training controller subscribed to: {self.address} (topic: {self.topic})\"\n        )\n\n    def __del__(self):\n        \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n        logger.info(\"Closing the training controller socket/context.\")\n        self.socket.close()\n        self.context.term()\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Called at the end of each training batch.\"\"\"\n        if trainer.is_global_zero:\n            if self.socket.poll(self.timeout, zmq.POLLIN):\n                msg = jsonpickle.decode(self.socket.recv_string())\n                logger.info(f\"Received control message: {msg}\")\n\n                # Stop training\n                if msg.get(\"command\") == \"stop\":\n                    trainer.should_stop = True\n\n        # Sync all processes after ZMQ operations\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ.__del__","title":"<code>__del__()</code>","text":"<p>Close zmq socket and context when callback is destroyed.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __del__(self):\n    \"\"\"Close zmq socket and context when callback is destroyed.\"\"\"\n    logger.info(\"Closing the training controller socket/context.\")\n    self.socket.close()\n    self.context.term()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ.__init__","title":"<code>__init__(address='tcp://127.0.0.1:9000', topic='', poll_timeout=10)</code>","text":"<p>Initialize the controller callback by connecting to the specified ZMQ PUB socket.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(self, address=\"tcp://127.0.0.1:9000\", topic=\"\", poll_timeout=10):\n    \"\"\"Initialize the controller callback by connecting to the specified ZMQ PUB socket.\"\"\"\n    super().__init__()\n    self.address = address\n    self.topic = topic\n    self.timeout = poll_timeout\n\n    # Initialize ZMQ\n    self.context = zmq.Context()\n    self.socket = self.context.socket(zmq.SUB)\n    self.socket.subscribe(self.topic)\n    self.socket.connect(self.address)\n    logger.info(\n        f\"Training controller subscribed to: {self.address} (topic: {self.topic})\"\n    )\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.TrainingControllerZMQ.on_train_batch_end","title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)</code>","text":"<p>Called at the end of each training batch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n    \"\"\"Called at the end of each training batch.\"\"\"\n    if trainer.is_global_zero:\n        if self.socket.poll(self.timeout, zmq.POLLIN):\n            msg = jsonpickle.decode(self.socket.recv_string())\n            logger.info(f\"Received control message: {msg}\")\n\n            # Stop training\n            if msg.get(\"command\") == \"stop\":\n                trainer.should_stop = True\n\n    # Sync all processes after ZMQ operations\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.UnifiedVizCallback","title":"<code>UnifiedVizCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Unified callback for all visualization outputs during training.</p> <p>This callback consolidates all visualization functionality into a single callback, eliminating redundant dataset copies and inference runs. It handles: - Local disk saving (matplotlib figures) - WandB logging (multiple modes: direct, boxes, masks) - Model-specific visualizations (PAFs for bottomup, class maps for multi_class_bottomup)</p> <p>Benefits over separate callbacks: - Uses ONE sample per epoch for all visualizations (no dataset deepcopy) - Runs inference ONCE per sample (vs 4-8x in previous implementation) - Outputs to multiple destinations from the same data - Simpler code with less duplication</p> <p>Attributes:</p> Name Type Description <code>model_trainer</code> <p>Reference to the ModelTrainer (for lazy access to lightning_model).</p> <code>train_pipeline</code> <p>Iterator over training visualization dataset.</p> <code>val_pipeline</code> <p>Iterator over validation visualization dataset.</p> <code>model_type</code> <p>Type of model (affects which visualizations are enabled).</p> <code>save_local</code> <p>Whether to save matplotlib figures to disk.</p> <code>local_save_dir</code> <p>Directory for local visualization saves.</p> <code>log_wandb</code> <p>Whether to log visualizations to wandb.</p> <code>wandb_modes</code> <p>List of wandb rendering modes (\"direct\", \"boxes\", \"masks\").</p> <code>wandb_box_size</code> <p>Size of keypoint boxes in pixels (for \"boxes\" mode).</p> <code>wandb_confmap_threshold</code> <p>Threshold for confmap masks (for \"masks\" mode).</p> <code>log_wandb_table</code> <p>Whether to also log to a wandb.Table.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the unified visualization callback.</p> <code>on_train_epoch_end</code> <p>Generate and output all visualizations at epoch end.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class UnifiedVizCallback(Callback):\n    \"\"\"Unified callback for all visualization outputs during training.\n\n    This callback consolidates all visualization functionality into a single callback,\n    eliminating redundant dataset copies and inference runs. It handles:\n    - Local disk saving (matplotlib figures)\n    - WandB logging (multiple modes: direct, boxes, masks)\n    - Model-specific visualizations (PAFs for bottomup, class maps for multi_class_bottomup)\n\n    Benefits over separate callbacks:\n    - Uses ONE sample per epoch for all visualizations (no dataset deepcopy)\n    - Runs inference ONCE per sample (vs 4-8x in previous implementation)\n    - Outputs to multiple destinations from the same data\n    - Simpler code with less duplication\n\n    Attributes:\n        model_trainer: Reference to the ModelTrainer (for lazy access to lightning_model).\n        train_pipeline: Iterator over training visualization dataset.\n        val_pipeline: Iterator over validation visualization dataset.\n        model_type: Type of model (affects which visualizations are enabled).\n        save_local: Whether to save matplotlib figures to disk.\n        local_save_dir: Directory for local visualization saves.\n        log_wandb: Whether to log visualizations to wandb.\n        wandb_modes: List of wandb rendering modes (\"direct\", \"boxes\", \"masks\").\n        wandb_box_size: Size of keypoint boxes in pixels (for \"boxes\" mode).\n        wandb_confmap_threshold: Threshold for confmap masks (for \"masks\" mode).\n        log_wandb_table: Whether to also log to a wandb.Table.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_trainer,\n        train_dataset,\n        val_dataset,\n        model_type: str,\n        save_local: bool = True,\n        local_save_dir: Optional[Path] = None,\n        log_wandb: bool = False,\n        wandb_modes: Optional[list] = None,\n        wandb_box_size: float = 5.0,\n        wandb_confmap_threshold: float = 0.1,\n        log_wandb_table: bool = False,\n    ):\n        \"\"\"Initialize the unified visualization callback.\n\n        Args:\n            model_trainer: ModelTrainer instance (lightning_model accessed lazily).\n            train_dataset: Training visualization dataset (will be cycled).\n            val_dataset: Validation visualization dataset (will be cycled).\n            model_type: Model type string (e.g., \"bottomup\", \"multi_class_bottomup\").\n            save_local: If True, save matplotlib figures to local_save_dir.\n            local_save_dir: Path to directory for saving visualization images.\n            log_wandb: If True, log visualizations to wandb.\n            wandb_modes: List of wandb rendering modes. Defaults to [\"direct\"].\n            wandb_box_size: Size of keypoint boxes in pixels.\n            wandb_confmap_threshold: Threshold for confidence map masks.\n            log_wandb_table: If True, also log to a wandb.Table.\n        \"\"\"\n        super().__init__()\n        from itertools import cycle\n\n        self.model_trainer = model_trainer\n        self.train_pipeline = cycle(train_dataset)\n        self.val_pipeline = cycle(val_dataset)\n        self.model_type = model_type\n\n        # Local disk config\n        self.save_local = save_local\n        self.local_save_dir = local_save_dir\n\n        # WandB config\n        self.log_wandb = log_wandb\n        self.wandb_modes = wandb_modes or [\"direct\"]\n        self.wandb_box_size = wandb_box_size\n        self.wandb_confmap_threshold = wandb_confmap_threshold\n        self.log_wandb_table = log_wandb_table\n\n        # Auto-enable model-specific visualizations\n        self.viz_pafs = model_type == \"bottomup\"\n        self.viz_class_maps = model_type == \"multi_class_bottomup\"\n\n        # Initialize renderers\n        from sleap_nn.training.utils import MatplotlibRenderer, WandBRenderer\n\n        self._mpl_renderer = MatplotlibRenderer()\n\n        # Create wandb renderers for each enabled mode\n        self._wandb_renderers = {}\n        if log_wandb:\n            for mode in self.wandb_modes:\n                self._wandb_renderers[mode] = WandBRenderer(\n                    mode=mode,\n                    box_size=wandb_box_size,\n                    confmap_threshold=wandb_confmap_threshold,\n                )\n\n    def _get_wandb_logger(self, trainer):\n        \"\"\"Get the WandbLogger from trainer's loggers.\"\"\"\n        from lightning.pytorch.loggers import WandbLogger\n\n        for log in trainer.loggers:\n            if isinstance(log, WandbLogger):\n                return log\n        return None\n\n    def _get_viz_data(self, sample):\n        \"\"\"Get visualization data with all needed fields based on model type.\n\n        Args:\n            sample: A sample from the visualization dataset.\n\n        Returns:\n            VisualizationData with appropriate fields populated.\n        \"\"\"\n        # Build kwargs based on model type\n        kwargs = {}\n        if self.viz_pafs:\n            kwargs[\"include_pafs\"] = True\n        if self.viz_class_maps:\n            kwargs[\"include_class_maps\"] = True\n\n        # Access lightning_model lazily from model_trainer\n        return self.model_trainer.lightning_model.get_visualization_data(\n            sample, **kwargs\n        )\n\n    def _save_local_viz(self, data, prefix: str, epoch: int):\n        \"\"\"Save visualization to local disk.\n\n        Args:\n            data: VisualizationData object.\n            prefix: Filename prefix (e.g., \"train\", \"validation\").\n            epoch: Current epoch number.\n        \"\"\"\n        if not self.save_local or self.local_save_dir is None:\n            return\n\n        # Confmaps visualization\n        fig = self._mpl_renderer.render(data)\n        fig_path = self.local_save_dir / f\"{prefix}.{epoch:04d}.png\"\n        fig.savefig(fig_path, format=\"png\")\n        plt.close(fig)\n\n        # PAFs visualization (for bottomup models)\n        if self.viz_pafs and data.pred_pafs is not None:\n            fig = self._mpl_renderer.render_pafs(data)\n            fig_path = self.local_save_dir / f\"{prefix}.pafs_magnitude.{epoch:04d}.png\"\n            fig.savefig(fig_path, format=\"png\")\n            plt.close(fig)\n\n        # Class maps visualization (for multi_class_bottomup models)\n        if self.viz_class_maps and data.pred_class_maps is not None:\n            fig = self._render_class_maps(data)\n            fig_path = self.local_save_dir / f\"{prefix}.class_maps.{epoch:04d}.png\"\n            fig.savefig(fig_path, format=\"png\")\n            plt.close(fig)\n\n    def _render_class_maps(self, data):\n        \"\"\"Render class maps visualization.\n\n        Args:\n            data: VisualizationData with pred_class_maps populated.\n\n        Returns:\n            A matplotlib Figure object.\n        \"\"\"\n        from sleap_nn.training.utils import plot_img, plot_confmaps\n\n        img = data.image\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(\n            data.pred_class_maps,\n            output_scale=data.pred_class_maps.shape[0] / img.shape[0],\n        )\n        return fig\n\n    def _log_wandb_viz(self, data, prefix: str, epoch: int, wandb_logger):\n        \"\"\"Log visualization to wandb.\n\n        Args:\n            data: VisualizationData object.\n            prefix: Log prefix (e.g., \"train\", \"val\").\n            epoch: Current epoch number.\n            wandb_logger: WandbLogger instance.\n        \"\"\"\n        if not self.log_wandb or wandb_logger is None:\n            return\n\n        from io import BytesIO\n        from PIL import Image as PILImage\n\n        log_dict = {}\n\n        # Render confmaps for each enabled mode\n        for mode_name, renderer in self._wandb_renderers.items():\n            suffix = \"\" if mode_name == \"direct\" else f\"_{mode_name}\"\n            img = renderer.render(data, caption=f\"{prefix.title()} Epoch {epoch}\")\n            log_dict[f\"viz/{prefix}/predictions{suffix}\"] = img\n\n        # PAFs visualization (for bottomup models)\n        if self.viz_pafs and data.pred_pafs is not None:\n            pafs_fig = self._mpl_renderer.render_pafs(data)\n            buf = BytesIO()\n            pafs_fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0)\n            buf.seek(0)\n            plt.close(pafs_fig)\n            pafs_pil = PILImage.open(buf)\n            log_dict[f\"viz/{prefix}/pafs\"] = wandb.Image(\n                pafs_pil, caption=f\"{prefix.title()} PAFs Epoch {epoch}\"\n            )\n\n        # Class maps visualization (for multi_class_bottomup models)\n        if self.viz_class_maps and data.pred_class_maps is not None:\n            class_fig = self._render_class_maps(data)\n            buf = BytesIO()\n            class_fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0)\n            buf.seek(0)\n            plt.close(class_fig)\n            class_pil = PILImage.open(buf)\n            log_dict[f\"viz/{prefix}/class_maps\"] = wandb.Image(\n                class_pil, caption=f\"{prefix.title()} Class Maps Epoch {epoch}\"\n            )\n\n        if log_dict:\n            log_dict[\"epoch\"] = epoch\n            wandb_logger.experiment.log(log_dict, commit=False)\n\n        # Optionally log to table for backwards compatibility\n        if self.log_wandb_table and \"direct\" in self._wandb_renderers:\n            train_img = self._wandb_renderers[\"direct\"].render(\n                data, caption=f\"{prefix.title()} Epoch {epoch}\"\n            )\n            table_data = [[epoch, train_img]]\n            columns = [\"Epoch\", prefix.title()]\n\n            if self.viz_pafs and data.pred_pafs is not None:\n                columns.append(f\"{prefix.title()} PAFs\")\n                table_data[0].append(log_dict.get(f\"viz/{prefix}/pafs\"))\n\n            if self.viz_class_maps and data.pred_class_maps is not None:\n                columns.append(f\"{prefix.title()} Class Maps\")\n                table_data[0].append(log_dict.get(f\"viz/{prefix}/class_maps\"))\n\n            table = wandb.Table(columns=columns, data=table_data)\n            wandb_logger.experiment.log(\n                {f\"predictions_table_{prefix}\": table}, commit=False\n            )\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Generate and output all visualizations at epoch end.\n\n        Args:\n            trainer: PyTorch Lightning trainer.\n            pl_module: Lightning module (not used, we use self.lightning_module).\n        \"\"\"\n        if trainer.is_global_zero:\n            epoch = trainer.current_epoch\n            wandb_logger = self._get_wandb_logger(trainer) if self.log_wandb else None\n\n            # Get ONE sample for train visualization\n            train_sample = next(self.train_pipeline)\n            # Run inference ONCE with all needed data\n            train_data = self._get_viz_data(train_sample)\n            # Output to all destinations\n            self._save_local_viz(train_data, \"train\", epoch)\n            self._log_wandb_viz(train_data, \"train\", epoch, wandb_logger)\n\n            # Same for validation\n            val_sample = next(self.val_pipeline)\n            val_data = self._get_viz_data(val_sample)\n            self._save_local_viz(val_data, \"validation\", epoch)\n            self._log_wandb_viz(val_data, \"val\", epoch, wandb_logger)\n\n        # Sync all processes - barrier must be reached by ALL ranks\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.UnifiedVizCallback.__init__","title":"<code>__init__(model_trainer, train_dataset, val_dataset, model_type, save_local=True, local_save_dir=None, log_wandb=False, wandb_modes=None, wandb_box_size=5.0, wandb_confmap_threshold=0.1, log_wandb_table=False)</code>","text":"<p>Initialize the unified visualization callback.</p> <p>Parameters:</p> Name Type Description Default <code>model_trainer</code> <p>ModelTrainer instance (lightning_model accessed lazily).</p> required <code>train_dataset</code> <p>Training visualization dataset (will be cycled).</p> required <code>val_dataset</code> <p>Validation visualization dataset (will be cycled).</p> required <code>model_type</code> <code>str</code> <p>Model type string (e.g., \"bottomup\", \"multi_class_bottomup\").</p> required <code>save_local</code> <code>bool</code> <p>If True, save matplotlib figures to local_save_dir.</p> <code>True</code> <code>local_save_dir</code> <code>Optional[Path]</code> <p>Path to directory for saving visualization images.</p> <code>None</code> <code>log_wandb</code> <code>bool</code> <p>If True, log visualizations to wandb.</p> <code>False</code> <code>wandb_modes</code> <code>Optional[list]</code> <p>List of wandb rendering modes. Defaults to [\"direct\"].</p> <code>None</code> <code>wandb_box_size</code> <code>float</code> <p>Size of keypoint boxes in pixels.</p> <code>5.0</code> <code>wandb_confmap_threshold</code> <code>float</code> <p>Threshold for confidence map masks.</p> <code>0.1</code> <code>log_wandb_table</code> <code>bool</code> <p>If True, also log to a wandb.Table.</p> <code>False</code> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    model_trainer,\n    train_dataset,\n    val_dataset,\n    model_type: str,\n    save_local: bool = True,\n    local_save_dir: Optional[Path] = None,\n    log_wandb: bool = False,\n    wandb_modes: Optional[list] = None,\n    wandb_box_size: float = 5.0,\n    wandb_confmap_threshold: float = 0.1,\n    log_wandb_table: bool = False,\n):\n    \"\"\"Initialize the unified visualization callback.\n\n    Args:\n        model_trainer: ModelTrainer instance (lightning_model accessed lazily).\n        train_dataset: Training visualization dataset (will be cycled).\n        val_dataset: Validation visualization dataset (will be cycled).\n        model_type: Model type string (e.g., \"bottomup\", \"multi_class_bottomup\").\n        save_local: If True, save matplotlib figures to local_save_dir.\n        local_save_dir: Path to directory for saving visualization images.\n        log_wandb: If True, log visualizations to wandb.\n        wandb_modes: List of wandb rendering modes. Defaults to [\"direct\"].\n        wandb_box_size: Size of keypoint boxes in pixels.\n        wandb_confmap_threshold: Threshold for confidence map masks.\n        log_wandb_table: If True, also log to a wandb.Table.\n    \"\"\"\n    super().__init__()\n    from itertools import cycle\n\n    self.model_trainer = model_trainer\n    self.train_pipeline = cycle(train_dataset)\n    self.val_pipeline = cycle(val_dataset)\n    self.model_type = model_type\n\n    # Local disk config\n    self.save_local = save_local\n    self.local_save_dir = local_save_dir\n\n    # WandB config\n    self.log_wandb = log_wandb\n    self.wandb_modes = wandb_modes or [\"direct\"]\n    self.wandb_box_size = wandb_box_size\n    self.wandb_confmap_threshold = wandb_confmap_threshold\n    self.log_wandb_table = log_wandb_table\n\n    # Auto-enable model-specific visualizations\n    self.viz_pafs = model_type == \"bottomup\"\n    self.viz_class_maps = model_type == \"multi_class_bottomup\"\n\n    # Initialize renderers\n    from sleap_nn.training.utils import MatplotlibRenderer, WandBRenderer\n\n    self._mpl_renderer = MatplotlibRenderer()\n\n    # Create wandb renderers for each enabled mode\n    self._wandb_renderers = {}\n    if log_wandb:\n        for mode in self.wandb_modes:\n            self._wandb_renderers[mode] = WandBRenderer(\n                mode=mode,\n                box_size=wandb_box_size,\n                confmap_threshold=wandb_confmap_threshold,\n            )\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.UnifiedVizCallback.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Generate and output all visualizations at epoch end.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <p>Lightning module (not used, we use self.lightning_module).</p> required Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Generate and output all visualizations at epoch end.\n\n    Args:\n        trainer: PyTorch Lightning trainer.\n        pl_module: Lightning module (not used, we use self.lightning_module).\n    \"\"\"\n    if trainer.is_global_zero:\n        epoch = trainer.current_epoch\n        wandb_logger = self._get_wandb_logger(trainer) if self.log_wandb else None\n\n        # Get ONE sample for train visualization\n        train_sample = next(self.train_pipeline)\n        # Run inference ONCE with all needed data\n        train_data = self._get_viz_data(train_sample)\n        # Output to all destinations\n        self._save_local_viz(train_data, \"train\", epoch)\n        self._log_wandb_viz(train_data, \"train\", epoch, wandb_logger)\n\n        # Same for validation\n        val_sample = next(self.val_pipeline)\n        val_data = self._get_viz_data(val_sample)\n        self._save_local_viz(val_data, \"validation\", epoch)\n        self._log_wandb_viz(val_data, \"val\", epoch, wandb_logger)\n\n    # Sync all processes - barrier must be reached by ALL ranks\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBPredImageLogger","title":"<code>WandBPredImageLogger</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for writing image predictions to wandb as a Table.</p> <p>.. deprecated::     This callback logs images to a wandb.Table which doesn't support     step sliders. Use WandBVizCallback instead for better UX.</p> <p>Attributes:</p> Name Type Description <code>viz_folder</code> <p>Path to viz directory.</p> <code>wandb_run_name</code> <p>WandB run name.</p> <code>is_bottomup</code> <p>If the model type is bottomup or not.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize attributes.</p> <code>on_train_epoch_end</code> <p>Called at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class WandBPredImageLogger(Callback):\n    \"\"\"Callback for writing image predictions to wandb as a Table.\n\n    .. deprecated::\n        This callback logs images to a wandb.Table which doesn't support\n        step sliders. Use WandBVizCallback instead for better UX.\n\n    Attributes:\n        viz_folder: Path to viz directory.\n        wandb_run_name: WandB run name.\n        is_bottomup: If the model type is bottomup or not.\n    \"\"\"\n\n    def __init__(\n        self,\n        viz_folder: str,\n        wandb_run_name: str,\n        is_bottomup: bool = False,\n    ):\n        \"\"\"Initialize attributes.\"\"\"\n        self.viz_folder = viz_folder\n        self.wandb_run_name = wandb_run_name\n        self.is_bottomup = is_bottomup\n        # Callback initialization\n        super().__init__()\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Called at the end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            epoch_num = trainer.current_epoch\n            train_img_path = (\n                Path(self.viz_folder) / f\"train.{epoch_num:04d}.png\"\n            ).as_posix()\n            val_img_path = (\n                Path(self.viz_folder) / f\"validation.{epoch_num:04d}.png\"\n            ).as_posix()\n            train_img = Image.open(train_img_path)\n            val_img = Image.open(val_img_path)\n\n            column_names = [\n                \"Run name\",\n                \"Epoch\",\n                \"Preds on train\",\n                \"Preds on validation\",\n            ]\n            data = [\n                [\n                    f\"{self.wandb_run_name}\",\n                    f\"{epoch_num}\",\n                    wandb.Image(train_img),\n                    wandb.Image(val_img),\n                ]\n            ]\n            if self.is_bottomup:\n                column_names.extend([\"Pafs Preds on train\", \"Pafs Preds on validation\"])\n                data = [\n                    [\n                        f\"{self.wandb_run_name}\",\n                        f\"{epoch_num}\",\n                        wandb.Image(train_img),\n                        wandb.Image(val_img),\n                        wandb.Image(\n                            Image.open(\n                                (\n                                    Path(self.viz_folder)\n                                    / f\"train.pafs_magnitude.{epoch_num:04d}.png\"\n                                ).as_posix()\n                            )\n                        ),\n                        wandb.Image(\n                            Image.open(\n                                (\n                                    Path(self.viz_folder)\n                                    / f\"validation.pafs_magnitude.{epoch_num:04d}.png\"\n                                ).as_posix()\n                            )\n                        ),\n                    ]\n                ]\n            table = wandb.Table(columns=column_names, data=data)\n            # Use commit=False to accumulate with other metrics in this step\n            wandb.log({f\"{self.wandb_run_name}\": table}, commit=False)\n\n        # Sync all processes after wandb logging\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBPredImageLogger.__init__","title":"<code>__init__(viz_folder, wandb_run_name, is_bottomup=False)</code>","text":"<p>Initialize attributes.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    viz_folder: str,\n    wandb_run_name: str,\n    is_bottomup: bool = False,\n):\n    \"\"\"Initialize attributes.\"\"\"\n    self.viz_folder = viz_folder\n    self.wandb_run_name = wandb_run_name\n    self.is_bottomup = is_bottomup\n    # Callback initialization\n    super().__init__()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBPredImageLogger.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Called at the end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        epoch_num = trainer.current_epoch\n        train_img_path = (\n            Path(self.viz_folder) / f\"train.{epoch_num:04d}.png\"\n        ).as_posix()\n        val_img_path = (\n            Path(self.viz_folder) / f\"validation.{epoch_num:04d}.png\"\n        ).as_posix()\n        train_img = Image.open(train_img_path)\n        val_img = Image.open(val_img_path)\n\n        column_names = [\n            \"Run name\",\n            \"Epoch\",\n            \"Preds on train\",\n            \"Preds on validation\",\n        ]\n        data = [\n            [\n                f\"{self.wandb_run_name}\",\n                f\"{epoch_num}\",\n                wandb.Image(train_img),\n                wandb.Image(val_img),\n            ]\n        ]\n        if self.is_bottomup:\n            column_names.extend([\"Pafs Preds on train\", \"Pafs Preds on validation\"])\n            data = [\n                [\n                    f\"{self.wandb_run_name}\",\n                    f\"{epoch_num}\",\n                    wandb.Image(train_img),\n                    wandb.Image(val_img),\n                    wandb.Image(\n                        Image.open(\n                            (\n                                Path(self.viz_folder)\n                                / f\"train.pafs_magnitude.{epoch_num:04d}.png\"\n                            ).as_posix()\n                        )\n                    ),\n                    wandb.Image(\n                        Image.open(\n                            (\n                                Path(self.viz_folder)\n                                / f\"validation.pafs_magnitude.{epoch_num:04d}.png\"\n                            ).as_posix()\n                        )\n                    ),\n                ]\n            ]\n        table = wandb.Table(columns=column_names, data=data)\n        # Use commit=False to accumulate with other metrics in this step\n        wandb.log({f\"{self.wandb_run_name}\": table}, commit=False)\n\n    # Sync all processes after wandb logging\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBVizCallback","title":"<code>WandBVizCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for logging visualization images directly to wandb with slider support.</p> <p>This callback logs images using wandb.log() which enables step slider navigation in the wandb UI. Multiple visualization modes can be enabled simultaneously: - viz_enabled: Pre-render with matplotlib (same as disk viz) - viz_boxes: Interactive keypoint boxes with filtering - viz_masks: Confidence map overlay with per-node toggling</p> <p>Attributes:</p> Name Type Description <code>train_viz_fn</code> <p>Function that returns VisualizationData for training sample.</p> <code>val_viz_fn</code> <p>Function that returns VisualizationData for validation sample.</p> <code>viz_enabled</code> <p>Whether to log pre-rendered matplotlib images.</p> <code>viz_boxes</code> <p>Whether to log interactive keypoint boxes.</p> <code>viz_masks</code> <p>Whether to log confidence map overlay masks.</p> <code>box_size</code> <p>Size of keypoint boxes in pixels (for viz_boxes).</p> <code>confmap_threshold</code> <p>Threshold for confmap masks (for viz_masks).</p> <code>log_table</code> <p>Whether to also log to a wandb.Table (backwards compat).</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the callback.</p> <code>on_train_epoch_end</code> <p>Log visualization images at end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class WandBVizCallback(Callback):\n    \"\"\"Callback for logging visualization images directly to wandb with slider support.\n\n    This callback logs images using wandb.log() which enables step slider navigation\n    in the wandb UI. Multiple visualization modes can be enabled simultaneously:\n    - viz_enabled: Pre-render with matplotlib (same as disk viz)\n    - viz_boxes: Interactive keypoint boxes with filtering\n    - viz_masks: Confidence map overlay with per-node toggling\n\n    Attributes:\n        train_viz_fn: Function that returns VisualizationData for training sample.\n        val_viz_fn: Function that returns VisualizationData for validation sample.\n        viz_enabled: Whether to log pre-rendered matplotlib images.\n        viz_boxes: Whether to log interactive keypoint boxes.\n        viz_masks: Whether to log confidence map overlay masks.\n        box_size: Size of keypoint boxes in pixels (for viz_boxes).\n        confmap_threshold: Threshold for confmap masks (for viz_masks).\n        log_table: Whether to also log to a wandb.Table (backwards compat).\n    \"\"\"\n\n    def __init__(\n        self,\n        train_viz_fn: Callable,\n        val_viz_fn: Callable,\n        viz_enabled: bool = True,\n        viz_boxes: bool = False,\n        viz_masks: bool = False,\n        box_size: float = 5.0,\n        confmap_threshold: float = 0.1,\n        log_table: bool = False,\n    ):\n        \"\"\"Initialize the callback.\n\n        Args:\n            train_viz_fn: Callable that returns VisualizationData for a training sample.\n            val_viz_fn: Callable that returns VisualizationData for a validation sample.\n            viz_enabled: If True, log pre-rendered matplotlib images.\n            viz_boxes: If True, log interactive keypoint boxes.\n            viz_masks: If True, log confidence map overlay masks.\n            box_size: Size of keypoint boxes in pixels (for viz_boxes).\n            confmap_threshold: Threshold for confmap mask generation (for viz_masks).\n            log_table: If True, also log images to a wandb.Table (for backwards compat).\n        \"\"\"\n        super().__init__()\n        self.train_viz_fn = train_viz_fn\n        self.val_viz_fn = val_viz_fn\n        self.viz_enabled = viz_enabled\n        self.viz_boxes = viz_boxes\n        self.viz_masks = viz_masks\n        self.log_table = log_table\n\n        # Import here to avoid circular imports\n        from sleap_nn.training.utils import WandBRenderer\n\n        self.box_size = box_size\n        self.confmap_threshold = confmap_threshold\n\n        # Create renderers for each enabled mode\n        self.renderers = {}\n        if viz_enabled:\n            self.renderers[\"direct\"] = WandBRenderer(\n                mode=\"direct\", box_size=box_size, confmap_threshold=confmap_threshold\n            )\n        if viz_boxes:\n            self.renderers[\"boxes\"] = WandBRenderer(\n                mode=\"boxes\", box_size=box_size, confmap_threshold=confmap_threshold\n            )\n        if viz_masks:\n            self.renderers[\"masks\"] = WandBRenderer(\n                mode=\"masks\", box_size=box_size, confmap_threshold=confmap_threshold\n            )\n\n    def _get_wandb_logger(self, trainer):\n        \"\"\"Get the WandbLogger from trainer's loggers.\"\"\"\n        from lightning.pytorch.loggers import WandbLogger\n\n        for logger in trainer.loggers:\n            if isinstance(logger, WandbLogger):\n                return logger\n        return None\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Log visualization images at end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            epoch = trainer.current_epoch\n\n            # Get the wandb logger to use its experiment for logging\n            wandb_logger = self._get_wandb_logger(trainer)\n\n            # Only do visualization work if wandb logger is available\n            if wandb_logger is not None:\n                # Get visualization data\n                train_data = self.train_viz_fn()\n                val_data = self.val_viz_fn()\n\n                # Render and log for each enabled mode\n                # Use the logger's experiment to let Lightning manage step tracking\n                log_dict = {}\n                for mode_name, renderer in self.renderers.items():\n                    suffix = \"\" if mode_name == \"direct\" else f\"_{mode_name}\"\n                    train_img = renderer.render(\n                        train_data, caption=f\"Train Epoch {epoch}\"\n                    )\n                    val_img = renderer.render(val_data, caption=f\"Val Epoch {epoch}\")\n                    log_dict[f\"viz/train/predictions{suffix}\"] = train_img\n                    log_dict[f\"viz/val/predictions{suffix}\"] = val_img\n\n                if log_dict:\n                    # Include epoch so wandb can use it as x-axis (via define_metric)\n                    log_dict[\"epoch\"] = epoch\n                    # Use commit=False to accumulate with other metrics in this step\n                    # Lightning will commit when it logs its own metrics\n                    wandb_logger.experiment.log(log_dict, commit=False)\n\n                # Optionally also log to table for backwards compat\n                if self.log_table and \"direct\" in self.renderers:\n                    train_img = self.renderers[\"direct\"].render(\n                        train_data, caption=f\"Train Epoch {epoch}\"\n                    )\n                    val_img = self.renderers[\"direct\"].render(\n                        val_data, caption=f\"Val Epoch {epoch}\"\n                    )\n                    table = wandb.Table(\n                        columns=[\"Epoch\", \"Train\", \"Validation\"],\n                        data=[[epoch, train_img, val_img]],\n                    )\n                    wandb_logger.experiment.log(\n                        {\"predictions_table\": table}, commit=False\n                    )\n\n        # Sync all processes - barrier must be reached by ALL ranks\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBVizCallback.__init__","title":"<code>__init__(train_viz_fn, val_viz_fn, viz_enabled=True, viz_boxes=False, viz_masks=False, box_size=5.0, confmap_threshold=0.1, log_table=False)</code>","text":"<p>Initialize the callback.</p> <p>Parameters:</p> Name Type Description Default <code>train_viz_fn</code> <code>Callable</code> <p>Callable that returns VisualizationData for a training sample.</p> required <code>val_viz_fn</code> <code>Callable</code> <p>Callable that returns VisualizationData for a validation sample.</p> required <code>viz_enabled</code> <code>bool</code> <p>If True, log pre-rendered matplotlib images.</p> <code>True</code> <code>viz_boxes</code> <code>bool</code> <p>If True, log interactive keypoint boxes.</p> <code>False</code> <code>viz_masks</code> <code>bool</code> <p>If True, log confidence map overlay masks.</p> <code>False</code> <code>box_size</code> <code>float</code> <p>Size of keypoint boxes in pixels (for viz_boxes).</p> <code>5.0</code> <code>confmap_threshold</code> <code>float</code> <p>Threshold for confmap mask generation (for viz_masks).</p> <code>0.1</code> <code>log_table</code> <code>bool</code> <p>If True, also log images to a wandb.Table (for backwards compat).</p> <code>False</code> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    train_viz_fn: Callable,\n    val_viz_fn: Callable,\n    viz_enabled: bool = True,\n    viz_boxes: bool = False,\n    viz_masks: bool = False,\n    box_size: float = 5.0,\n    confmap_threshold: float = 0.1,\n    log_table: bool = False,\n):\n    \"\"\"Initialize the callback.\n\n    Args:\n        train_viz_fn: Callable that returns VisualizationData for a training sample.\n        val_viz_fn: Callable that returns VisualizationData for a validation sample.\n        viz_enabled: If True, log pre-rendered matplotlib images.\n        viz_boxes: If True, log interactive keypoint boxes.\n        viz_masks: If True, log confidence map overlay masks.\n        box_size: Size of keypoint boxes in pixels (for viz_boxes).\n        confmap_threshold: Threshold for confmap mask generation (for viz_masks).\n        log_table: If True, also log images to a wandb.Table (for backwards compat).\n    \"\"\"\n    super().__init__()\n    self.train_viz_fn = train_viz_fn\n    self.val_viz_fn = val_viz_fn\n    self.viz_enabled = viz_enabled\n    self.viz_boxes = viz_boxes\n    self.viz_masks = viz_masks\n    self.log_table = log_table\n\n    # Import here to avoid circular imports\n    from sleap_nn.training.utils import WandBRenderer\n\n    self.box_size = box_size\n    self.confmap_threshold = confmap_threshold\n\n    # Create renderers for each enabled mode\n    self.renderers = {}\n    if viz_enabled:\n        self.renderers[\"direct\"] = WandBRenderer(\n            mode=\"direct\", box_size=box_size, confmap_threshold=confmap_threshold\n        )\n    if viz_boxes:\n        self.renderers[\"boxes\"] = WandBRenderer(\n            mode=\"boxes\", box_size=box_size, confmap_threshold=confmap_threshold\n        )\n    if viz_masks:\n        self.renderers[\"masks\"] = WandBRenderer(\n            mode=\"masks\", box_size=box_size, confmap_threshold=confmap_threshold\n        )\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBVizCallback.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Log visualization images at end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Log visualization images at end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        epoch = trainer.current_epoch\n\n        # Get the wandb logger to use its experiment for logging\n        wandb_logger = self._get_wandb_logger(trainer)\n\n        # Only do visualization work if wandb logger is available\n        if wandb_logger is not None:\n            # Get visualization data\n            train_data = self.train_viz_fn()\n            val_data = self.val_viz_fn()\n\n            # Render and log for each enabled mode\n            # Use the logger's experiment to let Lightning manage step tracking\n            log_dict = {}\n            for mode_name, renderer in self.renderers.items():\n                suffix = \"\" if mode_name == \"direct\" else f\"_{mode_name}\"\n                train_img = renderer.render(\n                    train_data, caption=f\"Train Epoch {epoch}\"\n                )\n                val_img = renderer.render(val_data, caption=f\"Val Epoch {epoch}\")\n                log_dict[f\"viz/train/predictions{suffix}\"] = train_img\n                log_dict[f\"viz/val/predictions{suffix}\"] = val_img\n\n            if log_dict:\n                # Include epoch so wandb can use it as x-axis (via define_metric)\n                log_dict[\"epoch\"] = epoch\n                # Use commit=False to accumulate with other metrics in this step\n                # Lightning will commit when it logs its own metrics\n                wandb_logger.experiment.log(log_dict, commit=False)\n\n            # Optionally also log to table for backwards compat\n            if self.log_table and \"direct\" in self.renderers:\n                train_img = self.renderers[\"direct\"].render(\n                    train_data, caption=f\"Train Epoch {epoch}\"\n                )\n                val_img = self.renderers[\"direct\"].render(\n                    val_data, caption=f\"Val Epoch {epoch}\"\n                )\n                table = wandb.Table(\n                    columns=[\"Epoch\", \"Train\", \"Validation\"],\n                    data=[[epoch, train_img, val_img]],\n                )\n                wandb_logger.experiment.log(\n                    {\"predictions_table\": table}, commit=False\n                )\n\n    # Sync all processes - barrier must be reached by ALL ranks\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBVizCallbackWithPAFs","title":"<code>WandBVizCallbackWithPAFs</code>","text":"<p>               Bases: <code>WandBVizCallback</code></p> <p>Extended WandBVizCallback that also logs PAF visualizations for bottom-up models.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the callback.</p> <code>on_train_epoch_end</code> <p>Log visualization images including PAFs at end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>class WandBVizCallbackWithPAFs(WandBVizCallback):\n    \"\"\"Extended WandBVizCallback that also logs PAF visualizations for bottom-up models.\"\"\"\n\n    def __init__(\n        self,\n        train_viz_fn: Callable,\n        val_viz_fn: Callable,\n        train_pafs_viz_fn: Callable,\n        val_pafs_viz_fn: Callable,\n        viz_enabled: bool = True,\n        viz_boxes: bool = False,\n        viz_masks: bool = False,\n        box_size: float = 5.0,\n        confmap_threshold: float = 0.1,\n        log_table: bool = False,\n    ):\n        \"\"\"Initialize the callback.\n\n        Args:\n            train_viz_fn: Callable returning VisualizationData for training sample.\n            val_viz_fn: Callable returning VisualizationData for validation sample.\n            train_pafs_viz_fn: Callable returning VisualizationData with PAFs for training.\n            val_pafs_viz_fn: Callable returning VisualizationData with PAFs for validation.\n            viz_enabled: If True, log pre-rendered matplotlib images.\n            viz_boxes: If True, log interactive keypoint boxes.\n            viz_masks: If True, log confidence map overlay masks.\n            box_size: Size of keypoint boxes in pixels.\n            confmap_threshold: Threshold for confmap mask generation.\n            log_table: If True, also log images to a wandb.Table.\n        \"\"\"\n        super().__init__(\n            train_viz_fn=train_viz_fn,\n            val_viz_fn=val_viz_fn,\n            viz_enabled=viz_enabled,\n            viz_boxes=viz_boxes,\n            viz_masks=viz_masks,\n            box_size=box_size,\n            confmap_threshold=confmap_threshold,\n            log_table=log_table,\n        )\n        self.train_pafs_viz_fn = train_pafs_viz_fn\n        self.val_pafs_viz_fn = val_pafs_viz_fn\n\n        # Import here to avoid circular imports\n        from sleap_nn.training.utils import MatplotlibRenderer\n\n        self._mpl_renderer = MatplotlibRenderer()\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Log visualization images including PAFs at end of each epoch.\"\"\"\n        if trainer.is_global_zero:\n            epoch = trainer.current_epoch\n\n            # Get the wandb logger to use its experiment for logging\n            wandb_logger = self._get_wandb_logger(trainer)\n\n            # Only do visualization work if wandb logger is available\n            if wandb_logger is not None:\n                # Get visualization data\n                train_data = self.train_viz_fn()\n                val_data = self.val_viz_fn()\n                train_pafs_data = self.train_pafs_viz_fn()\n                val_pafs_data = self.val_pafs_viz_fn()\n\n                # Render and log for each enabled mode\n                # Use the logger's experiment to let Lightning manage step tracking\n                log_dict = {}\n                for mode_name, renderer in self.renderers.items():\n                    suffix = \"\" if mode_name == \"direct\" else f\"_{mode_name}\"\n                    train_img = renderer.render(\n                        train_data, caption=f\"Train Epoch {epoch}\"\n                    )\n                    val_img = renderer.render(val_data, caption=f\"Val Epoch {epoch}\")\n                    log_dict[f\"viz/train/predictions{suffix}\"] = train_img\n                    log_dict[f\"viz/val/predictions{suffix}\"] = val_img\n\n                # Render PAFs (always use matplotlib/direct for PAFs)\n                from io import BytesIO\n                import matplotlib.pyplot as plt\n                from PIL import Image\n\n                train_pafs_fig = self._mpl_renderer.render_pafs(train_pafs_data)\n                buf = BytesIO()\n                train_pafs_fig.savefig(\n                    buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0\n                )\n                buf.seek(0)\n                plt.close(train_pafs_fig)\n                train_pafs_pil = Image.open(buf)\n                log_dict[\"viz/train/pafs\"] = wandb.Image(\n                    train_pafs_pil, caption=f\"Train PAFs Epoch {epoch}\"\n                )\n\n                val_pafs_fig = self._mpl_renderer.render_pafs(val_pafs_data)\n                buf = BytesIO()\n                val_pafs_fig.savefig(\n                    buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0\n                )\n                buf.seek(0)\n                plt.close(val_pafs_fig)\n                val_pafs_pil = Image.open(buf)\n                log_dict[\"viz/val/pafs\"] = wandb.Image(\n                    val_pafs_pil, caption=f\"Val PAFs Epoch {epoch}\"\n                )\n\n                if log_dict:\n                    # Include epoch so wandb can use it as x-axis (via define_metric)\n                    log_dict[\"epoch\"] = epoch\n                    # Use commit=False to accumulate with other metrics in this step\n                    # Lightning will commit when it logs its own metrics\n                    wandb_logger.experiment.log(log_dict, commit=False)\n\n                # Optionally also log to table\n                if self.log_table and \"direct\" in self.renderers:\n                    train_img = self.renderers[\"direct\"].render(\n                        train_data, caption=f\"Train Epoch {epoch}\"\n                    )\n                    val_img = self.renderers[\"direct\"].render(\n                        val_data, caption=f\"Val Epoch {epoch}\"\n                    )\n                    table = wandb.Table(\n                        columns=[\n                            \"Epoch\",\n                            \"Train\",\n                            \"Validation\",\n                            \"Train PAFs\",\n                            \"Val PAFs\",\n                        ],\n                        data=[\n                            [\n                                epoch,\n                                train_img,\n                                val_img,\n                                log_dict[\"viz/train/pafs\"],\n                                log_dict[\"viz/val/pafs\"],\n                            ]\n                        ],\n                    )\n                    wandb_logger.experiment.log(\n                        {\"predictions_table\": table}, commit=False\n                    )\n\n        # Sync all processes - barrier must be reached by ALL ranks\n        trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBVizCallbackWithPAFs.__init__","title":"<code>__init__(train_viz_fn, val_viz_fn, train_pafs_viz_fn, val_pafs_viz_fn, viz_enabled=True, viz_boxes=False, viz_masks=False, box_size=5.0, confmap_threshold=0.1, log_table=False)</code>","text":"<p>Initialize the callback.</p> <p>Parameters:</p> Name Type Description Default <code>train_viz_fn</code> <code>Callable</code> <p>Callable returning VisualizationData for training sample.</p> required <code>val_viz_fn</code> <code>Callable</code> <p>Callable returning VisualizationData for validation sample.</p> required <code>train_pafs_viz_fn</code> <code>Callable</code> <p>Callable returning VisualizationData with PAFs for training.</p> required <code>val_pafs_viz_fn</code> <code>Callable</code> <p>Callable returning VisualizationData with PAFs for validation.</p> required <code>viz_enabled</code> <code>bool</code> <p>If True, log pre-rendered matplotlib images.</p> <code>True</code> <code>viz_boxes</code> <code>bool</code> <p>If True, log interactive keypoint boxes.</p> <code>False</code> <code>viz_masks</code> <code>bool</code> <p>If True, log confidence map overlay masks.</p> <code>False</code> <code>box_size</code> <code>float</code> <p>Size of keypoint boxes in pixels.</p> <code>5.0</code> <code>confmap_threshold</code> <code>float</code> <p>Threshold for confmap mask generation.</p> <code>0.1</code> <code>log_table</code> <code>bool</code> <p>If True, also log images to a wandb.Table.</p> <code>False</code> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    train_viz_fn: Callable,\n    val_viz_fn: Callable,\n    train_pafs_viz_fn: Callable,\n    val_pafs_viz_fn: Callable,\n    viz_enabled: bool = True,\n    viz_boxes: bool = False,\n    viz_masks: bool = False,\n    box_size: float = 5.0,\n    confmap_threshold: float = 0.1,\n    log_table: bool = False,\n):\n    \"\"\"Initialize the callback.\n\n    Args:\n        train_viz_fn: Callable returning VisualizationData for training sample.\n        val_viz_fn: Callable returning VisualizationData for validation sample.\n        train_pafs_viz_fn: Callable returning VisualizationData with PAFs for training.\n        val_pafs_viz_fn: Callable returning VisualizationData with PAFs for validation.\n        viz_enabled: If True, log pre-rendered matplotlib images.\n        viz_boxes: If True, log interactive keypoint boxes.\n        viz_masks: If True, log confidence map overlay masks.\n        box_size: Size of keypoint boxes in pixels.\n        confmap_threshold: Threshold for confmap mask generation.\n        log_table: If True, also log images to a wandb.Table.\n    \"\"\"\n    super().__init__(\n        train_viz_fn=train_viz_fn,\n        val_viz_fn=val_viz_fn,\n        viz_enabled=viz_enabled,\n        viz_boxes=viz_boxes,\n        viz_masks=viz_masks,\n        box_size=box_size,\n        confmap_threshold=confmap_threshold,\n        log_table=log_table,\n    )\n    self.train_pafs_viz_fn = train_pafs_viz_fn\n    self.val_pafs_viz_fn = val_pafs_viz_fn\n\n    # Import here to avoid circular imports\n    from sleap_nn.training.utils import MatplotlibRenderer\n\n    self._mpl_renderer = MatplotlibRenderer()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.WandBVizCallbackWithPAFs.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Log visualization images including PAFs at end of each epoch.</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer, pl_module):\n    \"\"\"Log visualization images including PAFs at end of each epoch.\"\"\"\n    if trainer.is_global_zero:\n        epoch = trainer.current_epoch\n\n        # Get the wandb logger to use its experiment for logging\n        wandb_logger = self._get_wandb_logger(trainer)\n\n        # Only do visualization work if wandb logger is available\n        if wandb_logger is not None:\n            # Get visualization data\n            train_data = self.train_viz_fn()\n            val_data = self.val_viz_fn()\n            train_pafs_data = self.train_pafs_viz_fn()\n            val_pafs_data = self.val_pafs_viz_fn()\n\n            # Render and log for each enabled mode\n            # Use the logger's experiment to let Lightning manage step tracking\n            log_dict = {}\n            for mode_name, renderer in self.renderers.items():\n                suffix = \"\" if mode_name == \"direct\" else f\"_{mode_name}\"\n                train_img = renderer.render(\n                    train_data, caption=f\"Train Epoch {epoch}\"\n                )\n                val_img = renderer.render(val_data, caption=f\"Val Epoch {epoch}\")\n                log_dict[f\"viz/train/predictions{suffix}\"] = train_img\n                log_dict[f\"viz/val/predictions{suffix}\"] = val_img\n\n            # Render PAFs (always use matplotlib/direct for PAFs)\n            from io import BytesIO\n            import matplotlib.pyplot as plt\n            from PIL import Image\n\n            train_pafs_fig = self._mpl_renderer.render_pafs(train_pafs_data)\n            buf = BytesIO()\n            train_pafs_fig.savefig(\n                buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0\n            )\n            buf.seek(0)\n            plt.close(train_pafs_fig)\n            train_pafs_pil = Image.open(buf)\n            log_dict[\"viz/train/pafs\"] = wandb.Image(\n                train_pafs_pil, caption=f\"Train PAFs Epoch {epoch}\"\n            )\n\n            val_pafs_fig = self._mpl_renderer.render_pafs(val_pafs_data)\n            buf = BytesIO()\n            val_pafs_fig.savefig(\n                buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0\n            )\n            buf.seek(0)\n            plt.close(val_pafs_fig)\n            val_pafs_pil = Image.open(buf)\n            log_dict[\"viz/val/pafs\"] = wandb.Image(\n                val_pafs_pil, caption=f\"Val PAFs Epoch {epoch}\"\n            )\n\n            if log_dict:\n                # Include epoch so wandb can use it as x-axis (via define_metric)\n                log_dict[\"epoch\"] = epoch\n                # Use commit=False to accumulate with other metrics in this step\n                # Lightning will commit when it logs its own metrics\n                wandb_logger.experiment.log(log_dict, commit=False)\n\n            # Optionally also log to table\n            if self.log_table and \"direct\" in self.renderers:\n                train_img = self.renderers[\"direct\"].render(\n                    train_data, caption=f\"Train Epoch {epoch}\"\n                )\n                val_img = self.renderers[\"direct\"].render(\n                    val_data, caption=f\"Val Epoch {epoch}\"\n                )\n                table = wandb.Table(\n                    columns=[\n                        \"Epoch\",\n                        \"Train\",\n                        \"Validation\",\n                        \"Train PAFs\",\n                        \"Val PAFs\",\n                    ],\n                    data=[\n                        [\n                            epoch,\n                            train_img,\n                            val_img,\n                            log_dict[\"viz/train/pafs\"],\n                            log_dict[\"viz/val/pafs\"],\n                        ]\n                    ],\n                )\n                wandb_logger.experiment.log(\n                    {\"predictions_table\": table}, commit=False\n                )\n\n    # Sync all processes - barrier must be reached by ALL ranks\n    trainer.strategy.barrier()\n</code></pre>"},{"location":"api/training/callbacks/#sleap_nn.training.callbacks.match_centroids","title":"<code>match_centroids(pred_centroids, gt_centroids, max_distance=50.0)</code>","text":"<p>Match predicted centroids to ground truth using Hungarian algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>pred_centroids</code> <code>ndarray</code> <p>Predicted centroid locations, shape (n_pred, 2).</p> required <code>gt_centroids</code> <code>ndarray</code> <p>Ground truth centroid locations, shape (n_gt, 2).</p> required <code>max_distance</code> <code>float</code> <p>Maximum distance threshold for valid matches (in pixels).</p> <code>50.0</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of:     - matched_pred_indices: Indices of matched predictions     - matched_gt_indices: Indices of matched ground truth     - unmatched_pred_indices: Indices of unmatched predictions (false positives)     - unmatched_gt_indices: Indices of unmatched ground truth (false negatives)</p> Source code in <code>sleap_nn/training/callbacks.py</code> <pre><code>def match_centroids(\n    pred_centroids: \"np.ndarray\",\n    gt_centroids: \"np.ndarray\",\n    max_distance: float = 50.0,\n) -&gt; tuple:\n    \"\"\"Match predicted centroids to ground truth using Hungarian algorithm.\n\n    Args:\n        pred_centroids: Predicted centroid locations, shape (n_pred, 2).\n        gt_centroids: Ground truth centroid locations, shape (n_gt, 2).\n        max_distance: Maximum distance threshold for valid matches (in pixels).\n\n    Returns:\n        Tuple of:\n            - matched_pred_indices: Indices of matched predictions\n            - matched_gt_indices: Indices of matched ground truth\n            - unmatched_pred_indices: Indices of unmatched predictions (false positives)\n            - unmatched_gt_indices: Indices of unmatched ground truth (false negatives)\n    \"\"\"\n    import numpy as np\n    from scipy.optimize import linear_sum_assignment\n    from scipy.spatial.distance import cdist\n\n    n_pred = len(pred_centroids)\n    n_gt = len(gt_centroids)\n\n    # Handle edge cases\n    if n_pred == 0 and n_gt == 0:\n        return np.array([]), np.array([]), np.array([]), np.array([])\n    if n_pred == 0:\n        return np.array([]), np.array([]), np.array([]), np.arange(n_gt)\n    if n_gt == 0:\n        return np.array([]), np.array([]), np.arange(n_pred), np.array([])\n\n    # Compute pairwise distances\n    cost_matrix = cdist(pred_centroids, gt_centroids)\n\n    # Run Hungarian algorithm for optimal matching\n    pred_indices, gt_indices = linear_sum_assignment(cost_matrix)\n\n    # Filter matches that exceed max_distance\n    matched_pred = []\n    matched_gt = []\n    for p_idx, g_idx in zip(pred_indices, gt_indices):\n        if cost_matrix[p_idx, g_idx] &lt;= max_distance:\n            matched_pred.append(p_idx)\n            matched_gt.append(g_idx)\n\n    matched_pred = np.array(matched_pred)\n    matched_gt = np.array(matched_gt)\n\n    # Find unmatched indices\n    all_pred = set(range(n_pred))\n    all_gt = set(range(n_gt))\n    unmatched_pred = np.array(list(all_pred - set(matched_pred)))\n    unmatched_gt = np.array(list(all_gt - set(matched_gt)))\n\n    return matched_pred, matched_gt, unmatched_pred, unmatched_gt\n</code></pre>"},{"location":"api/training/lightning_modules/","title":"lightning_modules","text":""},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules","title":"<code>sleap_nn.training.lightning_modules</code>","text":"<p>This module has the LightningModule classes for all model types.</p> <p>Classes:</p> Name Description <code>BottomUpLightningModule</code> <p>Lightning Module for BottomUp Model.</p> <code>BottomUpMultiClassLightningModule</code> <p>Lightning Module for BottomUp ID Model.</p> <code>CentroidLightningModule</code> <p>Lightning Module for Centroid Model.</p> <code>LightningModel</code> <p>Base PyTorch Lightning Module for all sleap-nn models.</p> <code>SingleInstanceLightningModule</code> <p>Lightning Module for SingleInstance Model.</p> <code>TopDownCenteredInstanceLightningModule</code> <p>Lightning Module for TopDownCenteredInstance Model.</p> <code>TopDownCenteredInstanceMultiClassLightningModule</code> <p>Lightning Module for TopDownCenteredInstance ID Model.</p>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule","title":"<code>BottomUpLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for BottomUp Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to BottomUp model. Bottom-Up models predict all keypoints simultaneously and use Part Affinity Fields (PAFs) to group keypoints into individual animals.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_visualization_data</code> <p>Extract visualization data from a sample.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> <code>visualize_pafs_example</code> <p>Visualize PAF predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class BottomUpLightningModule(LightningModel):\n    \"\"\"Lightning Module for BottomUp Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to BottomUp model. Bottom-Up models predict all keypoints\n    simultaneously and use Part Affinity Fields (PAFs) to group keypoints into individual animals.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        paf_scorer = PAFScorer(\n            part_names=self.head_configs.bottomup.confmaps.part_names,\n            edges=self.head_configs.bottomup.pafs.edges,\n            pafs_stride=self.head_configs.bottomup.pafs.output_stride,\n        )\n        self.bottomup_inf_layer = BottomUpInferenceModel(\n            torch_model=self.forward,\n            paf_scorer=paf_scorer,\n            peak_threshold=0.1,  # Lower threshold for epoch-end eval during training\n            input_scale=1.0,\n            return_confmaps=True,\n            return_pafs=True,\n            cms_output_stride=self.head_configs.bottomup.confmaps.output_stride,\n            pafs_output_stride=self.head_configs.bottomup.pafs.output_stride,\n            max_peaks_per_node=100,  # Prevents combinatorial explosion in early training\n        )\n        self.node_names = list(self.head_configs.bottomup.confmaps.part_names)\n\n    def get_visualization_data(\n        self, sample, include_pafs: bool = False\n    ) -&gt; VisualizationData:\n        \"\"\"Extract visualization data from a sample.\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.bottomup_inf_layer(ex)[0]\n\n        peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n        peak_values = output[\"pred_peak_values\"][0].cpu().numpy()\n        img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        gt_instances = ex[\"instances\"][0].cpu().numpy()\n        confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n        pred_pafs = None\n        if include_pafs:\n            pafs = output[\"pred_part_affinity_fields\"].cpu().numpy()[0]\n            pred_pafs = pafs  # (h, w, 2*edges)\n\n        return VisualizationData(\n            image=img,\n            pred_confmaps=confmaps,\n            pred_peaks=peaks,\n            pred_peak_values=peak_values,\n            gt_instances=gt_instances,\n            node_names=self.node_names,\n            output_scale=confmaps.shape[0] / img.shape[0],\n            is_paired=False,\n            pred_pafs=pred_pafs,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n        plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n        plt.xlim(plt.xlim())\n        plt.ylim(plt.ylim())\n        plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n        return fig\n\n    def visualize_pafs_example(self, sample):\n        \"\"\"Visualize PAF predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample, include_pafs=True)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n\n        pafs = data.pred_pafs\n        pafs = pafs.reshape((pafs.shape[0], pafs.shape[1], -1, 2))\n        pafs_mag = np.sqrt(pafs[..., 0] ** 2 + pafs[..., 1] ** 2)\n        plot_confmaps(pafs_mag, output_scale=pafs_mag.shape[0] / data.image.shape[0])\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        img = normalize_on_gpu(img)\n        output = self.model(img)\n        return {\n            \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n            \"PartAffinityFieldsHead\": output[\"PartAffinityFieldsHead\"],\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_paf = batch[\"part_affinity_fields\"]\n        X = normalize_on_gpu(X)\n        preds = self.model(X)\n        pafs = preds[\"PartAffinityFieldsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            pafs_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_paf,\n                y_pr=pafs,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n            pafs_loss += pafs_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"PartAffinityFieldsHead\": pafs_loss,\n        }\n        loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        # Log step-level loss (every batch, uses global_step x-axis)\n        self.log(\n            \"loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n            sync_dist=True,\n        )\n        # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n        self._accumulate_loss(loss)\n        self.log(\n            \"train/confmaps_loss\",\n            confmap_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"train/paf_loss\",\n            pafs_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_paf = batch[\"part_affinity_fields\"]\n        X = normalize_on_gpu(X)\n\n        preds = self.model(X)\n        pafs = preds[\"PartAffinityFieldsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            pafs_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_paf,\n                y_pr=pafs,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n            pafs_loss += pafs_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"PartAffinityFieldsHead\": pafs_loss,\n        }\n\n        val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        self.log(\n            \"val/loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val/confmaps_loss\",\n            confmap_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val/paf_loss\",\n            pafs_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Collect predictions for epoch-end evaluation if enabled\n        if self._collect_val_predictions:\n            with torch.no_grad():\n                # Note: Do NOT squeeze the image here - the forward() method expects\n                # (batch, n_samples, C, H, W) and handles the n_samples squeeze internally\n                inference_output = self.bottomup_inf_layer(batch)\n                if isinstance(inference_output, list):\n                    inference_output = inference_output[0]\n\n            batch_size = len(batch[\"frame_idx\"])\n            for i in range(batch_size):\n                eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n                # Predictions are already in original space (variable number of instances)\n                pred_peaks = inference_output[\"pred_instance_peaks\"][i]\n                pred_scores = inference_output[\"pred_peak_values\"][i]\n                if torch.is_tensor(pred_peaks):\n                    pred_peaks = pred_peaks.cpu().numpy()\n                if torch.is_tensor(pred_scores):\n                    pred_scores = pred_scores.cpu().numpy()\n\n                # Transform GT to original space\n                # Note: instances have shape (1, max_inst, n_nodes, 2) - squeeze n_samples dim\n                gt_prep = batch[\"instances\"][i].cpu().numpy()\n                if gt_prep.ndim == 4:\n                    gt_prep = gt_prep.squeeze(0)  # (max_inst, n_nodes, 2)\n                gt_orig = gt_prep / eff\n                num_inst = batch[\"num_instances\"][i].item()\n                gt_orig = gt_orig[:num_inst]  # Only valid instances\n\n                self.val_predictions.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"pred_peaks\": pred_peaks,  # Original space, variable instances\n                        \"pred_scores\": pred_scores,\n                    }\n                )\n                self.val_ground_truth.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"gt_instances\": gt_orig,  # Original space\n                        \"num_instances\": num_inst,\n                    }\n                )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    paf_scorer = PAFScorer(\n        part_names=self.head_configs.bottomup.confmaps.part_names,\n        edges=self.head_configs.bottomup.pafs.edges,\n        pafs_stride=self.head_configs.bottomup.pafs.output_stride,\n    )\n    self.bottomup_inf_layer = BottomUpInferenceModel(\n        torch_model=self.forward,\n        paf_scorer=paf_scorer,\n        peak_threshold=0.1,  # Lower threshold for epoch-end eval during training\n        input_scale=1.0,\n        return_confmaps=True,\n        return_pafs=True,\n        cms_output_stride=self.head_configs.bottomup.confmaps.output_stride,\n        pafs_output_stride=self.head_configs.bottomup.pafs.output_stride,\n        max_peaks_per_node=100,  # Prevents combinatorial explosion in early training\n    )\n    self.node_names = list(self.head_configs.bottomup.confmaps.part_names)\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    img = normalize_on_gpu(img)\n    output = self.model(img)\n    return {\n        \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n        \"PartAffinityFieldsHead\": output[\"PartAffinityFieldsHead\"],\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.get_visualization_data","title":"<code>get_visualization_data(sample, include_pafs=False)</code>","text":"<p>Extract visualization data from a sample.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def get_visualization_data(\n    self, sample, include_pafs: bool = False\n) -&gt; VisualizationData:\n    \"\"\"Extract visualization data from a sample.\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.bottomup_inf_layer(ex)[0]\n\n    peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n    peak_values = output[\"pred_peak_values\"][0].cpu().numpy()\n    img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    gt_instances = ex[\"instances\"][0].cpu().numpy()\n    confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n    pred_pafs = None\n    if include_pafs:\n        pafs = output[\"pred_part_affinity_fields\"].cpu().numpy()[0]\n        pred_pafs = pafs  # (h, w, 2*edges)\n\n    return VisualizationData(\n        image=img,\n        pred_confmaps=confmaps,\n        pred_peaks=peaks,\n        pred_peak_values=peak_values,\n        gt_instances=gt_instances,\n        node_names=self.node_names,\n        output_scale=confmaps.shape[0] / img.shape[0],\n        is_paired=False,\n        pred_pafs=pred_pafs,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_paf = batch[\"part_affinity_fields\"]\n    X = normalize_on_gpu(X)\n    preds = self.model(X)\n    pafs = preds[\"PartAffinityFieldsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        pafs_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_paf,\n            y_pr=pafs,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n        pafs_loss += pafs_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"PartAffinityFieldsHead\": pafs_loss,\n    }\n    loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    # Log step-level loss (every batch, uses global_step x-axis)\n    self.log(\n        \"loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n        sync_dist=True,\n    )\n    # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n    self._accumulate_loss(loss)\n    self.log(\n        \"train/confmaps_loss\",\n        confmap_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"train/paf_loss\",\n        pafs_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_paf = batch[\"part_affinity_fields\"]\n    X = normalize_on_gpu(X)\n\n    preds = self.model(X)\n    pafs = preds[\"PartAffinityFieldsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    pafs_loss = nn.MSELoss()(pafs, y_paf)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        pafs_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_paf,\n            y_pr=pafs,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n        pafs_loss += pafs_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"PartAffinityFieldsHead\": pafs_loss,\n    }\n\n    val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    self.log(\n        \"val/loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val/confmaps_loss\",\n        confmap_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val/paf_loss\",\n        pafs_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Collect predictions for epoch-end evaluation if enabled\n    if self._collect_val_predictions:\n        with torch.no_grad():\n            # Note: Do NOT squeeze the image here - the forward() method expects\n            # (batch, n_samples, C, H, W) and handles the n_samples squeeze internally\n            inference_output = self.bottomup_inf_layer(batch)\n            if isinstance(inference_output, list):\n                inference_output = inference_output[0]\n\n        batch_size = len(batch[\"frame_idx\"])\n        for i in range(batch_size):\n            eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n            # Predictions are already in original space (variable number of instances)\n            pred_peaks = inference_output[\"pred_instance_peaks\"][i]\n            pred_scores = inference_output[\"pred_peak_values\"][i]\n            if torch.is_tensor(pred_peaks):\n                pred_peaks = pred_peaks.cpu().numpy()\n            if torch.is_tensor(pred_scores):\n                pred_scores = pred_scores.cpu().numpy()\n\n            # Transform GT to original space\n            # Note: instances have shape (1, max_inst, n_nodes, 2) - squeeze n_samples dim\n            gt_prep = batch[\"instances\"][i].cpu().numpy()\n            if gt_prep.ndim == 4:\n                gt_prep = gt_prep.squeeze(0)  # (max_inst, n_nodes, 2)\n            gt_orig = gt_prep / eff\n            num_inst = batch[\"num_instances\"][i].item()\n            gt_orig = gt_orig[:num_inst]  # Only valid instances\n\n            self.val_predictions.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"pred_peaks\": pred_peaks,  # Original space, variable instances\n                    \"pred_scores\": pred_scores,\n                }\n            )\n            self.val_ground_truth.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"gt_instances\": gt_orig,  # Original space\n                    \"num_instances\": num_inst,\n                }\n            )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n    plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n    plt.xlim(plt.xlim())\n    plt.ylim(plt.ylim())\n    plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpLightningModule.visualize_pafs_example","title":"<code>visualize_pafs_example(sample)</code>","text":"<p>Visualize PAF predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_pafs_example(self, sample):\n    \"\"\"Visualize PAF predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample, include_pafs=True)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n\n    pafs = data.pred_pafs\n    pafs = pafs.reshape((pafs.shape[0], pafs.shape[1], -1, 2))\n    pafs_mag = np.sqrt(pafs[..., 0] ** 2 + pafs[..., 1] ** 2)\n    plot_confmaps(pafs_mag, output_scale=pafs_mag.shape[0] / data.image.shape[0])\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule","title":"<code>BottomUpMultiClassLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for BottomUp ID Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to BottomUp ID model. Multi-Class Bottom-Up models predict all keypoints simultaneously and classify instances using class maps to identify individual animals across frames.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_visualization_data</code> <p>Extract visualization data from a sample.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_class_maps_example</code> <p>Visualize class map predictions during training (used with callbacks).</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class BottomUpMultiClassLightningModule(LightningModel):\n    \"\"\"Lightning Module for BottomUp ID Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to BottomUp ID model. Multi-Class Bottom-Up models predict\n    all keypoints simultaneously and classify instances using class maps to identify\n    individual animals across frames.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n        self.bottomup_inf_layer = BottomUpMultiClassInferenceModel(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            input_scale=1.0,\n            return_confmaps=True,\n            return_class_maps=True,\n            cms_output_stride=self.head_configs.multi_class_bottomup.confmaps.output_stride,\n            class_maps_output_stride=self.head_configs.multi_class_bottomup.class_maps.output_stride,\n        )\n        self.node_names = list(\n            self.head_configs.multi_class_bottomup.confmaps.part_names\n        )\n\n    def get_visualization_data(\n        self, sample, include_class_maps: bool = False\n    ) -&gt; VisualizationData:\n        \"\"\"Extract visualization data from a sample.\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.bottomup_inf_layer(ex)[0]\n\n        peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n        peak_values = output[\"pred_peak_values\"][0].cpu().numpy()\n        img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        gt_instances = ex[\"instances\"][0].cpu().numpy()\n        confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n        pred_class_maps = None\n        if include_class_maps:\n            pred_class_maps = (\n                output[\"pred_class_maps\"].cpu().numpy()[0].transpose(1, 2, 0)\n            )\n\n        return VisualizationData(\n            image=img,\n            pred_confmaps=confmaps,\n            pred_peaks=peaks,\n            pred_peak_values=peak_values,\n            gt_instances=gt_instances,\n            node_names=self.node_names,\n            output_scale=confmaps.shape[0] / img.shape[0],\n            is_paired=False,\n            pred_class_maps=pred_class_maps,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n        plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n        plt.xlim(plt.xlim())\n        plt.ylim(plt.ylim())\n        plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n        return fig\n\n    def visualize_class_maps_example(self, sample):\n        \"\"\"Visualize class map predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample, include_class_maps=True)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n        plot_confmaps(\n            data.pred_class_maps,\n            output_scale=data.pred_class_maps.shape[0] / data.image.shape[0],\n        )\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        img = normalize_on_gpu(img)\n        output = self.model(img)\n        return {\n            \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n            \"ClassMapsHead\": output[\"ClassMapsHead\"],\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n        X = normalize_on_gpu(X)\n        preds = self.model(X)\n        classmaps = preds[\"ClassMapsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"ClassMapsHead\": classmaps_loss,\n        }\n        loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        # Log step-level loss (every batch, uses global_step x-axis)\n        self.log(\n            \"loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n            sync_dist=True,\n        )\n        # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n        self._accumulate_loss(loss)\n        self.log(\n            \"train/confmaps_loss\",\n            confmap_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"train/classmap_loss\",\n            classmaps_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Compute classification accuracy at GT keypoint locations\n        with torch.no_grad():\n            # Get output stride for class maps\n            cms_stride = self.head_configs.multi_class_bottomup.class_maps.output_stride\n\n            # Get GT instances and sample class maps at those locations\n            instances = batch[\"instances\"]  # (batch, n_samples, max_inst, n_nodes, 2)\n            if instances.dim() == 5:\n                instances = instances.squeeze(1)  # (batch, max_inst, n_nodes, 2)\n            num_instances = batch[\"num_instances\"]  # (batch,)\n\n            correct = 0\n            total = 0\n            for b in range(instances.shape[0]):\n                n_inst = num_instances[b].item()\n                for inst_idx in range(n_inst):\n                    for node_idx in range(instances.shape[2]):\n                        # Get keypoint location (in input image space)\n                        kp = instances[b, inst_idx, node_idx]  # (2,) = (x, y)\n                        if torch.isnan(kp).any():\n                            continue\n\n                        # Convert to class map space\n                        x_cm = (\n                            (kp[0] / cms_stride)\n                            .long()\n                            .clamp(0, classmaps.shape[-1] - 1)\n                        )\n                        y_cm = (\n                            (kp[1] / cms_stride)\n                            .long()\n                            .clamp(0, classmaps.shape[-2] - 1)\n                        )\n\n                        # Sample predicted and GT class at this location\n                        pred_class = classmaps[b, :, y_cm, x_cm].argmax()\n                        gt_class = y_classmap[b, :, y_cm, x_cm].argmax()\n\n                        if pred_class == gt_class:\n                            correct += 1\n                        total += 1\n\n            if total &gt; 0:\n                class_accuracy = torch.tensor(correct / total, device=X.device)\n                self.log(\n                    \"train/class_accuracy\",\n                    class_accuracy,\n                    on_step=False,\n                    on_epoch=True,\n                    sync_dist=True,\n                )\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X = torch.squeeze(batch[\"image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n        X = normalize_on_gpu(X)\n\n        preds = self.model(X)\n        classmaps = preds[\"ClassMapsHead\"]\n        confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"MultiInstanceConfmapsHead\": confmap_loss,\n            \"ClassMapsHead\": classmaps_loss,\n        }\n\n        val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        self.log(\n            \"val/loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val/confmaps_loss\",\n            confmap_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val/classmap_loss\",\n            classmaps_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Compute classification accuracy at GT keypoint locations\n        with torch.no_grad():\n            # Get output stride for class maps\n            cms_stride = self.head_configs.multi_class_bottomup.class_maps.output_stride\n\n            # Get GT instances and sample class maps at those locations\n            instances = batch[\"instances\"]  # (batch, n_samples, max_inst, n_nodes, 2)\n            if instances.dim() == 5:\n                instances = instances.squeeze(1)  # (batch, max_inst, n_nodes, 2)\n            num_instances = batch[\"num_instances\"]  # (batch,)\n\n            correct = 0\n            total = 0\n            for b in range(instances.shape[0]):\n                n_inst = num_instances[b].item()\n                for inst_idx in range(n_inst):\n                    for node_idx in range(instances.shape[2]):\n                        # Get keypoint location (in input image space)\n                        kp = instances[b, inst_idx, node_idx]  # (2,) = (x, y)\n                        if torch.isnan(kp).any():\n                            continue\n\n                        # Convert to class map space\n                        x_cm = (\n                            (kp[0] / cms_stride)\n                            .long()\n                            .clamp(0, classmaps.shape[-1] - 1)\n                        )\n                        y_cm = (\n                            (kp[1] / cms_stride)\n                            .long()\n                            .clamp(0, classmaps.shape[-2] - 1)\n                        )\n\n                        # Sample predicted and GT class at this location\n                        pred_class = classmaps[b, :, y_cm, x_cm].argmax()\n                        gt_class = y_classmap[b, :, y_cm, x_cm].argmax()\n\n                        if pred_class == gt_class:\n                            correct += 1\n                        total += 1\n\n            if total &gt; 0:\n                class_accuracy = torch.tensor(correct / total, device=X.device)\n                self.log(\n                    \"val/class_accuracy\",\n                    class_accuracy,\n                    on_step=False,\n                    on_epoch=True,\n                    sync_dist=True,\n                )\n\n        # Collect predictions for epoch-end evaluation if enabled\n        if self._collect_val_predictions:\n            with torch.no_grad():\n                # Note: Do NOT squeeze the image here - the forward() method expects\n                # (batch, n_samples, C, H, W) and handles the n_samples squeeze internally\n                inference_output = self.bottomup_inf_layer(batch)\n                if isinstance(inference_output, list):\n                    inference_output = inference_output[0]\n\n            batch_size = len(batch[\"frame_idx\"])\n            for i in range(batch_size):\n                eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n                # Predictions are already in original space (variable number of instances)\n                pred_peaks = inference_output[\"pred_instance_peaks\"][i]\n                pred_scores = inference_output[\"pred_peak_values\"][i]\n                if torch.is_tensor(pred_peaks):\n                    pred_peaks = pred_peaks.cpu().numpy()\n                if torch.is_tensor(pred_scores):\n                    pred_scores = pred_scores.cpu().numpy()\n\n                # Transform GT to original space\n                # Note: instances have shape (1, max_inst, n_nodes, 2) - squeeze n_samples dim\n                gt_prep = batch[\"instances\"][i].cpu().numpy()\n                if gt_prep.ndim == 4:\n                    gt_prep = gt_prep.squeeze(0)  # (max_inst, n_nodes, 2)\n                gt_orig = gt_prep / eff\n                num_inst = batch[\"num_instances\"][i].item()\n                gt_orig = gt_orig[:num_inst]  # Only valid instances\n\n                self.val_predictions.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"pred_peaks\": pred_peaks,  # Original space, variable instances\n                        \"pred_scores\": pred_scores,\n                    }\n                )\n                self.val_ground_truth.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"gt_instances\": gt_orig,  # Original space\n                        \"num_instances\": num_inst,\n                    }\n                )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n    self.bottomup_inf_layer = BottomUpMultiClassInferenceModel(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        input_scale=1.0,\n        return_confmaps=True,\n        return_class_maps=True,\n        cms_output_stride=self.head_configs.multi_class_bottomup.confmaps.output_stride,\n        class_maps_output_stride=self.head_configs.multi_class_bottomup.class_maps.output_stride,\n    )\n    self.node_names = list(\n        self.head_configs.multi_class_bottomup.confmaps.part_names\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    img = normalize_on_gpu(img)\n    output = self.model(img)\n    return {\n        \"MultiInstanceConfmapsHead\": output[\"MultiInstanceConfmapsHead\"],\n        \"ClassMapsHead\": output[\"ClassMapsHead\"],\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.get_visualization_data","title":"<code>get_visualization_data(sample, include_class_maps=False)</code>","text":"<p>Extract visualization data from a sample.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def get_visualization_data(\n    self, sample, include_class_maps: bool = False\n) -&gt; VisualizationData:\n    \"\"\"Extract visualization data from a sample.\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.bottomup_inf_layer(ex)[0]\n\n    peaks = output[\"pred_instance_peaks\"][0].cpu().numpy()\n    peak_values = output[\"pred_peak_values\"][0].cpu().numpy()\n    img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    gt_instances = ex[\"instances\"][0].cpu().numpy()\n    confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n    pred_class_maps = None\n    if include_class_maps:\n        pred_class_maps = (\n            output[\"pred_class_maps\"].cpu().numpy()[0].transpose(1, 2, 0)\n        )\n\n    return VisualizationData(\n        image=img,\n        pred_confmaps=confmaps,\n        pred_peaks=peaks,\n        pred_peak_values=peak_values,\n        gt_instances=gt_instances,\n        node_names=self.node_names,\n        output_scale=confmaps.shape[0] / img.shape[0],\n        is_paired=False,\n        pred_class_maps=pred_class_maps,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n    X = normalize_on_gpu(X)\n    preds = self.model(X)\n    classmaps = preds[\"ClassMapsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"ClassMapsHead\": classmaps_loss,\n    }\n    loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    # Log step-level loss (every batch, uses global_step x-axis)\n    self.log(\n        \"loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n        sync_dist=True,\n    )\n    # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n    self._accumulate_loss(loss)\n    self.log(\n        \"train/confmaps_loss\",\n        confmap_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"train/classmap_loss\",\n        classmaps_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Compute classification accuracy at GT keypoint locations\n    with torch.no_grad():\n        # Get output stride for class maps\n        cms_stride = self.head_configs.multi_class_bottomup.class_maps.output_stride\n\n        # Get GT instances and sample class maps at those locations\n        instances = batch[\"instances\"]  # (batch, n_samples, max_inst, n_nodes, 2)\n        if instances.dim() == 5:\n            instances = instances.squeeze(1)  # (batch, max_inst, n_nodes, 2)\n        num_instances = batch[\"num_instances\"]  # (batch,)\n\n        correct = 0\n        total = 0\n        for b in range(instances.shape[0]):\n            n_inst = num_instances[b].item()\n            for inst_idx in range(n_inst):\n                for node_idx in range(instances.shape[2]):\n                    # Get keypoint location (in input image space)\n                    kp = instances[b, inst_idx, node_idx]  # (2,) = (x, y)\n                    if torch.isnan(kp).any():\n                        continue\n\n                    # Convert to class map space\n                    x_cm = (\n                        (kp[0] / cms_stride)\n                        .long()\n                        .clamp(0, classmaps.shape[-1] - 1)\n                    )\n                    y_cm = (\n                        (kp[1] / cms_stride)\n                        .long()\n                        .clamp(0, classmaps.shape[-2] - 1)\n                    )\n\n                    # Sample predicted and GT class at this location\n                    pred_class = classmaps[b, :, y_cm, x_cm].argmax()\n                    gt_class = y_classmap[b, :, y_cm, x_cm].argmax()\n\n                    if pred_class == gt_class:\n                        correct += 1\n                    total += 1\n\n        if total &gt; 0:\n            class_accuracy = torch.tensor(correct / total, device=X.device)\n            self.log(\n                \"train/class_accuracy\",\n                class_accuracy,\n                on_step=False,\n                on_epoch=True,\n                sync_dist=True,\n            )\n\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X = torch.squeeze(batch[\"image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classmap = torch.squeeze(batch[\"class_maps\"], dim=1)\n    X = normalize_on_gpu(X)\n\n    preds = self.model(X)\n    classmaps = preds[\"ClassMapsHead\"]\n    confmaps = preds[\"MultiInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classmaps_loss = nn.MSELoss()(classmaps, y_classmap)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"MultiInstanceConfmapsHead\": confmap_loss,\n        \"ClassMapsHead\": classmaps_loss,\n    }\n\n    val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    self.log(\n        \"val/loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val/confmaps_loss\",\n        confmap_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val/classmap_loss\",\n        classmaps_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Compute classification accuracy at GT keypoint locations\n    with torch.no_grad():\n        # Get output stride for class maps\n        cms_stride = self.head_configs.multi_class_bottomup.class_maps.output_stride\n\n        # Get GT instances and sample class maps at those locations\n        instances = batch[\"instances\"]  # (batch, n_samples, max_inst, n_nodes, 2)\n        if instances.dim() == 5:\n            instances = instances.squeeze(1)  # (batch, max_inst, n_nodes, 2)\n        num_instances = batch[\"num_instances\"]  # (batch,)\n\n        correct = 0\n        total = 0\n        for b in range(instances.shape[0]):\n            n_inst = num_instances[b].item()\n            for inst_idx in range(n_inst):\n                for node_idx in range(instances.shape[2]):\n                    # Get keypoint location (in input image space)\n                    kp = instances[b, inst_idx, node_idx]  # (2,) = (x, y)\n                    if torch.isnan(kp).any():\n                        continue\n\n                    # Convert to class map space\n                    x_cm = (\n                        (kp[0] / cms_stride)\n                        .long()\n                        .clamp(0, classmaps.shape[-1] - 1)\n                    )\n                    y_cm = (\n                        (kp[1] / cms_stride)\n                        .long()\n                        .clamp(0, classmaps.shape[-2] - 1)\n                    )\n\n                    # Sample predicted and GT class at this location\n                    pred_class = classmaps[b, :, y_cm, x_cm].argmax()\n                    gt_class = y_classmap[b, :, y_cm, x_cm].argmax()\n\n                    if pred_class == gt_class:\n                        correct += 1\n                    total += 1\n\n        if total &gt; 0:\n            class_accuracy = torch.tensor(correct / total, device=X.device)\n            self.log(\n                \"val/class_accuracy\",\n                class_accuracy,\n                on_step=False,\n                on_epoch=True,\n                sync_dist=True,\n            )\n\n    # Collect predictions for epoch-end evaluation if enabled\n    if self._collect_val_predictions:\n        with torch.no_grad():\n            # Note: Do NOT squeeze the image here - the forward() method expects\n            # (batch, n_samples, C, H, W) and handles the n_samples squeeze internally\n            inference_output = self.bottomup_inf_layer(batch)\n            if isinstance(inference_output, list):\n                inference_output = inference_output[0]\n\n        batch_size = len(batch[\"frame_idx\"])\n        for i in range(batch_size):\n            eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n            # Predictions are already in original space (variable number of instances)\n            pred_peaks = inference_output[\"pred_instance_peaks\"][i]\n            pred_scores = inference_output[\"pred_peak_values\"][i]\n            if torch.is_tensor(pred_peaks):\n                pred_peaks = pred_peaks.cpu().numpy()\n            if torch.is_tensor(pred_scores):\n                pred_scores = pred_scores.cpu().numpy()\n\n            # Transform GT to original space\n            # Note: instances have shape (1, max_inst, n_nodes, 2) - squeeze n_samples dim\n            gt_prep = batch[\"instances\"][i].cpu().numpy()\n            if gt_prep.ndim == 4:\n                gt_prep = gt_prep.squeeze(0)  # (max_inst, n_nodes, 2)\n            gt_orig = gt_prep / eff\n            num_inst = batch[\"num_instances\"][i].item()\n            gt_orig = gt_orig[:num_inst]  # Only valid instances\n\n            self.val_predictions.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"pred_peaks\": pred_peaks,  # Original space, variable instances\n                    \"pred_scores\": pred_scores,\n                }\n            )\n            self.val_ground_truth.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"gt_instances\": gt_orig,  # Original space\n                    \"num_instances\": num_inst,\n                }\n            )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.visualize_class_maps_example","title":"<code>visualize_class_maps_example(sample)</code>","text":"<p>Visualize class map predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_class_maps_example(self, sample):\n    \"\"\"Visualize class map predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample, include_class_maps=True)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n    plot_confmaps(\n        data.pred_class_maps,\n        output_scale=data.pred_class_maps.shape[0] / data.image.shape[0],\n    )\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.BottomUpMultiClassLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n    plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n    plt.xlim(plt.xlim())\n    plt.ylim(plt.ylim())\n    plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule","title":"<code>CentroidLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for Centroid Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to centroid model. Centroid models detect the center points of animals in the image, which are then used by Top-Down models for keypoint prediction.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_visualization_data</code> <p>Extract visualization data from a sample.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class CentroidLightningModule(LightningModel):\n    \"\"\"Lightning Module for Centroid Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to centroid model. Centroid models detect the center points\n    of animals in the image, which are then used by Top-Down models for keypoint prediction.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        self.centroid_inf_layer = CentroidCrop(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            return_confmaps=True,\n            output_stride=self.head_configs.centroid.confmaps.output_stride,\n            input_scale=1.0,\n        )\n        self.node_names = [\"centroid\"]\n\n    def get_visualization_data(self, sample) -&gt; VisualizationData:\n        \"\"\"Extract visualization data from a sample.\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        gt_centroids = ex[\"centroids\"].cpu().numpy()\n        output = self.centroid_inf_layer(ex)\n\n        peaks = output[\"centroids\"][0].cpu().numpy()\n        centroid_vals = output[\"centroid_vals\"][0].cpu().numpy()\n        img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        confmaps = output[\"pred_centroid_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n        return VisualizationData(\n            image=img,\n            pred_confmaps=confmaps,\n            pred_peaks=peaks,\n            pred_peak_values=centroid_vals,\n            gt_instances=gt_centroids,\n            node_names=self.node_names,\n            output_scale=confmaps.shape[0] / img.shape[0],\n            is_paired=False,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n        plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n        plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        img = normalize_on_gpu(img)\n        return self.model(img)[\"CentroidConfmapsHead\"]\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X, y = (\n            torch.squeeze(batch[\"image\"], dim=1),\n            torch.squeeze(batch[\"centroids_confidence_maps\"], dim=1),\n        )\n        X = normalize_on_gpu(X)\n\n        y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n        loss = nn.MSELoss()(y_preds, y)\n        # Log step-level loss (every batch, uses global_step x-axis)\n        self.log(\n            \"loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n            sync_dist=True,\n        )\n        # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n        self._accumulate_loss(loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X, y = (\n            torch.squeeze(batch[\"image\"], dim=1),\n            torch.squeeze(batch[\"centroids_confidence_maps\"], dim=1),\n        )\n        X = normalize_on_gpu(X)\n\n        y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n        val_loss = nn.MSELoss()(y_preds, y)\n        self.log(\n            \"val/loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Collect predictions for epoch-end evaluation if enabled\n        if self._collect_val_predictions:\n            # Save GT centroids before inference (inference overwrites batch[\"centroids\"])\n            batch[\"gt_centroids\"] = batch[\"centroids\"].clone()\n\n            with torch.no_grad():\n                inference_output = self.centroid_inf_layer(batch)\n\n            batch_size = len(batch[\"frame_idx\"])\n            for i in range(batch_size):\n                eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n                # Predictions are in original image space (inference divides by eff_scale)\n                # centroids shape: (batch, 1, max_instances, 2) - squeeze to (max_instances, 2)\n                pred_centroids = (\n                    inference_output[\"centroids\"][i].squeeze(0).cpu().numpy()\n                )\n                pred_vals = inference_output[\"centroid_vals\"][i].cpu().numpy()\n\n                # Transform GT centroids from preprocessed to original image space\n                # Use \"gt_centroids\" since inference overwrites \"centroids\" with predictions\n                gt_centroids_prep = (\n                    batch[\"gt_centroids\"][i].cpu().numpy()\n                )  # (n_samples=1, max_inst, 2)\n                gt_centroids_orig = gt_centroids_prep.squeeze(0) / eff  # (max_inst, 2)\n                num_inst = batch[\"num_instances\"][i].item()\n\n                # Filter to valid instances (non-NaN)\n                valid_pred_mask = ~np.isnan(pred_centroids).any(axis=1)\n                pred_centroids = pred_centroids[valid_pred_mask]\n                pred_vals = pred_vals[valid_pred_mask]\n\n                gt_centroids_valid = gt_centroids_orig[:num_inst]\n\n                self.val_predictions.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"pred_peaks\": pred_centroids.reshape(\n                            -1, 1, 2\n                        ),  # (n_inst, 1, 2)\n                        \"pred_scores\": pred_vals.reshape(-1, 1),  # (n_inst, 1)\n                    }\n                )\n                self.val_ground_truth.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"gt_instances\": gt_centroids_valid.reshape(\n                            -1, 1, 2\n                        ),  # (n_inst, 1, 2)\n                        \"num_instances\": num_inst,\n                    }\n                )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    self.centroid_inf_layer = CentroidCrop(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        return_confmaps=True,\n        output_stride=self.head_configs.centroid.confmaps.output_stride,\n        input_scale=1.0,\n    )\n    self.node_names = [\"centroid\"]\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    img = normalize_on_gpu(img)\n    return self.model(img)[\"CentroidConfmapsHead\"]\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.get_visualization_data","title":"<code>get_visualization_data(sample)</code>","text":"<p>Extract visualization data from a sample.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def get_visualization_data(self, sample) -&gt; VisualizationData:\n    \"\"\"Extract visualization data from a sample.\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    gt_centroids = ex[\"centroids\"].cpu().numpy()\n    output = self.centroid_inf_layer(ex)\n\n    peaks = output[\"centroids\"][0].cpu().numpy()\n    centroid_vals = output[\"centroid_vals\"][0].cpu().numpy()\n    img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    confmaps = output[\"pred_centroid_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n    return VisualizationData(\n        image=img,\n        pred_confmaps=confmaps,\n        pred_peaks=peaks,\n        pred_peak_values=centroid_vals,\n        gt_instances=gt_centroids,\n        node_names=self.node_names,\n        output_scale=confmaps.shape[0] / img.shape[0],\n        is_paired=False,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X, y = (\n        torch.squeeze(batch[\"image\"], dim=1),\n        torch.squeeze(batch[\"centroids_confidence_maps\"], dim=1),\n    )\n    X = normalize_on_gpu(X)\n\n    y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n    loss = nn.MSELoss()(y_preds, y)\n    # Log step-level loss (every batch, uses global_step x-axis)\n    self.log(\n        \"loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n        sync_dist=True,\n    )\n    # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n    self._accumulate_loss(loss)\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X, y = (\n        torch.squeeze(batch[\"image\"], dim=1),\n        torch.squeeze(batch[\"centroids_confidence_maps\"], dim=1),\n    )\n    X = normalize_on_gpu(X)\n\n    y_preds = self.model(X)[\"CentroidConfmapsHead\"]\n    val_loss = nn.MSELoss()(y_preds, y)\n    self.log(\n        \"val/loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Collect predictions for epoch-end evaluation if enabled\n    if self._collect_val_predictions:\n        # Save GT centroids before inference (inference overwrites batch[\"centroids\"])\n        batch[\"gt_centroids\"] = batch[\"centroids\"].clone()\n\n        with torch.no_grad():\n            inference_output = self.centroid_inf_layer(batch)\n\n        batch_size = len(batch[\"frame_idx\"])\n        for i in range(batch_size):\n            eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n            # Predictions are in original image space (inference divides by eff_scale)\n            # centroids shape: (batch, 1, max_instances, 2) - squeeze to (max_instances, 2)\n            pred_centroids = (\n                inference_output[\"centroids\"][i].squeeze(0).cpu().numpy()\n            )\n            pred_vals = inference_output[\"centroid_vals\"][i].cpu().numpy()\n\n            # Transform GT centroids from preprocessed to original image space\n            # Use \"gt_centroids\" since inference overwrites \"centroids\" with predictions\n            gt_centroids_prep = (\n                batch[\"gt_centroids\"][i].cpu().numpy()\n            )  # (n_samples=1, max_inst, 2)\n            gt_centroids_orig = gt_centroids_prep.squeeze(0) / eff  # (max_inst, 2)\n            num_inst = batch[\"num_instances\"][i].item()\n\n            # Filter to valid instances (non-NaN)\n            valid_pred_mask = ~np.isnan(pred_centroids).any(axis=1)\n            pred_centroids = pred_centroids[valid_pred_mask]\n            pred_vals = pred_vals[valid_pred_mask]\n\n            gt_centroids_valid = gt_centroids_orig[:num_inst]\n\n            self.val_predictions.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"pred_peaks\": pred_centroids.reshape(\n                        -1, 1, 2\n                    ),  # (n_inst, 1, 2)\n                    \"pred_scores\": pred_vals.reshape(-1, 1),  # (n_inst, 1)\n                }\n            )\n            self.val_ground_truth.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"gt_instances\": gt_centroids_valid.reshape(\n                        -1, 1, 2\n                    ),  # (n_inst, 1, 2)\n                    \"num_instances\": num_inst,\n                }\n            )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.CentroidLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n    plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n    plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel","title":"<code>LightningModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Base PyTorch Lightning Module for all sleap-nn models.</p> <p>This class is a sub-class of Torch Lightning Module to configure the training and validation steps.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>configure_optimizers</code> <p>Configure optimiser and learning rate scheduler.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_lightning_model_from_config</code> <p>Get lightning model from config.</p> <code>on_train_epoch_end</code> <p>Configure the train timer at the end of every epoch.</p> <code>on_train_epoch_start</code> <p>Configure the train timer at the beginning of each epoch.</p> <code>on_validation_epoch_end</code> <p>Configure the val timer at the end of every epoch.</p> <code>on_validation_epoch_start</code> <p>Configure the val timer at the beginning of each epoch.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class LightningModel(L.LightningModule):\n    \"\"\"Base PyTorch Lightning Module for all sleap-nn models.\n\n    This class is a sub-class of Torch Lightning Module to configure the training and validation steps.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__()\n        self.model_type = model_type\n        self.backbone_type = backbone_type\n        if not isinstance(backbone_config, DictConfig):\n            backbone_cfg = get_backbone_config(backbone_config)\n            config = OmegaConf.structured(backbone_cfg)\n            OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n            config = DictConfig(config)\n        else:\n            config = backbone_config\n        self.backbone_config = config\n        self.head_configs = head_configs\n        self.pretrained_backbone_weights = pretrained_backbone_weights\n        self.pretrained_head_weights = pretrained_head_weights\n        self.in_channels = self.backbone_config[f\"{self.backbone_type}\"][\"in_channels\"]\n        self.input_expand_channels = self.in_channels\n        self.init_weights = init_weights\n        self.lr_scheduler = lr_scheduler\n        self.online_mining = online_mining\n        self.hard_to_easy_ratio = hard_to_easy_ratio\n        self.min_hard_keypoints = min_hard_keypoints\n        self.max_hard_keypoints = max_hard_keypoints\n        self.loss_scale = loss_scale\n        self.optimizer = optimizer\n        self.lr = learning_rate\n        self.amsgrad = amsgrad\n\n        self.model = Model(\n            backbone_type=self.backbone_type,\n            backbone_config=self.backbone_config[f\"{self.backbone_type}\"],\n            head_configs=self.head_configs[self.model_type],\n            model_type=self.model_type,\n        )\n\n        if len(self.head_configs[self.model_type]) &gt; 1:\n            self.loss_weights = [\n                (\n                    self.head_configs[self.model_type][x].loss_weight\n                    if self.head_configs[self.model_type][x].loss_weight is not None\n                    else 1.0\n                )\n                for x in self.head_configs[self.model_type]\n            ]\n\n        self.training_loss = {}\n        self.val_loss = {}\n        self.learning_rate = {}\n\n        # For epoch-averaged loss tracking\n        self._epoch_loss_sum = 0.0\n        self._epoch_loss_count = 0\n\n        # For epoch-end evaluation\n        self.val_predictions: List[Dict] = []\n        self.val_ground_truth: List[Dict] = []\n        self._collect_val_predictions: bool = False\n\n        # Initialization for encoder and decoder stacks.\n        if self.init_weights == \"xavier\":\n            self.model.apply(xavier_init_weights)\n\n        # Pre-trained weights for the encoder stack - only for swint and convnext\n        if self.backbone_type == \"convnext\" or self.backbone_type == \"swint\":\n            if (\n                self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n                is not None\n            ):\n                ckpt = MODEL_WEIGHTS[\n                    self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n                ].DEFAULT.get_state_dict(progress=True, check_hash=True)\n                self.model.backbone.enc.load_state_dict(ckpt, strict=False)\n\n        # Initializing backbone (encoder + decoder) with trained ckpts\n        if self.pretrained_backbone_weights is not None:\n            logger.info(\n                f\"Loading backbone weights from `{self.pretrained_backbone_weights}` ...\"\n            )\n            if self.pretrained_backbone_weights.endswith(\".ckpt\"):\n                ckpt = torch.load(\n                    self.pretrained_backbone_weights,\n                    map_location=\"cpu\",\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".backbone\" in k\n                }\n                self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif self.pretrained_backbone_weights.endswith(\".h5\"):\n                # load from sleap model weights\n                load_legacy_model_weights(\n                    self.model.backbone,\n                    self.pretrained_backbone_weights,\n                    component=\"backbone\",\n                )\n\n            else:\n                message = f\"Unsupported file extension for pretrained backbone weights. Please provide a .ckpt or .h5 file.\"\n                logger.error(message)\n                raise ValueError(message)\n\n        # Initializing head layers with trained ckpts.\n        if self.pretrained_head_weights is not None:\n            logger.info(\n                f\"Loading head weights from `{self.pretrained_head_weights}` ...\"\n            )\n            if self.pretrained_head_weights.endswith(\".ckpt\"):\n                ckpt = torch.load(\n                    self.pretrained_head_weights,\n                    map_location=\"cpu\",\n                    weights_only=False,\n                )\n                ckpt[\"state_dict\"] = {\n                    k: ckpt[\"state_dict\"][k]\n                    for k in ckpt[\"state_dict\"].keys()\n                    if \".head_layers\" in k\n                }\n                self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n            elif self.pretrained_head_weights.endswith(\".h5\"):\n                # load from sleap model weights\n                load_legacy_model_weights(\n                    self.model.head_layers,\n                    self.pretrained_head_weights,\n                    component=\"head\",\n                )\n\n            else:\n                message = f\"Unsupported file extension for pretrained head weights. Please provide a .ckpt or .h5 file.\"\n                logger.error(message)\n                raise ValueError(message)\n\n    @classmethod\n    def get_lightning_model_from_config(cls, config: DictConfig):\n        \"\"\"Get lightning model from config.\"\"\"\n        model_type = get_model_type_from_cfg(config)\n        backbone_type = get_backbone_type_from_cfg(config)\n\n        lightning_models = {\n            \"single_instance\": SingleInstanceLightningModule,\n            \"centroid\": CentroidLightningModule,\n            \"centered_instance\": TopDownCenteredInstanceLightningModule,\n            \"bottomup\": BottomUpLightningModule,\n            \"multi_class_bottomup\": BottomUpMultiClassLightningModule,\n            \"multi_class_topdown\": TopDownCenteredInstanceMultiClassLightningModule,\n        }\n\n        if model_type not in lightning_models:\n            message = f\"Incorrect model type. Please check if one of the following keys in the head configs is not None: [`single_instance`, `centroid`, `centered_instance`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`]\"\n            logger.error(message)\n            raise ValueError(message)\n\n        lightning_model = lightning_models[model_type](\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=config.model_config.backbone_config,\n            head_configs=config.model_config.head_configs,\n            pretrained_backbone_weights=config.model_config.pretrained_backbone_weights,\n            pretrained_head_weights=config.model_config.pretrained_head_weights,\n            init_weights=config.model_config.init_weights,\n            lr_scheduler=config.trainer_config.lr_scheduler,\n            online_mining=config.trainer_config.online_hard_keypoint_mining.online_mining,\n            hard_to_easy_ratio=config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n            min_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n            max_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n            loss_scale=config.trainer_config.online_hard_keypoint_mining.loss_scale,\n            optimizer=config.trainer_config.optimizer_name,\n            learning_rate=config.trainer_config.optimizer.lr,\n            amsgrad=config.trainer_config.optimizer.amsgrad,\n        )\n\n        return lightning_model\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        pass\n\n    def on_train_epoch_start(self):\n        \"\"\"Configure the train timer at the beginning of each epoch.\"\"\"\n        self.train_start_time = time.time()\n        # Reset epoch loss tracking\n        self._epoch_loss_sum = 0.0\n        self._epoch_loss_count = 0\n\n    def _accumulate_loss(self, loss: torch.Tensor):\n        \"\"\"Accumulate loss for epoch-averaged logging. Call this in training_step.\"\"\"\n        self._epoch_loss_sum += loss.detach().item()\n        self._epoch_loss_count += 1\n\n    def on_train_epoch_end(self):\n        \"\"\"Configure the train timer at the end of every epoch.\"\"\"\n        train_time = time.time() - self.train_start_time\n        self.log(\n            \"train/time\",\n            train_time,\n            prog_bar=False,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        # Log epoch explicitly for custom x-axis support in wandb\n        self.log(\n            \"epoch\",\n            float(self.current_epoch),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        # Log epoch-averaged training loss\n        if self._epoch_loss_count &gt; 0:\n            avg_loss = self._epoch_loss_sum / self._epoch_loss_count\n            self.log(\n                \"train/loss\",\n                avg_loss,\n                prog_bar=False,\n                on_step=False,\n                on_epoch=True,\n                sync_dist=True,\n            )\n        # Log current learning rate (useful for monitoring LR schedulers)\n        if self.trainer.optimizers:\n            lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n            self.log(\n                \"train/lr\",\n                lr,\n                prog_bar=False,\n                on_step=False,\n                on_epoch=True,\n                sync_dist=True,\n            )\n\n    def on_validation_epoch_start(self):\n        \"\"\"Configure the val timer at the beginning of each epoch.\"\"\"\n        self.val_start_time = time.time()\n        # Clear accumulated predictions for new epoch\n        self.val_predictions = []\n        self.val_ground_truth = []\n\n    def on_validation_epoch_end(self):\n        \"\"\"Configure the val timer at the end of every epoch.\"\"\"\n        val_time = time.time() - self.val_start_time\n        self.log(\n            \"val/time\",\n            val_time,\n            prog_bar=False,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        # Log epoch explicitly so val/* metrics can use it as x-axis in wandb\n        # (mirrors what on_train_epoch_end does for train/* metrics)\n        self.log(\n            \"epoch\",\n            float(self.current_epoch),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        pass\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        pass\n\n    def configure_optimizers(self):\n        \"\"\"Configure optimiser and learning rate scheduler.\"\"\"\n        if self.optimizer == \"Adam\":\n            optim = torch.optim.Adam\n        elif self.optimizer == \"AdamW\":\n            optim = torch.optim.AdamW\n\n        optimizer = optim(\n            self.parameters(),\n            lr=self.lr,\n            amsgrad=self.amsgrad,\n        )\n\n        lr_scheduler_cfg = LRSchedulerConfig()\n        if self.lr_scheduler is None:\n            return {\n                \"optimizer\": optimizer,\n            }\n\n        scheduler = None\n        if isinstance(self.lr_scheduler, str):\n            if self.lr_scheduler == \"step_lr\":\n                lr_scheduler_cfg.step_lr = StepLRConfig()\n            elif self.lr_scheduler == \"reduce_lr_on_plateau\":\n                lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig()\n            elif self.lr_scheduler == \"cosine_annealing_warmup\":\n                lr_scheduler_cfg.cosine_annealing_warmup = CosineAnnealingWarmupConfig()\n            elif self.lr_scheduler == \"linear_warmup_linear_decay\":\n                lr_scheduler_cfg.linear_warmup_linear_decay = (\n                    LinearWarmupLinearDecayConfig()\n                )\n\n        elif isinstance(self.lr_scheduler, dict):\n            lr_scheduler_cfg = self.lr_scheduler\n\n        for k, v in self.lr_scheduler.items():\n            if v is not None:\n                if k == \"cosine_annealing_warmup\":\n                    cfg = self.lr_scheduler.cosine_annealing_warmup\n                    # Use trainer's max_epochs if not specified in config\n                    max_epochs = (\n                        cfg.max_epochs\n                        if cfg.max_epochs is not None\n                        else self.trainer.max_epochs\n                    )\n                    scheduler = LinearWarmupCosineAnnealingLR(\n                        optimizer=optimizer,\n                        warmup_epochs=cfg.warmup_epochs,\n                        max_epochs=max_epochs,\n                        warmup_start_lr=cfg.warmup_start_lr,\n                        eta_min=cfg.eta_min,\n                    )\n                    break\n                elif k == \"linear_warmup_linear_decay\":\n                    cfg = self.lr_scheduler.linear_warmup_linear_decay\n                    # Use trainer's max_epochs if not specified in config\n                    max_epochs = (\n                        cfg.max_epochs\n                        if cfg.max_epochs is not None\n                        else self.trainer.max_epochs\n                    )\n                    scheduler = LinearWarmupLinearDecayLR(\n                        optimizer=optimizer,\n                        warmup_epochs=cfg.warmup_epochs,\n                        max_epochs=max_epochs,\n                        warmup_start_lr=cfg.warmup_start_lr,\n                        end_lr=cfg.end_lr,\n                    )\n                    break\n                elif k == \"step_lr\":\n                    scheduler = torch.optim.lr_scheduler.StepLR(\n                        optimizer=optimizer,\n                        step_size=self.lr_scheduler.step_lr.step_size,\n                        gamma=self.lr_scheduler.step_lr.gamma,\n                    )\n                    break\n                elif k == \"reduce_lr_on_plateau\":\n                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                        optimizer,\n                        mode=\"min\",\n                        threshold=self.lr_scheduler.reduce_lr_on_plateau.threshold,\n                        threshold_mode=self.lr_scheduler.reduce_lr_on_plateau.threshold_mode,\n                        cooldown=self.lr_scheduler.reduce_lr_on_plateau.cooldown,\n                        patience=self.lr_scheduler.reduce_lr_on_plateau.patience,\n                        factor=self.lr_scheduler.reduce_lr_on_plateau.factor,\n                        min_lr=self.lr_scheduler.reduce_lr_on_plateau.min_lr,\n                    )\n                    break\n        if scheduler is None:\n            return {\n                \"optimizer\": optimizer,\n            }\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val/loss\",\n            },\n        }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__()\n    self.model_type = model_type\n    self.backbone_type = backbone_type\n    if not isinstance(backbone_config, DictConfig):\n        backbone_cfg = get_backbone_config(backbone_config)\n        config = OmegaConf.structured(backbone_cfg)\n        OmegaConf.to_container(config, resolve=True, throw_on_missing=True)\n        config = DictConfig(config)\n    else:\n        config = backbone_config\n    self.backbone_config = config\n    self.head_configs = head_configs\n    self.pretrained_backbone_weights = pretrained_backbone_weights\n    self.pretrained_head_weights = pretrained_head_weights\n    self.in_channels = self.backbone_config[f\"{self.backbone_type}\"][\"in_channels\"]\n    self.input_expand_channels = self.in_channels\n    self.init_weights = init_weights\n    self.lr_scheduler = lr_scheduler\n    self.online_mining = online_mining\n    self.hard_to_easy_ratio = hard_to_easy_ratio\n    self.min_hard_keypoints = min_hard_keypoints\n    self.max_hard_keypoints = max_hard_keypoints\n    self.loss_scale = loss_scale\n    self.optimizer = optimizer\n    self.lr = learning_rate\n    self.amsgrad = amsgrad\n\n    self.model = Model(\n        backbone_type=self.backbone_type,\n        backbone_config=self.backbone_config[f\"{self.backbone_type}\"],\n        head_configs=self.head_configs[self.model_type],\n        model_type=self.model_type,\n    )\n\n    if len(self.head_configs[self.model_type]) &gt; 1:\n        self.loss_weights = [\n            (\n                self.head_configs[self.model_type][x].loss_weight\n                if self.head_configs[self.model_type][x].loss_weight is not None\n                else 1.0\n            )\n            for x in self.head_configs[self.model_type]\n        ]\n\n    self.training_loss = {}\n    self.val_loss = {}\n    self.learning_rate = {}\n\n    # For epoch-averaged loss tracking\n    self._epoch_loss_sum = 0.0\n    self._epoch_loss_count = 0\n\n    # For epoch-end evaluation\n    self.val_predictions: List[Dict] = []\n    self.val_ground_truth: List[Dict] = []\n    self._collect_val_predictions: bool = False\n\n    # Initialization for encoder and decoder stacks.\n    if self.init_weights == \"xavier\":\n        self.model.apply(xavier_init_weights)\n\n    # Pre-trained weights for the encoder stack - only for swint and convnext\n    if self.backbone_type == \"convnext\" or self.backbone_type == \"swint\":\n        if (\n            self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n            is not None\n        ):\n            ckpt = MODEL_WEIGHTS[\n                self.backbone_config[f\"{self.backbone_type}\"][\"pre_trained_weights\"]\n            ].DEFAULT.get_state_dict(progress=True, check_hash=True)\n            self.model.backbone.enc.load_state_dict(ckpt, strict=False)\n\n    # Initializing backbone (encoder + decoder) with trained ckpts\n    if self.pretrained_backbone_weights is not None:\n        logger.info(\n            f\"Loading backbone weights from `{self.pretrained_backbone_weights}` ...\"\n        )\n        if self.pretrained_backbone_weights.endswith(\".ckpt\"):\n            ckpt = torch.load(\n                self.pretrained_backbone_weights,\n                map_location=\"cpu\",\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".backbone\" in k\n            }\n            self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif self.pretrained_backbone_weights.endswith(\".h5\"):\n            # load from sleap model weights\n            load_legacy_model_weights(\n                self.model.backbone,\n                self.pretrained_backbone_weights,\n                component=\"backbone\",\n            )\n\n        else:\n            message = f\"Unsupported file extension for pretrained backbone weights. Please provide a .ckpt or .h5 file.\"\n            logger.error(message)\n            raise ValueError(message)\n\n    # Initializing head layers with trained ckpts.\n    if self.pretrained_head_weights is not None:\n        logger.info(\n            f\"Loading head weights from `{self.pretrained_head_weights}` ...\"\n        )\n        if self.pretrained_head_weights.endswith(\".ckpt\"):\n            ckpt = torch.load(\n                self.pretrained_head_weights,\n                map_location=\"cpu\",\n                weights_only=False,\n            )\n            ckpt[\"state_dict\"] = {\n                k: ckpt[\"state_dict\"][k]\n                for k in ckpt[\"state_dict\"].keys()\n                if \".head_layers\" in k\n            }\n            self.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n        elif self.pretrained_head_weights.endswith(\".h5\"):\n            # load from sleap model weights\n            load_legacy_model_weights(\n                self.model.head_layers,\n                self.pretrained_head_weights,\n                component=\"head\",\n            )\n\n        else:\n            message = f\"Unsupported file extension for pretrained head weights. Please provide a .ckpt or .h5 file.\"\n            logger.error(message)\n            raise ValueError(message)\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimiser and learning rate scheduler.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimiser and learning rate scheduler.\"\"\"\n    if self.optimizer == \"Adam\":\n        optim = torch.optim.Adam\n    elif self.optimizer == \"AdamW\":\n        optim = torch.optim.AdamW\n\n    optimizer = optim(\n        self.parameters(),\n        lr=self.lr,\n        amsgrad=self.amsgrad,\n    )\n\n    lr_scheduler_cfg = LRSchedulerConfig()\n    if self.lr_scheduler is None:\n        return {\n            \"optimizer\": optimizer,\n        }\n\n    scheduler = None\n    if isinstance(self.lr_scheduler, str):\n        if self.lr_scheduler == \"step_lr\":\n            lr_scheduler_cfg.step_lr = StepLRConfig()\n        elif self.lr_scheduler == \"reduce_lr_on_plateau\":\n            lr_scheduler_cfg.reduce_lr_on_plateau = ReduceLROnPlateauConfig()\n        elif self.lr_scheduler == \"cosine_annealing_warmup\":\n            lr_scheduler_cfg.cosine_annealing_warmup = CosineAnnealingWarmupConfig()\n        elif self.lr_scheduler == \"linear_warmup_linear_decay\":\n            lr_scheduler_cfg.linear_warmup_linear_decay = (\n                LinearWarmupLinearDecayConfig()\n            )\n\n    elif isinstance(self.lr_scheduler, dict):\n        lr_scheduler_cfg = self.lr_scheduler\n\n    for k, v in self.lr_scheduler.items():\n        if v is not None:\n            if k == \"cosine_annealing_warmup\":\n                cfg = self.lr_scheduler.cosine_annealing_warmup\n                # Use trainer's max_epochs if not specified in config\n                max_epochs = (\n                    cfg.max_epochs\n                    if cfg.max_epochs is not None\n                    else self.trainer.max_epochs\n                )\n                scheduler = LinearWarmupCosineAnnealingLR(\n                    optimizer=optimizer,\n                    warmup_epochs=cfg.warmup_epochs,\n                    max_epochs=max_epochs,\n                    warmup_start_lr=cfg.warmup_start_lr,\n                    eta_min=cfg.eta_min,\n                )\n                break\n            elif k == \"linear_warmup_linear_decay\":\n                cfg = self.lr_scheduler.linear_warmup_linear_decay\n                # Use trainer's max_epochs if not specified in config\n                max_epochs = (\n                    cfg.max_epochs\n                    if cfg.max_epochs is not None\n                    else self.trainer.max_epochs\n                )\n                scheduler = LinearWarmupLinearDecayLR(\n                    optimizer=optimizer,\n                    warmup_epochs=cfg.warmup_epochs,\n                    max_epochs=max_epochs,\n                    warmup_start_lr=cfg.warmup_start_lr,\n                    end_lr=cfg.end_lr,\n                )\n                break\n            elif k == \"step_lr\":\n                scheduler = torch.optim.lr_scheduler.StepLR(\n                    optimizer=optimizer,\n                    step_size=self.lr_scheduler.step_lr.step_size,\n                    gamma=self.lr_scheduler.step_lr.gamma,\n                )\n                break\n            elif k == \"reduce_lr_on_plateau\":\n                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer,\n                    mode=\"min\",\n                    threshold=self.lr_scheduler.reduce_lr_on_plateau.threshold,\n                    threshold_mode=self.lr_scheduler.reduce_lr_on_plateau.threshold_mode,\n                    cooldown=self.lr_scheduler.reduce_lr_on_plateau.cooldown,\n                    patience=self.lr_scheduler.reduce_lr_on_plateau.patience,\n                    factor=self.lr_scheduler.reduce_lr_on_plateau.factor,\n                    min_lr=self.lr_scheduler.reduce_lr_on_plateau.min_lr,\n                )\n                break\n    if scheduler is None:\n        return {\n            \"optimizer\": optimizer,\n        }\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val/loss\",\n        },\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.get_lightning_model_from_config","title":"<code>get_lightning_model_from_config(config)</code>  <code>classmethod</code>","text":"<p>Get lightning model from config.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>@classmethod\ndef get_lightning_model_from_config(cls, config: DictConfig):\n    \"\"\"Get lightning model from config.\"\"\"\n    model_type = get_model_type_from_cfg(config)\n    backbone_type = get_backbone_type_from_cfg(config)\n\n    lightning_models = {\n        \"single_instance\": SingleInstanceLightningModule,\n        \"centroid\": CentroidLightningModule,\n        \"centered_instance\": TopDownCenteredInstanceLightningModule,\n        \"bottomup\": BottomUpLightningModule,\n        \"multi_class_bottomup\": BottomUpMultiClassLightningModule,\n        \"multi_class_topdown\": TopDownCenteredInstanceMultiClassLightningModule,\n    }\n\n    if model_type not in lightning_models:\n        message = f\"Incorrect model type. Please check if one of the following keys in the head configs is not None: [`single_instance`, `centroid`, `centered_instance`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`]\"\n        logger.error(message)\n        raise ValueError(message)\n\n    lightning_model = lightning_models[model_type](\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=config.model_config.backbone_config,\n        head_configs=config.model_config.head_configs,\n        pretrained_backbone_weights=config.model_config.pretrained_backbone_weights,\n        pretrained_head_weights=config.model_config.pretrained_head_weights,\n        init_weights=config.model_config.init_weights,\n        lr_scheduler=config.trainer_config.lr_scheduler,\n        online_mining=config.trainer_config.online_hard_keypoint_mining.online_mining,\n        hard_to_easy_ratio=config.trainer_config.online_hard_keypoint_mining.hard_to_easy_ratio,\n        min_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.min_hard_keypoints,\n        max_hard_keypoints=config.trainer_config.online_hard_keypoint_mining.max_hard_keypoints,\n        loss_scale=config.trainer_config.online_hard_keypoint_mining.loss_scale,\n        optimizer=config.trainer_config.optimizer_name,\n        learning_rate=config.trainer_config.optimizer.lr,\n        amsgrad=config.trainer_config.optimizer.amsgrad,\n    )\n\n    return lightning_model\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_train_epoch_end","title":"<code>on_train_epoch_end()</code>","text":"<p>Configure the train timer at the end of every epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_train_epoch_end(self):\n    \"\"\"Configure the train timer at the end of every epoch.\"\"\"\n    train_time = time.time() - self.train_start_time\n    self.log(\n        \"train/time\",\n        train_time,\n        prog_bar=False,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    # Log epoch explicitly for custom x-axis support in wandb\n    self.log(\n        \"epoch\",\n        float(self.current_epoch),\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    # Log epoch-averaged training loss\n    if self._epoch_loss_count &gt; 0:\n        avg_loss = self._epoch_loss_sum / self._epoch_loss_count\n        self.log(\n            \"train/loss\",\n            avg_loss,\n            prog_bar=False,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n    # Log current learning rate (useful for monitoring LR schedulers)\n    if self.trainer.optimizers:\n        lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n        self.log(\n            \"train/lr\",\n            lr,\n            prog_bar=False,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>Configure the train timer at the beginning of each epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_train_epoch_start(self):\n    \"\"\"Configure the train timer at the beginning of each epoch.\"\"\"\n    self.train_start_time = time.time()\n    # Reset epoch loss tracking\n    self._epoch_loss_sum = 0.0\n    self._epoch_loss_count = 0\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Configure the val timer at the end of every epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Configure the val timer at the end of every epoch.\"\"\"\n    val_time = time.time() - self.val_start_time\n    self.log(\n        \"val/time\",\n        val_time,\n        prog_bar=False,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    # Log epoch explicitly so val/* metrics can use it as x-axis in wandb\n    # (mirrors what on_train_epoch_end does for train/* metrics)\n    self.log(\n        \"epoch\",\n        float(self.current_epoch),\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.on_validation_epoch_start","title":"<code>on_validation_epoch_start()</code>","text":"<p>Configure the val timer at the beginning of each epoch.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def on_validation_epoch_start(self):\n    \"\"\"Configure the val timer at the beginning of each epoch.\"\"\"\n    self.val_start_time = time.time()\n    # Clear accumulated predictions for new epoch\n    self.val_predictions = []\n    self.val_ground_truth = []\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.LightningModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule","title":"<code>SingleInstanceLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for SingleInstance Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to Single Instance model. Single Instance models predict keypoint locations directly from the input image without requiring a separate detection step.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_visualization_data</code> <p>Extract visualization data from a sample.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class SingleInstanceLightningModule(LightningModel):\n    \"\"\"Lightning Module for SingleInstance Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps and\n    forward pass specific to Single Instance model. Single Instance models predict keypoint locations\n    directly from the input image without requiring a separate detection step.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        self.single_instance_inf_layer = SingleInstanceInferenceModel(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            input_scale=1.0,\n            return_confmaps=True,\n            output_stride=self.head_configs.single_instance.confmaps.output_stride,\n        )\n        self.node_names = self.head_configs.single_instance.confmaps.part_names\n\n    def get_visualization_data(self, sample) -&gt; VisualizationData:\n        \"\"\"Extract visualization data from a sample.\n\n        Args:\n            sample: A sample dictionary from the data pipeline.\n\n        Returns:\n            VisualizationData containing image, confmaps, peaks, etc.\n        \"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n        output = self.single_instance_inf_layer(ex)[0]\n\n        peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n        peak_values = output[\"pred_peak_values\"].cpu().numpy()\n        img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        gt_instances = ex[\"instances\"][0].cpu().numpy()\n        confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n        return VisualizationData(\n            image=img,\n            pred_confmaps=confmaps,\n            pred_peaks=peaks,\n            pred_peak_values=peak_values,\n            gt_instances=gt_instances,\n            node_names=list(self.node_names) if self.node_names else [],\n            output_scale=confmaps.shape[0] / img.shape[0],\n            is_paired=True,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n        plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n        plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        img = normalize_on_gpu(img)\n        return self.model(img)[\"SingleInstanceConfmapsHead\"]\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X, y = (\n            torch.squeeze(batch[\"image\"], dim=1),\n            torch.squeeze(batch[\"confidence_maps\"], dim=1),\n        )\n        X = normalize_on_gpu(X)\n\n        y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n\n        loss = nn.MSELoss()(y_preds, y)\n\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            loss = loss + ohkm_loss\n\n        # for part-wise loss\n        if self.node_names is not None:\n            batch_size, _, h, w = y.shape\n            mse = (y - y_preds) ** 2\n            channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n            for node_idx, name in enumerate(self.node_names):\n                self.log(\n                    f\"train/confmaps/{name}\",\n                    channel_wise_loss[node_idx],\n                    prog_bar=False,\n                    on_step=False,\n                    on_epoch=True,\n                    sync_dist=True,\n                )\n        # Log step-level loss (every batch, uses global_step x-axis)\n        self.log(\n            \"loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n            sync_dist=True,\n        )\n        # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n        self._accumulate_loss(loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        X, y = (\n            torch.squeeze(batch[\"image\"], dim=1),\n            torch.squeeze(batch[\"confidence_maps\"], dim=1),\n        )\n        X = normalize_on_gpu(X)\n\n        y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n        val_loss = nn.MSELoss()(y_preds, y)\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            val_loss = val_loss + ohkm_loss\n        self.log(\n            \"val/loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Collect predictions for epoch-end evaluation if enabled\n        if self._collect_val_predictions:\n            with torch.no_grad():\n                # Squeeze n_samples dim from image for inference (batch, 1, C, H, W) -&gt; (batch, C, H, W)\n                inference_batch = {k: v for k, v in batch.items()}\n                if inference_batch[\"image\"].ndim == 5:\n                    inference_batch[\"image\"] = inference_batch[\"image\"].squeeze(1)\n                inference_output = self.single_instance_inf_layer(inference_batch)\n                if isinstance(inference_output, list):\n                    inference_output = inference_output[0]\n\n            batch_size = len(batch[\"frame_idx\"])\n            for i in range(batch_size):\n                eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n                # Predictions are already in original image space (inference divides by eff_scale)\n                pred_peaks = inference_output[\"pred_instance_peaks\"][i].cpu().numpy()\n                pred_scores = inference_output[\"pred_peak_values\"][i].cpu().numpy()\n\n                # Transform GT from preprocessed to original image space\n                # Note: instances have shape (1, max_inst, n_nodes, 2) - squeeze n_samples dim\n                gt_prep = batch[\"instances\"][i].cpu().numpy()\n                if gt_prep.ndim == 4:\n                    gt_prep = gt_prep.squeeze(0)  # (max_inst, n_nodes, 2)\n                gt_orig = gt_prep / eff\n                num_inst = batch[\"num_instances\"][i].item()\n                gt_orig = gt_orig[:num_inst]  # Only valid instances\n\n                self.val_predictions.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"pred_peaks\": pred_peaks,\n                        \"pred_scores\": pred_scores,\n                    }\n                )\n                self.val_ground_truth.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"gt_instances\": gt_orig,\n                        \"num_instances\": num_inst,\n                    }\n                )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    self.single_instance_inf_layer = SingleInstanceInferenceModel(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        input_scale=1.0,\n        return_confmaps=True,\n        output_stride=self.head_configs.single_instance.confmaps.output_stride,\n    )\n    self.node_names = self.head_configs.single_instance.confmaps.part_names\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    img = normalize_on_gpu(img)\n    return self.model(img)[\"SingleInstanceConfmapsHead\"]\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.get_visualization_data","title":"<code>get_visualization_data(sample)</code>","text":"<p>Extract visualization data from a sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <p>A sample dictionary from the data pipeline.</p> required <p>Returns:</p> Type Description <code>VisualizationData</code> <p>VisualizationData containing image, confmaps, peaks, etc.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def get_visualization_data(self, sample) -&gt; VisualizationData:\n    \"\"\"Extract visualization data from a sample.\n\n    Args:\n        sample: A sample dictionary from the data pipeline.\n\n    Returns:\n        VisualizationData containing image, confmaps, peaks, etc.\n    \"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"image\"] = ex[\"image\"].unsqueeze(dim=0)\n    output = self.single_instance_inf_layer(ex)[0]\n\n    peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n    peak_values = output[\"pred_peak_values\"].cpu().numpy()\n    img = output[\"image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    gt_instances = ex[\"instances\"][0].cpu().numpy()\n    confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n    return VisualizationData(\n        image=img,\n        pred_confmaps=confmaps,\n        pred_peaks=peaks,\n        pred_peak_values=peak_values,\n        gt_instances=gt_instances,\n        node_names=list(self.node_names) if self.node_names else [],\n        output_scale=confmaps.shape[0] / img.shape[0],\n        is_paired=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X, y = (\n        torch.squeeze(batch[\"image\"], dim=1),\n        torch.squeeze(batch[\"confidence_maps\"], dim=1),\n    )\n    X = normalize_on_gpu(X)\n\n    y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n\n    loss = nn.MSELoss()(y_preds, y)\n\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        loss = loss + ohkm_loss\n\n    # for part-wise loss\n    if self.node_names is not None:\n        batch_size, _, h, w = y.shape\n        mse = (y - y_preds) ** 2\n        channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n        for node_idx, name in enumerate(self.node_names):\n            self.log(\n                f\"train/confmaps/{name}\",\n                channel_wise_loss[node_idx],\n                prog_bar=False,\n                on_step=False,\n                on_epoch=True,\n                sync_dist=True,\n            )\n    # Log step-level loss (every batch, uses global_step x-axis)\n    self.log(\n        \"loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n        sync_dist=True,\n    )\n    # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n    self._accumulate_loss(loss)\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    X, y = (\n        torch.squeeze(batch[\"image\"], dim=1),\n        torch.squeeze(batch[\"confidence_maps\"], dim=1),\n    )\n    X = normalize_on_gpu(X)\n\n    y_preds = self.model(X)[\"SingleInstanceConfmapsHead\"]\n    val_loss = nn.MSELoss()(y_preds, y)\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        val_loss = val_loss + ohkm_loss\n    self.log(\n        \"val/loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Collect predictions for epoch-end evaluation if enabled\n    if self._collect_val_predictions:\n        with torch.no_grad():\n            # Squeeze n_samples dim from image for inference (batch, 1, C, H, W) -&gt; (batch, C, H, W)\n            inference_batch = {k: v for k, v in batch.items()}\n            if inference_batch[\"image\"].ndim == 5:\n                inference_batch[\"image\"] = inference_batch[\"image\"].squeeze(1)\n            inference_output = self.single_instance_inf_layer(inference_batch)\n            if isinstance(inference_output, list):\n                inference_output = inference_output[0]\n\n        batch_size = len(batch[\"frame_idx\"])\n        for i in range(batch_size):\n            eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n            # Predictions are already in original image space (inference divides by eff_scale)\n            pred_peaks = inference_output[\"pred_instance_peaks\"][i].cpu().numpy()\n            pred_scores = inference_output[\"pred_peak_values\"][i].cpu().numpy()\n\n            # Transform GT from preprocessed to original image space\n            # Note: instances have shape (1, max_inst, n_nodes, 2) - squeeze n_samples dim\n            gt_prep = batch[\"instances\"][i].cpu().numpy()\n            if gt_prep.ndim == 4:\n                gt_prep = gt_prep.squeeze(0)  # (max_inst, n_nodes, 2)\n            gt_orig = gt_prep / eff\n            num_inst = batch[\"num_instances\"][i].item()\n            gt_orig = gt_orig[:num_inst]  # Only valid instances\n\n            self.val_predictions.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"pred_peaks\": pred_peaks,\n                    \"pred_scores\": pred_scores,\n                }\n            )\n            self.val_ground_truth.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"gt_instances\": gt_orig,\n                    \"num_instances\": num_inst,\n                }\n            )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.SingleInstanceLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n    plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n    plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule","title":"<code>TopDownCenteredInstanceLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for TopDownCenteredInstance Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to TopDown Centered instance model. Top-Down models use a two-stage approach: first detecting centroids, then predicting keypoints for each detected centroid.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_visualization_data</code> <p>Extract visualization data from a sample.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Perform validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class TopDownCenteredInstanceLightningModule(LightningModel):\n    \"\"\"Lightning Module for TopDownCenteredInstance Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to TopDown Centered instance model. Top-Down models use a two-stage\n    approach: first detecting centroids, then predicting keypoints for each detected centroid.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n\n        self.instance_peaks_inf_layer = FindInstancePeaks(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            return_confmaps=True,\n            output_stride=self.head_configs.centered_instance.confmaps.output_stride,\n        )\n\n        self.node_names = self.head_configs.centered_instance.confmaps.part_names\n\n    def get_visualization_data(self, sample) -&gt; VisualizationData:\n        \"\"\"Extract visualization data from a sample.\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n        output = self.instance_peaks_inf_layer(ex)\n\n        peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n        peak_values = output[\"pred_peak_values\"].cpu().numpy()\n        img = output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        gt_instances = ex[\"instance\"].cpu().numpy()\n        confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n        return VisualizationData(\n            image=img,\n            pred_confmaps=confmaps,\n            pred_peaks=peaks,\n            pred_peak_values=peak_values,\n            gt_instances=gt_instances,\n            node_names=list(self.node_names) if self.node_names else [],\n            output_scale=confmaps.shape[0] / img.shape[0],\n            is_paired=True,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n        plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n        plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        img = normalize_on_gpu(img)\n        return self.model(img)[\"CenteredInstanceConfmapsHead\"]\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X, y = (\n            torch.squeeze(batch[\"instance_image\"], dim=1),\n            torch.squeeze(batch[\"confidence_maps\"], dim=1),\n        )\n        X = normalize_on_gpu(X)\n\n        y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n\n        loss = nn.MSELoss()(y_preds, y)\n\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            loss = loss + ohkm_loss\n\n        # for part-wise loss\n        if self.node_names is not None:\n            batch_size, _, h, w = y.shape\n            mse = (y - y_preds) ** 2\n            channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n            for node_idx, name in enumerate(self.node_names):\n                self.log(\n                    f\"train/confmaps/{name}\",\n                    channel_wise_loss[node_idx],\n                    prog_bar=False,\n                    on_step=False,\n                    on_epoch=True,\n                    sync_dist=True,\n                )\n\n        # Log step-level loss (every batch, uses global_step x-axis)\n        self.log(\n            \"loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n            sync_dist=True,\n        )\n        # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n        self._accumulate_loss(loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Perform validation step.\"\"\"\n        X, y = (\n            torch.squeeze(batch[\"instance_image\"], dim=1),\n            torch.squeeze(batch[\"confidence_maps\"], dim=1),\n        )\n        X = normalize_on_gpu(X)\n\n        y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n        val_loss = nn.MSELoss()(y_preds, y)\n        if self.online_mining is not None and self.online_mining:\n            ohkm_loss = compute_ohkm_loss(\n                y_gt=y,\n                y_pr=y_preds,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            val_loss = val_loss + ohkm_loss\n        self.log(\n            \"val/loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Collect predictions for epoch-end evaluation if enabled\n        if self._collect_val_predictions:\n            # SAVE bbox BEFORE inference (it modifies in-place!)\n            bbox_prep_saved = batch[\"instance_bbox\"].clone()\n\n            with torch.no_grad():\n                inference_output = self.instance_peaks_inf_layer(batch)\n\n            batch_size = len(batch[\"frame_idx\"])\n            for i in range(batch_size):\n                eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n                # Predictions from inference (crop-relative, original scale)\n                pred_peaks_crop = (\n                    inference_output[\"pred_instance_peaks\"][i].cpu().numpy()\n                )\n                pred_scores = inference_output[\"pred_peak_values\"][i].cpu().numpy()\n\n                # Compute bbox offset in original space from SAVED prep bbox\n                # bbox has shape (n_samples=1, 4, 2) where 4 corners\n                bbox_prep = bbox_prep_saved[i].squeeze(0).cpu().numpy()  # (4, 2)\n                bbox_top_left_orig = (\n                    bbox_prep[0] / eff\n                )  # Top-left corner in original space\n\n                # Full image coordinates (original space)\n                pred_peaks_full = pred_peaks_crop + bbox_top_left_orig\n\n                # GT transform: crop-relative preprocessed -&gt; full image original\n                gt_crop_prep = (\n                    batch[\"instance\"][i].squeeze(0).cpu().numpy()\n                )  # (n_nodes, 2)\n                gt_crop_orig = gt_crop_prep / eff\n                gt_full_orig = gt_crop_orig + bbox_top_left_orig\n\n                self.val_predictions.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"pred_peaks\": pred_peaks_full.reshape(\n                            1, -1, 2\n                        ),  # (1, n_nodes, 2)\n                        \"pred_scores\": pred_scores.reshape(1, -1),  # (1, n_nodes)\n                    }\n                )\n                self.val_ground_truth.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"gt_instances\": gt_full_orig.reshape(\n                            1, -1, 2\n                        ),  # (1, n_nodes, 2)\n                        \"num_instances\": 1,\n                    }\n                )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n\n    self.instance_peaks_inf_layer = FindInstancePeaks(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        return_confmaps=True,\n        output_stride=self.head_configs.centered_instance.confmaps.output_stride,\n    )\n\n    self.node_names = self.head_configs.centered_instance.confmaps.part_names\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    img = normalize_on_gpu(img)\n    return self.model(img)[\"CenteredInstanceConfmapsHead\"]\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.get_visualization_data","title":"<code>get_visualization_data(sample)</code>","text":"<p>Extract visualization data from a sample.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def get_visualization_data(self, sample) -&gt; VisualizationData:\n    \"\"\"Extract visualization data from a sample.\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n    output = self.instance_peaks_inf_layer(ex)\n\n    peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n    peak_values = output[\"pred_peak_values\"].cpu().numpy()\n    img = output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    gt_instances = ex[\"instance\"].cpu().numpy()\n    confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n    return VisualizationData(\n        image=img,\n        pred_confmaps=confmaps,\n        pred_peaks=peaks,\n        pred_peak_values=peak_values,\n        gt_instances=gt_instances,\n        node_names=list(self.node_names) if self.node_names else [],\n        output_scale=confmaps.shape[0] / img.shape[0],\n        is_paired=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X, y = (\n        torch.squeeze(batch[\"instance_image\"], dim=1),\n        torch.squeeze(batch[\"confidence_maps\"], dim=1),\n    )\n    X = normalize_on_gpu(X)\n\n    y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n\n    loss = nn.MSELoss()(y_preds, y)\n\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        loss = loss + ohkm_loss\n\n    # for part-wise loss\n    if self.node_names is not None:\n        batch_size, _, h, w = y.shape\n        mse = (y - y_preds) ** 2\n        channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n        for node_idx, name in enumerate(self.node_names):\n            self.log(\n                f\"train/confmaps/{name}\",\n                channel_wise_loss[node_idx],\n                prog_bar=False,\n                on_step=False,\n                on_epoch=True,\n                sync_dist=True,\n            )\n\n    # Log step-level loss (every batch, uses global_step x-axis)\n    self.log(\n        \"loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n        sync_dist=True,\n    )\n    # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n    self._accumulate_loss(loss)\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Perform validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Perform validation step.\"\"\"\n    X, y = (\n        torch.squeeze(batch[\"instance_image\"], dim=1),\n        torch.squeeze(batch[\"confidence_maps\"], dim=1),\n    )\n    X = normalize_on_gpu(X)\n\n    y_preds = self.model(X)[\"CenteredInstanceConfmapsHead\"]\n    val_loss = nn.MSELoss()(y_preds, y)\n    if self.online_mining is not None and self.online_mining:\n        ohkm_loss = compute_ohkm_loss(\n            y_gt=y,\n            y_pr=y_preds,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        val_loss = val_loss + ohkm_loss\n    self.log(\n        \"val/loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Collect predictions for epoch-end evaluation if enabled\n    if self._collect_val_predictions:\n        # SAVE bbox BEFORE inference (it modifies in-place!)\n        bbox_prep_saved = batch[\"instance_bbox\"].clone()\n\n        with torch.no_grad():\n            inference_output = self.instance_peaks_inf_layer(batch)\n\n        batch_size = len(batch[\"frame_idx\"])\n        for i in range(batch_size):\n            eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n            # Predictions from inference (crop-relative, original scale)\n            pred_peaks_crop = (\n                inference_output[\"pred_instance_peaks\"][i].cpu().numpy()\n            )\n            pred_scores = inference_output[\"pred_peak_values\"][i].cpu().numpy()\n\n            # Compute bbox offset in original space from SAVED prep bbox\n            # bbox has shape (n_samples=1, 4, 2) where 4 corners\n            bbox_prep = bbox_prep_saved[i].squeeze(0).cpu().numpy()  # (4, 2)\n            bbox_top_left_orig = (\n                bbox_prep[0] / eff\n            )  # Top-left corner in original space\n\n            # Full image coordinates (original space)\n            pred_peaks_full = pred_peaks_crop + bbox_top_left_orig\n\n            # GT transform: crop-relative preprocessed -&gt; full image original\n            gt_crop_prep = (\n                batch[\"instance\"][i].squeeze(0).cpu().numpy()\n            )  # (n_nodes, 2)\n            gt_crop_orig = gt_crop_prep / eff\n            gt_full_orig = gt_crop_orig + bbox_top_left_orig\n\n            self.val_predictions.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"pred_peaks\": pred_peaks_full.reshape(\n                        1, -1, 2\n                    ),  # (1, n_nodes, 2)\n                    \"pred_scores\": pred_scores.reshape(1, -1),  # (1, n_nodes)\n                }\n            )\n            self.val_ground_truth.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"gt_instances\": gt_full_orig.reshape(\n                        1, -1, 2\n                    ),  # (1, n_nodes, 2)\n                    \"num_instances\": 1,\n                }\n            )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n    plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n    plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n    return fig\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule","title":"<code>TopDownCenteredInstanceMultiClassLightningModule</code>","text":"<p>               Bases: <code>LightningModel</code></p> <p>Lightning Module for TopDownCenteredInstance ID Model.</p> <p>This is a subclass of the <code>LightningModel</code> to configure the training/ validation steps and forward pass specific to TopDown Centered instance model. Multi-Class Top-Down models use a two-stage approach: first detecting centroids, then predicting keypoints and classifying instances using supervised learning with ground truth track IDs.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <code>str</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>backbone_config</code> <code>Union[str, Dict[str, Any], DictConfig]</code> <p>Backbone configuration. Can be: - String: One of the preset backbone types:     - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]     - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]     - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"] - Dictionary: Custom configuration with structure:     {         \"unet\": {UNetConfig parameters},         \"convnext\": {ConvNextConfig parameters},         \"swint\": {SwinTConfig parameters}     }     Only one backbone type should be specified in the dictionary. - DictConfig: OmegaConf DictConfig object containing backbone configuration.</p> required <code>head_configs</code> <code>DictConfig</code> <p>Head configuration dictionary containing model-specific parameters. For Single Instance: confmaps with part_names, sigma, output_stride. For Centroid: confmaps with anchor_part, sigma, output_stride. For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride. For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight. For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight. For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.</p> required <code>pretrained_backbone_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.</p> <code>None</code> <code>pretrained_head_weights</code> <code>Optional[str]</code> <p>Path to checkpoint <code>.ckpt</code> (or <code>.h5</code> file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.</p> <code>None</code> <code>init_weights</code> <code>Optional[str]</code> <p>Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.</p> <code>'xavier'</code> <code>lr_scheduler</code> <code>Optional[Union[str, DictConfig]]</code> <p>Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.</p> <code>None</code> <code>online_mining</code> <code>Optional[bool]</code> <p>If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).</p> <code>False</code> <code>hard_to_easy_ratio</code> <code>Optional[float]</code> <p>Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.</p> <code>2.0</code> <code>min_hard_keypoints</code> <code>Optional[int]</code> <p>Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.</p> <code>2</code> <code>max_hard_keypoints</code> <code>Optional[int]</code> <p>Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.</p> <code>None</code> <code>loss_scale</code> <code>Optional[float]</code> <p>Factor to scale hard keypoint losses by. Default: 5.0.</p> <code>5.0</code> <code>optimizer</code> <code>Optional[str]</code> <p>Optimizer name. One of [\"Adam\", \"AdamW\"].</p> <code>'Adam'</code> <code>learning_rate</code> <code>Optional[float]</code> <p>Learning rate for the optimizer. Default: 1e-3.</p> <code>0.001</code> <code>amsgrad</code> <code>Optional[bool]</code> <p>Enable AMSGrad with the optimizer. Default: False.</p> <code>False</code> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialise the configs and the model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>get_visualization_data</code> <p>Extract visualization data from a sample.</p> <code>training_step</code> <p>Training step.</p> <code>validation_step</code> <p>Perform validation step.</p> <code>visualize_example</code> <p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>class TopDownCenteredInstanceMultiClassLightningModule(LightningModel):\n    \"\"\"Lightning Module for TopDownCenteredInstance ID Model.\n\n    This is a subclass of the `LightningModel` to configure the training/ validation steps\n    and forward pass specific to TopDown Centered instance model. Multi-Class Top-Down models\n    use a two-stage approach: first detecting centroids, then predicting keypoints and\n    classifying instances using supervised learning with ground truth track IDs.\n\n    Args:\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        backbone_config: Backbone configuration. Can be:\n            - String: One of the preset backbone types:\n                - UNet variants: [\"unet\", \"unet_medium_rf\", \"unet_large_rf\"]\n                - ConvNeXt variants: [\"convnext\", \"convnext_tiny\", \"convnext_small\", \"convnext_base\", \"convnext_large\"]\n                - SwinT variants: [\"swint\", \"swint_tiny\", \"swint_small\", \"swint_base\"]\n            - Dictionary: Custom configuration with structure:\n                {\n                    \"unet\": {UNetConfig parameters},\n                    \"convnext\": {ConvNextConfig parameters},\n                    \"swint\": {SwinTConfig parameters}\n                }\n                Only one backbone type should be specified in the dictionary.\n            - DictConfig: OmegaConf DictConfig object containing backbone configuration.\n        head_configs: Head configuration dictionary containing model-specific parameters.\n            For Single Instance: confmaps with part_names, sigma, output_stride.\n            For Centroid: confmaps with anchor_part, sigma, output_stride.\n            For Centered Instance: confmaps with part_names, anchor_part, sigma, output_stride.\n            For Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; pafs with edges, sigma, output_stride, loss_weight.\n            For Multi-Class Bottom-Up: confmaps with part_names, sigma, output_stride, loss_weight; class_maps with classes, sigma, output_stride, loss_weight.\n            For Multi-Class Top-Down: confmaps with part_names, anchor_part, sigma, output_stride, loss_weight; class_vectors with classes, num_fc_layers, num_fc_units, global_pool, output_stride, loss_weight.\n        pretrained_backbone_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for backbone initialization. If None, random initialization is used.\n        pretrained_head_weights: Path to checkpoint `.ckpt` (or `.h5` file from SLEAP - only UNet backbone is supported) file for head layers initialization. If None, random initialization is used.\n        init_weights: Model weights initialization method. \"default\" uses kaiming uniform initialization, \"xavier\" uses Xavier initialization.\n        lr_scheduler: Learning rate scheduler configuration. Can be string (\"step_lr\", \"reduce_lr_on_plateau\") or dictionary with scheduler-specific parameters.\n        online_mining: If True, online hard keypoint mining (OHKM) is enabled. Loss is computed per keypoint and sorted from lowest (easy) to highest (hard).\n        hard_to_easy_ratio: Minimum ratio of individual keypoint loss to lowest keypoint loss to be considered \"hard\". Default: 2.0.\n        min_hard_keypoints: Minimum number of keypoints considered as \"hard\", even if below hard_to_easy_ratio. Default: 2.\n        max_hard_keypoints: Maximum number of hard keypoints to apply scaling to. If None, no limit is applied.\n        loss_scale: Factor to scale hard keypoint losses by. Default: 5.0.\n        optimizer: Optimizer name. One of [\"Adam\", \"AdamW\"].\n        learning_rate: Learning rate for the optimizer. Default: 1e-3.\n        amsgrad: Enable AMSGrad with the optimizer. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,\n        backbone_type: str,\n        backbone_config: Union[str, Dict[str, Any], DictConfig],\n        head_configs: DictConfig,\n        pretrained_backbone_weights: Optional[str] = None,\n        pretrained_head_weights: Optional[str] = None,\n        init_weights: Optional[str] = \"xavier\",\n        lr_scheduler: Optional[Union[str, DictConfig]] = None,\n        online_mining: Optional[bool] = False,\n        hard_to_easy_ratio: Optional[float] = 2.0,\n        min_hard_keypoints: Optional[int] = 2,\n        max_hard_keypoints: Optional[int] = None,\n        loss_scale: Optional[float] = 5.0,\n        optimizer: Optional[str] = \"Adam\",\n        learning_rate: Optional[float] = 1e-3,\n        amsgrad: Optional[bool] = False,\n    ):\n        \"\"\"Initialise the configs and the model.\"\"\"\n        super().__init__(\n            model_type=model_type,\n            backbone_type=backbone_type,\n            backbone_config=backbone_config,\n            head_configs=head_configs,\n            pretrained_backbone_weights=pretrained_backbone_weights,\n            pretrained_head_weights=pretrained_head_weights,\n            init_weights=init_weights,\n            lr_scheduler=lr_scheduler,\n            online_mining=online_mining,\n            hard_to_easy_ratio=hard_to_easy_ratio,\n            min_hard_keypoints=min_hard_keypoints,\n            max_hard_keypoints=max_hard_keypoints,\n            loss_scale=loss_scale,\n            optimizer=optimizer,\n            learning_rate=learning_rate,\n            amsgrad=amsgrad,\n        )\n        self.instance_peaks_inf_layer = TopDownMultiClassFindInstancePeaks(\n            torch_model=self.forward,\n            peak_threshold=0.2,\n            return_confmaps=True,\n            output_stride=self.head_configs.multi_class_topdown.confmaps.output_stride,\n        )\n\n        self.node_names = self.head_configs.multi_class_topdown.confmaps.part_names\n\n    def get_visualization_data(self, sample) -&gt; VisualizationData:\n        \"\"\"Extract visualization data from a sample.\"\"\"\n        ex = sample.copy()\n        ex[\"eff_scale\"] = torch.tensor([1.0])\n        for k, v in ex.items():\n            if isinstance(v, torch.Tensor):\n                ex[k] = v.to(device=self.device)\n        ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n        output = self.instance_peaks_inf_layer(ex)\n\n        peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n        peak_values = output[\"pred_peak_values\"].cpu().numpy()\n        img = output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n        gt_instances = ex[\"instance\"].cpu().numpy()\n        confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n        return VisualizationData(\n            image=img,\n            pred_confmaps=confmaps,\n            pred_peaks=peaks,\n            pred_peak_values=peak_values,\n            gt_instances=gt_instances,\n            node_names=list(self.node_names) if self.node_names else [],\n            output_scale=confmaps.shape[0] / img.shape[0],\n            is_paired=True,\n        )\n\n    def visualize_example(self, sample):\n        \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n        data = self.get_visualization_data(sample)\n        scale = 1.0\n        if data.image.shape[0] &lt; 512:\n            scale = 2.0\n        if data.image.shape[0] &lt; 256:\n            scale = 4.0\n        fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n        plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n        plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n        return fig\n\n    def forward(self, img):\n        \"\"\"Forward pass of the model.\"\"\"\n        img = torch.squeeze(img, dim=1).to(self.device)\n        img = normalize_on_gpu(img)\n        output = self.model(img)\n        return {\n            \"CenteredInstanceConfmapsHead\": output[\"CenteredInstanceConfmapsHead\"],\n            \"ClassVectorsHead\": output[\"ClassVectorsHead\"],\n        }\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step.\"\"\"\n        X = torch.squeeze(batch[\"instance_image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classvector = batch[\"class_vectors\"]\n        X = normalize_on_gpu(X)\n        preds = self.model(X)\n        classvector = preds[\"ClassVectorsHead\"]\n        confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"CenteredInstanceConfmapsHead\": confmap_loss,\n            \"ClassVectorsHead\": classvector_loss,\n        }\n        loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n\n        # for part-wise loss\n        if self.node_names is not None:\n            batch_size, _, h, w = y_confmap.shape\n            mse = (y_confmap - confmaps) ** 2\n            channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n            for node_idx, name in enumerate(self.node_names):\n                self.log(\n                    f\"train/confmaps/{name}\",\n                    channel_wise_loss[node_idx],\n                    prog_bar=False,\n                    on_step=False,\n                    on_epoch=True,\n                    sync_dist=True,\n                )\n\n        # Log step-level loss (every batch, uses global_step x-axis)\n        self.log(\n            \"loss\",\n            loss,\n            prog_bar=True,\n            on_step=True,\n            on_epoch=False,\n            sync_dist=True,\n        )\n        # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n        self._accumulate_loss(loss)\n        self.log(\n            \"train/confmaps_loss\",\n            confmap_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"train/classvector_loss\",\n            classvector_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Compute classification accuracy\n        with torch.no_grad():\n            pred_classes = torch.argmax(classvector, dim=1)\n            gt_classes = torch.argmax(y_classvector, dim=1)\n            class_accuracy = (pred_classes == gt_classes).float().mean()\n        self.log(\n            \"train/class_accuracy\",\n            class_accuracy,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Perform validation step.\"\"\"\n        X = torch.squeeze(batch[\"instance_image\"], dim=1)\n        y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n        y_classvector = batch[\"class_vectors\"]\n        X = normalize_on_gpu(X)\n        preds = self.model(X)\n        classvector = preds[\"ClassVectorsHead\"]\n        confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n        confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n        classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n        if self.online_mining is not None and self.online_mining:\n            confmap_ohkm_loss = compute_ohkm_loss(\n                y_gt=y_confmap,\n                y_pr=confmaps,\n                hard_to_easy_ratio=self.hard_to_easy_ratio,\n                min_hard_keypoints=self.min_hard_keypoints,\n                max_hard_keypoints=self.max_hard_keypoints,\n                loss_scale=self.loss_scale,\n            )\n            confmap_loss += confmap_ohkm_loss\n\n        losses = {\n            \"CenteredInstanceConfmapsHead\": confmap_loss,\n            \"ClassVectorsHead\": classvector_loss,\n        }\n        val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n        self.log(\n            \"val/loss\",\n            val_loss,\n            prog_bar=True,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val/confmaps_loss\",\n            confmap_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"val/classvector_loss\",\n            classvector_loss,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Compute classification accuracy\n        with torch.no_grad():\n            pred_classes = torch.argmax(classvector, dim=1)\n            gt_classes = torch.argmax(y_classvector, dim=1)\n            class_accuracy = (pred_classes == gt_classes).float().mean()\n        self.log(\n            \"val/class_accuracy\",\n            class_accuracy,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Collect predictions for epoch-end evaluation if enabled\n        if self._collect_val_predictions:\n            # SAVE bbox BEFORE inference (it modifies in-place!)\n            bbox_prep_saved = batch[\"instance_bbox\"].clone()\n\n            with torch.no_grad():\n                inference_output = self.instance_peaks_inf_layer(batch)\n\n            batch_size = len(batch[\"frame_idx\"])\n            for i in range(batch_size):\n                eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n                # Predictions from inference (crop-relative, original scale)\n                pred_peaks_crop = (\n                    inference_output[\"pred_instance_peaks\"][i].cpu().numpy()\n                )\n                pred_scores = inference_output[\"pred_peak_values\"][i].cpu().numpy()\n\n                # Compute bbox offset in original space from SAVED prep bbox\n                # bbox has shape (n_samples=1, 4, 2) where 4 corners\n                bbox_prep = bbox_prep_saved[i].squeeze(0).cpu().numpy()  # (4, 2)\n                bbox_top_left_orig = (\n                    bbox_prep[0] / eff\n                )  # Top-left corner in original space\n\n                # Full image coordinates (original space)\n                pred_peaks_full = pred_peaks_crop + bbox_top_left_orig\n\n                # GT transform: crop-relative preprocessed -&gt; full image original\n                gt_crop_prep = (\n                    batch[\"instance\"][i].squeeze(0).cpu().numpy()\n                )  # (n_nodes, 2)\n                gt_crop_orig = gt_crop_prep / eff\n                gt_full_orig = gt_crop_orig + bbox_top_left_orig\n\n                self.val_predictions.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"pred_peaks\": pred_peaks_full.reshape(\n                            1, -1, 2\n                        ),  # (1, n_nodes, 2)\n                        \"pred_scores\": pred_scores.reshape(1, -1),  # (1, n_nodes)\n                    }\n                )\n                self.val_ground_truth.append(\n                    {\n                        \"video_idx\": batch[\"video_idx\"][i].item(),\n                        \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                        \"gt_instances\": gt_full_orig.reshape(\n                            1, -1, 2\n                        ),  # (1, n_nodes, 2)\n                        \"num_instances\": 1,\n                    }\n                )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.__init__","title":"<code>__init__(model_type, backbone_type, backbone_config, head_configs, pretrained_backbone_weights=None, pretrained_head_weights=None, init_weights='xavier', lr_scheduler=None, online_mining=False, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0, optimizer='Adam', learning_rate=0.001, amsgrad=False)</code>","text":"<p>Initialise the configs and the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def __init__(\n    self,\n    model_type: str,\n    backbone_type: str,\n    backbone_config: Union[str, Dict[str, Any], DictConfig],\n    head_configs: DictConfig,\n    pretrained_backbone_weights: Optional[str] = None,\n    pretrained_head_weights: Optional[str] = None,\n    init_weights: Optional[str] = \"xavier\",\n    lr_scheduler: Optional[Union[str, DictConfig]] = None,\n    online_mining: Optional[bool] = False,\n    hard_to_easy_ratio: Optional[float] = 2.0,\n    min_hard_keypoints: Optional[int] = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: Optional[float] = 5.0,\n    optimizer: Optional[str] = \"Adam\",\n    learning_rate: Optional[float] = 1e-3,\n    amsgrad: Optional[bool] = False,\n):\n    \"\"\"Initialise the configs and the model.\"\"\"\n    super().__init__(\n        model_type=model_type,\n        backbone_type=backbone_type,\n        backbone_config=backbone_config,\n        head_configs=head_configs,\n        pretrained_backbone_weights=pretrained_backbone_weights,\n        pretrained_head_weights=pretrained_head_weights,\n        init_weights=init_weights,\n        lr_scheduler=lr_scheduler,\n        online_mining=online_mining,\n        hard_to_easy_ratio=hard_to_easy_ratio,\n        min_hard_keypoints=min_hard_keypoints,\n        max_hard_keypoints=max_hard_keypoints,\n        loss_scale=loss_scale,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        amsgrad=amsgrad,\n    )\n    self.instance_peaks_inf_layer = TopDownMultiClassFindInstancePeaks(\n        torch_model=self.forward,\n        peak_threshold=0.2,\n        return_confmaps=True,\n        output_stride=self.head_configs.multi_class_topdown.confmaps.output_stride,\n    )\n\n    self.node_names = self.head_configs.multi_class_topdown.confmaps.part_names\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.forward","title":"<code>forward(img)</code>","text":"<p>Forward pass of the model.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def forward(self, img):\n    \"\"\"Forward pass of the model.\"\"\"\n    img = torch.squeeze(img, dim=1).to(self.device)\n    img = normalize_on_gpu(img)\n    output = self.model(img)\n    return {\n        \"CenteredInstanceConfmapsHead\": output[\"CenteredInstanceConfmapsHead\"],\n        \"ClassVectorsHead\": output[\"ClassVectorsHead\"],\n    }\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.get_visualization_data","title":"<code>get_visualization_data(sample)</code>","text":"<p>Extract visualization data from a sample.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def get_visualization_data(self, sample) -&gt; VisualizationData:\n    \"\"\"Extract visualization data from a sample.\"\"\"\n    ex = sample.copy()\n    ex[\"eff_scale\"] = torch.tensor([1.0])\n    for k, v in ex.items():\n        if isinstance(v, torch.Tensor):\n            ex[k] = v.to(device=self.device)\n    ex[\"instance_image\"] = ex[\"instance_image\"].unsqueeze(dim=0)\n    output = self.instance_peaks_inf_layer(ex)\n\n    peaks = output[\"pred_instance_peaks\"].cpu().numpy()\n    peak_values = output[\"pred_peak_values\"].cpu().numpy()\n    img = output[\"instance_image\"][0, 0].cpu().numpy().transpose(1, 2, 0)\n    gt_instances = ex[\"instance\"].cpu().numpy()\n    confmaps = output[\"pred_confmaps\"][0].cpu().numpy().transpose(1, 2, 0)\n\n    return VisualizationData(\n        image=img,\n        pred_confmaps=confmaps,\n        pred_peaks=peaks,\n        pred_peak_values=peak_values,\n        gt_instances=gt_instances,\n        node_names=list(self.node_names) if self.node_names else [],\n        output_scale=confmaps.shape[0] / img.shape[0],\n        is_paired=True,\n    )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    X = torch.squeeze(batch[\"instance_image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classvector = batch[\"class_vectors\"]\n    X = normalize_on_gpu(X)\n    preds = self.model(X)\n    classvector = preds[\"ClassVectorsHead\"]\n    confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"CenteredInstanceConfmapsHead\": confmap_loss,\n        \"ClassVectorsHead\": classvector_loss,\n    }\n    loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n\n    # for part-wise loss\n    if self.node_names is not None:\n        batch_size, _, h, w = y_confmap.shape\n        mse = (y_confmap - confmaps) ** 2\n        channel_wise_loss = torch.sum(mse, dim=(0, 2, 3)) / (batch_size * h * w)\n        for node_idx, name in enumerate(self.node_names):\n            self.log(\n                f\"train/confmaps/{name}\",\n                channel_wise_loss[node_idx],\n                prog_bar=False,\n                on_step=False,\n                on_epoch=True,\n                sync_dist=True,\n            )\n\n    # Log step-level loss (every batch, uses global_step x-axis)\n    self.log(\n        \"loss\",\n        loss,\n        prog_bar=True,\n        on_step=True,\n        on_epoch=False,\n        sync_dist=True,\n    )\n    # Accumulate for epoch-averaged loss (logged in on_train_epoch_end)\n    self._accumulate_loss(loss)\n    self.log(\n        \"train/confmaps_loss\",\n        confmap_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"train/classvector_loss\",\n        classvector_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Compute classification accuracy\n    with torch.no_grad():\n        pred_classes = torch.argmax(classvector, dim=1)\n        gt_classes = torch.argmax(y_classvector, dim=1)\n        class_accuracy = (pred_classes == gt_classes).float().mean()\n    self.log(\n        \"train/class_accuracy\",\n        class_accuracy,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    return loss\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Perform validation step.</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Perform validation step.\"\"\"\n    X = torch.squeeze(batch[\"instance_image\"], dim=1)\n    y_confmap = torch.squeeze(batch[\"confidence_maps\"], dim=1)\n    y_classvector = batch[\"class_vectors\"]\n    X = normalize_on_gpu(X)\n    preds = self.model(X)\n    classvector = preds[\"ClassVectorsHead\"]\n    confmaps = preds[\"CenteredInstanceConfmapsHead\"]\n\n    confmap_loss = nn.MSELoss()(confmaps, y_confmap)\n    classvector_loss = nn.CrossEntropyLoss()(classvector, y_classvector)\n\n    if self.online_mining is not None and self.online_mining:\n        confmap_ohkm_loss = compute_ohkm_loss(\n            y_gt=y_confmap,\n            y_pr=confmaps,\n            hard_to_easy_ratio=self.hard_to_easy_ratio,\n            min_hard_keypoints=self.min_hard_keypoints,\n            max_hard_keypoints=self.max_hard_keypoints,\n            loss_scale=self.loss_scale,\n        )\n        confmap_loss += confmap_ohkm_loss\n\n    losses = {\n        \"CenteredInstanceConfmapsHead\": confmap_loss,\n        \"ClassVectorsHead\": classvector_loss,\n    }\n    val_loss = sum([s * losses[t] for s, t in zip(self.loss_weights, losses)])\n    self.log(\n        \"val/loss\",\n        val_loss,\n        prog_bar=True,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val/confmaps_loss\",\n        confmap_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n    self.log(\n        \"val/classvector_loss\",\n        classvector_loss,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Compute classification accuracy\n    with torch.no_grad():\n        pred_classes = torch.argmax(classvector, dim=1)\n        gt_classes = torch.argmax(y_classvector, dim=1)\n        class_accuracy = (pred_classes == gt_classes).float().mean()\n    self.log(\n        \"val/class_accuracy\",\n        class_accuracy,\n        on_step=False,\n        on_epoch=True,\n        sync_dist=True,\n    )\n\n    # Collect predictions for epoch-end evaluation if enabled\n    if self._collect_val_predictions:\n        # SAVE bbox BEFORE inference (it modifies in-place!)\n        bbox_prep_saved = batch[\"instance_bbox\"].clone()\n\n        with torch.no_grad():\n            inference_output = self.instance_peaks_inf_layer(batch)\n\n        batch_size = len(batch[\"frame_idx\"])\n        for i in range(batch_size):\n            eff = batch[\"eff_scale\"][i].cpu().numpy()\n\n            # Predictions from inference (crop-relative, original scale)\n            pred_peaks_crop = (\n                inference_output[\"pred_instance_peaks\"][i].cpu().numpy()\n            )\n            pred_scores = inference_output[\"pred_peak_values\"][i].cpu().numpy()\n\n            # Compute bbox offset in original space from SAVED prep bbox\n            # bbox has shape (n_samples=1, 4, 2) where 4 corners\n            bbox_prep = bbox_prep_saved[i].squeeze(0).cpu().numpy()  # (4, 2)\n            bbox_top_left_orig = (\n                bbox_prep[0] / eff\n            )  # Top-left corner in original space\n\n            # Full image coordinates (original space)\n            pred_peaks_full = pred_peaks_crop + bbox_top_left_orig\n\n            # GT transform: crop-relative preprocessed -&gt; full image original\n            gt_crop_prep = (\n                batch[\"instance\"][i].squeeze(0).cpu().numpy()\n            )  # (n_nodes, 2)\n            gt_crop_orig = gt_crop_prep / eff\n            gt_full_orig = gt_crop_orig + bbox_top_left_orig\n\n            self.val_predictions.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"pred_peaks\": pred_peaks_full.reshape(\n                        1, -1, 2\n                    ),  # (1, n_nodes, 2)\n                    \"pred_scores\": pred_scores.reshape(1, -1),  # (1, n_nodes)\n                }\n            )\n            self.val_ground_truth.append(\n                {\n                    \"video_idx\": batch[\"video_idx\"][i].item(),\n                    \"frame_idx\": batch[\"frame_idx\"][i].item(),\n                    \"gt_instances\": gt_full_orig.reshape(\n                        1, -1, 2\n                    ),  # (1, n_nodes, 2)\n                    \"num_instances\": 1,\n                }\n            )\n</code></pre>"},{"location":"api/training/lightning_modules/#sleap_nn.training.lightning_modules.TopDownCenteredInstanceMultiClassLightningModule.visualize_example","title":"<code>visualize_example(sample)</code>","text":"<p>Visualize predictions during training (used with callbacks).</p> Source code in <code>sleap_nn/training/lightning_modules.py</code> <pre><code>def visualize_example(self, sample):\n    \"\"\"Visualize predictions during training (used with callbacks).\"\"\"\n    data = self.get_visualization_data(sample)\n    scale = 1.0\n    if data.image.shape[0] &lt; 512:\n        scale = 2.0\n    if data.image.shape[0] &lt; 256:\n        scale = 4.0\n    fig = plot_img(data.image, dpi=72 * scale, scale=scale)\n    plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n    plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n    return fig\n</code></pre>"},{"location":"api/training/losses/","title":"losses","text":""},{"location":"api/training/losses/#sleap_nn.training.losses","title":"<code>sleap_nn.training.losses</code>","text":"<p>Custom loss functions.</p> <p>Functions:</p> Name Description <code>compute_ohkm_loss</code> <p>Compute the online hard keypoint mining loss.</p>"},{"location":"api/training/losses/#sleap_nn.training.losses.compute_ohkm_loss","title":"<code>compute_ohkm_loss(y_gt, y_pr, hard_to_easy_ratio=2.0, min_hard_keypoints=2, max_hard_keypoints=None, loss_scale=5.0)</code>","text":"<p>Compute the online hard keypoint mining loss.</p> Source code in <code>sleap_nn/training/losses.py</code> <pre><code>def compute_ohkm_loss(\n    y_gt: torch.Tensor,\n    y_pr: torch.Tensor,\n    hard_to_easy_ratio: float = 2.0,\n    min_hard_keypoints: int = 2,\n    max_hard_keypoints: Optional[int] = None,\n    loss_scale: float = 5.0,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the online hard keypoint mining loss.\"\"\"\n    if max_hard_keypoints is None:\n        max_hard_keypoints = -1\n    # Compute elementwise squared difference.\n    loss = (y_pr - y_gt) ** 2\n\n    # Store initial shape for normalization.\n    batch_shape = loss.shape\n\n    # Reduce over everything but channels axis.\n    l = torch.sum(loss, dim=(0, 2, 3))\n\n    # Compute the loss for the \"easy\" keypoint.\n    best_loss = torch.min(l)\n\n    # Find the number of hard keypoints.\n    is_hard_keypoint = (l / best_loss) &gt;= hard_to_easy_ratio\n    n_hard_keypoints = torch.sum(is_hard_keypoint.to(torch.int32))\n\n    # Work out the actual final number of keypoints to consider as hard.\n    if max_hard_keypoints &lt; 0:\n        max_hard_keypoints = l.shape[0]\n    else:\n        max_hard_keypoints = min(\n            max_hard_keypoints,\n            l.shape[0],\n        )\n    k = min(\n        max(\n            n_hard_keypoints,\n            min_hard_keypoints,\n        ),\n        max_hard_keypoints,\n    )\n\n    # Pull out the top hard values.\n    k_vals, k_inds = torch.topk(l, k=k, largest=True, sorted=False)\n\n    # Apply weights.\n    k_loss = k_vals * loss_scale\n\n    # Reduce over all channels.\n    n_elements = batch_shape[0] * batch_shape[2] * batch_shape[3] * k\n    k_loss = torch.sum(k_loss) / n_elements\n\n    return k_loss\n</code></pre>"},{"location":"api/training/model_trainer/","title":"model_trainer","text":""},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer","title":"<code>sleap_nn.training.model_trainer</code>","text":"<p>This module is to train a sleap-nn model using Lightning.</p> <p>Classes:</p> Name Description <code>ModelTrainer</code> <p>Train sleap-nn model using PyTorch Lightning.</p>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer","title":"<code>ModelTrainer</code>","text":"<p>Train sleap-nn model using PyTorch Lightning.</p> <p>This class is used to create dataloaders, train a sleap-nn model and save the model checkpoints/ logs with options to logging with wandb and csvlogger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>OmegaConf dictionary which has the following:     (i) data_config: data loading pre-processing configs.     (ii) model_config: backbone and head configs to be passed to <code>Model</code> class.     (iii) trainer_config: trainer configs like accelerator, optimiser params, etc.</p> required <code>train_labels</code> <p>List of <code>sio.Labels</code> objects for training dataset.</p> required <code>val_labels</code> <p>List of <code>sio.Labels</code> objects for validation dataset.</p> required <code>skeletons</code> <p>List of <code>sio.Skeleton</code> objects in a single slp file.</p> required <code>lightning_model</code> <p>One of the child classes of <code>sleap_nn.training.lightning_modules.LightningModel</code>.</p> required <code>model_type</code> <p>Type of the model. One of <code>single_instance</code>, <code>centered_instance</code>, <code>centroid</code>, <code>bottomup</code>, <code>multi_class_bottomup</code>, <code>multi_class_topdown</code>.</p> required <code>backbone_type</code> <p>Backbone model. One of <code>unet</code>, <code>convnext</code> and <code>swint</code>.</p> required <code>trainer</code> <p>Instance of the <code>lightning.Trainer</code> initialized with loggers and callbacks.</p> required <p>Methods:</p> Name Description <code>get_model_trainer_from_config</code> <p>Create a model trainer instance from config.</p> <code>setup_config</code> <p>Compute config parameters.</p> <code>train</code> <p>Train the lightning model.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>@attrs.define\nclass ModelTrainer:\n    \"\"\"Train sleap-nn model using PyTorch Lightning.\n\n    This class is used to create dataloaders, train a sleap-nn model and save the model checkpoints/ logs with options to logging\n    with wandb and csvlogger.\n\n    Args:\n        config: OmegaConf dictionary which has the following:\n                (i) data_config: data loading pre-processing configs.\n                (ii) model_config: backbone and head configs to be passed to `Model` class.\n                (iii) trainer_config: trainer configs like accelerator, optimiser params, etc.\n        train_labels: List of `sio.Labels` objects for training dataset.\n        val_labels: List of `sio.Labels` objects for validation dataset.\n        skeletons: List of `sio.Skeleton` objects in a single slp file.\n        lightning_model: One of the child classes of `sleap_nn.training.lightning_modules.LightningModel`.\n        model_type: Type of the model. One of `single_instance`, `centered_instance`, `centroid`, `bottomup`, `multi_class_bottomup`, `multi_class_topdown`.\n        backbone_type: Backbone model. One of `unet`, `convnext` and `swint`.\n        trainer: Instance of the `lightning.Trainer` initialized with loggers and callbacks.\n    \"\"\"\n\n    config: DictConfig\n    _initial_config: Optional[DictConfig] = None\n    train_labels: List[sio.Labels] = attrs.field(factory=list)\n    val_labels: List[sio.Labels] = attrs.field(factory=list)\n    skeletons: Optional[List[sio.Skeleton]] = None\n\n    lightning_model: Optional[LightningModel] = None\n    model_type: Optional[str] = None\n    backbone_type: Optional[str] = None\n\n    _profilers: dict = {\n        \"advanced\": AdvancedProfiler(),\n        \"passthrough\": PassThroughProfiler(),\n        \"pytorch\": PyTorchProfiler(),\n        \"simple\": SimpleProfiler(),\n    }\n\n    trainer: Optional[L.Trainer] = None\n\n    @classmethod\n    def get_model_trainer_from_config(\n        cls,\n        config: DictConfig,\n        train_labels: Optional[List[sio.Labels]] = None,\n        val_labels: Optional[List[sio.Labels]] = None,\n    ):\n        \"\"\"Create a model trainer instance from config.\"\"\"\n        # Verify config structure.\n        config = verify_training_cfg(config)\n\n        model_trainer = cls(config=config)\n\n        model_trainer.model_type = get_model_type_from_cfg(model_trainer.config)\n        model_trainer.backbone_type = get_backbone_type_from_cfg(model_trainer.config)\n\n        if model_trainer.config.trainer_config.seed is not None:\n            model_trainer._set_seed()\n\n        if train_labels is None and val_labels is None:\n            # read labels from paths provided in the config\n            train_labels = [\n                sio.load_slp(path)\n                for path in model_trainer.config.data_config.train_labels_path\n            ]\n            val_labels = (\n                [\n                    sio.load_slp(path)\n                    for path in model_trainer.config.data_config.val_labels_path\n                ]\n                if model_trainer.config.data_config.val_labels_path is not None\n                else None\n            )\n            model_trainer._setup_train_val_labels(\n                labels=train_labels, val_labels=val_labels\n            )\n        else:\n            model_trainer._setup_train_val_labels(\n                labels=train_labels, val_labels=val_labels\n            )\n\n        model_trainer._initial_config = model_trainer.config.copy()\n        # update config parameters\n        model_trainer.setup_config()\n\n        # Check if all videos exist across all labels\n        all_videos_exist = all(\n            video.exists(check_all=True)\n            for labels in [*model_trainer.train_labels, *model_trainer.val_labels]\n            for video in labels.videos\n        )\n\n        if not all_videos_exist:\n            raise FileNotFoundError(\n                \"One or more video files do not exist or are not accessible.\"\n            )\n\n        return model_trainer\n\n    def _set_seed(self):\n        \"\"\"Set seed for the current experiment.\"\"\"\n        seed = self.config.trainer_config.seed\n\n        random.seed(seed)\n\n        # torch\n        torch.manual_seed(seed)\n\n        # if cuda is available\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n\n        # lightning\n        L.seed_everything(seed)\n\n        # numpy\n        np.random.seed(seed)\n\n    def _get_trainer_devices(self):\n        \"\"\"Get trainer devices.\"\"\"\n        trainer_devices = (\n            self.config.trainer_config.trainer_devices\n            if self.config.trainer_config.trainer_devices is not None\n            else \"auto\"\n        )\n        if (\n            trainer_devices == \"auto\"\n            and OmegaConf.select(\n                self.config, \"trainer_config.trainer_device_indices\", default=None\n            )\n            is not None\n        ):\n            trainer_devices = len(\n                OmegaConf.select(\n                    self.config,\n                    \"trainer_config.trainer_device_indices\",\n                    default=None,\n                )\n            )\n        elif trainer_devices == \"auto\":\n            if torch.cuda.is_available():\n                trainer_devices = torch.cuda.device_count()\n            elif torch.backends.mps.is_available():\n                trainer_devices = 1\n            elif torch.xpu.is_available():\n                trainer_devices = torch.xpu.device_count()\n            else:\n                trainer_devices = 1\n        return trainer_devices\n\n    def _count_labeled_frames(\n        self, labels_list: List[sio.Labels], user_only: bool = True\n    ) -&gt; int:\n        \"\"\"Count labeled frames, optionally filtering to user-labeled only.\n\n        Args:\n            labels_list: List of Labels objects to count frames from.\n            user_only: If True, count only frames with user instances.\n\n        Returns:\n            Total count of labeled frames.\n        \"\"\"\n        total = 0\n        for label in labels_list:\n            if user_only:\n                total += sum(1 for lf in label if lf.has_user_instances)\n            else:\n                total += len(label)\n        return total\n\n    def _filter_to_user_labeled(self, labels: sio.Labels) -&gt; sio.Labels:\n        \"\"\"Filter a Labels object to only include user-labeled frames.\n\n        Args:\n            labels: Labels object to filter.\n\n        Returns:\n            New Labels object containing only frames with user instances.\n        \"\"\"\n        # Filter labeled frames to only those with user instances\n        user_lfs = [lf for lf in labels if lf.has_user_instances]\n\n        # Set instances to user instances only\n        for lf in user_lfs:\n            lf.instances = lf.user_instances\n\n        # Create new Labels with filtered frames\n        return sio.Labels(\n            labeled_frames=user_lfs,\n            videos=labels.videos,\n            skeletons=labels.skeletons,\n            tracks=labels.tracks,\n            suggestions=labels.suggestions,\n            provenance=labels.provenance,\n        )\n\n    def _setup_train_val_labels(\n        self,\n        labels: Optional[List[sio.Labels]] = None,\n        val_labels: Optional[List[sio.Labels]] = None,\n    ):\n        \"\"\"Create train and val labels objects. (Initialize `self.train_labels` and `self.val_labels`).\"\"\"\n        logger.info(f\"Creating train-val split...\")\n        total_train_lfs = 0\n        total_val_lfs = 0\n        self.skeletons = labels[0].skeletons\n\n        # Check if we should count only user-labeled frames\n        user_instances_only = OmegaConf.select(\n            self.config, \"data_config.user_instances_only\", default=True\n        )\n\n        # check if all `.slp` file shave same skeleton structure (if multiple slp file paths are provided)\n        skeleton = self.skeletons[0]\n        for index, train_label in enumerate(labels):\n            skel_temp = train_label.skeletons[0]\n            skeletons_equal = skeleton.matches(skel_temp)\n            if not skeletons_equal:\n                message = f\"The skeletons in the training labels: {index + 1} do not match the skeleton in the first training label file.\"\n                logger.error(message)\n                raise ValueError(message)\n\n        # Check for same-data mode (train = val, for intentional overfitting)\n        use_same = OmegaConf.select(\n            self.config, \"data_config.use_same_data_for_val\", default=False\n        )\n\n        if use_same:\n            # Same mode: use identical data for train and val (for overfitting)\n            logger.info(\"Using same data for train and val (overfit mode)\")\n            self.train_labels = labels\n            self.val_labels = labels\n            total_train_lfs = self._count_labeled_frames(labels, user_instances_only)\n            total_val_lfs = total_train_lfs\n        elif val_labels is None or not len(val_labels):\n            # if val labels are not provided, split from train\n            val_fraction = OmegaConf.select(\n                self.config, \"data_config.validation_fraction\", default=0.1\n            )\n            seed = (\n                42\n                if (\n                    self.config.trainer_config.seed is None\n                    and self._get_trainer_devices() &gt; 1\n                )\n                else self.config.trainer_config.seed\n            )\n            for label in labels:\n                train_split, val_split = label.make_training_splits(\n                    n_train=1 - val_fraction, n_val=val_fraction, seed=seed\n                )\n                self.train_labels.append(train_split)\n                self.val_labels.append(val_split)\n                # make_training_splits returns only user-labeled frames\n                total_train_lfs += len(train_split)\n                total_val_lfs += len(val_split)\n        else:\n            self.train_labels = labels\n            self.val_labels = val_labels\n            total_train_lfs = self._count_labeled_frames(labels, user_instances_only)\n            total_val_lfs = self._count_labeled_frames(val_labels, user_instances_only)\n\n        logger.info(f\"# Train Labeled frames: {total_train_lfs}\")\n        logger.info(f\"# Val Labeled frames: {total_val_lfs}\")\n\n    def _setup_preprocessing_config(self):\n        \"\"\"Setup preprocessing config.\"\"\"\n        # compute max_heigt, max_width, and crop_size (if not provided in the config)\n        max_height = self.config.data_config.preprocessing.max_height\n        max_width = self.config.data_config.preprocessing.max_width\n        if (\n            self.model_type == \"centered_instance\"\n            or self.model_type == \"multi_class_topdown\"\n        ):\n            crop_size = self.config.data_config.preprocessing.crop_size\n\n        max_h, max_w = 0, 0\n        max_crop_size = 0\n\n        for train_label in self.train_labels:\n            # compute max h and w from slp file if not provided\n            if max_height is None or max_width is None:\n                current_max_h, current_max_w = get_max_height_width(train_label)\n\n                if current_max_h &gt; max_h:\n                    max_h = current_max_h\n                if current_max_w &gt; max_w:\n                    max_w = current_max_w\n\n            if (\n                self.model_type == \"centered_instance\"\n                or self.model_type == \"multi_class_topdown\"\n            ):\n                # compute crop size if not provided in config\n                if crop_size is None:\n                    # Get padding from config or auto-compute from augmentation settings\n                    padding = self.config.data_config.preprocessing.crop_padding\n                    if padding is None:\n                        # Auto-compute padding based on augmentation settings\n                        aug_config = self.config.data_config.augmentation_config\n                        if (\n                            self.config.data_config.use_augmentations_train\n                            and aug_config is not None\n                            and aug_config.geometric is not None\n                        ):\n                            geo = aug_config.geometric\n                            # Check if rotation is enabled (via rotation_p or affine_p)\n                            rotation_enabled = (\n                                geo.rotation_p is not None and geo.rotation_p &gt; 0\n                            ) or (\n                                geo.rotation_p is None\n                                and geo.scale_p is None\n                                and geo.translate_p is None\n                                and geo.affine_p &gt; 0\n                            )\n                            # Check if scale is enabled (via scale_p or affine_p)\n                            scale_enabled = (\n                                geo.scale_p is not None and geo.scale_p &gt; 0\n                            ) or (\n                                geo.rotation_p is None\n                                and geo.scale_p is None\n                                and geo.translate_p is None\n                                and geo.affine_p &gt; 0\n                            )\n\n                            if rotation_enabled or scale_enabled:\n                                # First find the actual max bbox size from labels\n                                bbox_size = find_max_instance_bbox_size(train_label)\n                                bbox_size = max(\n                                    bbox_size,\n                                    self.config.data_config.preprocessing.min_crop_size\n                                    or 100,\n                                )\n                                rotation_max = (\n                                    max(\n                                        abs(geo.rotation_min),\n                                        abs(geo.rotation_max),\n                                    )\n                                    if rotation_enabled\n                                    else 0.0\n                                )\n                                scale_max = geo.scale_max if scale_enabled else 1.0\n                                padding = compute_augmentation_padding(\n                                    bbox_size=bbox_size,\n                                    rotation_max=rotation_max,\n                                    scale_max=scale_max,\n                                )\n                            else:\n                                padding = 0\n                        else:\n                            padding = 0\n\n                    crop_sz = find_instance_crop_size(\n                        labels=train_label,\n                        padding=padding,\n                        maximum_stride=self.config.model_config.backbone_config[\n                            f\"{self.backbone_type}\"\n                        ][\"max_stride\"],\n                        min_crop_size=self.config.data_config.preprocessing.min_crop_size,\n                    )\n\n                    if crop_sz &gt; max_crop_size:\n                        max_crop_size = crop_sz\n\n        # if preprocessing params were None, replace with computed params\n        if max_height is None or max_width is None:\n            self.config.data_config.preprocessing.max_height = max_h\n            self.config.data_config.preprocessing.max_width = max_w\n\n        if (\n            self.model_type == \"centered_instance\"\n            or self.model_type == \"multi_class_topdown\"\n        ) and crop_size is None:\n            self.config.data_config.preprocessing.crop_size = max_crop_size\n\n    def _setup_head_config(self):\n        \"\"\"Setup node, edge and class names in head config.\"\"\"\n        # if edges and part names aren't set in head configs, get it from labels object.\n        head_config = self.config.model_config.head_configs[self.model_type]\n        for key in head_config:\n            if \"part_names\" in head_config[key].keys():\n                if head_config[key][\"part_names\"] is None:\n                    self.config.model_config.head_configs[self.model_type][key][\n                        \"part_names\"\n                    ] = self.skeletons[0].node_names\n\n            if \"edges\" in head_config[key].keys():\n                if head_config[key][\"edges\"] is None:\n                    edges = [\n                        (x.source.name, x.destination.name)\n                        for x in self.skeletons[0].edges\n                    ]\n                    self.config.model_config.head_configs[self.model_type][key][\n                        \"edges\"\n                    ] = edges\n\n            if \"classes\" in head_config[key].keys():\n                if head_config[key][\"classes\"] is None:\n                    tracks = []\n                    for train_label in self.train_labels:\n                        tracks.extend(\n                            [x.name for x in train_label.tracks if x is not None]\n                        )\n                    classes = list(set(tracks))\n                    if not len(classes):\n                        message = (\n                            f\"No tracks found. ID models need tracks to be defined.\"\n                        )\n                        logger.error(message)\n                        raise Exception(message)\n                    self.config.model_config.head_configs[self.model_type][key][\n                        \"classes\"\n                    ] = classes\n\n    def _setup_ckpt_path(self):\n        \"\"\"Setup checkpoint path.\"\"\"\n        # if run_name is None, assign a new dir name\n        ckpt_dir = self.config.trainer_config.ckpt_dir\n        if ckpt_dir is None or ckpt_dir == \"\" or ckpt_dir == \"None\":\n            ckpt_dir = \".\"\n            self.config.trainer_config.ckpt_dir = ckpt_dir\n        run_name = self.config.trainer_config.run_name\n        run_name_is_empty = run_name is None or run_name == \"\" or run_name == \"None\"\n\n        # Validate: multi-GPU + disk cache requires explicit run_name\n        if run_name_is_empty:\n            is_disk_caching = (\n                self.config.data_config.data_pipeline_fw\n                == \"torch_dataset_cache_img_disk\"\n            )\n            num_devices = self._get_trainer_devices()\n\n            if is_disk_caching and num_devices &gt; 1:\n                raise ValueError(\n                    f\"Multi-GPU training with disk caching requires an explicit `run_name`.\\n\\n\"\n                    f\"Detected {num_devices} device(s) with \"\n                    f\"`data_pipeline_fw='torch_dataset_cache_img_disk'`.\\n\"\n                    f\"Without an explicit run_name, each GPU worker generates a different \"\n                    f\"timestamp-based directory, causing cache synchronization failures.\\n\\n\"\n                    f\"Please provide a run_name using one of these methods:\\n\"\n                    f\"  - CLI: sleap-nn train config.yaml trainer_config.run_name=my_experiment\\n\"\n                    f\"  - Config file: Set `trainer_config.run_name: my_experiment`\\n\"\n                    f\"  - Python API: train(..., run_name='my_experiment')\"\n                )\n\n            # Auto-generate timestamp-based run_name (safe for single GPU or non-disk-cache)\n            sum_train_lfs = sum([len(train_label) for train_label in self.train_labels])\n            sum_val_lfs = sum([len(val_label) for val_label in self.val_labels])\n            run_name = (\n                datetime.now().strftime(\"%y%m%d_%H%M%S\")\n                + f\".{self.model_type}.n={sum_train_lfs + sum_val_lfs}\"\n            )\n\n        # If checkpoint path already exists, add suffix to prevent overwriting\n        if (Path(ckpt_dir) / run_name).exists() and (\n            Path(ckpt_dir) / run_name / \"best.ckpt\"\n        ).exists():\n            logger.info(\n                f\"Checkpoint path already exists: {Path(ckpt_dir) / run_name}... adding suffix to prevent overwriting.\"\n            )\n            for i in count(1):\n                new_run_name = f\"{run_name}-{i}\"\n                if not (Path(ckpt_dir) / new_run_name).exists():\n                    run_name = new_run_name\n                    break\n\n        self.config.trainer_config.run_name = run_name\n\n        # set output dir for cache img\n        if self.config.data_config.data_pipeline_fw == \"torch_dataset_cache_img_disk\":\n            if self.config.data_config.cache_img_path is None:\n                self.config.data_config.cache_img_path = (\n                    Path(self.config.trainer_config.ckpt_dir)\n                    / self.config.trainer_config.run_name\n                )\n\n    def _verify_model_input_channels(self):\n        \"\"\"Verify input channels in model_config based on input image and pretrained model weights.\"\"\"\n        # check in channels, verify with img channels / ensure_rgb/ ensure_grayscale\n        if self.train_labels[0] is not None:\n            img_channels = self.train_labels[0][0].image.shape[-1]\n            if self.config.data_config.preprocessing.ensure_rgb:\n                img_channels = 3\n            if self.config.data_config.preprocessing.ensure_grayscale:\n                img_channels = 1\n            if (\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels\n                != img_channels\n            ):\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels = img_channels\n                logger.info(\n                    f\"Updating backbone in_channels to {img_channels} based on the input image channels.\"\n                )\n\n        # verify input img channels with pretrained model ckpts (if any)\n        if (\n            self.backbone_type == \"convnext\" or self.backbone_type == \"swint\"\n        ) and self.config.model_config.backbone_config[\n            f\"{self.backbone_type}\"\n        ].pre_trained_weights is not None:\n            if (\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels\n                != 3\n            ):\n                self.config.model_config.backbone_config[\n                    f\"{self.backbone_type}\"\n                ].in_channels = 3\n                self.config.data_config.preprocessing.ensure_rgb = True\n                self.config.data_config.preprocessing.ensure_grayscale = False\n                logger.info(\n                    f\"Updating backbone in_channels to 3 based on the pretrained model weights.\"\n                )\n\n        elif (\n            self.backbone_type == \"unet\"\n            and self.config.model_config.pretrained_backbone_weights is not None\n        ):\n            if self.config.model_config.pretrained_backbone_weights.endswith(\".ckpt\"):\n                pretrained_backbone_ckpt = torch.load(\n                    self.config.model_config.pretrained_backbone_weights,\n                    map_location=\"cpu\",  # this will be loaded on cpu as it's just used to get the input channels\n                    weights_only=False,\n                )\n                input_channels = list(pretrained_backbone_ckpt[\"state_dict\"].values())[\n                    0\n                ].shape[\n                    -3\n                ]  # get input channels from first layer\n                if (\n                    self.config.model_config.backbone_config.unet.in_channels\n                    != input_channels\n                ):\n                    self.config.model_config.backbone_config.unet.in_channels = (\n                        input_channels\n                    )\n                    logger.info(\n                        f\"Updating backbone in_channels to {input_channels} based on the pretrained model weights.\"\n                    )\n\n                    if input_channels == 1:\n                        self.config.data_config.preprocessing.ensure_grayscale = True\n                        self.config.data_config.preprocessing.ensure_rgb = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_grayscale to True based on the pretrained model weights.\"\n                        )\n                    elif input_channels == 3:\n                        self.config.data_config.preprocessing.ensure_rgb = True\n                        self.config.data_config.preprocessing.ensure_grayscale = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_rgb to True based on the pretrained model weights.\"\n                        )\n\n            elif self.config.model_config.pretrained_backbone_weights.endswith(\".h5\"):\n                input_channels = get_keras_first_layer_channels(\n                    self.config.model_config.pretrained_backbone_weights\n                )\n                if (\n                    self.config.model_config.backbone_config.unet.in_channels\n                    != input_channels\n                ):\n                    self.config.model_config.backbone_config.unet.in_channels = (\n                        input_channels\n                    )\n                    logger.info(\n                        f\"Updating backbone in_channels to {input_channels} based on the pretrained model weights.\"\n                    )\n\n                    if input_channels == 1:\n                        self.config.data_config.preprocessing.ensure_grayscale = True\n                        self.config.data_config.preprocessing.ensure_rgb = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_grayscale to True based on the pretrained model weights.\"\n                        )\n                    elif input_channels == 3:\n                        self.config.data_config.preprocessing.ensure_rgb = True\n                        self.config.data_config.preprocessing.ensure_grayscale = False\n                        logger.info(\n                            f\"Updating data preprocessing to ensure_rgb to True based on the pretrained model weights.\"\n                        )\n\n    def setup_config(self):\n        \"\"\"Compute config parameters.\"\"\"\n        logger.info(\"Setting up config...\")\n\n        # Normalize empty strings to None for optional wandb fields\n        if self.config.trainer_config.wandb.prv_runid == \"\":\n            self.config.trainer_config.wandb.prv_runid = None\n\n        # compute preprocessing parameters from the labels objects and fill in the config\n        self._setup_preprocessing_config()\n\n        # save skeleton to config\n        skeleton_yaml = yaml.safe_load(SkeletonYAMLEncoder().encode(self.skeletons))\n        skeleton_names = skeleton_yaml.keys()\n        self.config[\"data_config\"][\"skeletons\"] = []\n        for skeleton_name in skeleton_names:\n            skl = skeleton_yaml[skeleton_name]\n            skl[\"name\"] = skeleton_name\n            self.config[\"data_config\"][\"skeletons\"].append(skl)\n\n        # setup head config - partnames, edges and class names\n        self._setup_head_config()\n\n        # set max stride for the backbone: convnext and swint\n        if self.backbone_type == \"convnext\":\n            self.config.model_config.backbone_config.convnext.max_stride = (\n                self.config.model_config.backbone_config.convnext.stem_patch_stride\n                * (2**3)\n                * 2\n            )\n        elif self.backbone_type == \"swint\":\n            self.config.model_config.backbone_config.swint.max_stride = (\n                self.config.model_config.backbone_config.swint.stem_patch_stride\n                * (2**3)\n                * 2\n            )\n\n        # set output stride for backbone from head config and verify max stride\n        self.config = check_output_strides(self.config)\n\n        # if trainer_devices is None, set it to \"auto\"\n        if self.config.trainer_config.trainer_devices is None:\n            self.config.trainer_config.trainer_devices = (\n                \"auto\"\n                if OmegaConf.select(\n                    self.config, \"trainer_config.trainer_device_indices\", default=None\n                )\n                is None\n                else len(\n                    OmegaConf.select(\n                        self.config,\n                        \"trainer_config.trainer_device_indices\",\n                        default=None,\n                    )\n                )\n            )\n\n        # setup checkpoint path (generates run_name if not specified)\n        self._setup_ckpt_path()\n\n        # Default wandb run name to trainer run_name if not specified\n        # Note: This must come after _setup_ckpt_path() which generates run_name\n        if self.config.trainer_config.wandb.name is None:\n            self.config.trainer_config.wandb.name = self.config.trainer_config.run_name\n\n        # verify input_channels in model_config based on input image and pretrained model weights\n        self._verify_model_input_channels()\n\n    def _setup_model_ckpt_dir(self):\n        \"\"\"Create the model ckpt folder and save ground truth labels.\"\"\"\n        ckpt_path = (\n            Path(self.config.trainer_config.ckpt_dir)\n            / self.config.trainer_config.run_name\n        ).as_posix()\n        logger.info(f\"Setting up model ckpt dir: `{ckpt_path}`...\")\n\n        # Only rank 0 (or non-distributed) should create directories and save files\n        if RANK in [0, -1]:\n            if not Path(ckpt_path).exists():\n                try:\n                    Path(ckpt_path).mkdir(parents=True, exist_ok=True)\n                except OSError as e:\n                    message = f\"Cannot create a new folder in {ckpt_path}.\\n {e}\"\n                    logger.error(message)\n                    raise OSError(message)\n            # Check if we should filter to user-labeled frames only\n            user_instances_only = OmegaConf.select(\n                self.config, \"data_config.user_instances_only\", default=True\n            )\n\n            # Save train and val ground truth labels\n            for idx, (train, val) in enumerate(zip(self.train_labels, self.val_labels)):\n                # Filter to user-labeled frames if needed (for evaluation)\n                if user_instances_only:\n                    train_filtered = self._filter_to_user_labeled(train)\n                    val_filtered = self._filter_to_user_labeled(val)\n                else:\n                    train_filtered = train\n                    val_filtered = val\n\n                train_filtered.save(\n                    Path(ckpt_path) / f\"labels_gt.train.{idx}.slp\",\n                    restore_original_videos=False,\n                )\n                val_filtered.save(\n                    Path(ckpt_path) / f\"labels_gt.val.{idx}.slp\",\n                    restore_original_videos=False,\n                )\n\n            # Save test ground truth labels if test paths are provided\n            test_file_path = OmegaConf.select(\n                self.config, \"data_config.test_file_path\", default=None\n            )\n            if test_file_path is not None:\n                # Normalize to list of strings\n                if isinstance(test_file_path, str):\n                    test_paths = [test_file_path]\n                else:\n                    test_paths = list(test_file_path)\n\n                for idx, test_path in enumerate(test_paths):\n                    # Only save if it's a .slp file (not a video file)\n                    if test_path.endswith(\".slp\") or test_path.endswith(\".pkg.slp\"):\n                        try:\n                            test_labels = sio.load_slp(test_path)\n                            if user_instances_only:\n                                test_filtered = self._filter_to_user_labeled(\n                                    test_labels\n                                )\n                            else:\n                                test_filtered = test_labels\n                            test_filtered.save(\n                                Path(ckpt_path) / f\"labels_gt.test.{idx}.slp\",\n                                restore_original_videos=False,\n                            )\n                        except Exception as e:\n                            logger.warning(\n                                f\"Could not save test ground truth for {test_path}: {e}\"\n                            )\n\n    def _setup_viz_datasets(self):\n        \"\"\"Setup dataloaders.\"\"\"\n        data_viz_config = self.config.copy()\n        data_viz_config.data_config.data_pipeline_fw = \"torch_dataset\"\n\n        return get_train_val_datasets(\n            train_labels=self.train_labels,\n            val_labels=self.val_labels,\n            config=data_viz_config,\n            rank=-1,\n        )\n\n    def _setup_datasets(self):\n        \"\"\"Setup dataloaders.\"\"\"\n        base_cache_img_path = None\n        if self.config.data_config.data_pipeline_fw == \"torch_dataset_cache_img_memory\":\n            # check available memory. If insufficient memory, default to disk caching.\n            # Account for DataLoader worker memory overhead\n            train_num_workers = self.config.trainer_config.train_data_loader.num_workers\n            val_num_workers = self.config.trainer_config.val_data_loader.num_workers\n            max_num_workers = max(train_num_workers, val_num_workers)\n\n            mem_available = check_cache_memory(\n                self.train_labels,\n                self.val_labels,\n                memory_buffer=MEMORY_BUFFER,\n                num_workers=max_num_workers,\n            )\n            if not mem_available:\n                # Validate: multi-GPU + auto-generated run_name + fallback to disk cache\n                original_run_name = self._initial_config.trainer_config.run_name\n                run_name_was_auto = (\n                    original_run_name is None\n                    or original_run_name == \"\"\n                    or original_run_name == \"None\"\n                )\n                if run_name_was_auto and self.trainer.num_devices &gt; 1:\n                    raise ValueError(\n                        f\"Memory caching failed and disk caching fallback requires an \"\n                        f\"explicit `run_name` for multi-GPU training.\\n\\n\"\n                        f\"Detected {self.trainer.num_devices} device(s) with insufficient \"\n                        f\"memory for in-memory caching.\\n\"\n                        f\"Without an explicit run_name, each GPU worker generates a different \"\n                        f\"timestamp-based directory, causing cache synchronization failures.\\n\\n\"\n                        f\"Please provide a run_name using one of these methods:\\n\"\n                        f\"  - CLI: sleap-nn train config.yaml trainer_config.run_name=my_experiment\\n\"\n                        f\"  - Config file: Set `trainer_config.run_name: my_experiment`\\n\"\n                        f\"  - Python API: train(..., run_name='my_experiment')\\n\\n\"\n                        f\"Alternatively, use `data_pipeline_fw='torch_dataset'` to disable caching.\"\n                    )\n\n                self.config.data_config.data_pipeline_fw = (\n                    \"torch_dataset_cache_img_disk\"\n                )\n                base_cache_img_path = Path(\"./\")\n                logger.info(\n                    f\"Insufficient memory for in-memory caching. `jpg` files will be created for disk-caching.\"\n                )\n            self.config.data_config.cache_img_path = base_cache_img_path\n\n        elif self.config.data_config.data_pipeline_fw == \"torch_dataset_cache_img_disk\":\n            # Get cache img path\n            base_cache_img_path = (\n                Path(self.config.data_config.cache_img_path)\n                if self.config.data_config.cache_img_path is not None\n                else Path(self.config.trainer_config.ckpt_dir)\n                / self.config.trainer_config.run_name\n            )\n\n            if self.config.data_config.cache_img_path is None:\n                self.config.data_config.cache_img_path = base_cache_img_path\n\n        return get_train_val_datasets(\n            train_labels=self.train_labels,\n            val_labels=self.val_labels,\n            config=self.config,\n            rank=self.trainer.global_rank,\n        )\n\n    def _setup_loggers_callbacks(self, viz_train_dataset, viz_val_dataset):\n        \"\"\"Create loggers and callbacks.\"\"\"\n        logger.info(\"Setting up callbacks and loggers...\")\n        loggers = []\n        callbacks = []\n        if self.config.trainer_config.save_ckpt:\n            # checkpoint callback\n            checkpoint_callback = ModelCheckpoint(\n                save_top_k=self.config.trainer_config.model_ckpt.save_top_k,\n                save_last=self.config.trainer_config.model_ckpt.save_last,\n                dirpath=(\n                    Path(self.config.trainer_config.ckpt_dir)\n                    / self.config.trainer_config.run_name\n                ).as_posix(),\n                filename=\"best\",\n                monitor=\"val/loss\",\n                mode=\"min\",\n            )\n            callbacks.append(checkpoint_callback)\n\n            # csv log callback\n            csv_log_keys = [\n                \"epoch\",\n                \"train/loss\",\n                \"val/loss\",\n                \"learning_rate\",\n                \"train/time\",\n                \"val/time\",\n            ]\n            # Add model-specific keys for wandb parity\n            if self.model_type in [\n                \"single_instance\",\n                \"centered_instance\",\n                \"multi_class_topdown\",\n            ]:\n                csv_log_keys.extend(\n                    [f\"train/confmaps/{name}\" for name in self.skeletons[0].node_names]\n                )\n            if self.model_type == \"bottomup\":\n                csv_log_keys.extend(\n                    [\n                        \"train/confmaps_loss\",\n                        \"train/paf_loss\",\n                        \"val/confmaps_loss\",\n                        \"val/paf_loss\",\n                    ]\n                )\n            if self.model_type == \"multi_class_bottomup\":\n                csv_log_keys.extend(\n                    [\n                        \"train/confmaps_loss\",\n                        \"train/classmap_loss\",\n                        \"train/class_accuracy\",\n                        \"val/confmaps_loss\",\n                        \"val/classmap_loss\",\n                        \"val/class_accuracy\",\n                    ]\n                )\n            if self.model_type == \"multi_class_topdown\":\n                csv_log_keys.extend(\n                    [\n                        \"train/confmaps_loss\",\n                        \"train/classvector_loss\",\n                        \"train/class_accuracy\",\n                        \"val/confmaps_loss\",\n                        \"val/classvector_loss\",\n                        \"val/class_accuracy\",\n                    ]\n                )\n            csv_logger = CSVLoggerCallback(\n                filepath=Path(self.config.trainer_config.ckpt_dir)\n                / self.config.trainer_config.run_name\n                / \"training_log.csv\",\n                keys=csv_log_keys,\n            )\n            callbacks.append(csv_logger)\n\n        if self.config.trainer_config.early_stopping.stop_training_on_plateau:\n            # early stopping callback\n            callbacks.append(\n                EarlyStopping(\n                    monitor=\"val/loss\",\n                    mode=\"min\",\n                    verbose=False,\n                    min_delta=self.config.trainer_config.early_stopping.min_delta,\n                    patience=self.config.trainer_config.early_stopping.patience,\n                )\n            )\n\n        if self.config.trainer_config.use_wandb:\n            # wandb logger\n            wandb_config = self.config.trainer_config.wandb\n            if wandb_config.wandb_mode == \"offline\":\n                os.environ[\"WANDB_MODE\"] = \"offline\"\n            else:\n                if RANK in [0, -1]:\n                    wandb.login(key=self.config.trainer_config.wandb.api_key)\n            wandb_logger = WandbLogger(\n                entity=wandb_config.entity,\n                project=wandb_config.project,\n                name=wandb_config.name,\n                save_dir=(\n                    Path(self.config.trainer_config.ckpt_dir)\n                    / self.config.trainer_config.run_name\n                ).as_posix(),\n                id=self.config.trainer_config.wandb.prv_runid,\n                group=self.config.trainer_config.wandb.group,\n            )\n            loggers.append(wandb_logger)\n\n            # Log message about wandb local logs cleanup\n            should_delete_wandb_logs = wandb_config.delete_local_logs is True or (\n                wandb_config.delete_local_logs is None\n                and wandb_config.wandb_mode != \"offline\"\n            )\n            if should_delete_wandb_logs:\n                logger.info(\n                    \"WandB local logs will be deleted after training completes. \"\n                    \"To keep logs, set trainer_config.wandb.delete_local_logs=false\"\n                )\n\n            # save the configs as yaml in the checkpoint dir\n            # Mask API key in both configs to prevent saving to disk\n            self.config.trainer_config.wandb.api_key = \"\"\n            if self._initial_config is not None:\n                self._initial_config.trainer_config.wandb.api_key = \"\"\n\n        # zmq callbacks\n        if self.config.trainer_config.zmq.controller_port is not None:\n            controller_address = \"tcp://127.0.0.1:\" + str(\n                self.config.trainer_config.zmq.controller_port\n            )\n            callbacks.append(TrainingControllerZMQ(address=controller_address))\n        if self.config.trainer_config.zmq.publish_port is not None:\n            publish_address = \"tcp://127.0.0.1:\" + str(\n                self.config.trainer_config.zmq.publish_port\n            )\n            callbacks.append(ProgressReporterZMQ(address=publish_address))\n\n        # viz callbacks - use unified callback for all visualization outputs\n        if self.config.trainer_config.visualize_preds_during_training:\n            viz_dir = (\n                Path(self.config.trainer_config.ckpt_dir)\n                / self.config.trainer_config.run_name\n                / \"viz\"\n            )\n            if not Path(viz_dir).exists():\n                if RANK in [0, -1]:\n                    Path(viz_dir).mkdir(parents=True, exist_ok=True)\n\n            # Get wandb viz config options\n            log_wandb = self.config.trainer_config.use_wandb and OmegaConf.select(\n                self.config, \"trainer_config.wandb.save_viz_imgs_wandb\", default=False\n            )\n            wandb_modes = []\n            if log_wandb:\n                if OmegaConf.select(\n                    self.config, \"trainer_config.wandb.viz_enabled\", default=True\n                ):\n                    wandb_modes.append(\"direct\")\n                if OmegaConf.select(\n                    self.config, \"trainer_config.wandb.viz_boxes\", default=False\n                ):\n                    wandb_modes.append(\"boxes\")\n                if OmegaConf.select(\n                    self.config, \"trainer_config.wandb.viz_masks\", default=False\n                ):\n                    wandb_modes.append(\"masks\")\n\n            # Single unified callback handles all visualization outputs\n            callbacks.append(\n                UnifiedVizCallback(\n                    model_trainer=self,\n                    train_dataset=viz_train_dataset,\n                    val_dataset=viz_val_dataset,\n                    model_type=self.model_type,\n                    save_local=self.config.trainer_config.save_ckpt,\n                    local_save_dir=viz_dir,\n                    log_wandb=log_wandb,\n                    wandb_modes=wandb_modes if wandb_modes else [\"direct\"],\n                    wandb_box_size=OmegaConf.select(\n                        self.config, \"trainer_config.wandb.viz_box_size\", default=5.0\n                    ),\n                    wandb_confmap_threshold=OmegaConf.select(\n                        self.config,\n                        \"trainer_config.wandb.viz_confmap_threshold\",\n                        default=0.1,\n                    ),\n                    log_wandb_table=OmegaConf.select(\n                        self.config, \"trainer_config.wandb.log_viz_table\", default=False\n                    ),\n                )\n            )\n\n        # Add custom progress bar with better metric formatting\n        if self.config.trainer_config.enable_progress_bar:\n            callbacks.append(SleapProgressBar())\n\n        # Add epoch-end evaluation callback if enabled\n        if self.config.trainer_config.eval.enabled:\n            if self.model_type == \"centroid\":\n                # Use centroid-specific evaluation with distance-based metrics\n                callbacks.append(\n                    CentroidEvaluationCallback(\n                        videos=self.val_labels[0].videos,\n                        eval_frequency=self.config.trainer_config.eval.frequency,\n                        match_threshold=self.config.trainer_config.eval.match_threshold,\n                    )\n                )\n            else:\n                # Use standard OKS/PCK evaluation for pose models\n                callbacks.append(\n                    EpochEndEvaluationCallback(\n                        skeleton=self.skeletons[0],\n                        videos=self.val_labels[0].videos,\n                        eval_frequency=self.config.trainer_config.eval.frequency,\n                        oks_stddev=self.config.trainer_config.eval.oks_stddev,\n                        oks_scale=self.config.trainer_config.eval.oks_scale,\n                    )\n                )\n\n        return loggers, callbacks\n\n    def _delete_cache_imgs(self):\n        \"\"\"Delete cache images in disk.\"\"\"\n        base_cache_img_path = Path(self.config.data_config.cache_img_path)\n        train_cache_img_path = Path(base_cache_img_path) / \"train_imgs\"\n        val_cache_img_path = Path(base_cache_img_path) / \"val_imgs\"\n\n        if (train_cache_img_path).exists():\n            logger.info(f\"Deleting cache imgs from `{train_cache_img_path}`...\")\n            shutil.rmtree(\n                (train_cache_img_path).as_posix(),\n                ignore_errors=True,\n            )\n\n        if (val_cache_img_path).exists():\n            logger.info(f\"Deleting cache imgs from `{val_cache_img_path}`...\")\n            shutil.rmtree(\n                (val_cache_img_path).as_posix(),\n                ignore_errors=True,\n            )\n\n    def train(self):\n        \"\"\"Train the lightning model.\"\"\"\n        logger.info(f\"Setting up for training...\")\n        start_setup_time = time.time()\n\n        # initialize the labels object and update config.\n        if not len(self.train_labels) or not len(self.val_labels):\n            self._setup_train_val_labels(self.config)\n            self.setup_config()\n\n        # create the ckpt dir.\n        self._setup_model_ckpt_dir()\n\n        # create the train and val datasets for visualization.\n        viz_train_dataset = None\n        viz_val_dataset = None\n        if self.config.trainer_config.visualize_preds_during_training:\n            logger.info(f\"Setting up visualization train and val datasets...\")\n            viz_train_dataset, viz_val_dataset = self._setup_viz_datasets()\n\n        # setup loggers and callbacks for Trainer.\n        logger.info(f\"Setting up Trainer...\")\n        loggers, callbacks = self._setup_loggers_callbacks(\n            viz_train_dataset=viz_train_dataset, viz_val_dataset=viz_val_dataset\n        )\n        # set up the strategy (for multi-gpu training)\n        strategy = OmegaConf.select(\n            self.config, \"trainer_config.trainer_strategy\", default=\"auto\"\n        )\n        # set up profilers\n        cfg_profiler = self.config.trainer_config.profiler\n        profiler = None\n        if cfg_profiler is not None:\n            if cfg_profiler in self._profilers:\n                profiler = self._profilers[cfg_profiler]\n            else:\n                message = f\"{cfg_profiler} is not a valid option. Please choose one of {list(self._profilers.keys())}\"\n                logger.error(message)\n                raise ValueError(message)\n\n        devices = (\n            OmegaConf.select(\n                self.config, \"trainer_config.trainer_device_indices\", default=None\n            )\n            if OmegaConf.select(\n                self.config, \"trainer_config.trainer_device_indices\", default=None\n            )\n            is not None\n            else self.config.trainer_config.trainer_devices\n        )\n        logger.info(f\"Trainer devices: {devices}\")\n\n        # if trainer devices is set to less than the number of available GPUs, use the least used GPUs\n        if (\n            torch.cuda.is_available()\n            and self.config.trainer_config.trainer_accelerator != \"cpu\"\n            and isinstance(self.config.trainer_config.trainer_devices, int)\n            and self.config.trainer_config.trainer_devices &lt; torch.cuda.device_count()\n            and self.config.trainer_config.trainer_device_indices is None\n        ):\n            devices = [\n                int(x)\n                for x in np.argsort(get_gpu_memory())[::-1][\n                    : self.config.trainer_config.trainer_devices\n                ]\n            ]\n            # Sort device indices in ascending order for NCCL compatibility.\n            # NCCL expects devices in consistent ascending order across ranks\n            # to properly set up communication rings. Without sorting, DDP may\n            # assign multiple ranks to the same GPU, causing \"Duplicate GPU detected\" errors.\n            devices.sort()\n            logger.info(f\"Using GPUs with most available memory: {devices}\")\n\n        # create lightning.Trainer instance.\n        self.trainer = L.Trainer(\n            callbacks=callbacks,\n            logger=loggers,\n            enable_checkpointing=self.config.trainer_config.save_ckpt,\n            devices=devices,\n            max_epochs=self.config.trainer_config.max_epochs,\n            accelerator=self.config.trainer_config.trainer_accelerator,\n            enable_progress_bar=self.config.trainer_config.enable_progress_bar,\n            strategy=strategy,\n            profiler=profiler,\n            log_every_n_steps=1,\n        )\n\n        self.trainer.strategy.barrier()\n\n        # setup datasets\n        train_dataset, val_dataset = self._setup_datasets()\n\n        # Barrier after dataset creation to ensure all workers wait for disk caching\n        # (rank 0 caches to disk, others must wait before reading cached files)\n        self.trainer.strategy.barrier()\n\n        # set-up steps per epoch\n        train_steps_per_epoch = self.config.trainer_config.train_steps_per_epoch\n        if train_steps_per_epoch is None:\n            train_steps_per_epoch = get_steps_per_epoch(\n                dataset=train_dataset,\n                batch_size=self.config.trainer_config.train_data_loader.batch_size,\n            )\n        if self.config.trainer_config.min_train_steps_per_epoch &gt; train_steps_per_epoch:\n            train_steps_per_epoch = self.config.trainer_config.min_train_steps_per_epoch\n        self.config.trainer_config.train_steps_per_epoch = train_steps_per_epoch\n\n        val_steps_per_epoch = get_steps_per_epoch(\n            dataset=val_dataset,\n            batch_size=self.config.trainer_config.val_data_loader.batch_size,\n        )\n\n        logger.info(f\"Training on {self.trainer.num_devices} device(s)\")\n        logger.info(f\"Training on {self.trainer.strategy.root_device} accelerator\")\n\n        # initialize the lightning model.\n        # need to initialize after Trainer is initialized (for trainer accelerator)\n        logger.info(f\"Setting up lightning module for {self.model_type} model...\")\n        self.lightning_model = LightningModel.get_lightning_model_from_config(\n            config=self.config,\n        )\n        logger.info(f\"Backbone model: {self.lightning_model.model.backbone}\")\n        logger.info(f\"Head model: {self.lightning_model.model.head_layers}\")\n        total_params = sum(p.numel() for p in self.lightning_model.parameters())\n        logger.info(f\"Total model parameters: {total_params:,}\")\n        self.config.model_config.total_params = total_params\n\n        # setup dataloaders\n        # need to set up dataloaders after Trainer is initialized (for ddp). DistributedSampler depends on the rank\n        logger.info(\n            f\"Input image shape: {train_dataset[0]['image'].shape if 'image' in train_dataset[0] else train_dataset[0]['instance_image'].shape}\"\n        )\n        train_dataloader, val_dataloader = get_train_val_dataloaders(\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            config=self.config,\n            rank=self.trainer.global_rank,\n            train_steps_per_epoch=self.config.trainer_config.train_steps_per_epoch,\n            val_steps_per_epoch=val_steps_per_epoch,\n            trainer_devices=self.trainer.num_devices,\n        )\n\n        if self.trainer.global_rank == 0:  # save config only in rank 0 process\n            ckpt_path = (\n                Path(self.config.trainer_config.ckpt_dir)\n                / self.config.trainer_config.run_name\n            ).as_posix()\n            OmegaConf.save(\n                self._initial_config,\n                (Path(ckpt_path) / \"initial_config.yaml\").as_posix(),\n            )\n\n            if self.config.trainer_config.use_wandb:\n                if wandb.run is None:\n                    wandb.init(\n                        dir=(\n                            Path(self.config.trainer_config.ckpt_dir)\n                            / self.config.trainer_config.run_name\n                        ).as_posix(),\n                        project=self.config.trainer_config.wandb.project,\n                        entity=self.config.trainer_config.wandb.entity,\n                        name=self.config.trainer_config.wandb.name,\n                        id=self.config.trainer_config.wandb.prv_runid,\n                        group=self.config.trainer_config.wandb.group,\n                    )\n\n                # Define custom x-axes for wandb metrics\n                # Epoch-level metrics use epoch as x-axis, step-level use default global_step\n                wandb.define_metric(\"epoch\")\n\n                # Training metrics (train/ prefix for grouping) - all use epoch x-axis\n                wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n                wandb.define_metric(\"train/confmaps/*\", step_metric=\"epoch\")\n\n                # Validation metrics (val/ prefix for grouping)\n                wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n\n                # Evaluation metrics (eval/ prefix for grouping)\n                wandb.define_metric(\"eval/*\", step_metric=\"epoch\")\n\n                # Visualization images (need explicit nested paths)\n                wandb.define_metric(\"viz/*\", step_metric=\"epoch\")\n                wandb.define_metric(\"viz/train/*\", step_metric=\"epoch\")\n                wandb.define_metric(\"viz/val/*\", step_metric=\"epoch\")\n\n                self.config.trainer_config.wandb.current_run_id = wandb.run.id\n                wandb.config[\"run_name\"] = self.config.trainer_config.wandb.name\n                wandb.config[\"run_config\"] = OmegaConf.to_container(\n                    self.config, resolve=True\n                )\n\n            OmegaConf.save(\n                self.config,\n                (\n                    Path(self.config.trainer_config.ckpt_dir)\n                    / self.config.trainer_config.run_name\n                    / \"training_config.yaml\"\n                ).as_posix(),\n            )\n\n        self.trainer.strategy.barrier()\n\n        # Flag to track if training was interrupted (not completed normally)\n        training_interrupted = False\n\n        try:\n            logger.info(\n                f\"Finished trainer set up. [{time.time() - start_setup_time:.1f}s]\"\n            )\n            logger.info(f\"Starting training loop...\")\n            start_train_time = time.time()\n            self.trainer.fit(\n                self.lightning_model,\n                train_dataloader,\n                val_dataloader,\n                ckpt_path=self.config.trainer_config.resume_ckpt_path,\n            )\n\n        except KeyboardInterrupt:\n            logger.info(\"Stopping training...\")\n            training_interrupted = True\n\n        finally:\n            logger.info(\n                f\"Finished training loop. [{(time.time() - start_train_time) / 60:.1f} min]\"\n            )\n            # Note: wandb.finish() is called in train.py after post-training evaluation\n\n            # delete image disk caching\n            if (\n                self.config.data_config.data_pipeline_fw\n                == \"torch_dataset_cache_img_disk\"\n                and self.config.data_config.delete_cache_imgs_after_training\n            ):\n                if self.trainer.global_rank == 0:\n                    self._delete_cache_imgs()\n\n            # delete viz folder if requested\n            if (\n                self.config.trainer_config.visualize_preds_during_training\n                and not self.config.trainer_config.keep_viz\n            ):\n                if self.trainer.global_rank == 0:\n                    viz_dir = (\n                        Path(self.config.trainer_config.ckpt_dir)\n                        / self.config.trainer_config.run_name\n                        / \"viz\"\n                    )\n                    if viz_dir.exists():\n                        logger.info(f\"Deleting viz folder at {viz_dir}...\")\n                        shutil.rmtree(viz_dir, ignore_errors=True)\n\n            # Clean up entire run folder if training was interrupted (KeyboardInterrupt)\n            if training_interrupted and self.trainer.global_rank == 0:\n                run_dir = (\n                    Path(self.config.trainer_config.ckpt_dir)\n                    / self.config.trainer_config.run_name\n                )\n                if run_dir.exists():\n                    logger.info(\n                        f\"Training canceled - cleaning up run folder at {run_dir}...\"\n                    )\n                    shutil.rmtree(run_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer.get_model_trainer_from_config","title":"<code>get_model_trainer_from_config(config, train_labels=None, val_labels=None)</code>  <code>classmethod</code>","text":"<p>Create a model trainer instance from config.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>@classmethod\ndef get_model_trainer_from_config(\n    cls,\n    config: DictConfig,\n    train_labels: Optional[List[sio.Labels]] = None,\n    val_labels: Optional[List[sio.Labels]] = None,\n):\n    \"\"\"Create a model trainer instance from config.\"\"\"\n    # Verify config structure.\n    config = verify_training_cfg(config)\n\n    model_trainer = cls(config=config)\n\n    model_trainer.model_type = get_model_type_from_cfg(model_trainer.config)\n    model_trainer.backbone_type = get_backbone_type_from_cfg(model_trainer.config)\n\n    if model_trainer.config.trainer_config.seed is not None:\n        model_trainer._set_seed()\n\n    if train_labels is None and val_labels is None:\n        # read labels from paths provided in the config\n        train_labels = [\n            sio.load_slp(path)\n            for path in model_trainer.config.data_config.train_labels_path\n        ]\n        val_labels = (\n            [\n                sio.load_slp(path)\n                for path in model_trainer.config.data_config.val_labels_path\n            ]\n            if model_trainer.config.data_config.val_labels_path is not None\n            else None\n        )\n        model_trainer._setup_train_val_labels(\n            labels=train_labels, val_labels=val_labels\n        )\n    else:\n        model_trainer._setup_train_val_labels(\n            labels=train_labels, val_labels=val_labels\n        )\n\n    model_trainer._initial_config = model_trainer.config.copy()\n    # update config parameters\n    model_trainer.setup_config()\n\n    # Check if all videos exist across all labels\n    all_videos_exist = all(\n        video.exists(check_all=True)\n        for labels in [*model_trainer.train_labels, *model_trainer.val_labels]\n        for video in labels.videos\n    )\n\n    if not all_videos_exist:\n        raise FileNotFoundError(\n            \"One or more video files do not exist or are not accessible.\"\n        )\n\n    return model_trainer\n</code></pre>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer.setup_config","title":"<code>setup_config()</code>","text":"<p>Compute config parameters.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>def setup_config(self):\n    \"\"\"Compute config parameters.\"\"\"\n    logger.info(\"Setting up config...\")\n\n    # Normalize empty strings to None for optional wandb fields\n    if self.config.trainer_config.wandb.prv_runid == \"\":\n        self.config.trainer_config.wandb.prv_runid = None\n\n    # compute preprocessing parameters from the labels objects and fill in the config\n    self._setup_preprocessing_config()\n\n    # save skeleton to config\n    skeleton_yaml = yaml.safe_load(SkeletonYAMLEncoder().encode(self.skeletons))\n    skeleton_names = skeleton_yaml.keys()\n    self.config[\"data_config\"][\"skeletons\"] = []\n    for skeleton_name in skeleton_names:\n        skl = skeleton_yaml[skeleton_name]\n        skl[\"name\"] = skeleton_name\n        self.config[\"data_config\"][\"skeletons\"].append(skl)\n\n    # setup head config - partnames, edges and class names\n    self._setup_head_config()\n\n    # set max stride for the backbone: convnext and swint\n    if self.backbone_type == \"convnext\":\n        self.config.model_config.backbone_config.convnext.max_stride = (\n            self.config.model_config.backbone_config.convnext.stem_patch_stride\n            * (2**3)\n            * 2\n        )\n    elif self.backbone_type == \"swint\":\n        self.config.model_config.backbone_config.swint.max_stride = (\n            self.config.model_config.backbone_config.swint.stem_patch_stride\n            * (2**3)\n            * 2\n        )\n\n    # set output stride for backbone from head config and verify max stride\n    self.config = check_output_strides(self.config)\n\n    # if trainer_devices is None, set it to \"auto\"\n    if self.config.trainer_config.trainer_devices is None:\n        self.config.trainer_config.trainer_devices = (\n            \"auto\"\n            if OmegaConf.select(\n                self.config, \"trainer_config.trainer_device_indices\", default=None\n            )\n            is None\n            else len(\n                OmegaConf.select(\n                    self.config,\n                    \"trainer_config.trainer_device_indices\",\n                    default=None,\n                )\n            )\n        )\n\n    # setup checkpoint path (generates run_name if not specified)\n    self._setup_ckpt_path()\n\n    # Default wandb run name to trainer run_name if not specified\n    # Note: This must come after _setup_ckpt_path() which generates run_name\n    if self.config.trainer_config.wandb.name is None:\n        self.config.trainer_config.wandb.name = self.config.trainer_config.run_name\n\n    # verify input_channels in model_config based on input image and pretrained model weights\n    self._verify_model_input_channels()\n</code></pre>"},{"location":"api/training/model_trainer/#sleap_nn.training.model_trainer.ModelTrainer.train","title":"<code>train()</code>","text":"<p>Train the lightning model.</p> Source code in <code>sleap_nn/training/model_trainer.py</code> <pre><code>def train(self):\n    \"\"\"Train the lightning model.\"\"\"\n    logger.info(f\"Setting up for training...\")\n    start_setup_time = time.time()\n\n    # initialize the labels object and update config.\n    if not len(self.train_labels) or not len(self.val_labels):\n        self._setup_train_val_labels(self.config)\n        self.setup_config()\n\n    # create the ckpt dir.\n    self._setup_model_ckpt_dir()\n\n    # create the train and val datasets for visualization.\n    viz_train_dataset = None\n    viz_val_dataset = None\n    if self.config.trainer_config.visualize_preds_during_training:\n        logger.info(f\"Setting up visualization train and val datasets...\")\n        viz_train_dataset, viz_val_dataset = self._setup_viz_datasets()\n\n    # setup loggers and callbacks for Trainer.\n    logger.info(f\"Setting up Trainer...\")\n    loggers, callbacks = self._setup_loggers_callbacks(\n        viz_train_dataset=viz_train_dataset, viz_val_dataset=viz_val_dataset\n    )\n    # set up the strategy (for multi-gpu training)\n    strategy = OmegaConf.select(\n        self.config, \"trainer_config.trainer_strategy\", default=\"auto\"\n    )\n    # set up profilers\n    cfg_profiler = self.config.trainer_config.profiler\n    profiler = None\n    if cfg_profiler is not None:\n        if cfg_profiler in self._profilers:\n            profiler = self._profilers[cfg_profiler]\n        else:\n            message = f\"{cfg_profiler} is not a valid option. Please choose one of {list(self._profilers.keys())}\"\n            logger.error(message)\n            raise ValueError(message)\n\n    devices = (\n        OmegaConf.select(\n            self.config, \"trainer_config.trainer_device_indices\", default=None\n        )\n        if OmegaConf.select(\n            self.config, \"trainer_config.trainer_device_indices\", default=None\n        )\n        is not None\n        else self.config.trainer_config.trainer_devices\n    )\n    logger.info(f\"Trainer devices: {devices}\")\n\n    # if trainer devices is set to less than the number of available GPUs, use the least used GPUs\n    if (\n        torch.cuda.is_available()\n        and self.config.trainer_config.trainer_accelerator != \"cpu\"\n        and isinstance(self.config.trainer_config.trainer_devices, int)\n        and self.config.trainer_config.trainer_devices &lt; torch.cuda.device_count()\n        and self.config.trainer_config.trainer_device_indices is None\n    ):\n        devices = [\n            int(x)\n            for x in np.argsort(get_gpu_memory())[::-1][\n                : self.config.trainer_config.trainer_devices\n            ]\n        ]\n        # Sort device indices in ascending order for NCCL compatibility.\n        # NCCL expects devices in consistent ascending order across ranks\n        # to properly set up communication rings. Without sorting, DDP may\n        # assign multiple ranks to the same GPU, causing \"Duplicate GPU detected\" errors.\n        devices.sort()\n        logger.info(f\"Using GPUs with most available memory: {devices}\")\n\n    # create lightning.Trainer instance.\n    self.trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=loggers,\n        enable_checkpointing=self.config.trainer_config.save_ckpt,\n        devices=devices,\n        max_epochs=self.config.trainer_config.max_epochs,\n        accelerator=self.config.trainer_config.trainer_accelerator,\n        enable_progress_bar=self.config.trainer_config.enable_progress_bar,\n        strategy=strategy,\n        profiler=profiler,\n        log_every_n_steps=1,\n    )\n\n    self.trainer.strategy.barrier()\n\n    # setup datasets\n    train_dataset, val_dataset = self._setup_datasets()\n\n    # Barrier after dataset creation to ensure all workers wait for disk caching\n    # (rank 0 caches to disk, others must wait before reading cached files)\n    self.trainer.strategy.barrier()\n\n    # set-up steps per epoch\n    train_steps_per_epoch = self.config.trainer_config.train_steps_per_epoch\n    if train_steps_per_epoch is None:\n        train_steps_per_epoch = get_steps_per_epoch(\n            dataset=train_dataset,\n            batch_size=self.config.trainer_config.train_data_loader.batch_size,\n        )\n    if self.config.trainer_config.min_train_steps_per_epoch &gt; train_steps_per_epoch:\n        train_steps_per_epoch = self.config.trainer_config.min_train_steps_per_epoch\n    self.config.trainer_config.train_steps_per_epoch = train_steps_per_epoch\n\n    val_steps_per_epoch = get_steps_per_epoch(\n        dataset=val_dataset,\n        batch_size=self.config.trainer_config.val_data_loader.batch_size,\n    )\n\n    logger.info(f\"Training on {self.trainer.num_devices} device(s)\")\n    logger.info(f\"Training on {self.trainer.strategy.root_device} accelerator\")\n\n    # initialize the lightning model.\n    # need to initialize after Trainer is initialized (for trainer accelerator)\n    logger.info(f\"Setting up lightning module for {self.model_type} model...\")\n    self.lightning_model = LightningModel.get_lightning_model_from_config(\n        config=self.config,\n    )\n    logger.info(f\"Backbone model: {self.lightning_model.model.backbone}\")\n    logger.info(f\"Head model: {self.lightning_model.model.head_layers}\")\n    total_params = sum(p.numel() for p in self.lightning_model.parameters())\n    logger.info(f\"Total model parameters: {total_params:,}\")\n    self.config.model_config.total_params = total_params\n\n    # setup dataloaders\n    # need to set up dataloaders after Trainer is initialized (for ddp). DistributedSampler depends on the rank\n    logger.info(\n        f\"Input image shape: {train_dataset[0]['image'].shape if 'image' in train_dataset[0] else train_dataset[0]['instance_image'].shape}\"\n    )\n    train_dataloader, val_dataloader = get_train_val_dataloaders(\n        train_dataset=train_dataset,\n        val_dataset=val_dataset,\n        config=self.config,\n        rank=self.trainer.global_rank,\n        train_steps_per_epoch=self.config.trainer_config.train_steps_per_epoch,\n        val_steps_per_epoch=val_steps_per_epoch,\n        trainer_devices=self.trainer.num_devices,\n    )\n\n    if self.trainer.global_rank == 0:  # save config only in rank 0 process\n        ckpt_path = (\n            Path(self.config.trainer_config.ckpt_dir)\n            / self.config.trainer_config.run_name\n        ).as_posix()\n        OmegaConf.save(\n            self._initial_config,\n            (Path(ckpt_path) / \"initial_config.yaml\").as_posix(),\n        )\n\n        if self.config.trainer_config.use_wandb:\n            if wandb.run is None:\n                wandb.init(\n                    dir=(\n                        Path(self.config.trainer_config.ckpt_dir)\n                        / self.config.trainer_config.run_name\n                    ).as_posix(),\n                    project=self.config.trainer_config.wandb.project,\n                    entity=self.config.trainer_config.wandb.entity,\n                    name=self.config.trainer_config.wandb.name,\n                    id=self.config.trainer_config.wandb.prv_runid,\n                    group=self.config.trainer_config.wandb.group,\n                )\n\n            # Define custom x-axes for wandb metrics\n            # Epoch-level metrics use epoch as x-axis, step-level use default global_step\n            wandb.define_metric(\"epoch\")\n\n            # Training metrics (train/ prefix for grouping) - all use epoch x-axis\n            wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n            wandb.define_metric(\"train/confmaps/*\", step_metric=\"epoch\")\n\n            # Validation metrics (val/ prefix for grouping)\n            wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n\n            # Evaluation metrics (eval/ prefix for grouping)\n            wandb.define_metric(\"eval/*\", step_metric=\"epoch\")\n\n            # Visualization images (need explicit nested paths)\n            wandb.define_metric(\"viz/*\", step_metric=\"epoch\")\n            wandb.define_metric(\"viz/train/*\", step_metric=\"epoch\")\n            wandb.define_metric(\"viz/val/*\", step_metric=\"epoch\")\n\n            self.config.trainer_config.wandb.current_run_id = wandb.run.id\n            wandb.config[\"run_name\"] = self.config.trainer_config.wandb.name\n            wandb.config[\"run_config\"] = OmegaConf.to_container(\n                self.config, resolve=True\n            )\n\n        OmegaConf.save(\n            self.config,\n            (\n                Path(self.config.trainer_config.ckpt_dir)\n                / self.config.trainer_config.run_name\n                / \"training_config.yaml\"\n            ).as_posix(),\n        )\n\n    self.trainer.strategy.barrier()\n\n    # Flag to track if training was interrupted (not completed normally)\n    training_interrupted = False\n\n    try:\n        logger.info(\n            f\"Finished trainer set up. [{time.time() - start_setup_time:.1f}s]\"\n        )\n        logger.info(f\"Starting training loop...\")\n        start_train_time = time.time()\n        self.trainer.fit(\n            self.lightning_model,\n            train_dataloader,\n            val_dataloader,\n            ckpt_path=self.config.trainer_config.resume_ckpt_path,\n        )\n\n    except KeyboardInterrupt:\n        logger.info(\"Stopping training...\")\n        training_interrupted = True\n\n    finally:\n        logger.info(\n            f\"Finished training loop. [{(time.time() - start_train_time) / 60:.1f} min]\"\n        )\n        # Note: wandb.finish() is called in train.py after post-training evaluation\n\n        # delete image disk caching\n        if (\n            self.config.data_config.data_pipeline_fw\n            == \"torch_dataset_cache_img_disk\"\n            and self.config.data_config.delete_cache_imgs_after_training\n        ):\n            if self.trainer.global_rank == 0:\n                self._delete_cache_imgs()\n\n        # delete viz folder if requested\n        if (\n            self.config.trainer_config.visualize_preds_during_training\n            and not self.config.trainer_config.keep_viz\n        ):\n            if self.trainer.global_rank == 0:\n                viz_dir = (\n                    Path(self.config.trainer_config.ckpt_dir)\n                    / self.config.trainer_config.run_name\n                    / \"viz\"\n                )\n                if viz_dir.exists():\n                    logger.info(f\"Deleting viz folder at {viz_dir}...\")\n                    shutil.rmtree(viz_dir, ignore_errors=True)\n\n        # Clean up entire run folder if training was interrupted (KeyboardInterrupt)\n        if training_interrupted and self.trainer.global_rank == 0:\n            run_dir = (\n                Path(self.config.trainer_config.ckpt_dir)\n                / self.config.trainer_config.run_name\n            )\n            if run_dir.exists():\n                logger.info(\n                    f\"Training canceled - cleaning up run folder at {run_dir}...\"\n                )\n                shutil.rmtree(run_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/training/schedulers/","title":"schedulers","text":""},{"location":"api/training/schedulers/#sleap_nn.training.schedulers","title":"<code>sleap_nn.training.schedulers</code>","text":"<p>Custom learning rate schedulers for sleap-nn training.</p> <p>This module provides learning rate schedulers with warmup phases that are commonly used in deep learning for pose estimation and computer vision tasks.</p> <p>Classes:</p> Name Description <code>LinearWarmupCosineAnnealingLR</code> <p>Cosine annealing learning rate scheduler with linear warmup.</p> <code>LinearWarmupLinearDecayLR</code> <p>Linear warmup followed by linear decay learning rate scheduler.</p>"},{"location":"api/training/schedulers/#sleap_nn.training.schedulers.LinearWarmupCosineAnnealingLR","title":"<code>LinearWarmupCosineAnnealingLR</code>","text":"<p>               Bases: <code>LRScheduler</code></p> <p>Cosine annealing learning rate scheduler with linear warmup.</p> <p>The learning rate increases linearly from <code>warmup_start_lr</code> to the optimizer's base learning rate over <code>warmup_epochs</code>, then decreases following a cosine curve to <code>eta_min</code> over the remaining epochs.</p> <p>This schedule is widely used in vision transformers and modern CNN architectures as it provides stable early training (warmup) and smooth convergence (cosine decay).</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <p>Wrapped optimizer.</p> required <code>warmup_epochs</code> <code>int</code> <p>Number of epochs for the linear warmup phase.</p> required <code>max_epochs</code> <code>int</code> <p>Total number of training epochs.</p> required <code>warmup_start_lr</code> <code>float</code> <p>Learning rate at the start of warmup. Default: 0.0.</p> <code>0.0</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate at the end of the schedule. Default: 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>The index of the last epoch. Default: -1.</p> <code>-1</code> Example <p>optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) scheduler = LinearWarmupCosineAnnealingLR( ...     optimizer, warmup_epochs=5, max_epochs=100, eta_min=1e-6 ... ) for epoch in range(100): ...     train(...) ...     scheduler.step()</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the scheduler.</p> <code>get_lr</code> <p>Compute the learning rate at the current epoch.</p> Source code in <code>sleap_nn/training/schedulers.py</code> <pre><code>class LinearWarmupCosineAnnealingLR(LRScheduler):\n    \"\"\"Cosine annealing learning rate scheduler with linear warmup.\n\n    The learning rate increases linearly from `warmup_start_lr` to the optimizer's\n    base learning rate over `warmup_epochs`, then decreases following a cosine\n    curve to `eta_min` over the remaining epochs.\n\n    This schedule is widely used in vision transformers and modern CNN architectures\n    as it provides stable early training (warmup) and smooth convergence (cosine decay).\n\n    Args:\n        optimizer: Wrapped optimizer.\n        warmup_epochs: Number of epochs for the linear warmup phase.\n        max_epochs: Total number of training epochs.\n        warmup_start_lr: Learning rate at the start of warmup. Default: 0.0.\n        eta_min: Minimum learning rate at the end of the schedule. Default: 0.0.\n        last_epoch: The index of the last epoch. Default: -1.\n\n    Example:\n        &gt;&gt;&gt; optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        &gt;&gt;&gt; scheduler = LinearWarmupCosineAnnealingLR(\n        ...     optimizer, warmup_epochs=5, max_epochs=100, eta_min=1e-6\n        ... )\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     train(...)\n        ...     scheduler.step()\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        warmup_epochs: int,\n        max_epochs: int,\n        warmup_start_lr: float = 0.0,\n        eta_min: float = 0.0,\n        last_epoch: int = -1,\n    ):\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            optimizer: Wrapped optimizer.\n            warmup_epochs: Number of epochs for the linear warmup phase.\n            max_epochs: Total number of training epochs.\n            warmup_start_lr: Learning rate at the start of warmup. Default: 0.0.\n            eta_min: Minimum learning rate at the end of the schedule. Default: 0.0.\n            last_epoch: The index of the last epoch. Default: -1.\n        \"\"\"\n        if warmup_epochs &lt; 0:\n            raise ValueError(f\"warmup_epochs must be &gt;= 0, got {warmup_epochs}\")\n        if max_epochs &lt;= 0:\n            raise ValueError(f\"max_epochs must be &gt; 0, got {max_epochs}\")\n        if warmup_epochs &gt;= max_epochs:\n            raise ValueError(\n                f\"warmup_epochs ({warmup_epochs}) must be &lt; max_epochs ({max_epochs})\"\n            )\n        if warmup_start_lr &lt; 0:\n            raise ValueError(f\"warmup_start_lr must be &gt;= 0, got {warmup_start_lr}\")\n        if eta_min &lt; 0:\n            raise ValueError(f\"eta_min must be &gt;= 0, got {eta_min}\")\n\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.warmup_start_lr = warmup_start_lr\n        self.eta_min = eta_min\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"Compute the learning rate at the current epoch.\"\"\"\n        if self.last_epoch &lt; self.warmup_epochs:\n            # Linear warmup phase\n            if self.warmup_epochs == 0:\n                return list(self.base_lrs)\n            alpha = self.last_epoch / self.warmup_epochs\n            return [\n                self.warmup_start_lr + alpha * (base_lr - self.warmup_start_lr)\n                for base_lr in self.base_lrs\n            ]\n        else:\n            # Cosine annealing phase\n            decay_epochs = self.max_epochs - self.warmup_epochs\n            if decay_epochs == 0:\n                return [self.eta_min for _ in self.base_lrs]\n            progress = (self.last_epoch - self.warmup_epochs) / decay_epochs\n            # Clamp progress to [0, 1] to handle epochs beyond max_epochs\n            progress = min(1.0, progress)\n            return [\n                self.eta_min\n                + (base_lr - self.eta_min) * (1 + math.cos(math.pi * progress)) / 2\n                for base_lr in self.base_lrs\n            ]\n</code></pre>"},{"location":"api/training/schedulers/#sleap_nn.training.schedulers.LinearWarmupCosineAnnealingLR.__init__","title":"<code>__init__(optimizer, warmup_epochs, max_epochs, warmup_start_lr=0.0, eta_min=0.0, last_epoch=-1)</code>","text":"<p>Initialize the scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <p>Wrapped optimizer.</p> required <code>warmup_epochs</code> <code>int</code> <p>Number of epochs for the linear warmup phase.</p> required <code>max_epochs</code> <code>int</code> <p>Total number of training epochs.</p> required <code>warmup_start_lr</code> <code>float</code> <p>Learning rate at the start of warmup. Default: 0.0.</p> <code>0.0</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate at the end of the schedule. Default: 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>The index of the last epoch. Default: -1.</p> <code>-1</code> Source code in <code>sleap_nn/training/schedulers.py</code> <pre><code>def __init__(\n    self,\n    optimizer,\n    warmup_epochs: int,\n    max_epochs: int,\n    warmup_start_lr: float = 0.0,\n    eta_min: float = 0.0,\n    last_epoch: int = -1,\n):\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        optimizer: Wrapped optimizer.\n        warmup_epochs: Number of epochs for the linear warmup phase.\n        max_epochs: Total number of training epochs.\n        warmup_start_lr: Learning rate at the start of warmup. Default: 0.0.\n        eta_min: Minimum learning rate at the end of the schedule. Default: 0.0.\n        last_epoch: The index of the last epoch. Default: -1.\n    \"\"\"\n    if warmup_epochs &lt; 0:\n        raise ValueError(f\"warmup_epochs must be &gt;= 0, got {warmup_epochs}\")\n    if max_epochs &lt;= 0:\n        raise ValueError(f\"max_epochs must be &gt; 0, got {max_epochs}\")\n    if warmup_epochs &gt;= max_epochs:\n        raise ValueError(\n            f\"warmup_epochs ({warmup_epochs}) must be &lt; max_epochs ({max_epochs})\"\n        )\n    if warmup_start_lr &lt; 0:\n        raise ValueError(f\"warmup_start_lr must be &gt;= 0, got {warmup_start_lr}\")\n    if eta_min &lt; 0:\n        raise ValueError(f\"eta_min must be &gt;= 0, got {eta_min}\")\n\n    self.warmup_epochs = warmup_epochs\n    self.max_epochs = max_epochs\n    self.warmup_start_lr = warmup_start_lr\n    self.eta_min = eta_min\n    super().__init__(optimizer, last_epoch)\n</code></pre>"},{"location":"api/training/schedulers/#sleap_nn.training.schedulers.LinearWarmupCosineAnnealingLR.get_lr","title":"<code>get_lr()</code>","text":"<p>Compute the learning rate at the current epoch.</p> Source code in <code>sleap_nn/training/schedulers.py</code> <pre><code>def get_lr(self):\n    \"\"\"Compute the learning rate at the current epoch.\"\"\"\n    if self.last_epoch &lt; self.warmup_epochs:\n        # Linear warmup phase\n        if self.warmup_epochs == 0:\n            return list(self.base_lrs)\n        alpha = self.last_epoch / self.warmup_epochs\n        return [\n            self.warmup_start_lr + alpha * (base_lr - self.warmup_start_lr)\n            for base_lr in self.base_lrs\n        ]\n    else:\n        # Cosine annealing phase\n        decay_epochs = self.max_epochs - self.warmup_epochs\n        if decay_epochs == 0:\n            return [self.eta_min for _ in self.base_lrs]\n        progress = (self.last_epoch - self.warmup_epochs) / decay_epochs\n        # Clamp progress to [0, 1] to handle epochs beyond max_epochs\n        progress = min(1.0, progress)\n        return [\n            self.eta_min\n            + (base_lr - self.eta_min) * (1 + math.cos(math.pi * progress)) / 2\n            for base_lr in self.base_lrs\n        ]\n</code></pre>"},{"location":"api/training/schedulers/#sleap_nn.training.schedulers.LinearWarmupLinearDecayLR","title":"<code>LinearWarmupLinearDecayLR</code>","text":"<p>               Bases: <code>LRScheduler</code></p> <p>Linear warmup followed by linear decay learning rate scheduler.</p> <p>The learning rate increases linearly from <code>warmup_start_lr</code> to the optimizer's base learning rate over <code>warmup_epochs</code>, then decreases linearly to <code>end_lr</code> over the remaining epochs.</p> <p>This schedule provides a simple, interpretable learning rate trajectory and is commonly used in transformer-based models and NLP tasks.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <p>Wrapped optimizer.</p> required <code>warmup_epochs</code> <code>int</code> <p>Number of epochs for the linear warmup phase.</p> required <code>max_epochs</code> <code>int</code> <p>Total number of training epochs.</p> required <code>warmup_start_lr</code> <code>float</code> <p>Learning rate at the start of warmup. Default: 0.0.</p> <code>0.0</code> <code>end_lr</code> <code>float</code> <p>Learning rate at the end of training. Default: 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>The index of the last epoch. Default: -1.</p> <code>-1</code> Example <p>optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) scheduler = LinearWarmupLinearDecayLR( ...     optimizer, warmup_epochs=5, max_epochs=100, end_lr=1e-6 ... ) for epoch in range(100): ...     train(...) ...     scheduler.step()</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the scheduler.</p> <code>get_lr</code> <p>Compute the learning rate at the current epoch.</p> Source code in <code>sleap_nn/training/schedulers.py</code> <pre><code>class LinearWarmupLinearDecayLR(LRScheduler):\n    \"\"\"Linear warmup followed by linear decay learning rate scheduler.\n\n    The learning rate increases linearly from `warmup_start_lr` to the optimizer's\n    base learning rate over `warmup_epochs`, then decreases linearly to `end_lr`\n    over the remaining epochs.\n\n    This schedule provides a simple, interpretable learning rate trajectory and is\n    commonly used in transformer-based models and NLP tasks.\n\n    Args:\n        optimizer: Wrapped optimizer.\n        warmup_epochs: Number of epochs for the linear warmup phase.\n        max_epochs: Total number of training epochs.\n        warmup_start_lr: Learning rate at the start of warmup. Default: 0.0.\n        end_lr: Learning rate at the end of training. Default: 0.0.\n        last_epoch: The index of the last epoch. Default: -1.\n\n    Example:\n        &gt;&gt;&gt; optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        &gt;&gt;&gt; scheduler = LinearWarmupLinearDecayLR(\n        ...     optimizer, warmup_epochs=5, max_epochs=100, end_lr=1e-6\n        ... )\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     train(...)\n        ...     scheduler.step()\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        warmup_epochs: int,\n        max_epochs: int,\n        warmup_start_lr: float = 0.0,\n        end_lr: float = 0.0,\n        last_epoch: int = -1,\n    ):\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            optimizer: Wrapped optimizer.\n            warmup_epochs: Number of epochs for the linear warmup phase.\n            max_epochs: Total number of training epochs.\n            warmup_start_lr: Learning rate at the start of warmup. Default: 0.0.\n            end_lr: Learning rate at the end of training. Default: 0.0.\n            last_epoch: The index of the last epoch. Default: -1.\n        \"\"\"\n        if warmup_epochs &lt; 0:\n            raise ValueError(f\"warmup_epochs must be &gt;= 0, got {warmup_epochs}\")\n        if max_epochs &lt;= 0:\n            raise ValueError(f\"max_epochs must be &gt; 0, got {max_epochs}\")\n        if warmup_epochs &gt;= max_epochs:\n            raise ValueError(\n                f\"warmup_epochs ({warmup_epochs}) must be &lt; max_epochs ({max_epochs})\"\n            )\n        if warmup_start_lr &lt; 0:\n            raise ValueError(f\"warmup_start_lr must be &gt;= 0, got {warmup_start_lr}\")\n        if end_lr &lt; 0:\n            raise ValueError(f\"end_lr must be &gt;= 0, got {end_lr}\")\n\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.warmup_start_lr = warmup_start_lr\n        self.end_lr = end_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"Compute the learning rate at the current epoch.\"\"\"\n        if self.last_epoch &lt; self.warmup_epochs:\n            # Linear warmup phase\n            if self.warmup_epochs == 0:\n                return list(self.base_lrs)\n            alpha = self.last_epoch / self.warmup_epochs\n            return [\n                self.warmup_start_lr + alpha * (base_lr - self.warmup_start_lr)\n                for base_lr in self.base_lrs\n            ]\n        else:\n            # Linear decay phase\n            decay_epochs = self.max_epochs - self.warmup_epochs\n            if decay_epochs == 0:\n                return [self.end_lr for _ in self.base_lrs]\n            progress = (self.last_epoch - self.warmup_epochs) / decay_epochs\n            # Clamp progress to [0, 1] to handle epochs beyond max_epochs\n            progress = min(1.0, progress)\n            return [\n                base_lr + progress * (self.end_lr - base_lr)\n                for base_lr in self.base_lrs\n            ]\n</code></pre>"},{"location":"api/training/schedulers/#sleap_nn.training.schedulers.LinearWarmupLinearDecayLR.__init__","title":"<code>__init__(optimizer, warmup_epochs, max_epochs, warmup_start_lr=0.0, end_lr=0.0, last_epoch=-1)</code>","text":"<p>Initialize the scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <p>Wrapped optimizer.</p> required <code>warmup_epochs</code> <code>int</code> <p>Number of epochs for the linear warmup phase.</p> required <code>max_epochs</code> <code>int</code> <p>Total number of training epochs.</p> required <code>warmup_start_lr</code> <code>float</code> <p>Learning rate at the start of warmup. Default: 0.0.</p> <code>0.0</code> <code>end_lr</code> <code>float</code> <p>Learning rate at the end of training. Default: 0.0.</p> <code>0.0</code> <code>last_epoch</code> <code>int</code> <p>The index of the last epoch. Default: -1.</p> <code>-1</code> Source code in <code>sleap_nn/training/schedulers.py</code> <pre><code>def __init__(\n    self,\n    optimizer,\n    warmup_epochs: int,\n    max_epochs: int,\n    warmup_start_lr: float = 0.0,\n    end_lr: float = 0.0,\n    last_epoch: int = -1,\n):\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        optimizer: Wrapped optimizer.\n        warmup_epochs: Number of epochs for the linear warmup phase.\n        max_epochs: Total number of training epochs.\n        warmup_start_lr: Learning rate at the start of warmup. Default: 0.0.\n        end_lr: Learning rate at the end of training. Default: 0.0.\n        last_epoch: The index of the last epoch. Default: -1.\n    \"\"\"\n    if warmup_epochs &lt; 0:\n        raise ValueError(f\"warmup_epochs must be &gt;= 0, got {warmup_epochs}\")\n    if max_epochs &lt;= 0:\n        raise ValueError(f\"max_epochs must be &gt; 0, got {max_epochs}\")\n    if warmup_epochs &gt;= max_epochs:\n        raise ValueError(\n            f\"warmup_epochs ({warmup_epochs}) must be &lt; max_epochs ({max_epochs})\"\n        )\n    if warmup_start_lr &lt; 0:\n        raise ValueError(f\"warmup_start_lr must be &gt;= 0, got {warmup_start_lr}\")\n    if end_lr &lt; 0:\n        raise ValueError(f\"end_lr must be &gt;= 0, got {end_lr}\")\n\n    self.warmup_epochs = warmup_epochs\n    self.max_epochs = max_epochs\n    self.warmup_start_lr = warmup_start_lr\n    self.end_lr = end_lr\n    super().__init__(optimizer, last_epoch)\n</code></pre>"},{"location":"api/training/schedulers/#sleap_nn.training.schedulers.LinearWarmupLinearDecayLR.get_lr","title":"<code>get_lr()</code>","text":"<p>Compute the learning rate at the current epoch.</p> Source code in <code>sleap_nn/training/schedulers.py</code> <pre><code>def get_lr(self):\n    \"\"\"Compute the learning rate at the current epoch.\"\"\"\n    if self.last_epoch &lt; self.warmup_epochs:\n        # Linear warmup phase\n        if self.warmup_epochs == 0:\n            return list(self.base_lrs)\n        alpha = self.last_epoch / self.warmup_epochs\n        return [\n            self.warmup_start_lr + alpha * (base_lr - self.warmup_start_lr)\n            for base_lr in self.base_lrs\n        ]\n    else:\n        # Linear decay phase\n        decay_epochs = self.max_epochs - self.warmup_epochs\n        if decay_epochs == 0:\n            return [self.end_lr for _ in self.base_lrs]\n        progress = (self.last_epoch - self.warmup_epochs) / decay_epochs\n        # Clamp progress to [0, 1] to handle epochs beyond max_epochs\n        progress = min(1.0, progress)\n        return [\n            base_lr + progress * (self.end_lr - base_lr)\n            for base_lr in self.base_lrs\n        ]\n</code></pre>"},{"location":"api/training/utils/","title":"utils","text":""},{"location":"api/training/utils/#sleap_nn.training.utils","title":"<code>sleap_nn.training.utils</code>","text":"<p>Miscellaneous utility functions for training.</p> <p>Classes:</p> Name Description <code>MatplotlibRenderer</code> <p>Renders VisualizationData to matplotlib figures.</p> <code>VisualizationData</code> <p>Container for visualization data from a single sample.</p> <code>WandBRenderer</code> <p>Renders VisualizationData to wandb.Image objects.</p> <p>Functions:</p> Name Description <code>get_dist_rank</code> <p>Return the rank of the current process if torch.distributed is initialized.</p> <code>get_gpu_memory</code> <p>Get the available memory on each GPU.</p> <code>imgfig</code> <p>Create a tight figure for image plotting.</p> <code>is_distributed_initialized</code> <p>Check if distributed processes are initialized.</p> <code>plot_confmaps</code> <p>Plot confidence maps reduced over channels.</p> <code>plot_img</code> <p>Plot an image in a tight figure.</p> <code>plot_peaks</code> <p>Plot ground truth and detected peaks.</p> <code>xavier_init_weights</code> <p>Function to initilaise the model weights with Xavier initialization method.</p>"},{"location":"api/training/utils/#sleap_nn.training.utils.MatplotlibRenderer","title":"<code>MatplotlibRenderer</code>","text":"<p>Renders VisualizationData to matplotlib figures.</p> <p>Methods:</p> Name Description <code>render</code> <p>Render visualization data to a matplotlib figure.</p> <code>render_pafs</code> <p>Render PAF magnitude visualization.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>class MatplotlibRenderer:\n    \"\"\"Renders VisualizationData to matplotlib figures.\"\"\"\n\n    def render(self, data: VisualizationData) -&gt; matplotlib.figure.Figure:\n        \"\"\"Render visualization data to a matplotlib figure.\n\n        Args:\n            data: VisualizationData containing image, confmaps, peaks, etc.\n\n        Returns:\n            A matplotlib Figure object.\n        \"\"\"\n        img = data.image\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n        plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n        return fig\n\n    def render_pafs(self, data: VisualizationData) -&gt; matplotlib.figure.Figure:\n        \"\"\"Render PAF magnitude visualization.\n\n        Args:\n            data: VisualizationData with pred_pafs populated.\n\n        Returns:\n            A matplotlib Figure object showing PAF magnitudes.\n        \"\"\"\n        if data.pred_pafs is None:\n            raise ValueError(\"pred_pafs is None, cannot render PAFs\")\n\n        img = data.image\n        scale = 1.0\n        if img.shape[0] &lt; 512:\n            scale = 2.0\n        if img.shape[0] &lt; 256:\n            scale = 4.0\n\n        # Compute PAF magnitude\n        pafs = data.pred_pafs  # (H, W, 2*edges) or (H, W, edges, 2)\n        if pafs.ndim == 3:\n            n_edges = pafs.shape[-1] // 2\n            pafs = pafs.reshape(pafs.shape[0], pafs.shape[1], n_edges, 2)\n        magnitude = np.sqrt(pafs[..., 0] ** 2 + pafs[..., 1] ** 2)\n        magnitude = magnitude.max(axis=-1)  # Max over edges\n\n        fig = plot_img(img, dpi=72 * scale, scale=scale)\n        ax = plt.gca()\n\n        # Calculate PAF output scale from actual PAF dimensions, not confmap output_scale\n        # PAFs may have a different output_stride than confmaps\n        paf_output_scale = magnitude.shape[0] / img.shape[0]\n\n        ax.imshow(\n            magnitude,\n            alpha=0.5,\n            origin=\"upper\",\n            cmap=\"viridis\",\n            extent=[\n                -0.5,\n                magnitude.shape[1] / paf_output_scale - 0.5,\n                magnitude.shape[0] / paf_output_scale - 0.5,\n                -0.5,\n            ],\n        )\n        return fig\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.MatplotlibRenderer.render","title":"<code>render(data)</code>","text":"<p>Render visualization data to a matplotlib figure.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>VisualizationData</code> <p>VisualizationData containing image, confmaps, peaks, etc.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib Figure object.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def render(self, data: VisualizationData) -&gt; matplotlib.figure.Figure:\n    \"\"\"Render visualization data to a matplotlib figure.\n\n    Args:\n        data: VisualizationData containing image, confmaps, peaks, etc.\n\n    Returns:\n        A matplotlib Figure object.\n    \"\"\"\n    img = data.image\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    plot_confmaps(data.pred_confmaps, output_scale=data.output_scale)\n    plot_peaks(data.gt_instances, data.pred_peaks, paired=data.is_paired)\n    return fig\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.MatplotlibRenderer.render_pafs","title":"<code>render_pafs(data)</code>","text":"<p>Render PAF magnitude visualization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>VisualizationData</code> <p>VisualizationData with pred_pafs populated.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib Figure object showing PAF magnitudes.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def render_pafs(self, data: VisualizationData) -&gt; matplotlib.figure.Figure:\n    \"\"\"Render PAF magnitude visualization.\n\n    Args:\n        data: VisualizationData with pred_pafs populated.\n\n    Returns:\n        A matplotlib Figure object showing PAF magnitudes.\n    \"\"\"\n    if data.pred_pafs is None:\n        raise ValueError(\"pred_pafs is None, cannot render PAFs\")\n\n    img = data.image\n    scale = 1.0\n    if img.shape[0] &lt; 512:\n        scale = 2.0\n    if img.shape[0] &lt; 256:\n        scale = 4.0\n\n    # Compute PAF magnitude\n    pafs = data.pred_pafs  # (H, W, 2*edges) or (H, W, edges, 2)\n    if pafs.ndim == 3:\n        n_edges = pafs.shape[-1] // 2\n        pafs = pafs.reshape(pafs.shape[0], pafs.shape[1], n_edges, 2)\n    magnitude = np.sqrt(pafs[..., 0] ** 2 + pafs[..., 1] ** 2)\n    magnitude = magnitude.max(axis=-1)  # Max over edges\n\n    fig = plot_img(img, dpi=72 * scale, scale=scale)\n    ax = plt.gca()\n\n    # Calculate PAF output scale from actual PAF dimensions, not confmap output_scale\n    # PAFs may have a different output_stride than confmaps\n    paf_output_scale = magnitude.shape[0] / img.shape[0]\n\n    ax.imshow(\n        magnitude,\n        alpha=0.5,\n        origin=\"upper\",\n        cmap=\"viridis\",\n        extent=[\n            -0.5,\n            magnitude.shape[1] / paf_output_scale - 0.5,\n            magnitude.shape[0] / paf_output_scale - 0.5,\n            -0.5,\n        ],\n    )\n    return fig\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.VisualizationData","title":"<code>VisualizationData</code>  <code>dataclass</code>","text":"<p>Container for visualization data from a single sample.</p> <p>This dataclass decouples data extraction from rendering, allowing the same data to be rendered to different output targets (matplotlib, wandb, etc.).</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>ndarray</code> <p>Input image as (H, W, C) numpy array, normalized to [0, 1].</p> <code>pred_confmaps</code> <code>ndarray</code> <p>Predicted confidence maps as (H, W, nodes) array, values in [0, 1].</p> <code>pred_peaks</code> <code>ndarray</code> <p>Predicted keypoints as (instances, nodes, 2) or (nodes, 2) array.</p> <code>pred_peak_values</code> <code>ndarray</code> <p>Confidence values as (instances, nodes) or (nodes,) array.</p> <code>gt_instances</code> <code>ndarray</code> <p>Ground truth keypoints, same shape as pred_peaks.</p> <code>node_names</code> <code>List[str]</code> <p>List of node/keypoint names, e.g., [\"head\", \"thorax\", ...].</p> <code>output_scale</code> <code>float</code> <p>Ratio of confmap size to image size (confmap_h / image_h).</p> <code>is_paired</code> <code>bool</code> <p>Whether GT and predictions can be paired for error visualization.</p> <code>pred_pafs</code> <code>Optional[ndarray]</code> <p>Part affinity fields for bottom-up models, optional.</p> <code>pred_class_maps</code> <code>Optional[ndarray]</code> <p>Class maps for multi-class models, optional.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>@dataclass\nclass VisualizationData:\n    \"\"\"Container for visualization data from a single sample.\n\n    This dataclass decouples data extraction from rendering, allowing the same\n    data to be rendered to different output targets (matplotlib, wandb, etc.).\n\n    Attributes:\n        image: Input image as (H, W, C) numpy array, normalized to [0, 1].\n        pred_confmaps: Predicted confidence maps as (H, W, nodes) array, values in [0, 1].\n        pred_peaks: Predicted keypoints as (instances, nodes, 2) or (nodes, 2) array.\n        pred_peak_values: Confidence values as (instances, nodes) or (nodes,) array.\n        gt_instances: Ground truth keypoints, same shape as pred_peaks.\n        node_names: List of node/keypoint names, e.g., [\"head\", \"thorax\", ...].\n        output_scale: Ratio of confmap size to image size (confmap_h / image_h).\n        is_paired: Whether GT and predictions can be paired for error visualization.\n        pred_pafs: Part affinity fields for bottom-up models, optional.\n        pred_class_maps: Class maps for multi-class models, optional.\n    \"\"\"\n\n    image: np.ndarray\n    pred_confmaps: np.ndarray\n    pred_peaks: np.ndarray\n    pred_peak_values: np.ndarray\n    gt_instances: np.ndarray\n    node_names: List[str] = field(default_factory=list)\n    output_scale: float = 1.0\n    is_paired: bool = True\n    pred_pafs: Optional[np.ndarray] = None\n    pred_class_maps: Optional[np.ndarray] = None\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.WandBRenderer","title":"<code>WandBRenderer</code>","text":"<p>Renders VisualizationData to wandb.Image objects.</p> <p>Supports multiple rendering modes: - \"direct\": Pre-render with matplotlib, convert to wandb.Image - \"boxes\": Use wandb boxes for interactive keypoint visualization - \"masks\": Use wandb masks for confidence map overlay</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the renderer.</p> <code>render</code> <p>Render visualization data to a wandb.Image.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>class WandBRenderer:\n    \"\"\"Renders VisualizationData to wandb.Image objects.\n\n    Supports multiple rendering modes:\n    - \"direct\": Pre-render with matplotlib, convert to wandb.Image\n    - \"boxes\": Use wandb boxes for interactive keypoint visualization\n    - \"masks\": Use wandb masks for confidence map overlay\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = \"direct\",\n        box_size: float = 5.0,\n        confmap_threshold: float = 0.1,\n        min_size: int = 512,\n    ):\n        \"\"\"Initialize the renderer.\n\n        Args:\n            mode: Rendering mode - \"direct\", \"boxes\", or \"masks\".\n            box_size: Size of keypoint boxes in pixels (for \"boxes\" mode).\n            confmap_threshold: Threshold for confmap mask (for \"masks\" mode).\n            min_size: Minimum image dimension. Smaller images will be upscaled.\n        \"\"\"\n        self.mode = mode\n        self.box_size = box_size\n        self.confmap_threshold = confmap_threshold\n        self.min_size = min_size\n        self._mpl_renderer = MatplotlibRenderer()\n\n    def render(\n        self, data: VisualizationData, caption: Optional[str] = None\n    ) -&gt; \"wandb.Image\":\n        \"\"\"Render visualization data to a wandb.Image.\n\n        Args:\n            data: VisualizationData containing image, confmaps, peaks, etc.\n            caption: Optional caption for the image.\n\n        Returns:\n            A wandb.Image object.\n        \"\"\"\n        import wandb\n\n        if self.mode == \"boxes\":\n            return self._render_with_boxes(data, caption)\n        elif self.mode == \"masks\":\n            return self._render_with_masks(data, caption)\n        else:  # \"direct\"\n            return self._render_direct(data, caption)\n\n    def _get_scale_factor(self, img_h: int, img_w: int) -&gt; int:\n        \"\"\"Calculate scale factor to ensure minimum image size.\"\"\"\n        min_dim = min(img_h, img_w)\n        if min_dim &gt;= self.min_size:\n            return 1\n        return int(np.ceil(self.min_size / min_dim))\n\n    def _render_direct(\n        self, data: VisualizationData, caption: Optional[str] = None\n    ) -&gt; \"wandb.Image\":\n        \"\"\"Pre-render with matplotlib, return as wandb.Image.\"\"\"\n        import wandb\n        from PIL import Image\n\n        fig = self._mpl_renderer.render(data)\n\n        # Convert figure to PIL Image\n        buf = BytesIO()\n        fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0)\n        buf.seek(0)\n        plt.close(fig)\n\n        pil_image = Image.open(buf)\n        return wandb.Image(pil_image, caption=caption)\n\n    def _render_with_boxes(\n        self, data: VisualizationData, caption: Optional[str] = None\n    ) -&gt; \"wandb.Image\":\n        \"\"\"Use wandb boxes for interactive keypoint visualization.\"\"\"\n        import wandb\n        from PIL import Image\n\n        # Prepare class labels from node names\n        class_labels = {i: name for i, name in enumerate(data.node_names)}\n        if not class_labels:\n            class_labels = {i: f\"node_{i}\" for i in range(data.pred_peaks.shape[-2])}\n\n        # Convert image to uint8\n        img_uint8 = (np.clip(data.image, 0, 1) * 255).astype(np.uint8)\n        # Handle single-channel images: squeeze (H, W, 1) -&gt; (H, W)\n        if img_uint8.ndim == 3 and img_uint8.shape[2] == 1:\n            img_uint8 = img_uint8.squeeze(axis=2)\n        img_h, img_w = img_uint8.shape[:2]\n\n        # Scale up small images for better visibility in wandb\n        scale = self._get_scale_factor(img_h, img_w)\n        if scale &gt; 1:\n            pil_img = Image.fromarray(img_uint8)\n            pil_img = pil_img.resize(\n                (img_w * scale, img_h * scale), resample=Image.BILINEAR\n            )\n            img_uint8 = np.array(pil_img)\n\n        # Build ground truth boxes (use percent domain for proper thumbnail scaling)\n        gt_box_data = self._peaks_to_boxes(\n            data.gt_instances, data.node_names, img_w, img_h, is_gt=True\n        )\n\n        # Build prediction boxes\n        pred_box_data = self._peaks_to_boxes(\n            data.pred_peaks,\n            data.node_names,\n            img_w,\n            img_h,\n            peak_values=data.pred_peak_values,\n            is_gt=False,\n        )\n\n        return wandb.Image(\n            img_uint8,\n            boxes={\n                \"ground_truth\": {\"box_data\": gt_box_data, \"class_labels\": class_labels},\n                \"predictions\": {\n                    \"box_data\": pred_box_data,\n                    \"class_labels\": class_labels,\n                },\n            },\n            caption=caption,\n        )\n\n    def _peaks_to_boxes(\n        self,\n        peaks: np.ndarray,\n        node_names: List[str],\n        img_w: int,\n        img_h: int,\n        peak_values: Optional[np.ndarray] = None,\n        is_gt: bool = False,\n    ) -&gt; List[dict]:\n        \"\"\"Convert peaks array to wandb box_data format.\n\n        Args:\n            peaks: Keypoints as (instances, nodes, 2) or (nodes, 2).\n            node_names: List of node names.\n            img_w: Image width in pixels.\n            img_h: Image height in pixels.\n            peak_values: Optional confidence values.\n            is_gt: Whether these are ground truth points.\n\n        Returns:\n            List of box dictionaries for wandb.\n        \"\"\"\n        box_data = []\n\n        # Normalize shape to (instances, nodes, 2)\n        if peaks.ndim == 2:\n            peaks = peaks[np.newaxis, ...]\n            if peak_values is not None and peak_values.ndim == 1:\n                peak_values = peak_values[np.newaxis, ...]\n\n        # Convert box_size from pixels to percent\n        box_w_pct = self.box_size / img_w\n        box_h_pct = self.box_size / img_h\n\n        for inst_idx, instance in enumerate(peaks):\n            for node_idx, (x, y) in enumerate(instance):\n                if np.isnan(x) or np.isnan(y):\n                    continue\n\n                node_name = (\n                    node_names[node_idx]\n                    if node_idx &lt; len(node_names)\n                    else f\"node_{node_idx}\"\n                )\n\n                # Convert pixel coordinates to percent (0-1 range)\n                x_pct = float(x) / img_w\n                y_pct = float(y) / img_h\n\n                box = {\n                    \"position\": {\n                        \"middle\": [x_pct, y_pct],\n                        \"width\": box_w_pct,\n                        \"height\": box_h_pct,\n                    },\n                    \"domain\": \"percent\",\n                    \"class_id\": node_idx,\n                }\n\n                if is_gt:\n                    box[\"box_caption\"] = f\"GT: {node_name}\"\n                else:\n                    if peak_values is not None:\n                        conf = float(peak_values[inst_idx, node_idx])\n                        box[\"box_caption\"] = f\"{node_name} ({conf:.2f})\"\n                        box[\"scores\"] = {\"confidence\": conf}\n                    else:\n                        box[\"box_caption\"] = node_name\n\n                box_data.append(box)\n\n        return box_data\n\n    def _render_with_masks(\n        self, data: VisualizationData, caption: Optional[str] = None\n    ) -&gt; \"wandb.Image\":\n        \"\"\"Use wandb masks for confidence map overlay.\n\n        Uses argmax approach: each pixel shows the dominant node.\n        \"\"\"\n        import wandb\n\n        # Prepare class labels (0 = background, 1+ = nodes)\n        class_labels = {0: \"background\"}\n        for i, name in enumerate(data.node_names):\n            class_labels[i + 1] = name\n        if not data.node_names:\n            n_nodes = data.pred_confmaps.shape[-1]\n            for i in range(n_nodes):\n                class_labels[i + 1] = f\"node_{i}\"\n\n        # Create argmax mask from confmaps\n        confmaps = data.pred_confmaps  # (H/stride, W/stride, nodes)\n        max_vals = confmaps.max(axis=-1)\n        argmax_map = confmaps.argmax(axis=-1) + 1  # +1 for background offset\n        argmax_map[max_vals &lt; self.confmap_threshold] = 0  # Background\n\n        # Convert image to uint8\n        img_uint8 = (np.clip(data.image, 0, 1) * 255).astype(np.uint8)\n        # Handle single-channel images: (H, W, 1) -&gt; (H, W)\n        if img_uint8.ndim == 3 and img_uint8.shape[2] == 1:\n            img_uint8 = img_uint8.squeeze(axis=2)\n        img_h, img_w = img_uint8.shape[:2]\n\n        # Resize mask to match image dimensions (confmaps are H/stride, W/stride)\n        from PIL import Image\n\n        mask_pil = Image.fromarray(argmax_map.astype(np.uint8))\n        mask_pil = mask_pil.resize((img_w, img_h), resample=Image.NEAREST)\n        argmax_map = np.array(mask_pil)\n\n        return wandb.Image(\n            img_uint8,\n            masks={\n                \"confidence_maps\": {\n                    \"mask_data\": argmax_map.astype(np.uint8),\n                    \"class_labels\": class_labels,\n                }\n            },\n            caption=caption,\n        )\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.WandBRenderer.__init__","title":"<code>__init__(mode='direct', box_size=5.0, confmap_threshold=0.1, min_size=512)</code>","text":"<p>Initialize the renderer.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Rendering mode - \"direct\", \"boxes\", or \"masks\".</p> <code>'direct'</code> <code>box_size</code> <code>float</code> <p>Size of keypoint boxes in pixels (for \"boxes\" mode).</p> <code>5.0</code> <code>confmap_threshold</code> <code>float</code> <p>Threshold for confmap mask (for \"masks\" mode).</p> <code>0.1</code> <code>min_size</code> <code>int</code> <p>Minimum image dimension. Smaller images will be upscaled.</p> <code>512</code> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def __init__(\n    self,\n    mode: str = \"direct\",\n    box_size: float = 5.0,\n    confmap_threshold: float = 0.1,\n    min_size: int = 512,\n):\n    \"\"\"Initialize the renderer.\n\n    Args:\n        mode: Rendering mode - \"direct\", \"boxes\", or \"masks\".\n        box_size: Size of keypoint boxes in pixels (for \"boxes\" mode).\n        confmap_threshold: Threshold for confmap mask (for \"masks\" mode).\n        min_size: Minimum image dimension. Smaller images will be upscaled.\n    \"\"\"\n    self.mode = mode\n    self.box_size = box_size\n    self.confmap_threshold = confmap_threshold\n    self.min_size = min_size\n    self._mpl_renderer = MatplotlibRenderer()\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.WandBRenderer.render","title":"<code>render(data, caption=None)</code>","text":"<p>Render visualization data to a wandb.Image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>VisualizationData</code> <p>VisualizationData containing image, confmaps, peaks, etc.</p> required <code>caption</code> <code>Optional[str]</code> <p>Optional caption for the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>A wandb.Image object.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def render(\n    self, data: VisualizationData, caption: Optional[str] = None\n) -&gt; \"wandb.Image\":\n    \"\"\"Render visualization data to a wandb.Image.\n\n    Args:\n        data: VisualizationData containing image, confmaps, peaks, etc.\n        caption: Optional caption for the image.\n\n    Returns:\n        A wandb.Image object.\n    \"\"\"\n    import wandb\n\n    if self.mode == \"boxes\":\n        return self._render_with_boxes(data, caption)\n    elif self.mode == \"masks\":\n        return self._render_with_masks(data, caption)\n    else:  # \"direct\"\n        return self._render_direct(data, caption)\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.get_dist_rank","title":"<code>get_dist_rank()</code>","text":"<p>Return the rank of the current process if torch.distributed is initialized.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def get_dist_rank():\n    \"\"\"Return the rank of the current process if torch.distributed is initialized.\"\"\"\n    return dist.get_rank() if is_distributed_initialized() else None\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.get_gpu_memory","title":"<code>get_gpu_memory()</code>","text":"<p>Get the available memory on each GPU.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>A list of the available memory on each GPU in MiB.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def get_gpu_memory() -&gt; List[int]:\n    \"\"\"Get the available memory on each GPU.\n\n    Returns:\n        A list of the available memory on each GPU in MiB.\n    \"\"\"\n    if shutil.which(\"nvidia-smi\") is None:\n        return []\n\n    command = [\n        \"nvidia-smi\",\n        \"--query-gpu=index,memory.free\",\n        \"--format=csv\",\n    ]\n\n    try:\n        memory_poll = subprocess.run(command, capture_output=True)\n    except (subprocess.SubprocessError, FileNotFoundError):\n        return []\n\n    subprocess_result = memory_poll.stdout\n    memory_string = subprocess_result.decode(\"ascii\").split(\"\\n\")[1:-1]\n\n    if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n        cuda_visible_devices = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n    else:\n        cuda_visible_devices = None\n\n    memory_list = []\n    for row in memory_string:\n        gpu_index, available_memory = row.split(\", \")\n        available_memory = available_memory.split(\" MiB\")[0]\n\n        if cuda_visible_devices is None or gpu_index in cuda_visible_devices:\n            memory_list.append(int(available_memory))\n\n    return memory_list\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.imgfig","title":"<code>imgfig(size=6, dpi=72, scale=1.0)</code>","text":"<p>Create a tight figure for image plotting.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>float | tuple</code> <p>Scalar or 2-tuple specifying the (width, height) of the figure in inches. If scalar, will assume equal width and height.</p> <code>6</code> <code>dpi</code> <code>int</code> <p>Dots per inch, controlling the resolution of the image.</p> <code>72</code> <code>scale</code> <code>float</code> <p>Factor to scale the size of the figure by. This is a convenience for increasing the size of the plot at the same DPI.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def imgfig(\n    size: float | tuple = 6, dpi: int = 72, scale: float = 1.0\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Create a tight figure for image plotting.\n\n    Args:\n        size: Scalar or 2-tuple specifying the (width, height) of the figure in inches.\n            If scalar, will assume equal width and height.\n        dpi: Dots per inch, controlling the resolution of the image.\n        scale: Factor to scale the size of the figure by. This is a convenience for\n            increasing the size of the plot at the same DPI.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    if not isinstance(size, (tuple, list)):\n        size = (size, size)\n    fig = plt.figure(figsize=(scale * size[0], scale * size[1]), dpi=dpi)\n    ax = fig.add_axes([0, 0, 1, 1], frameon=False)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.autoscale(tight=True)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.grid(False)\n    return fig\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.is_distributed_initialized","title":"<code>is_distributed_initialized()</code>","text":"<p>Check if distributed processes are initialized.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def is_distributed_initialized():\n    \"\"\"Check if distributed processes are initialized.\"\"\"\n    return dist.is_available() and dist.is_initialized()\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.plot_confmaps","title":"<code>plot_confmaps(confmaps, output_scale=1.0)</code>","text":"<p>Plot confidence maps reduced over channels.</p> <p>Parameters:</p> Name Type Description Default <code>confmaps</code> <code>ndarray</code> <p>Confidence maps to plot with shape (height, width, channel).</p> required <code>output_scale</code> <code>float</code> <p>Factor to scale the size of the figure by.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def plot_confmaps(confmaps: np.ndarray, output_scale: float = 1.0):\n    \"\"\"Plot confidence maps reduced over channels.\n\n    Args:\n        confmaps: Confidence maps to plot with shape (height, width, channel).\n        output_scale: Factor to scale the size of the figure by.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    ax = plt.gca()\n    return ax.imshow(\n        np.squeeze(confmaps.max(axis=-1)),\n        alpha=0.5,\n        origin=\"upper\",\n        vmin=0,\n        vmax=1,\n        extent=[\n            -0.5,\n            confmaps.shape[1] / output_scale - 0.5,\n            confmaps.shape[0] / output_scale - 0.5,\n            -0.5,\n        ],\n    )\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.plot_img","title":"<code>plot_img(img, dpi=72, scale=1.0)</code>","text":"<p>Plot an image in a tight figure.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Image to plot of shape (height, width, channel).</p> required <code>dpi</code> <code>int</code> <p>Dots per inch, controlling the resolution of the image.</p> <code>72</code> <code>scale</code> <code>float</code> <p>Factor to scale the size of the figure by. This is a convenience for increasing the size of the plot at the same DPI.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def plot_img(\n    img: np.ndarray, dpi: int = 72, scale: float = 1.0\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot an image in a tight figure.\n\n    Args:\n        img: Image to plot of shape (height, width, channel).\n        dpi: Dots per inch, controlling the resolution of the image.\n        scale: Factor to scale the size of the figure by. This is a convenience for\n            increasing the size of the plot at the same DPI.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    if hasattr(img, \"numpy\"):\n        img = img.numpy()\n\n    if img.shape[0] == 1:\n        # Squeeze out batch singleton dimension.\n        img = img.squeeze(axis=0)\n\n    # Check if image is grayscale (single channel).\n    grayscale = img.shape[-1] == 1\n    if grayscale:\n        # Squeeze out singleton channel.\n        img = img.squeeze(axis=-1)\n\n    # Normalize the range of pixel values.\n    img_min = img.min()\n    img_max = img.max()\n    if img_min &lt; 0.0 or img_max &gt; 1.0:\n        img = (img - img_min) / (img_max - img_min)\n\n    fig = imgfig(\n        size=(float(img.shape[1]) / dpi, float(img.shape[0]) / dpi),\n        dpi=dpi,\n        scale=scale,\n    )\n\n    ax = fig.gca()\n    ax.imshow(\n        img,\n        cmap=\"gray\" if grayscale else None,\n        origin=\"upper\",\n        extent=[-0.5, img.shape[1] - 0.5, img.shape[0] - 0.5, -0.5],\n    )\n    return fig\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.plot_peaks","title":"<code>plot_peaks(pts_gt, pts_pr=None, paired=False)</code>","text":"<p>Plot ground truth and detected peaks.</p> <p>Parameters:</p> Name Type Description Default <code>pts_gt</code> <code>ndarray</code> <p>Ground-truth keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2).</p> required <code>pts_pr</code> <code>ndarray | None</code> <p>Predicted keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2)</p> <code>None</code> <code>paired</code> <code>bool</code> <p>True if error lines should be plotted else False.</p> <code>False</code> <p>Returns:</p> Type Description <p>A matplotlib.figure.Figure to use for plotting.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def plot_peaks(\n    pts_gt: np.ndarray, pts_pr: np.ndarray | None = None, paired: bool = False\n):\n    \"\"\"Plot ground truth and detected peaks.\n\n    Args:\n        pts_gt: Ground-truth keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2).\n        pts_pr: Predicted keypoints of shape (num_instances, nodes, 2). To plot centroids, shape: (1, num_instances, 2)\n        paired: True if error lines should be plotted else False.\n\n    Returns:\n        A matplotlib.figure.Figure to use for plotting.\n    \"\"\"\n    handles = []\n    ax = plt.gca()\n    if paired and pts_pr is not None:\n        for pt_gt, pt_pr in zip(pts_gt, pts_pr):\n            for p_gt, p_pr in zip(pt_gt, pt_pr):\n                handles.append(\n                    ax.plot(\n                        [p_gt[0], p_pr[0]], [p_gt[1], p_pr[1]], \"r-\", alpha=0.5, lw=2\n                    )\n                )\n    if pts_pr is not None:\n        handles.append(\n            ax.plot(\n                pts_gt[..., 0].ravel(),\n                pts_gt[..., 1].ravel(),\n                \"g.\",\n                alpha=0.7,\n                ms=10,\n                mew=1,\n                mec=\"w\",\n            )\n        )\n        handles.append(\n            ax.plot(\n                pts_pr[..., 0].ravel(),\n                pts_pr[..., 1].ravel(),\n                \"r.\",\n                alpha=0.7,\n                ms=10,\n                mew=1,\n                mec=\"w\",\n            )\n        )\n    else:\n        cmap = sns.color_palette(\"tab20\")\n        for i, pts in enumerate(pts_gt):\n            handles.append(\n                ax.plot(\n                    pts[:, 0],\n                    pts[:, 1],\n                    \".\",\n                    alpha=0.7,\n                    ms=15,\n                    mew=1,\n                    mfc=cmap[i % len(cmap)],\n                    mec=\"w\",\n                )\n            )\n    return handles\n</code></pre>"},{"location":"api/training/utils/#sleap_nn.training.utils.xavier_init_weights","title":"<code>xavier_init_weights(x)</code>","text":"<p>Function to initilaise the model weights with Xavier initialization method.</p> Source code in <code>sleap_nn/training/utils.py</code> <pre><code>def xavier_init_weights(x):\n    \"\"\"Function to initilaise the model weights with Xavier initialization method.\"\"\"\n    if isinstance(x, nn.Conv2d) or isinstance(x, nn.Linear):\n        if x.weight is not None:\n            nn.init.xavier_uniform_(x.weight)\n        if x.bias is not None:\n            nn.init.constant_(x.bias, 0)\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"<p>SLEAP-NN uses YAML configuration files with three main sections.</p> Section Description Data Config Data paths, preprocessing, augmentation Model Config Backbone and head architecture Trainer Config Epochs, optimizer, learning rate, logging Sample Configs Ready-to-use templates"},{"location":"configuration/#config-structure","title":"Config Structure","text":"<pre><code>data_config:      # Data loading and preprocessing\n  train_labels_path: [...]\n  preprocessing: {...}\n  augmentation_config: {...}\n\nmodel_config:     # Model architecture\n  backbone_config: {...}\n  head_configs: {...}\n\ntrainer_config:   # Training settings\n  max_epochs: 100\n  optimizer: {...}\n</code></pre>"},{"location":"configuration/#usage","title":"Usage","text":"<pre><code>sleap-nn train --config config.yaml\n</code></pre>"},{"location":"configuration/#converting-from-legacy-sleap","title":"Converting from Legacy SLEAP","text":"<p>If you have a <code>training_config.json</code> from SLEAP &lt; v1.5, you can convert it to the new YAML format:</p> <pre><code>from sleap_nn.config.training_job_config import TrainingJobConfig\nfrom omegaconf import OmegaConf\n\n# Load legacy config and convert\nconfig = TrainingJobConfig.load_sleap_config(\"path/to/training_config.json\")\n\n# Save as YAML\nOmegaConf.save(config, \"config.yaml\")\n</code></pre>"},{"location":"configuration/data/","title":"Data Config","text":"<p>Configure data loading, preprocessing, and augmentation.</p>"},{"location":"configuration/data/#data-paths","title":"Data Paths","text":"<pre><code>data_config:\n  train_labels_path:\n    - train.slp\n    - more_train.slp      # Multiple files supported\n  val_labels_path:\n    - val.slp             # Optional\n  validation_fraction: 0.1  # Used if val_labels_path not set\n  test_file_path: test.slp  # For post-training evaluation\n</code></pre> Option Description Default <code>train_labels_path</code> Training data files Required <code>val_labels_path</code> Validation data files <code>null</code> <code>validation_fraction</code> Fraction for auto-split <code>0.1</code> <code>test_file_path</code> Test data for evaluation <code>null</code> <code>user_instances_only</code> Only use user-labeled instances <code>true</code>"},{"location":"configuration/data/#data-pipeline","title":"Data Pipeline","text":"<pre><code>data_config:\n  data_pipeline_fw: torch_dataset  # or cache options\n  cache_img_path: /path/to/cache   # For disk caching\n</code></pre> Option Description When to Use <code>torch_dataset</code> Load on demand Default, works everywhere <code>torch_dataset_cache_img_memory</code> Cache in RAM Faster, needs enough RAM <code>torch_dataset_cache_img_disk</code> Cache to disk Large datasets, multi-GPU <p>With caching, you can use <code>num_workers &gt; 0</code></p>"},{"location":"configuration/data/#preprocessing","title":"Preprocessing","text":"<pre><code>data_config:\n  preprocessing:\n    ensure_rgb: false          # Force 3 channels\n    ensure_grayscale: false    # Force 1 channel\n    scale: 1.0                 # Resize factor (0.5 = half size)\n    max_height: null           # Pad to this height\n    max_width: null            # Pad to this width\n    crop_size: null            # For centered-instance models\n    min_crop_size: 100         # Minimum auto crop size\n    crop_padding: null         # Extra padding for crops\n</code></pre>"},{"location":"configuration/data/#common-patterns","title":"Common Patterns","text":"Downscale for speedFixed dimensionsCentered-instance model <pre><code>preprocessing:\n  scale: 0.5  # Half resolution, 4x faster\n</code></pre> <pre><code>preprocessing:\n  max_height: 512\n  max_width: 512\n</code></pre> <pre><code>preprocessing:\n  crop_size: 256  # Fixed crop size\n  # or\n  crop_size: null  # Auto-compute from data\n  min_crop_size: 100\n  crop_padding: 20\n</code></pre>"},{"location":"configuration/data/#augmentation","title":"Augmentation","text":"<pre><code>data_config:\n  use_augmentations_train: true\n  augmentation_config:\n    intensity:\n      # ... intensity settings\n    geometric:\n      # ... geometric settings\n</code></pre>"},{"location":"configuration/data/#intensity-augmentation","title":"Intensity Augmentation","text":"<pre><code>augmentation_config:\n  intensity:\n    # Noise\n    uniform_noise_p: 0.0\n    gaussian_noise_p: 0.0\n    gaussian_noise_mean: 0.0\n    gaussian_noise_std: 1.0\n\n    # Contrast\n    contrast_min: 0.9\n    contrast_max: 1.1\n    contrast_p: 0.0\n\n    # Brightness\n    brightness_min: 1.0\n    brightness_max: 1.0\n    brightness_p: 0.0\n</code></pre>"},{"location":"configuration/data/#geometric-augmentation","title":"Geometric Augmentation","text":"<pre><code>augmentation_config:\n  geometric:\n    # Rotation (degrees)\n    rotation_min: -15.0\n    rotation_max: 15.0\n\n    # Scale\n    scale_min: 0.9\n    scale_max: 1.1\n\n    # Translation (fraction of image size)\n    translate_width: 0.0\n    translate_height: 0.0\n\n    # Combined probability (used when individual *_p values are null)\n    affine_p: 0.0\n\n    # Independent probabilities (override affine_p when set)\n    rotation_p: 1.0   # Always apply rotation by default\n    scale_p: 1.0      # Always apply scaling by default\n    translate_p: null # Uses affine_p if null\n\n    # Random erase\n    erase_p: 0.0\n    erase_scale_min: 0.0001\n    erase_scale_max: 0.01\n\n    # Mixup\n    mixup_p: 0.0\n    mixup_lambda_min: 0.01\n    mixup_lambda_max: 0.05\n</code></pre>"},{"location":"configuration/data/#examples","title":"Examples","text":""},{"location":"configuration/data/#no-augmentation","title":"No Augmentation","text":"<pre><code>data_config:\n  use_augmentations_train: false\n</code></pre>"},{"location":"configuration/data/#geometric-only-using-defaults","title":"Geometric Only (using defaults)","text":"<pre><code>data_config:\n  use_augmentations_train: true\n  # Default augmentation_config applies rotation \u00b115\u00b0 and scale 0.9-1.1 with p=1.0\n</code></pre>"},{"location":"configuration/data/#geometric-only-explicit","title":"Geometric Only (explicit)","text":"<pre><code>data_config:\n  use_augmentations_train: true\n  augmentation_config:\n    intensity: null\n    geometric:\n      rotation_min: -30.0\n      rotation_max: 30.0\n      rotation_p: 0.5        # 50% chance of rotation\n      scale_min: 0.8\n      scale_max: 1.2\n      scale_p: 0.5           # 50% chance of scaling\n</code></pre>"},{"location":"configuration/data/#full-augmentation","title":"Full Augmentation","text":"<pre><code>data_config:\n  use_augmentations_train: true\n  augmentation_config:\n    intensity:\n      contrast_p: 0.3\n      brightness_p: 0.3\n      gaussian_noise_p: 0.2\n      gaussian_noise_std: 0.1\n    geometric:\n      rotation_min: -30.0\n      rotation_max: 30.0\n      scale_min: 0.8\n      scale_max: 1.2\n      affine_p: 0.7\n</code></pre>"},{"location":"configuration/data/#full-reference","title":"Full Reference","text":""},{"location":"configuration/data/#dataconfig","title":"DataConfig","text":"Option Type Default Description <code>train_labels_path</code> list <code>null</code> List of paths to training <code>.slp</code> files <code>val_labels_path</code> list <code>null</code> List of paths to validation <code>.slp</code> files <code>validation_fraction</code> float <code>0.1</code> Fraction of training data for auto-split validation <code>use_same_data_for_val</code> bool <code>false</code> Use same data for both training and validation (useful for intentional overfitting) <code>test_file_path</code> str/list <code>null</code> Path(s) to test data for post-training evaluation <code>provider</code> str <code>LabelsReader</code> Data provider class (only <code>LabelsReader</code> supported) <code>user_instances_only</code> bool <code>true</code> Only use user-labeled instances (not predicted) <code>data_pipeline_fw</code> str <code>torch_dataset</code> Data loading framework: <code>torch_dataset</code>, <code>torch_dataset_cache_img_memory</code>, <code>torch_dataset_cache_img_disk</code> <code>cache_img_path</code> str <code>null</code> Path for disk caching (used with <code>torch_dataset_cache_img_disk</code>) <code>use_existing_imgs</code> bool <code>false</code> Use existing cached images instead of regenerating <code>delete_cache_imgs_after_training</code> bool <code>true</code> Delete cached images after training completes <code>parallel_caching</code> bool <code>true</code> Use parallel processing for caching (faster for large datasets) <code>cache_workers</code> int <code>0</code> Number of workers for parallel caching (0 = auto: min(4, cpu_count)) <code>use_augmentations_train</code> bool <code>true</code> Apply augmentations during training"},{"location":"configuration/data/#preprocessingconfig","title":"PreprocessingConfig","text":"Option Type Default Description <code>ensure_rgb</code> bool <code>false</code> Convert images to 3 channels (RGB). Single-channel images are replicated <code>ensure_grayscale</code> bool <code>false</code> Convert images to 1 channel (grayscale) <code>max_height</code> int <code>null</code> Pad images to this height <code>max_width</code> int <code>null</code> Pad images to this width <code>scale</code> float <code>1.0</code> Resize factor (e.g., 0.5 = half size) <code>crop_size</code> int <code>null</code> Crop size for centered-instance models. If <code>null</code>, auto-computed from data <code>min_crop_size</code> int <code>100</code> Minimum crop size when auto-computing <code>crop_padding</code> int <code>null</code> Extra padding around crops. If <code>null</code>, auto-computed from augmentation settings"},{"location":"configuration/data/#intensityconfig-augmentation_configintensity","title":"IntensityConfig (augmentation_config.intensity)","text":"Option Type Default Description <code>uniform_noise_min</code> float <code>0.0</code> Minimum uniform noise value (0-1 scale) <code>uniform_noise_max</code> float <code>1.0</code> Maximum uniform noise value (0-1 scale) <code>uniform_noise_p</code> float <code>0.0</code> Probability of applying uniform noise <code>gaussian_noise_mean</code> float <code>0.0</code> Mean of Gaussian noise distribution <code>gaussian_noise_std</code> float <code>1.0</code> Standard deviation of Gaussian noise <code>gaussian_noise_p</code> float <code>0.0</code> Probability of applying Gaussian noise <code>contrast_min</code> float <code>0.9</code> Minimum contrast factor <code>contrast_max</code> float <code>1.1</code> Maximum contrast factor <code>contrast_p</code> float <code>0.0</code> Probability of applying contrast adjustment <code>brightness_min</code> float <code>1.0</code> Minimum brightness factor <code>brightness_max</code> float <code>1.0</code> Maximum brightness factor (max 2.0) <code>brightness_p</code> float <code>0.0</code> Probability of applying brightness adjustment"},{"location":"configuration/data/#geometricconfig-augmentation_configgeometric","title":"GeometricConfig (augmentation_config.geometric)","text":"Option Type Default Description <code>rotation_min</code> float <code>-15.0</code> Minimum rotation angle in degrees <code>rotation_max</code> float <code>15.0</code> Maximum rotation angle in degrees <code>rotation_p</code> float <code>1.0</code> Probability of rotation (independent). If <code>null</code>, uses <code>affine_p</code> <code>scale_min</code> float <code>0.9</code> Minimum scale factor <code>scale_max</code> float <code>1.1</code> Maximum scale factor <code>scale_p</code> float <code>1.0</code> Probability of scaling (independent). If <code>null</code>, uses <code>affine_p</code> <code>translate_width</code> float <code>0.0</code> Maximum horizontal translation as fraction of width <code>translate_height</code> float <code>0.0</code> Maximum vertical translation as fraction of height <code>translate_p</code> float <code>null</code> Probability of translation (independent). If <code>null</code>, uses <code>affine_p</code> <code>affine_p</code> float <code>0.0</code> Probability of bundled affine transform (rotation + scale + translate) <code>erase_scale_min</code> float <code>0.0001</code> Minimum erased area as proportion of image <code>erase_scale_max</code> float <code>0.01</code> Maximum erased area as proportion of image <code>erase_ratio_min</code> float <code>1.0</code> Minimum aspect ratio of erased area <code>erase_ratio_max</code> float <code>1.0</code> Maximum aspect ratio of erased area <code>erase_p</code> float <code>0.0</code> Probability of random erasing <code>mixup_lambda_min</code> float <code>0.01</code> Minimum mixup strength <code>mixup_lambda_max</code> float <code>0.05</code> Maximum mixup strength <code>mixup_p</code> float <code>0.0</code> Probability of applying mixup"},{"location":"configuration/model/","title":"Model Config","text":"<p>Configure model architecture: backbone and heads.</p>"},{"location":"configuration/model/#structure","title":"Structure","text":"<pre><code>model_config:\n  init_weights: default\n  pretrained_backbone_weights: null\n  pretrained_head_weights: null\n\n  backbone_config:\n    unet: {...}       # Only one backbone\n    convnext: null\n    swint: null\n\n  head_configs:\n    single_instance: {...}  # Only one head type\n    centroid: null\n    centered_instance: null\n    bottomup: null\n    multi_class_bottomup: null\n    multi_class_topdown: null\n</code></pre>"},{"location":"configuration/model/#backbones","title":"Backbones","text":"<p>Choose one backbone. Set others to <code>null</code>.</p>"},{"location":"configuration/model/#unet","title":"UNet","text":"<p>Most flexible, good for any resolution:</p> <pre><code>backbone_config:\n  unet:\n    in_channels: 1          # 1=grayscale, 3=RGB\n    filters: 32             # Base filters (16, 32, 64)\n    filters_rate: 2.0       # Filter multiplier per level\n    max_stride: 16          # Receptive field (16 or 32)\n    kernel_size: 3\n    middle_block: true\n    up_interpolate: true\n    stacks: 1\n    convs_per_block: 2\n    output_stride: 2        # Output resolution divisor\n</code></pre> <p>Size presets:</p> Preset filters max_stride Speed Accuracy Small 16 16 Fast Good Medium 32 16 Medium Better Large 64 32 Slow Best"},{"location":"configuration/model/#convnext","title":"ConvNeXt","text":"<p>Modern CNN, good with pretrained weights:</p> <pre><code>backbone_config:\n  convnext:\n    model_type: tiny       # tiny, small, base, large\n    in_channels: 1\n    max_stride: 32\n    output_stride: 2\n    up_interpolate: true\n    pre_trained_weights: ConvNeXt_Tiny_Weights  # ImageNet\n</code></pre>"},{"location":"configuration/model/#swin-transformer","title":"Swin Transformer","text":"<p>Vision transformer, captures global context:</p> <pre><code>backbone_config:\n  swint:\n    model_type: tiny       # tiny, small, base\n    in_channels: 1\n    max_stride: 32\n    output_stride: 2\n    up_interpolate: true\n    pre_trained_weights: Swin_T_Weights  # ImageNet\n</code></pre>"},{"location":"configuration/model/#heads","title":"Heads","text":"<p>Choose the head type for your task. Set others to <code>null</code>.</p>"},{"location":"configuration/model/#single-instance","title":"Single Instance","text":"<p>One animal per frame:</p> <pre><code>head_configs:\n  single_instance:\n    confmaps:\n      part_names: null    # null = all from skeleton\n      sigma: 5.0          # Gaussian spread\n      output_stride: 2\n</code></pre>"},{"location":"configuration/model/#centroid-top-down-stage-1","title":"Centroid (Top-Down Stage 1)","text":"<p>Detect instance centers:</p> <pre><code>head_configs:\n  centroid:\n    confmaps:\n      anchor_part: null   # null = bbox center\n      sigma: 5.0\n      output_stride: 2\n</code></pre>"},{"location":"configuration/model/#centered-instance-top-down-stage-2","title":"Centered Instance (Top-Down Stage 2)","text":"<p>Pose on cropped instances:</p> <pre><code>head_configs:\n  centered_instance:\n    confmaps:\n      anchor_part: null\n      sigma: 5.0\n      output_stride: 2\n</code></pre>"},{"location":"configuration/model/#bottom-up","title":"Bottom-Up","text":"<p>All keypoints + grouping:</p> <pre><code>head_configs:\n  bottomup:\n    confmaps:\n      sigma: 2.5\n      output_stride: 4\n      loss_weight: 1.0\n    pafs:\n      sigma: 75.0\n      output_stride: 8\n      loss_weight: 1.0\n</code></pre>"},{"location":"configuration/model/#multi-class-bottom-up","title":"Multi-Class Bottom-Up","text":"<p>Bottom-up with identity:</p> <pre><code>head_configs:\n  multi_class_bottomup:\n    confmaps:\n      sigma: 5.0\n      output_stride: 2\n      loss_weight: 1.0\n    class_maps:\n      classes: null       # null = from track names\n      sigma: 5.0\n      output_stride: 2\n      loss_weight: 1.0\n</code></pre>"},{"location":"configuration/model/#multi-class-top-down","title":"Multi-Class Top-Down","text":"<p>Top-down with identity:</p> <pre><code>head_configs:\n  multi_class_topdown:\n    confmaps:\n      anchor_part: null\n      sigma: 5.0\n      output_stride: 2\n      loss_weight: 1.0\n    class_vectors:\n      classes: null\n      num_fc_layers: 1\n      num_fc_units: 64\n      output_stride: 16   # Match backbone max_stride\n      loss_weight: 1.0\n</code></pre>"},{"location":"configuration/model/#pretrained-weights","title":"Pretrained Weights","text":""},{"location":"configuration/model/#from-previous-training","title":"From Previous Training","text":"<pre><code>model_config:\n  pretrained_backbone_weights: /path/to/best.ckpt\n  pretrained_head_weights: /path/to/best.ckpt\n</code></pre>"},{"location":"configuration/model/#from-legacy-sleap","title":"From Legacy SLEAP","text":"<pre><code>model_config:\n  pretrained_backbone_weights: /path/to/best_model.h5\n  pretrained_head_weights: /path/to/best_model.h5\n</code></pre> <p>UNet only</p> <p>Legacy SLEAP weights only work with UNet backbone.</p>"},{"location":"configuration/model/#common-configurations","title":"Common Configurations","text":""},{"location":"configuration/model/#single-instance-medium","title":"Single Instance (Medium)","text":"<pre><code>model_config:\n  backbone_config:\n    unet:\n      filters: 32\n      max_stride: 16\n      output_stride: 2\n  head_configs:\n    single_instance:\n      confmaps:\n        sigma: 5.0\n        output_stride: 2\n</code></pre>"},{"location":"configuration/model/#bottom-up-large","title":"Bottom-Up (Large)","text":"<pre><code>model_config:\n  backbone_config:\n    unet:\n      filters: 32\n      max_stride: 32\n      output_stride: 4\n  head_configs:\n    bottomup:\n      confmaps:\n        sigma: 2.5\n        output_stride: 4\n      pafs:\n        sigma: 75.0\n        output_stride: 8\n</code></pre>"},{"location":"configuration/model/#top-down-with-convnext","title":"Top-Down with ConvNeXt","text":"<pre><code># Centroid model\nmodel_config:\n  backbone_config:\n    convnext:\n      model_type: tiny\n      pre_trained_weights: ConvNeXt_Tiny_Weights\n  head_configs:\n    centroid:\n      confmaps:\n        sigma: 5.0\n\n# Instance model (separate training)\nmodel_config:\n  backbone_config:\n    convnext:\n      model_type: tiny\n  head_configs:\n    centered_instance:\n      confmaps:\n        sigma: 5.0\n</code></pre>"},{"location":"configuration/model/#tips","title":"Tips","text":"<p>Match output_stride</p> <p>Set <code>backbone_config.*.output_stride</code> to match the minimum <code>head_configs.*.output_stride</code>.</p> <p>Sigma tuning</p> <ul> <li>Larger sigma (5-10): Easier to learn, less precise</li> <li>Smaller sigma (1-3): More precise, harder to learn</li> </ul> <p>PAF sigma</p> <p>Use larger sigma for PAFs (50-100) than for confmaps.</p>"},{"location":"configuration/model/#full-reference","title":"Full Reference","text":""},{"location":"configuration/model/#modelconfig","title":"ModelConfig","text":"Option Type Default Description <code>init_weights</code> str <code>default</code> Weight initialization: <code>default</code> (kaiming) or <code>xavier</code> <code>pretrained_backbone_weights</code> str <code>null</code> Path to <code>.ckpt</code> or <code>.h5</code> file for backbone weights <code>pretrained_head_weights</code> str <code>null</code> Path to <code>.ckpt</code> or <code>.h5</code> file for head weights"},{"location":"configuration/model/#unetconfig","title":"UNetConfig","text":"Option Type Default Description <code>in_channels</code> int <code>1</code> Input channels (1=grayscale, 3=RGB) <code>kernel_size</code> int <code>3</code> Convolution kernel size <code>filters</code> int <code>32</code> Base number of filters <code>filters_rate</code> float <code>1.5</code> Filter multiplier per level <code>max_stride</code> int <code>16</code> Maximum stride (16 or 32) <code>stem_stride</code> int <code>null</code> Additional downsampling in stem <code>middle_block</code> bool <code>true</code> Add block at encoder end <code>up_interpolate</code> bool <code>true</code> Use interpolation (vs transposed conv) for upsampling <code>stacks</code> int <code>1</code> Number of decoder stacks <code>convs_per_block</code> int <code>2</code> Convolutions per block <code>output_stride</code> int <code>1</code> Output resolution divisor"},{"location":"configuration/model/#convnextconfig","title":"ConvNextConfig","text":"Option Type Default Description <code>model_type</code> str <code>tiny</code> Architecture: <code>tiny</code>, <code>small</code>, <code>base</code>, <code>large</code> <code>pre_trained_weights</code> str <code>null</code> ImageNet weights: <code>ConvNeXt_Tiny_Weights</code>, etc. <code>in_channels</code> int <code>1</code> Input channels <code>kernel_size</code> int <code>3</code> Convolution kernel size <code>filters_rate</code> float <code>2</code> Filter multiplier <code>convs_per_block</code> int <code>2</code> Convolutions per block <code>stem_patch_kernel</code> int <code>4</code> Stem layer kernel size <code>stem_patch_stride</code> int <code>2</code> Stem layer stride <code>up_interpolate</code> bool <code>true</code> Use interpolation for upsampling <code>output_stride</code> int <code>1</code> Output resolution divisor <code>max_stride</code> int <code>32</code> Fixed at 32 for all ConvNeXt"},{"location":"configuration/model/#swintconfig","title":"SwinTConfig","text":"Option Type Default Description <code>model_type</code> str <code>tiny</code> Architecture: <code>tiny</code>, <code>small</code>, <code>base</code> <code>pre_trained_weights</code> str <code>null</code> ImageNet weights: <code>Swin_T_Weights</code>, <code>Swin_S_Weights</code>, <code>Swin_B_Weights</code> <code>in_channels</code> int <code>1</code> Input channels <code>kernel_size</code> int <code>3</code> Convolution kernel size <code>filters_rate</code> float <code>2</code> Filter multiplier <code>convs_per_block</code> int <code>2</code> Convolutions per block <code>patch_size</code> int <code>4</code> Patch size for stem <code>stem_patch_stride</code> int <code>2</code> Stem stride <code>window_size</code> int <code>7</code> Attention window size <code>up_interpolate</code> bool <code>true</code> Use interpolation for upsampling <code>output_stride</code> int <code>1</code> Output resolution divisor <code>max_stride</code> int <code>32</code> Fixed at 32 for all SwinT"},{"location":"configuration/model/#single_instanceconfmaps","title":"single_instance.confmaps","text":"<p>For single animal per frame.</p> Option Type Default Description <code>part_names</code> list <code>null</code> Body parts to predict (null = all from skeleton) <code>sigma</code> float <code>5.0</code> Gaussian spread in pixels <code>output_stride</code> int <code>1</code> Output resolution divisor <pre><code>head_configs:\n  single_instance:\n    confmaps:\n      part_names: null\n      sigma: 5.0\n      output_stride: 2\n</code></pre>"},{"location":"configuration/model/#centroidconfmaps","title":"centroid.confmaps","text":"<p>For detecting instance centers (top-down stage 1).</p> Option Type Default Description <code>anchor_part</code> str <code>null</code> Anchor point (null = bbox center) <code>sigma</code> float <code>5.0</code> Gaussian spread in pixels <code>output_stride</code> int <code>1</code> Output resolution divisor <pre><code>head_configs:\n  centroid:\n    confmaps:\n      anchor_part: null\n      sigma: 5.0\n      output_stride: 2\n</code></pre>"},{"location":"configuration/model/#centered_instanceconfmaps","title":"centered_instance.confmaps","text":"<p>For pose estimation on cropped instances (top-down stage 2).</p> Option Type Default Description <code>part_names</code> list <code>null</code> Body parts to predict (null = all from skeleton) <code>anchor_part</code> str <code>null</code> Anchor point (null = bbox center) <code>sigma</code> float <code>5.0</code> Gaussian spread in pixels <code>output_stride</code> int <code>1</code> Output resolution divisor <code>loss_weight</code> float <code>1.0</code> Loss weighting <pre><code>head_configs:\n  centered_instance:\n    confmaps:\n      part_names: null\n      anchor_part: null\n      sigma: 5.0\n      output_stride: 2\n</code></pre>"},{"location":"configuration/model/#bottomupconfmaps","title":"bottomup.confmaps","text":"<p>For detecting all keypoints in bottom-up models.</p> Option Type Default Description <code>part_names</code> list <code>null</code> Body parts to predict (null = all from skeleton) <code>sigma</code> float <code>5.0</code> Gaussian spread in pixels <code>output_stride</code> int <code>1</code> Output resolution divisor <code>loss_weight</code> float <code>null</code> Loss weighting <pre><code>head_configs:\n  bottomup:\n    confmaps:\n      part_names: null\n      sigma: 2.5\n      output_stride: 4\n      loss_weight: 1.0\n</code></pre>"},{"location":"configuration/model/#bottomuppafs","title":"bottomup.pafs","text":"<p>Part Affinity Fields for grouping keypoints into instances.</p> Option Type Default Description <code>edges</code> list <code>null</code> Edge connections (null = from skeleton) <code>sigma</code> float <code>15.0</code> PAF spread (use larger than confmaps, typically 50-100) <code>output_stride</code> int <code>1</code> Output resolution divisor <code>loss_weight</code> float <code>null</code> Loss weighting <pre><code>head_configs:\n  bottomup:\n    confmaps:\n      sigma: 2.5\n      output_stride: 4\n      loss_weight: 1.0\n    pafs:\n      sigma: 75.0\n      output_stride: 8\n      loss_weight: 1.0\n</code></pre>"},{"location":"configuration/model/#multi_class_bottomupconfmaps","title":"multi_class_bottomup.confmaps","text":"<p>Confidence maps for bottom-up models with identity.</p> Option Type Default Description <code>part_names</code> list <code>null</code> Body parts to predict <code>sigma</code> float <code>5.0</code> Gaussian spread <code>output_stride</code> int <code>1</code> Output resolution divisor <code>loss_weight</code> float <code>null</code> Loss weighting"},{"location":"configuration/model/#multi_class_bottomupclass_maps","title":"multi_class_bottomup.class_maps","text":"<p>Class/identity maps for bottom-up ID models.</p> Option Type Default Description <code>classes</code> list <code>null</code> Class names (null = from track names) <code>sigma</code> float <code>5.0</code> Gaussian spread <code>output_stride</code> int <code>1</code> Output resolution divisor <code>loss_weight</code> float <code>null</code> Loss weighting <pre><code>head_configs:\n  multi_class_bottomup:\n    confmaps:\n      sigma: 5.0\n      output_stride: 2\n      loss_weight: 1.0\n    class_maps:\n      classes: null  # inferred from track names\n      sigma: 5.0\n      output_stride: 2\n      loss_weight: 1.0\n</code></pre>"},{"location":"configuration/model/#multi_class_topdownconfmaps","title":"multi_class_topdown.confmaps","text":"<p>Confidence maps for top-down models with identity.</p> Option Type Default Description <code>part_names</code> list <code>null</code> Body parts to predict <code>anchor_part</code> str <code>null</code> Anchor point <code>sigma</code> float <code>5.0</code> Gaussian spread <code>output_stride</code> int <code>1</code> Output resolution divisor <code>loss_weight</code> float <code>1.0</code> Loss weighting"},{"location":"configuration/model/#multi_class_topdownclass_vectors","title":"multi_class_topdown.class_vectors","text":"<p>Classification head for top-down ID models.</p> Option Type Default Description <code>classes</code> list <code>null</code> Class names (null = from track names) <code>num_fc_layers</code> int <code>1</code> Fully-connected layers before output <code>num_fc_units</code> int <code>64</code> Units per FC layer <code>global_pool</code> bool <code>true</code> Use global pooling <code>output_stride</code> int <code>1</code> Should match backbone max_stride <code>loss_weight</code> float <code>1.0</code> Loss weighting <pre><code>head_configs:\n  multi_class_topdown:\n    confmaps:\n      anchor_part: null\n      sigma: 5.0\n      output_stride: 2\n      loss_weight: 1.0\n    class_vectors:\n      classes: null  # inferred from track names\n      num_fc_layers: 1\n      num_fc_units: 64\n      output_stride: 16  # match backbone max_stride\n      loss_weight: 1.0\n</code></pre>"},{"location":"configuration/samples/","title":"Sample Configs","text":"<p>Ready-to-use configuration templates for common scenarios.</p>"},{"location":"configuration/samples/#by-model-type","title":"By Model Type","text":""},{"location":"configuration/samples/#single-instance","title":"Single Instance","text":"<p>One animal per frame.</p> Config Backbone Receptive Field single_instance_unet_medium UNet Medium single_instance_unet_large UNet Large"},{"location":"configuration/samples/#top-down","title":"Top-Down","text":"<p>Two-stage: centroid detection + pose estimation.</p> Config Backbone Notes centroid_unet UNet Stage 1 centroid_swint Swin-T Stage 1 centered_instance_unet_medium UNet Stage 2, Medium centered_instance_unet_large UNet Stage 2, Large"},{"location":"configuration/samples/#bottom-up","title":"Bottom-Up","text":"<p>Single-stage multi-instance.</p> Config Backbone Receptive Field bottomup_unet_medium UNet Medium bottomup_unet_large UNet Large bottomup_convnext ConvNeXt -"},{"location":"configuration/samples/#multi-class-identity","title":"Multi-Class (Identity)","text":"<p>With supervised identity tracking.</p> Config Model Type Backbone multi_class_bottomup Bottom-Up UNet multi_class_topdown Top-Down UNet"},{"location":"configuration/samples/#quick-start-templates","title":"Quick Start Templates","text":""},{"location":"configuration/samples/#minimal-single-instance","title":"Minimal Single Instance","text":"<pre><code>data_config:\n  train_labels_path:\n    - train.slp\n\nmodel_config:\n  backbone_config:\n    unet:\n      filters: 32\n      max_stride: 16\n  head_configs:\n    single_instance:\n      confmaps:\n        sigma: 5.0\n\ntrainer_config:\n  max_epochs: 100\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: single_instance\n</code></pre>"},{"location":"configuration/samples/#minimal-bottom-up","title":"Minimal Bottom-Up","text":"<pre><code>data_config:\n  train_labels_path:\n    - train.slp\n\nmodel_config:\n  backbone_config:\n    unet:\n      filters: 32\n      max_stride: 32\n      output_stride: 4\n  head_configs:\n    bottomup:\n      confmaps:\n        sigma: 2.5\n        output_stride: 4\n      pafs:\n        sigma: 75.0\n        output_stride: 8\n\ntrainer_config:\n  max_epochs: 200\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: bottomup\n</code></pre>"},{"location":"configuration/samples/#minimal-top-down-centroid","title":"Minimal Top-Down (Centroid)","text":"<pre><code>data_config:\n  train_labels_path:\n    - train.slp\n\nmodel_config:\n  backbone_config:\n    unet:\n      filters: 32\n      max_stride: 16\n  head_configs:\n    centroid:\n      confmaps:\n        sigma: 5.0\n\ntrainer_config:\n  max_epochs: 100\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: centroid\n</code></pre>"},{"location":"configuration/samples/#minimal-top-down-instance","title":"Minimal Top-Down (Instance)","text":"<pre><code>data_config:\n  train_labels_path:\n    - train.slp\n  preprocessing:\n    crop_size: 256\n\nmodel_config:\n  backbone_config:\n    unet:\n      filters: 32\n      max_stride: 16\n  head_configs:\n    centered_instance:\n      confmaps:\n        sigma: 5.0\n\ntrainer_config:\n  max_epochs: 100\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: centered_instance\n</code></pre>"},{"location":"configuration/samples/#download-all-samples","title":"Download All Samples","text":"<pre><code># Clone the repo\ngit clone https://github.com/talmolab/sleap-nn.git\n\n# Configs are in docs/sample_configs/\nls sleap-nn/docs/sample_configs/\n</code></pre>"},{"location":"configuration/samples/#customize-a-sample","title":"Customize a Sample","text":"<ol> <li>Download a sample config</li> <li>Update <code>train_labels_path</code> and <code>val_labels_path</code></li> <li>Adjust <code>ckpt_dir</code> and <code>run_name</code></li> <li>Run training:</li> </ol> <pre><code>sleap-nn train --config my_config.yaml\n</code></pre>"},{"location":"configuration/samples/#tips","title":"Tips","text":"<p>Start with medium receptive field</p> <p>Medium RF configs are a good balance of speed and accuracy.</p> <p>Use augmentation</p> <p>All sample configs have augmentation enabled. Disable with <code>use_augmentations_train: false</code> for debugging.</p> <p>Check your data first</p> <p>Open your <code>.slp</code> file in SLEAP to verify labels before training.</p>"},{"location":"configuration/trainer/","title":"Trainer Config","text":"<p>Configure training hyperparameters, optimization, and logging.</p>"},{"location":"configuration/trainer/#essential-settings","title":"Essential Settings","text":"<pre><code>trainer_config:\n  max_epochs: 100\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: my_experiment\n</code></pre> Option Description Default <code>max_epochs</code> Training epochs <code>10</code> <code>save_ckpt</code> Save checkpoints <code>false</code> <code>ckpt_dir</code> Checkpoint directory <code>null</code> <code>run_name</code> Run folder name auto-generated"},{"location":"configuration/trainer/#data-loading","title":"Data Loading","text":"<pre><code>trainer_config:\n  train_data_loader:\n    batch_size: 4\n    shuffle: true\n    num_workers: 0    # &gt;0 only with caching\n  val_data_loader:\n    batch_size: 4\n    shuffle: false\n    num_workers: 0\n</code></pre> <p>Workers without caching</p> <p>Only use <code>num_workers &gt; 0</code> with data caching enabled.</p>"},{"location":"configuration/trainer/#optimization","title":"Optimization","text":""},{"location":"configuration/trainer/#optimizer","title":"Optimizer","text":"<pre><code>trainer_config:\n  optimizer_name: Adam    # Adam or AdamW\n  optimizer:\n    lr: 0.0001\n    amsgrad: false\n</code></pre>"},{"location":"configuration/trainer/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Choose one scheduler:</p> Reduce on PlateauStep LRCosine Annealing + WarmupLinear Warmup + Decay <pre><code>lr_scheduler:\n  reduce_lr_on_plateau:\n    patience: 5\n    factor: 0.5\n    min_lr: 1e-8\n</code></pre> <pre><code>lr_scheduler:\n  step_lr:\n    step_size: 20    # Every N epochs\n    gamma: 0.5       # Multiply by this\n</code></pre> <pre><code>lr_scheduler:\n  cosine_annealing_warmup:\n    warmup_epochs: 5\n    warmup_start_lr: 0.0\n    eta_min: 1e-6\n</code></pre> <pre><code>lr_scheduler:\n  linear_warmup_linear_decay:\n    warmup_epochs: 5\n    warmup_start_lr: 0.0\n    end_lr: 1e-6\n</code></pre>"},{"location":"configuration/trainer/#early-stopping","title":"Early Stopping","text":"<pre><code>trainer_config:\n  early_stopping:\n    stop_training_on_plateau: true\n    patience: 10        # Epochs without improvement\n    min_delta: 1e-8     # Minimum improvement\n</code></pre>"},{"location":"configuration/trainer/#hardware","title":"Hardware","text":"<pre><code>trainer_config:\n  trainer_accelerator: auto     # auto, gpu, cpu, mps\n  trainer_devices: auto         # Number of devices\n  trainer_device_indices: null  # Specific GPUs [0, 2]\n  trainer_strategy: auto        # auto, ddp, fsdp\n</code></pre>"},{"location":"configuration/trainer/#visualization","title":"Visualization","text":"<pre><code>trainer_config:\n  visualize_preds_during_training: true\n  keep_viz: false    # Keep viz folder after training\n</code></pre>"},{"location":"configuration/trainer/#wandb-logging","title":"WandB Logging","text":"<pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: your-username\n    project: your-project\n    name: run-name\n    api_key: null             # Or set WANDB_API_KEY env\n    wandb_mode: online        # online, offline\n    save_viz_imgs_wandb: true\n    delete_local_logs: null   # Auto-delete online logs\n</code></pre>"},{"location":"configuration/trainer/#checkpointing","title":"Checkpointing","text":"<pre><code>trainer_config:\n  model_ckpt:\n    save_top_k: 1     # Keep N best models\n    save_last: false  # Also save last.ckpt\n  resume_ckpt_path: null  # Resume from this path\n</code></pre>"},{"location":"configuration/trainer/#training-control","title":"Training Control","text":"<pre><code>trainer_config:\n  min_train_steps_per_epoch: 200  # Minimum steps\n  train_steps_per_epoch: null     # Exact steps (null=auto)\n  enable_progress_bar: true\n  seed: null                      # Random seed\n</code></pre>"},{"location":"configuration/trainer/#online-hard-keypoint-mining","title":"Online Hard Keypoint Mining","text":"<p>Focus on difficult keypoints:</p> <pre><code>trainer_config:\n  online_hard_keypoint_mining:\n    online_mining: false\n    hard_to_easy_ratio: 2.0\n    min_hard_keypoints: 2\n    max_hard_keypoints: null\n    loss_scale: 5.0\n</code></pre>"},{"location":"configuration/trainer/#zmq-gui-integration","title":"ZMQ (GUI Integration)","text":"<p>For SLEAP GUI communication:</p> <pre><code>trainer_config:\n  zmq:\n    publish_port: 9001\n    controller_port: 9000\n    controller_polling_timeout: 10\n</code></pre>"},{"location":"configuration/trainer/#complete-example","title":"Complete Example","text":"<pre><code>trainer_config:\n  # Training\n  max_epochs: 200\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: fly_bottomup_v1\n\n  # Data loading\n  train_data_loader:\n    batch_size: 4\n    shuffle: true\n    num_workers: 0\n  val_data_loader:\n    batch_size: 4\n    shuffle: false\n    num_workers: 0\n\n  # Optimization\n  optimizer_name: Adam\n  optimizer:\n    lr: 0.0001\n    amsgrad: false\n\n  lr_scheduler:\n    reduce_lr_on_plateau:\n      patience: 5\n      factor: 0.5\n      min_lr: 1e-8\n\n  early_stopping:\n    stop_training_on_plateau: true\n    patience: 10\n\n  # Hardware\n  trainer_accelerator: auto\n  trainer_devices: 1\n\n  # Logging\n  use_wandb: true\n  wandb:\n    project: sleap-experiments\n    save_viz_imgs_wandb: true\n\n  # Visualization\n  visualize_preds_during_training: true\n  keep_viz: false\n</code></pre>"},{"location":"configuration/trainer/#full-reference","title":"Full Reference","text":""},{"location":"configuration/trainer/#trainerconfig","title":"TrainerConfig","text":"Option Type Default Description <code>max_epochs</code> int <code>100</code> Maximum training epochs <code>save_ckpt</code> bool <code>false</code> Save model checkpoints <code>ckpt_dir</code> str <code>.</code> Directory for checkpoints <code>run_name</code> str <code>null</code> Run folder name (auto-generated if null) <code>seed</code> int <code>null</code> Random seed for reproducibility <code>trainer_accelerator</code> str <code>auto</code> Hardware: <code>auto</code>, <code>gpu</code>, <code>cpu</code>, <code>mps</code> <code>trainer_devices</code> int/str <code>null</code> Number of devices or <code>auto</code> <code>trainer_device_indices</code> list <code>null</code> Specific device indices (e.g., <code>[0, 2]</code>) <code>trainer_strategy</code> str <code>auto</code> Strategy: <code>auto</code>, <code>ddp</code>, <code>fsdp</code> <code>profiler</code> str <code>null</code> PyTorch profiler: <code>simple</code>, <code>advanced</code>, <code>pytorch</code> <code>enable_progress_bar</code> bool <code>true</code> Show training progress <code>min_train_steps_per_epoch</code> int <code>200</code> Minimum batches per epoch <code>train_steps_per_epoch</code> int <code>null</code> Exact steps per epoch (null = auto) <code>visualize_preds_during_training</code> bool <code>false</code> Save prediction visualizations <code>keep_viz</code> bool <code>false</code> Keep viz folder after training <code>use_wandb</code> bool <code>false</code> Enable WandB logging <code>resume_ckpt_path</code> str <code>null</code> Path to checkpoint to resume from <code>optimizer_name</code> str <code>Adam</code> Optimizer: <code>Adam</code> or <code>AdamW</code>"},{"location":"configuration/trainer/#dataloaderconfig","title":"DataLoaderConfig","text":"Option Type Default Description <code>batch_size</code> int <code>4</code> Samples per batch <code>shuffle</code> bool <code>true</code> (train) / <code>false</code> (val) Shuffle data each epoch <code>num_workers</code> int <code>0</code> Parallel data loading workers (use with caching only)"},{"location":"configuration/trainer/#optimizerconfig","title":"OptimizerConfig","text":"Option Type Default Description <code>lr</code> float <code>1e-4</code> Learning rate <code>amsgrad</code> bool <code>false</code> Enable AMSGrad variant"},{"location":"configuration/trainer/#lrschedulerconfig","title":"LRSchedulerConfig","text":"<p>Only one scheduler should be set at a time.</p>"},{"location":"configuration/trainer/#reducelronplateauconfig","title":"ReduceLROnPlateauConfig","text":"Option Type Default Description <code>threshold</code> float <code>1e-6</code> Minimum improvement threshold <code>threshold_mode</code> str <code>abs</code> Mode: <code>rel</code> or <code>abs</code> <code>cooldown</code> int <code>3</code> Epochs to wait after reduction <code>patience</code> int <code>5</code> Epochs without improvement before reducing <code>factor</code> float <code>0.5</code> LR multiplication factor <code>min_lr</code> float <code>1e-8</code> Minimum learning rate"},{"location":"configuration/trainer/#steplrconfig","title":"StepLRConfig","text":"Option Type Default Description <code>step_size</code> int <code>10</code> Epochs between LR reductions <code>gamma</code> float <code>0.1</code> LR multiplication factor"},{"location":"configuration/trainer/#cosineannealingwarmupconfig","title":"CosineAnnealingWarmupConfig","text":"Option Type Default Description <code>warmup_epochs</code> int <code>5</code> Linear warmup epochs <code>warmup_start_lr</code> float <code>0.0</code> Starting LR for warmup <code>eta_min</code> float <code>0.0</code> Minimum LR at end of cosine decay <code>max_epochs</code> int <code>null</code> Total epochs (auto from trainer)"},{"location":"configuration/trainer/#linearwarmuplineardecayconfig","title":"LinearWarmupLinearDecayConfig","text":"Option Type Default Description <code>warmup_epochs</code> int <code>5</code> Linear warmup epochs <code>warmup_start_lr</code> float <code>0.0</code> Starting LR for warmup <code>end_lr</code> float <code>0.0</code> Final LR at end of training <code>max_epochs</code> int <code>null</code> Total epochs (auto from trainer)"},{"location":"configuration/trainer/#earlystoppingconfig","title":"EarlyStoppingConfig","text":"Option Type Default Description <code>stop_training_on_plateau</code> bool <code>true</code> Enable early stopping <code>patience</code> int <code>10</code> Epochs without improvement <code>min_delta</code> float <code>1e-8</code> Minimum improvement"},{"location":"configuration/trainer/#modelckptconfig","title":"ModelCkptConfig","text":"Option Type Default Description <code>save_top_k</code> int <code>1</code> Keep N best models <code>save_last</code> bool <code>null</code> Also save last.ckpt"},{"location":"configuration/trainer/#wandbconfig","title":"WandBConfig","text":"Option Type Default Description <code>entity</code> str <code>null</code> WandB entity/username <code>project</code> str <code>null</code> WandB project name <code>name</code> str <code>null</code> Run name <code>api_key</code> str <code>null</code> API key (or use WANDB_API_KEY env) <code>wandb_mode</code> str <code>null</code> Mode: <code>online</code> or <code>offline</code> <code>prv_runid</code> str <code>null</code> Previous run ID (for resuming) <code>group</code> str <code>null</code> Run group <code>save_viz_imgs_wandb</code> bool <code>false</code> Upload viz images to WandB <code>viz_enabled</code> bool <code>true</code> Log pre-rendered matplotlib images <code>viz_boxes</code> bool <code>false</code> Log interactive keypoint boxes <code>viz_masks</code> bool <code>false</code> Log confidence map overlay masks <code>viz_box_size</code> float <code>5.0</code> Keypoint box size in pixels <code>viz_confmap_threshold</code> float <code>0.1</code> Confidence map mask threshold <code>log_viz_table</code> bool <code>false</code> Log images to wandb.Table <code>delete_local_logs</code> bool <code>null</code> Delete local logs (auto if online)"},{"location":"configuration/trainer/#hardkeypointminingconfig","title":"HardKeypointMiningConfig","text":"Option Type Default Description <code>online_mining</code> bool <code>false</code> Enable online hard keypoint mining <code>hard_to_easy_ratio</code> float <code>2.0</code> Ratio threshold for \"hard\" keypoints <code>min_hard_keypoints</code> int <code>2</code> Minimum hard keypoints <code>max_hard_keypoints</code> int <code>null</code> Maximum hard keypoints <code>loss_scale</code> float <code>5.0</code> Scale factor for hard keypoint losses"},{"location":"configuration/trainer/#evalconfig","title":"EvalConfig","text":"Option Type Default Description <code>enabled</code> bool <code>false</code> Enable epoch-end evaluation metrics <code>frequency</code> int <code>1</code> Evaluate every N epochs <code>oks_stddev</code> float <code>0.025</code> OKS standard deviation (pose models only) <code>oks_scale</code> float <code>null</code> OKS scale override (pose models only) <code>match_threshold</code> float <code>50.0</code> Max distance (px) for centroid matching (centroid models only) <p>Model-Type-Dependent Evaluation</p> <p>SLEAP-NN automatically selects the appropriate evaluation callback based on model type:</p> <ul> <li>Pose models (single instance, bottom-up, centered instance): Uses OKS/PCK metrics with <code>oks_stddev</code> and <code>oks_scale</code></li> <li>Centroid models: Uses distance-based metrics with <code>match_threshold</code> for prediction-to-GT matching</li> </ul> <p>See the Monitoring Guide for details.</p>"},{"location":"configuration/trainer/#zmqconfig","title":"ZMQConfig","text":"Option Type Default Description <code>publish_port</code> int <code>null</code> Port for publishing updates <code>controller_port</code> int <code>null</code> Port for receiving commands <code>controller_polling_timeout</code> int <code>10</code> Polling timeout in microseconds"},{"location":"getting-started/","title":"Getting Started","text":"<p>New to SLEAP-NN? Start here.</p> <ul> <li> <p> Installation</p> <p>Set up SLEAP-NN on your system with GPU or CPU support.</p> <p> Install now</p> </li> <li> <p> Quick Start</p> <p>Train and run inference in under 5 minutes.</p> <p> Let's go</p> </li> <li> <p> Your First Model</p> <p>A complete walkthrough from data to predictions.</p> <p> Tutorial</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11, 3.12, or 3.13 (3.14 not yet supported)</li> <li>SLEAP labels file (<code>.slp</code> or <code>.pkg.slp</code>) with annotated data</li> <li>GPU recommended for training (NVIDIA with CUDA, or Apple Silicon with MPS)</li> </ul>"},{"location":"getting-started/#choose-your-path","title":"Choose Your Path","text":""},{"location":"getting-started/#i-want-to-try-it-quickly","title":"I want to try it quickly","text":"<p> Go to Quick Start</p>"},{"location":"getting-started/#im-setting-up-for-a-project","title":"I'm setting up for a project","text":"<p> Go to Installation for full setup options</p>"},{"location":"getting-started/#i-have-data-and-want-to-train","title":"I have data and want to train","text":"<p> Go to Your First Model for a complete tutorial</p>"},{"location":"getting-started/first-model/","title":"Your First Model","text":"<p>A complete walkthrough from labeled data to predictions.</p>"},{"location":"getting-started/first-model/#overview","title":"Overview","text":"<p>In this tutorial, you'll:</p> <ol> <li>Choose the right model type for your data</li> <li>Create a configuration file</li> <li>Train the model</li> <li>Run inference on new videos</li> <li>Evaluate your results</li> </ol> <p>Time: ~20 minutes (plus training time)</p>"},{"location":"getting-started/first-model/#step-1-choose-your-model-type","title":"Step 1: Choose Your Model Type","text":"<p>The right model depends on your data:</p> Scenario Model Type Config One animal per frame Single Instance <code>single_instance</code> Multiple animals, not touching Top-Down <code>centroid</code> + <code>centered_instance</code> Multiple animals, overlapping Bottom-Up <code>bottomup</code> Known identities (tracking by ID) Multi-Class <code>multi_class_bottomup</code> or <code>multi_class_topdown</code> <p>For this tutorial, we'll use Single Instance (simplest case).</p> <p> Learn more about model types</p>"},{"location":"getting-started/first-model/#step-2-prepare-your-data","title":"Step 2: Prepare Your Data","text":"<p>You need a SLEAP labels file (<code>.slp</code> or <code>.pkg.slp</code>) with:</p> <ul> <li>Labeled frames showing your animal's pose</li> <li>At least 50-100 labeled frames (more is better)</li> </ul> <p>Don't have data?</p> <p>Use our sample dataset: <pre><code># Download sample fly data\ncurl -O https://storage.googleapis.com/sleap-data/datasets/BermanFlies/random_split1/train.pkg.slp\ncurl -O https://storage.googleapis.com/sleap-data/datasets/BermanFlies/random_split1/val.pkg.slp\n</code></pre></p>"},{"location":"getting-started/first-model/#step-3-create-your-config","title":"Step 3: Create Your Config","text":"<p>Create a file called <code>config.yaml</code>:</p> config.yaml<pre><code># === DATA ===\ndata_config:\n  train_labels_path:\n    - train.pkg.slp       # Your training data\n  val_labels_path:\n    - val.pkg.slp         # Your validation data (optional)\n  validation_fraction: 0.1  # Use 10% for validation if no val file\n\n  # Preprocessing\n  preprocessing:\n    scale: 1.0            # Image scale (0.5 = half size, faster)\n\n  # Augmentation (helps generalization)\n  use_augmentations_train: true\n  augmentation_config:\n    geometric:\n      rotation_min: -15.0\n      rotation_max: 15.0\n      scale_min: 0.9\n      scale_max: 1.1\n      affine_p: 0.5       # 50% chance of augmentation\n\n# === MODEL ===\nmodel_config:\n  backbone_config:\n    unet:\n      filters: 32         # Model size (16=small, 32=medium, 64=large)\n      max_stride: 16      # Receptive field size\n      output_stride: 2    # Output resolution (lower = more precise)\n\n  head_configs:\n    single_instance:      # Change this for different model types\n      confmaps:\n        sigma: 5.0        # Gaussian spread for keypoints\n\n# === TRAINING ===\ntrainer_config:\n  max_epochs: 100         # Training epochs\n\n  # Saving\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: fly_single_instance\n\n  # Data loading\n  train_data_loader:\n    batch_size: 4         # Increase if you have more GPU memory\n\n  # Optimization\n  optimizer:\n    lr: 0.0001            # Learning rate\n\n  # Early stopping\n  early_stopping:\n    stop_training_on_plateau: true\n    patience: 10          # Stop if no improvement for 10 epochs\n</code></pre>"},{"location":"getting-started/first-model/#step-4-train","title":"Step 4: Train","text":"<p>Run training:</p> <pre><code>sleap-nn train --config config.yaml\n</code></pre>"},{"location":"getting-started/first-model/#monitor-training","title":"Monitor Training","text":"<p>Watch the terminal output for:</p> <ul> <li>Loss decreasing over epochs (good!)</li> <li>Validation loss not increasing (no overfitting)</li> </ul>"},{"location":"getting-started/first-model/#optional-enable-wandb","title":"Optional: Enable WandB","text":"<p>Add to your config for beautiful training dashboards:</p> <pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    project: sleap-nn-tutorial\n    entity: your-username  # Your WandB username\n</code></pre>"},{"location":"getting-started/first-model/#training-output","title":"Training Output","text":"<p>After training, you'll find in <code>models/fly_single_instance/</code>:</p> <pre><code>models/fly_single_instance/\n\u251c\u2500\u2500 best.ckpt                  # Best model weights\n\u251c\u2500\u2500 initial_config.yaml        # Your original config\n\u251c\u2500\u2500 training_config.yaml       # Full config with auto-computed values\n\u251c\u2500\u2500 labels_gt.train.0.slp      # Training data split (ground truth)\n\u251c\u2500\u2500 labels_gt.val.0.slp        # Validation data split (ground truth)\n\u251c\u2500\u2500 labels_pr.train.slp        # Predictions on training data after training\n\u251c\u2500\u2500 labels_pr.val.slp          # Predictions on validation data after training\n\u251c\u2500\u2500 metrics.train.0.npz        # Training metrics\n\u251c\u2500\u2500 metrics.val.0.npz          # Validation metrics\n\u2514\u2500\u2500 training_log.csv           # Loss per epoch\n</code></pre>"},{"location":"getting-started/first-model/#step-5-run-inference","title":"Step 5: Run Inference","text":"<p>Once training completes, run on <code>val.pkg.slp</code> (or a video):</p> <pre><code>sleap-nn track \\\n    --data_path val.pkg.slp \\\n    --model_paths models/fly_single_instance/ \\\n    -o predictions.slp\n</code></pre>"},{"location":"getting-started/first-model/#useful-options","title":"Useful Options","text":"<pre><code># Faster inference with larger batches\nsleap-nn track -i val.pkg.slp -m models/fly_single_instance/ --batch_size 8\n\n# Process specific frames\nsleap-nn track -i val.pkg.slp -m models/fly_single_instance/ --frames 0-1000\n\n# Save to custom path\nsleap-nn track -i val.pkg.slp -m models/fly_single_instance/ -o predictions.slp\n</code></pre>"},{"location":"getting-started/first-model/#step-6-evaluate","title":"Step 6: Evaluate","text":"<p>Compare predictions to ground truth:</p> <pre><code>sleap-nn eval \\\n    --ground_truth_path val.pkg.slp \\\n    --predicted_path predictions.slp \\\n    --save_metrics metrics.npz\n</code></pre> <p>Or in Python:</p> <pre><code>import sleap_io as sio\nfrom sleap_nn.evaluation import Evaluator\n\ngt = sio.load_slp(\"val.pkg.slp\")\npred = sio.load_slp(\"predictions.slp\")\n\nevaluator = Evaluator(gt, pred)\nmetrics = evaluator.evaluate()\n\nprint(f\"OKS mAP: {metrics['voc_metrics']['oks_voc.mAP']:.3f}\")\nprint(f\"Distance error (90th %ile): {metrics['distance_metrics']['p90']:.2f} px\")\n</code></pre>"},{"location":"getting-started/first-model/#understanding-metrics","title":"Understanding Metrics","text":"Metric What it measures Good values OKS mAP Overall keypoint accuracy (0-1) &gt; 0.7 PCK % of keypoints within threshold &gt; 90% Distance p50 Median error in pixels &lt; 5 px Distance p90 90<sup>th</sup> percentile error &lt; 10 px"},{"location":"getting-started/first-model/#visualize-predictions","title":"Visualize Predictions","text":"<pre><code>import sleap_io as sio\nimport matplotlib.pyplot as plt\n\ngt = sio.load_slp(\"val.pkg.slp\")\npred = sio.load_slp(\"predictions.slp\")\n\n# Plot a frame\nframe_idx = 0\nfig, ax = plt.subplots()\nax.imshow(gt[frame_idx].image, cmap=\"gray\")\n\n# Ground truth (green circles)\nfor inst in gt[frame_idx].instances:\n    pts = inst.numpy()\n    ax.plot(pts[:, 0], pts[:, 1], \"go\", markersize=6, label=\"GT\")\n\n# Predictions (red crosses)\nfor inst in pred[frame_idx].instances:\n    pts = inst.numpy()\n    ax.plot(pts[:, 0], pts[:, 1], \"rx\", markersize=6, label=\"Pred\")\n\nax.legend()\nplt.show()\n</code></pre>"},{"location":"getting-started/first-model/#whats-next","title":"What's Next?","text":""},{"location":"getting-started/first-model/#improve-your-model","title":"Improve Your Model","text":"<ul> <li>More data: Label more frames, especially failure cases</li> <li>Augmentation: Enable more augmentations for robustness</li> <li>Fine-tuning: Start from pre-trained weights</li> </ul>"},{"location":"getting-started/first-model/#multi-animal-models","title":"Multi-Animal Models","text":"<p>Ready for multiple animals?</p> <ul> <li> Top-Down models</li> <li> Bottom-Up models</li> </ul>"},{"location":"getting-started/first-model/#production-deployment","title":"Production Deployment","text":"<p>Need faster inference?</p> <ul> <li> Export to ONNX/TensorRT</li> </ul>"},{"location":"getting-started/first-model/#troubleshooting","title":"Troubleshooting","text":"Training is slow <ul> <li>Enable caching: <code>data_config.data_pipeline_fw: torch_dataset_cache_img_memory</code></li> <li>Reduce image size: <code>data_config.preprocessing.scale: 0.5</code></li> <li>Use a smaller model: <code>model_config.backbone_config.unet.filters: 16</code></li> </ul> Poor predictions <ul> <li>Train longer: increase <code>max_epochs</code></li> <li>Check your labels for errors</li> <li>Add more training data</li> </ul> Out of memory <ul> <li>Reduce batch size: <code>train_data_loader.batch_size: 2</code></li> <li>Reduce image size: <code>data_config.preprocessing.scale: 0.5</code></li> <li>Use a smaller model</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Train a model and run inference in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>uv tool install sleap-nn[torch] --torch-backend auto\n</code></pre> <p>See full installation guide for other methods and troubleshooting.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>A training dataset (<code>.slp</code> or <code>.pkg.slp</code> file)</li> </ul> <p>Sample Data</p> <p>Download sample data to try it out:</p> <ul> <li>Train data</li> <li>Validation data</li> </ul>"},{"location":"getting-started/quickstart/#step-1-create-a-config-file","title":"Step 1: Create a Config File","text":"<p>Create <code>config.yaml</code>:</p> config.yaml<pre><code>data_config:\n  train_labels_path:\n    - train.pkg.slp\n  val_labels_path:\n    - val.pkg.slp\n\nmodel_config:\n  backbone_config:\n    unet:\n      filters: 32\n      max_stride: 16\n  head_configs:\n    single_instance:\n      confmaps:\n        sigma: 5.0\n\ntrainer_config:\n  max_epochs: 50\n  save_ckpt: true\n  ckpt_dir: models\n  run_name: my_first_model\n</code></pre> <p>Or grab a sample config for your model type.</p>"},{"location":"getting-started/quickstart/#step-2-train","title":"Step 2: Train","text":"<pre><code>sleap-nn train --config config.yaml\n</code></pre> <p>That's it! Training will:</p> <ol> <li>Load your data</li> <li>Build the model</li> <li>Train for 50 epochs</li> <li>Save the best checkpoint to <code>models/my_first_model/</code></li> </ol>"},{"location":"getting-started/quickstart/#step-3-run-inference","title":"Step 3: Run Inference","text":"<pre><code>sleap-nn track --data_path val.pkg.slp --model_paths models/my_first_model/ -o val.predictions.slp\n</code></pre> <p>This creates <code>val.predictions.slp</code> with your predictions.</p>"},{"location":"getting-started/quickstart/#step-4-view-results","title":"Step 4: View Results","text":"<p>Open the predictions in the SLEAP GUI:</p> <pre><code>sleap-label val.predictions.slp\n</code></pre> <p>Or load in Python:</p> <pre><code>import sleap_io as sio\n\nlabels = sio.load_slp(\"val.predictions.slp\")\nprint(f\"Found {len(labels)} frames with predictions\")\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li> <p>Train a multi-animal model</p> <p> Model types</p> </li> <li> <p>Customize your config</p> <p> Configuration guide</p> </li> <li> <p>Enable tracking</p> <p> Tracking guide</p> </li> <li> <p>Export for production</p> <p> ONNX/TensorRT export</p> </li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>Task-oriented guides for common workflows.</p> Guide Description Training Configure training, fine-tune models Monitoring WandB, visualizations, epoch-end evaluation Multi-GPU Scale training across multiple GPUs Inference Run predictions on videos and label files Evaluation Assess model performance with metrics Tracking Assign consistent IDs across frames Export ONNX/TensorRT for production inference <p>Looking for step-by-step learning? Check out the Tutorials.</p>"},{"location":"guides/evaluation/","title":"Evaluation","text":"<p>Compare predictions to ground truth and assess model performance.</p>"},{"location":"guides/evaluation/#cli-usage","title":"CLI Usage","text":"<pre><code>sleap-nn eval \\\n    -g ground_truth.slp \\\n    -p predictions.slp \\\n    -s metrics.npz\n</code></pre>"},{"location":"guides/evaluation/#parameters","title":"Parameters","text":"Parameter Description Default <code>-g</code> / <code>--ground_truth_path</code> Ground truth labels file Required <code>-p</code> / <code>--predicted_path</code> Predicted labels file Required <code>-s</code> / <code>--save_metrics</code> Save metrics to .npz file None <code>--oks_stddev</code> OKS standard deviation <code>0.05</code> <code>--user_labels_only</code> Only evaluate user-labeled frames <code>False</code>"},{"location":"guides/evaluation/#python-api","title":"Python API","text":""},{"location":"guides/evaluation/#basic-usage","title":"Basic Usage","text":"<pre><code>import sleap_io as sio\nfrom sleap_nn.evaluation import Evaluator\n\ngt = sio.load_slp(\"ground_truth.slp\")\npred = sio.load_slp(\"predictions.slp\")\n\nevaluator = Evaluator(gt, pred)\nmetrics = evaluator.evaluate()\n</code></pre>"},{"location":"guides/evaluation/#accessing-metrics","title":"Accessing Metrics","text":"<pre><code># Overall metrics\nprint(f\"OKS mAP: {metrics['voc_metrics']['oks_voc.mAP']:.3f}\")\nprint(f\"mOKS: {metrics['mOKS']:.3f}\")\n\n# Distance errors\nprint(f\"Mean error: {metrics['distance_metrics']['mean']:.2f} px\")\nprint(f\"Median error: {metrics['distance_metrics']['p50']:.2f} px\")\nprint(f\"90th %ile error: {metrics['distance_metrics']['p90']:.2f} px\")\n\n# Per-node metrics\nfor node, oks in metrics['per_node_oks'].items():\n    print(f\"  {node}: {oks:.3f}\")\n</code></pre>"},{"location":"guides/evaluation/#metrics-reference","title":"Metrics Reference","text":"<p>For a detailed explanation of all evaluation metrics, see the Evaluation Metrics Reference.</p>"},{"location":"guides/evaluation/#oks-object-keypoint-similarity","title":"OKS (Object Keypoint Similarity)","text":"<p>Measures pose similarity accounting for keypoint visibility and scale:</p> Metric Description Range <code>mOKS</code> Mean OKS across all instances 0-1 <code>oks_voc.mAP</code> COCO-style mean Average Precision 0-1 <code>oks_voc.AP@0.5</code> AP at OKS threshold 0.5 0-1 <code>oks_voc.AP@0.75</code> AP at OKS threshold 0.75 0-1 <p>Higher is better. mAP &gt; 0.7 is generally good.</p>"},{"location":"guides/evaluation/#distance-metrics","title":"Distance Metrics","text":"<p>Euclidean distance between predicted and ground truth keypoints (in pixels):</p> Metric Description <code>mean</code> Mean error <code>std</code> Standard deviation <code>p50</code> Median (50<sup>th</sup> percentile) <code>p90</code> 90<sup>th</sup> percentile <code>p95</code> 95<sup>th</sup> percentile <code>p99</code> 99<sup>th</sup> percentile <p>Lower is better. Values depend on image resolution and animal size.</p>"},{"location":"guides/evaluation/#per-node-metrics","title":"Per-Node Metrics","text":"<p>OKS computed separately for each body part:</p> <pre><code>for node, oks in metrics['per_node_oks'].items():\n    print(f\"{node}: {oks:.3f}\")\n</code></pre> <p>Useful for identifying which keypoints are harder to predict.</p>"},{"location":"guides/evaluation/#loading-saved-metrics","title":"Loading Saved Metrics","text":"<pre><code>import numpy as np\n\ndata = np.load(\"metrics.npz\", allow_pickle=True)\nmetrics = data['metrics'].item()\n\nprint(metrics.keys())\n</code></pre>"},{"location":"guides/evaluation/#next-steps","title":"Next Steps","text":"<ul> <li> Evaluation Metrics Reference - Deep dive into OKS, PCK, and other metrics</li> <li> Tracking - Assign IDs across frames</li> <li> Export - Deploy models</li> </ul>"},{"location":"guides/export/","title":"ONNX &amp; TensorRT Export","text":"<p>Export models for high-performance production inference.</p> <p>Experimental</p> <p>Export functionality is experimental. Report issues at github.com/talmolab/sleap-nn/issues.</p>"},{"location":"guides/export/#why-export","title":"Why Export?","text":"Format Use Case Speedup ONNX Cross-platform, CPU inference ~1x TensorRT FP16 Maximum GPU throughput 3-6x"},{"location":"guides/export/#installation","title":"Installation","text":"<p>Export requires additional dependencies. Install them with your preferred method:</p> uv (recommended)pip <pre><code># ONNX export (CPU)\nuv tool install \"sleap-nn[torch,export]\" --torch-backend auto\n\n# ONNX export (GPU runtime)\nuv tool install \"sleap-nn[torch,export-gpu]\" --torch-backend auto\n\n# TensorRT export (Linux/Windows with NVIDIA GPU)\nuv tool install \"sleap-nn[torch,export-gpu,tensorrt]\" --torch-backend auto\n</code></pre> <pre><code># ONNX export (CPU)\npip install sleap-nn[torch,export]\n\n# TensorRT export (NVIDIA GPUs)\npip install sleap-nn[torch,export-gpu,tensorrt]\n</code></pre> <p>TensorRT availability</p> <p>TensorRT is only available on Linux and Windows with NVIDIA GPUs.</p>"},{"location":"guides/export/#quick-start","title":"Quick Start","text":""},{"location":"guides/export/#export","title":"Export","text":"<pre><code># ONNX only\nsleap-nn export models/my_model -o exports/ --format onnx\n\n# Both ONNX and TensorRT\nsleap-nn export models/my_model -o exports/ --format both\n</code></pre>"},{"location":"guides/export/#run-inference","title":"Run Inference","text":"<pre><code>sleap-nn predict exports/my_model video.mp4 -o predictions.slp\n</code></pre>"},{"location":"guides/export/#export-options","title":"Export Options","text":"<pre><code>sleap-nn export MODEL_PATH [options]\n</code></pre> Option Description Default <code>-o</code>, <code>--output-dir</code> Output directory Required <code>-f</code>, <code>--format</code> <code>onnx</code>, <code>tensorrt</code>, <code>both</code> <code>onnx</code> <code>--precision</code> TensorRT: <code>fp32</code>, <code>fp16</code> <code>fp16</code> <code>-n</code>, <code>--max-instances</code> Max instances per frame <code>20</code> <code>-b</code>, <code>--max-batch-size</code> Max batch size <code>8</code>"},{"location":"guides/export/#model-types","title":"Model Types","text":""},{"location":"guides/export/#single-instance-bottom-up","title":"Single Instance / Bottom-Up","text":"<pre><code>sleap-nn export models/bottomup -o exports/bottomup --format both\n</code></pre>"},{"location":"guides/export/#top-down","title":"Top-Down","text":"<p>Export both models together:</p> <pre><code>sleap-nn export models/centroid models/centered_instance \\\n    -o exports/topdown --format both\n</code></pre>"},{"location":"guides/export/#multi-class","title":"Multi-Class","text":"<pre><code>sleap-nn export models/multi_class_bottomup \\\n    -o exports/multiclass --format onnx\n</code></pre>"},{"location":"guides/export/#inference-options","title":"Inference Options","text":"<pre><code>sleap-nn predict EXPORT_DIR VIDEO [options]\n</code></pre> Option Description Default <code>-o</code>, <code>--output</code> Output path <code>&lt;video&gt;.predictions.slp</code> <code>-r</code>, <code>--runtime</code> <code>auto</code>, <code>onnx</code>, <code>tensorrt</code> <code>auto</code> <code>-b</code>, <code>--batch-size</code> Batch size <code>4</code> <code>-n</code>, <code>--n-frames</code> Frames to process (0=all) <code>0</code>"},{"location":"guides/export/#examples","title":"Examples","text":"<pre><code># Maximum speed with TensorRT\nsleap-nn predict exports/model video.mp4 --runtime tensorrt --batch-size 8\n\n# CPU inference\nsleap-nn predict exports/model video.mp4 --runtime onnx --device cpu\n\n# First 1000 frames\nsleap-nn predict exports/model video.mp4 --n-frames 1000\n</code></pre>"},{"location":"guides/export/#performance","title":"Performance","text":"<p>Benchmarks on NVIDIA RTX A6000:</p> Model Resolution PyTorch TensorRT FP16 Speedup Single Instance 192x192 1.8 ms 0.31 ms 5.9x Top-Down 1024x1024 11.4 ms 2.31 ms 4.9x Bottom-Up 1024x1280 12.3 ms 2.52 ms 4.9x"},{"location":"guides/export/#python-api","title":"Python API","text":""},{"location":"guides/export/#export_1","title":"Export","text":"<pre><code>from sleap_nn.export import export_to_onnx, export_to_tensorrt\n\nexport_to_onnx(model, \"model.onnx\", input_shape=(1, 1, 192, 192))\nexport_to_tensorrt(model, \"model.trt\", input_shape=(1, 1, 192, 192))\n</code></pre>"},{"location":"guides/export/#inference","title":"Inference","text":"<pre><code>from sleap_nn.export.predictors import ONNXPredictor\nimport numpy as np\n\npredictor = ONNXPredictor(\"model.onnx\")\nframes = np.random.randint(0, 256, (4, 1, 192, 192), dtype=np.uint8)\noutputs = predictor.predict(frames)\n</code></pre>"},{"location":"guides/export/#output-files","title":"Output Files","text":"<pre><code>exports/my_model/\n\u251c\u2500\u2500 model.onnx\n\u251c\u2500\u2500 model.onnx.metadata.json\n\u251c\u2500\u2500 model.trt                   # If TensorRT\n\u2514\u2500\u2500 model.trt.metadata.json     # If TensorRT\n</code></pre>"},{"location":"guides/export/#limitations","title":"Limitations","text":"<ul> <li>TensorRT engines are GPU-specific - Regenerate for different hardware</li> <li>Fixed image size - Height/width set at export time</li> <li>Bottom-up grouping on CPU - PAF matching can't be GPU-accelerated</li> <li>No standalone centroid/instance - Use combined top-down or Python API</li> </ul>"},{"location":"guides/export/#troubleshooting","title":"Troubleshooting","text":"ONNX Runtime falls back to CPU <pre><code># Check available providers\npython -c \"import onnxruntime as ort; print(ort.get_available_providers())\"\n\n# Install GPU support\npip install onnxruntime-gpu\n</code></pre> TensorRT export fails <ul> <li>Check TensorRT version: <code>python -c \"import tensorrt; print(tensorrt.__version__)\"</code></li> <li>Ensure CUDA version matches</li> <li>Check GPU memory during export</li> </ul> Bottom-up inference is slow <p>Bottom-up requires CPU-side Hungarian matching. Use larger batch sizes for better throughput.</p>"},{"location":"guides/inference/","title":"Running Inference","text":"<p>Run predictions on videos and label files.</p> <p>Using uv workflow</p> <ul> <li>If using <code>uvx</code>, no installation needed</li> <li>If using <code>uv sync</code>, prefix commands with <code>uv run</code>:   <pre><code>uv run sleap-nn track ...\n</code></pre></li> </ul>"},{"location":"guides/inference/#basic-inference","title":"Basic Inference","text":"<pre><code>sleap-nn track --data_path video.mp4 --model_paths models/my_model/\n</code></pre> <p>Output: <code>video.mp4.predictions.slp</code></p> <p>See the CLI Reference for all available parameters.</p>"},{"location":"guides/inference/#essential-parameters","title":"Essential Parameters","text":"Parameter Description Default <code>--data_path</code> / <code>-i</code> Video or labels file Required <code>--model_paths</code> / <code>-m</code> Model directory (repeat for top-down) Required* <code>--output_path</code> / <code>-o</code> Output file path <code>&lt;input&gt;.predictions.slp</code> <code>--device</code> / <code>-d</code> Device (auto/cuda/cpu/mps) <code>auto</code> <code>--batch_size</code> / <code>-b</code> Frames per batch <code>4</code> <code>--max_instances</code> / <code>-n</code> Max instances per frame <code>None</code> <code>--tracking</code> / <code>-t</code> Enable tracking <code>False</code> <code>--peak_threshold</code> Min confidence for peaks <code>0.2</code> <p>*Not required for track-only mode.</p> <p>For all parameters including image pre-processing and data selection options, see the CLI Reference.</p>"},{"location":"guides/inference/#model-types","title":"Model Types","text":""},{"location":"guides/inference/#single-instance-bottom-up","title":"Single Instance / Bottom-Up","text":"<pre><code>sleap-nn track -i video.mp4 -m models/bottomup/\n</code></pre>"},{"location":"guides/inference/#top-down","title":"Top-Down","text":"<p>Provide both models:</p> <pre><code>sleap-nn track -i video.mp4 \\\n    -m models/centroid/ \\\n    -m models/centered_instance/\n</code></pre>"},{"location":"guides/inference/#filtering-overlapping-instances","title":"Filtering Overlapping Instances","text":"<p>Remove duplicate detections with greedy NMS:</p> <pre><code># Enable with default IOU\nsleap-nn track -i video.mp4 -m models/ --filter_overlapping\n\n# Use OKS with custom threshold\nsleap-nn track -i video.mp4 -m models/ \\\n    --filter_overlapping \\\n    --filter_overlapping_method oks \\\n    --filter_overlapping_threshold 0.5\n</code></pre> Method Description <code>iou</code> Bounding box intersection-over-union <code>oks</code> Object Keypoint Similarity (pose-aware) Threshold Effect 0.3 Aggressive filtering 0.5 Moderate 0.8 Permissive (default)"},{"location":"guides/inference/#python-api","title":"Python API","text":""},{"location":"guides/inference/#basic-usage","title":"Basic Usage","text":"<pre><code>from sleap_nn.predict import run_inference\n\nlabels = run_inference(\n    data_path=\"video.mp4\",\n    model_paths=[\"models/my_model/\"],\n    output_path=\"predictions.slp\",\n    make_labels=True,\n)\n</code></pre>"},{"location":"guides/inference/#get-raw-outputs","title":"Get Raw Outputs","text":"<pre><code>results = run_inference(\n    data_path=\"video.mp4\",\n    model_paths=[\"models/my_model/\"],\n    make_labels=False,\n    return_confmaps=True,\n)\n</code></pre>"},{"location":"guides/inference/#provenance-metadata","title":"Provenance Metadata","text":"<p>Output files include metadata about how predictions were generated:</p> <pre><code>import sleap_io as sio\n\nlabels = sio.load_slp(\"predictions.slp\")\nprovenance = labels.provenance\n\nprint(f\"sleap-nn version: {provenance.get('sleap_nn_version')}\")\nprint(f\"Model type: {provenance.get('model_type')}\")\nprint(f\"Runtime: {provenance.get('runtime_sec')}s\")\n</code></pre>"},{"location":"guides/inference/#recorded-information","title":"Recorded Information","text":"Category Fields Timestamps <code>timestamp_start</code>, <code>timestamp_end</code>, <code>runtime_sec</code> Versions <code>sleap_nn_version</code>, <code>sleap_io_version</code>, <code>torch_version</code> Model <code>model_paths</code>, <code>model_type</code>, <code>head_type</code> Input <code>source_path</code>, <code>source_video_paths</code> Config <code>peak_threshold</code>, <code>batch_size</code>, <code>max_instances</code> System <code>device</code>, <code>python_version</code>, <code>cuda_version</code>, <code>gpu_names</code>"},{"location":"guides/inference/#legacy-sleap-model-support","title":"Legacy SLEAP Model Support","text":"<p>Run inference with SLEAP &lt;=v1.4 models (UNet only):</p> <pre><code>sleap-nn track -i video.mp4 -m /path/to/sleap_model/\n</code></pre> <p>The directory should contain: - <code>best_model.h5</code> - <code>training_config.json</code></p> <p>UNet only</p> <p>Only UNet backbone models from SLEAP \u22641.4 are supported.</p>"},{"location":"guides/inference/#troubleshooting","title":"Troubleshooting","text":"Out of memory <p>Reduce batch size: <code>--batch_size 2</code></p> Slow inference <ul> <li>Increase batch size (if memory allows)</li> <li>Use GPU: <code>--device cuda</code></li> <li>Consider ONNX/TensorRT export</li> </ul> Poor predictions <ul> <li>Lower <code>--peak_threshold</code></li> <li>Verify preprocessing matches training</li> <li>Check model was trained on similar data</li> </ul>"},{"location":"guides/inference/#next-steps","title":"Next Steps","text":"<ul> <li> Evaluation - Assess model performance</li> <li> Tracking - Assign IDs across frames</li> <li> Export - Deploy models</li> </ul>"},{"location":"guides/monitoring/","title":"Monitoring &amp; Visualization","text":"<p>Monitor training progress with logging, visualization, and evaluation callbacks.</p>"},{"location":"guides/monitoring/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<p>Enable WandB for experiment tracking:</p> <pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: your-username\n    project: your-project\n</code></pre>"},{"location":"guides/monitoring/#visualization-options","title":"Visualization Options","text":"<pre><code>trainer_config:\n  wandb:\n    viz_enabled: true           # Pre-rendered matplotlib images\n    viz_boxes: false            # Interactive keypoint boxes with epoch slider\n    viz_masks: false            # Confidence map overlay masks\n    viz_box_size: 5.0           # Size of keypoint boxes in pixels\n    viz_confmap_threshold: 0.1  # Threshold for confmap masks\n</code></pre> Option Description Default <code>viz_enabled</code> Log pre-rendered prediction images <code>true</code> <code>viz_boxes</code> Interactive keypoint boxes (epoch slider) <code>false</code> <code>viz_masks</code> Confidence map overlay masks <code>false</code> <code>viz_box_size</code> Size of keypoint boxes in pixels <code>5.0</code> <code>viz_confmap_threshold</code> Threshold for confmap mask generation <code>0.1</code> <p>Interactive Epoch Slider</p> <p>Enable <code>viz_boxes: true</code> to scrub through epochs and see predictions improve over time.</p>"},{"location":"guides/monitoring/#local-visualizations","title":"Local Visualizations","text":"<p>Save prediction visualizations to disk during training:</p> <pre><code>trainer_config:\n  visualize_preds_during_training: true\n  keep_viz: false  # Set true to keep viz folder after training\n</code></pre> <p>Visualizations are saved to a <code>viz/</code> folder in the checkpoint directory.</p>"},{"location":"guides/monitoring/#per-head-loss-monitoring","title":"Per-Head Loss Monitoring","text":"<p>Multi-head models (e.g., bottom-up) log individual losses:</p> Metric Description <code>train_confmap_loss</code> / <code>val_confmap_loss</code> Confidence map head loss <code>train_paf_loss</code> / <code>val_paf_loss</code> Part affinity field loss (bottom-up only) <p>This helps diagnose when individual heads aren't learning effectively.</p>"},{"location":"guides/monitoring/#epoch-end-evaluation","title":"Epoch-End Evaluation","text":"<p>Compute evaluation metrics during training to track model quality beyond loss values.</p>"},{"location":"guides/monitoring/#enable-evaluation","title":"Enable Evaluation","text":"<pre><code>trainer_config:\n  eval:\n    enabled: true\n    frequency: 1  # Evaluate every N epochs\n</code></pre>"},{"location":"guides/monitoring/#how-it-works","title":"How It Works","text":"<p>SLEAP-NN automatically selects the appropriate evaluation callback based on model type:</p> Model Type Callback Metrics Single Instance <code>EpochEndEvaluationCallback</code> OKS, PCK, distance metrics Bottom-Up <code>EpochEndEvaluationCallback</code> OKS, PCK, distance metrics Top-Down (Centered Instance) <code>EpochEndEvaluationCallback</code> OKS, PCK, distance metrics Centroid <code>CentroidEvaluationCallback</code> Distance, precision/recall"},{"location":"guides/monitoring/#pose-model-evaluation-oks-based","title":"Pose Model Evaluation (OKS-based)","text":"<p>For pose models (single instance, bottom-up, centered instance), evaluation uses Object Keypoint Similarity (OKS) metrics:</p> <pre><code>trainer_config:\n  eval:\n    enabled: true\n    frequency: 1\n    oks_stddev: 0.025   # OKS standard deviation\n    oks_scale: null     # OKS scale override (null = auto)\n</code></pre> <p>Metrics logged to WandB:</p> Metric Description <code>eval/val/oks_voc</code> OKS-based VOC score <code>eval/val/pck</code> Percentage of Correct Keypoints <code>eval/val/dist_p50</code> Median distance (pixels) <code>eval/val/dist_p90</code> 90<sup>th</sup> percentile distance <code>eval/val/dist_p95</code> 95<sup>th</sup> percentile distance"},{"location":"guides/monitoring/#centroid-model-evaluation-distance-based","title":"Centroid Model Evaluation (Distance-based)","text":"<p>Centroid models use distance-based metrics appropriate for point detection:</p> <pre><code>trainer_config:\n  eval:\n    enabled: true\n    frequency: 1\n    match_threshold: 50.0  # Max distance (px) for matching pred to GT\n</code></pre> <p>Hungarian Matching</p> <p>Predictions are matched to ground truth using the Hungarian algorithm for optimal assignment. Only matches within <code>match_threshold</code> pixels are considered true positives.</p> <p>Metrics logged to WandB:</p> Metric Description <code>eval/val/centroid_dist_avg</code> Mean Euclidean distance (pixels) <code>eval/val/centroid_dist_median</code> Median distance <code>eval/val/centroid_dist_p90</code> 90<sup>th</sup> percentile distance <code>eval/val/centroid_dist_p95</code> 95<sup>th</sup> percentile distance <code>eval/val/centroid_dist_max</code> Maximum distance <code>eval/val/centroid_precision</code> TP / (TP + FP) <code>eval/val/centroid_recall</code> TP / (TP + FN) <code>eval/val/centroid_f1</code> F1 score <code>eval/val/centroid_n_tp</code> True positive count <code>eval/val/centroid_n_fp</code> False positive count <code>eval/val/centroid_n_fn</code> False negative count"},{"location":"guides/monitoring/#configuration-reference","title":"Configuration Reference","text":""},{"location":"guides/monitoring/#full-eval-config","title":"Full Eval Config","text":"<pre><code>trainer_config:\n  eval:\n    enabled: bool       # Enable epoch-end evaluation (default: false)\n    frequency: int      # Evaluate every N epochs (default: 1)\n    oks_stddev: float   # OKS standard deviation (default: 0.025)\n    oks_scale: float    # OKS scale override, null for auto (default: null)\n    match_threshold: float  # Centroid matching threshold in pixels (default: 50.0)\n</code></pre> <p>See Trainer Configuration for complete reference.</p>"},{"location":"guides/monitoring/#example-configurations","title":"Example Configurations","text":""},{"location":"guides/monitoring/#bottom-up-with-full-monitoring","title":"Bottom-Up with Full Monitoring","text":"<pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: my-team\n    project: pose-estimation\n    viz_enabled: true\n    viz_boxes: true\n\n  eval:\n    enabled: true\n    frequency: 5  # Evaluate every 5 epochs\n    oks_stddev: 0.025\n\n  visualize_preds_during_training: true\n</code></pre>"},{"location":"guides/monitoring/#centroid-model-with-evaluation","title":"Centroid Model with Evaluation","text":"<pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: my-team\n    project: pose-estimation\n    viz_enabled: true\n\n  eval:\n    enabled: true\n    frequency: 1\n    match_threshold: 30.0  # Stricter matching for small animals\n</code></pre>"},{"location":"guides/monitoring/#next-steps","title":"Next Steps","text":"<ul> <li> Evaluation Guide - Post-training evaluation</li> <li> Evaluation Metrics Reference - Detailed metric explanations</li> <li> Configuration Reference - Full trainer config options</li> </ul>"},{"location":"guides/multi-gpu/","title":"Multi-GPU Training","text":"<p>Scale training across multiple GPUs.</p>"},{"location":"guides/multi-gpu/#auto-detection","title":"Auto-Detection","text":"<p>Let SLEAP-NN detect available GPUs:</p> <pre><code>trainer_config:\n  trainer_accelerator: auto\n  trainer_devices: auto\n  trainer_strategy: auto\n  run_name: multi_gpu_training\n</code></pre>"},{"location":"guides/multi-gpu/#specify-gpu-count","title":"Specify GPU Count","text":"<pre><code>trainer_config:\n  trainer_accelerator: gpu\n  trainer_devices: 4          # Use 4 GPUs\n  trainer_strategy: ddp       # Distributed Data Parallel\n  run_name: multi_gpu_training\n</code></pre>"},{"location":"guides/multi-gpu/#specific-gpus","title":"Specific GPUs","text":"<p>Use specific GPU indices:</p> <pre><code>trainer_config:\n  trainer_accelerator: gpu\n  trainer_devices: 2\n  trainer_device_indices:\n    - 0\n    - 2  # Use first and third GPU\n  trainer_strategy: ddp\n  run_name: multi_gpu_training\n</code></pre>"},{"location":"guides/multi-gpu/#validation-splitting","title":"Validation Splitting","text":"<p>Multi-GPU training seeds validation splits for reproducibility:</p> <pre><code>trainer_config:\n  seed: 42  # Fixed seed for reproducible train/val split\n</code></pre> <p>Without a seed, the split is seeded with 42 by default to ensure all GPU workers get the same split.</p>"},{"location":"guides/multi-gpu/#best-practices","title":"Best Practices","text":""},{"location":"guides/multi-gpu/#batch-size","title":"Batch Size","text":"<p>Scale batch size with GPU count:</p> <pre><code># Single GPU\ntrainer_config:\n  train_data_loader:\n    batch_size: 4\n\n# 4 GPUs - increase batch size\ntrainer_config:\n  train_data_loader:\n    batch_size: 16  # 4 per GPU\n</code></pre>"},{"location":"guides/multi-gpu/#caching","title":"Caching","text":"<p>Use disk caching for multi-GPU (memory caching doesn't share across processes):</p> <pre><code>data_config:\n  data_pipeline_fw: torch_dataset_cache_img_disk\n  cache_img_path: /path/to/shared/cache\n</code></pre>"},{"location":"guides/multi-gpu/#troubleshooting","title":"Troubleshooting","text":"GPUs not detected <pre><code># Check visible GPUs\nsleap-nn system\n</code></pre> Training freezes at epoch start <p>This is usually a DDP synchronization issue. Try:</p> <ul> <li>Use disk caching instead of memory caching</li> <li>Reduce <code>num_workers</code></li> <li>Check that all GPUs are accessible</li> </ul> Out of memory on some GPUs <ul> <li>Reduce batch size</li> <li>Use <code>trainer_device_indices</code> to skip problematic GPUs</li> <li>Check for other processes using GPU memory</li> </ul> Slow multi-GPU training <ul> <li>Ensure NVLink is available for fast GPU communication</li> <li>Use local SSD for disk caching</li> <li>Reduce logging frequency</li> </ul>"},{"location":"guides/multi-gpu/#multi-node-training","title":"Multi-Node Training","text":"<p>Experimental</p> <p>Multi-node training is not fully validated. Use at your own risk.</p> <p>For multi-node, configure your cluster's job scheduler to set the appropriate environment variables (<code>MASTER_ADDR</code>, <code>MASTER_PORT</code>, etc.).</p>"},{"location":"guides/tracking/","title":"Tracking","text":"<p>Assign consistent IDs to instances across frames.</p>"},{"location":"guides/tracking/#enable-tracking","title":"Enable Tracking","text":"<p>Add <code>--tracking</code> to your inference command:</p> <pre><code>sleap-nn track -i video.mp4 -m models/bottomup/ --tracking\n</code></pre>"},{"location":"guides/tracking/#tracking-parameters","title":"Tracking Parameters","text":"Parameter Description Default <code>--tracking</code> / <code>-t</code> Enable tracking <code>False</code> <code>--tracking_window_size</code> Frames to look back <code>5</code> <code>--min_new_track_points</code> Min points for new track <code>0</code> <code>--candidates_method</code> <code>fixed_window</code> or <code>local_queues</code> <code>fixed_window</code> <code>--min_match_points</code> Min non-NaN points for matching <code>0</code> <code>--features</code> <code>keypoints</code>/<code>centroids</code>/<code>bboxes</code>/<code>image</code> <code>keypoints</code> <code>--scoring_method</code> <code>oks</code>/<code>cosine_sim</code>/<code>iou</code>/<code>euclidean_dist</code> <code>oks</code> <code>--scoring_reduction</code> <code>mean</code>/<code>max</code>/<code>robust_quantile</code> <code>mean</code> <code>--track_matching_method</code> <code>hungarian</code> or <code>greedy</code> <code>hungarian</code> <code>--max_tracks</code> Maximum track count <code>None</code> <code>--use_flow</code> Enable optical flow <code>False</code>"},{"location":"guides/tracking/#tracking-methods","title":"Tracking Methods","text":""},{"location":"guides/tracking/#fixed-window-default","title":"Fixed Window (Default)","text":"<p>Uses instances from the last N frames as matching candidates:</p> <pre><code>sleap-nn track -i video.mp4 -m models/ \\\n    -t \\\n    --candidates_method fixed_window \\\n    --tracking_window_size 10\n</code></pre> <p>Best for: Most scenarios, good balance of speed and accuracy.</p>"},{"location":"guides/tracking/#local-queues","title":"Local Queues","text":"<p>Maintains separate history for each track ID:</p> <pre><code>sleap-nn track -i video.mp4 -m models/ \\\n    -t \\\n    --candidates_method local_queues \\\n    --tracking_window_size 5\n</code></pre> <p>Best for: Robust to track breaks, handles occlusions better.</p>"},{"location":"guides/tracking/#optical-flow","title":"Optical Flow","text":"<p>Uses optical flow to predict instance positions:</p> <pre><code>sleap-nn track -i video.mp4 -m models/ \\\n    -t \\\n    --use_flow\n</code></pre> <p>Best for: Fast-moving animals.</p>"},{"location":"guides/tracking/#optical-flow-parameters","title":"Optical Flow Parameters","text":"Parameter Description Default <code>--of_img_scale</code> Image scale (lower = faster) <code>1.0</code> <code>--of_window_size</code> Window size per pyramid level <code>21</code> <code>--of_max_levels</code> Pyramid levels <code>3</code>"},{"location":"guides/tracking/#track-only-mode","title":"Track-Only Mode","text":"<p>Assign tracks to existing predictions (no inference):</p> <pre><code>sleap-nn track -i labels.slp --tracking\n</code></pre> <p>Note: Omit <code>--model_paths</code> for track-only mode.</p> <p>With specific frames:</p> <pre><code>sleap-nn track -i labels.slp -t --frames 0-100 --video_index 0\n</code></pre>"},{"location":"guides/tracking/#limit-instances","title":"Limit Instances","text":"<pre><code># Maximum 5 instances per frame\nsleap-nn track -i video.mp4 -m models/ --max_instances 5\n</code></pre>"},{"location":"guides/tracking/#example-configurations","title":"Example Configurations","text":""},{"location":"guides/tracking/#fast-animals","title":"Fast Animals","text":"<pre><code>sleap-nn track -i video.mp4 -m models/ \\\n    -t \\\n    --use_flow \\\n    --of_img_scale 0.5\n</code></pre>"},{"location":"guides/tracking/#crowded-scenes","title":"Crowded Scenes","text":"<pre><code>sleap-nn track -i video.mp4 -m models/ \\\n    -t \\\n    --candidates_method local_queues \\\n    --tracking_window_size 10 \\\n    --max_tracks 10\n</code></pre>"},{"location":"guides/tracking/#high-accuracy","title":"High Accuracy","text":"<pre><code>sleap-nn track -i video.mp4 -m models/ \\\n    -t \\\n    --scoring_method oks \\\n    --scoring_reduction mean \\\n    --track_matching_method hungarian\n</code></pre>"},{"location":"guides/tracking/#troubleshooting","title":"Troubleshooting","text":"Tracks switch identities <ul> <li>Increase <code>--tracking_window_size</code></li> <li>Try <code>--candidates_method local_queues</code></li> <li>Use <code>--use_flow</code> for fast motion</li> </ul> Too many tracks <ul> <li>Set <code>--max_tracks</code> to limit track count</li> <li>Increase <code>--min_new_track_points</code></li> </ul> Tracking is slow <ul> <li>Reduce <code>--tracking_window_size</code></li> <li>Use <code>--of_img_scale 0.5</code> with optical flow</li> </ul>"},{"location":"guides/training/","title":"Training Models","text":"<p>Train pose estimation models with SLEAP-NN.</p> <p>New to SLEAP-NN?</p> <p>See Model Types to understand the different model architectures (single instance, top-down, bottom-up) and when to use each.</p> <p>Using uv workflow</p> <ul> <li>If using <code>uvx</code>, no installation needed</li> <li>If using <code>uv sync</code>, prefix commands with <code>uv run</code>:   <pre><code>uv run sleap-nn train ...\n</code></pre></li> </ul>"},{"location":"guides/training/#basic-training","title":"Basic Training","text":""},{"location":"guides/training/#using-cli","title":"Using CLI","text":"<pre><code>sleap-nn train --config config.yaml\n</code></pre> <p>Or with separate config directory and name:</p> <pre><code>sleap-nn train --config-dir /path/to/configs --config-name my_config\n</code></pre> <p>See the CLI Reference for all available parameters.</p>"},{"location":"guides/training/#using-python-api","title":"Using Python API","text":"<pre><code>from omegaconf import OmegaConf\nfrom sleap_nn.train import run_training\n\nconfig = OmegaConf.load(\"config.yaml\")\nrun_training(config=config)\n</code></pre>"},{"location":"guides/training/#with-custom-labels","title":"With Custom Labels","text":"<pre><code>import sleap_io as sio\nfrom sleap_nn.train import run_training\n\nconfig = OmegaConf.load(\"config.yaml\")\ntrain_labels = sio.load_slp(\"train.slp\")\nval_labels = sio.load_slp(\"val.slp\")\n\nrun_training(config=config,\n            train_labels=[train_labels],\n            val_labels=[val_labels])\n</code></pre>"},{"location":"guides/training/#config-overrides","title":"Config Overrides","text":"<p>Override any config value from the command line:</p> <pre><code># Change epochs\nsleap-nn train --config config.yaml trainer_config.max_epochs=200\n\n# Change learning rate\nsleap-nn train --config config.yaml trainer_config.optimizer.lr=0.0005\n\n# Change batch size\nsleap-nn train --config config.yaml trainer_config.train_data_loader.batch_size=8\n\n# Set training data\nsleap-nn train --config config.yaml \"data_config.train_labels_path=[train.slp]\"\n\n# Set number of GPUs\nsleap-nn train --config config.yaml trainer_config.trainer_devices=1\n</code></pre>"},{"location":"guides/training/#video-path-remapping","title":"Video Path Remapping","text":"<p>When training on a different machine than where labels were created:</p> Replace by orderMap specific pathsReplace prefix <pre><code>sleap-nn train --config config.yaml \\\n    --video-paths /new/path/video1.mp4 \\\n    --video-paths /new/path/video2.mp4\n</code></pre> <pre><code>sleap-nn train --config config.yaml \\\n    --video-path-map /old/video.mp4 /new/video.mp4\n</code></pre> <pre><code>sleap-nn train --config config.yaml \\\n    --prefix-map /old/server/data /new/local/data\n</code></pre> <p>Choose one option</p> <p>You can only use one of <code>--video-paths</code>, <code>--video-path-map</code>, or <code>--prefix-map</code> at a time.</p>"},{"location":"guides/training/#training-without-config","title":"Training Without Config","text":"<p>Quick training with minimal setup using presets:</p> <pre><code>from sleap_nn.train import train\n\ntrain(\n    train_labels_path=[\"labels.slp\"],\n    backbone_config=\"unet_medium_rf\",  # or \"unet_large_rf\"\n    head_configs=\"bottomup\",           # or \"single_instance\", etc.\n    save_ckpt=True,\n)\n</code></pre> <p>With augmentation:</p> <pre><code>train(\n    train_labels_path=[\"labels.slp\"],\n    backbone_config=\"unet_medium_rf\",\n    head_configs=\"bottomup\",\n    use_augmentations_train=True,\n    intensity_aug=\"uniform_noise\",\n    geometric_aug=[\"rotation\", \"scale\"],\n)\n</code></pre>"},{"location":"guides/training/#top-down-training","title":"Top-Down Training","text":"<p>Top-down models need two separate training runs. See Model Types for details on when to use top-down vs bottom-up.</p> <pre><code># Train centroid model\nsleap-nn train -d /path/to/configs -c centroid_unet \\\n    \"data_config.train_labels_path=[labels.pkg.slp]\"\n\n# Train centered instance model\nsleap-nn train -d /path/to/configs -c centered_instance_unet \\\n    \"data_config.train_labels_path=[labels.pkg.slp]\"\n</code></pre>"},{"location":"guides/training/#monitoring-training","title":"Monitoring Training","text":"<p>Track training progress with WandB logging, visualizations, and evaluation metrics.</p> <pre><code>trainer_config:\n  use_wandb: true\n  wandb:\n    entity: your-username\n    project: your-project\n    viz_enabled: true   # Log prediction visualizations\n\n  eval:\n    enabled: true       # Compute evaluation metrics during training\n    frequency: 1        # Evaluate every epoch\n</code></pre> <p>For detailed configuration options including:</p> <ul> <li>WandB visualization settings (interactive boxes, confidence map masks)</li> <li>Epoch-end evaluation metrics (OKS, PCK, centroid metrics)</li> <li>Local visualization output</li> <li>Per-head loss monitoring</li> </ul> <p>See the dedicated guide:</p> <p> Monitoring &amp; Visualization Guide</p>"},{"location":"guides/training/#checkpointing-artifacts","title":"Checkpointing &amp; Artifacts","text":"<p>Each training run creates a checkpoint directory with:</p> File Description <code>best.ckpt</code> Best model weights <code>initial_config.yaml</code> Original user config <code>training_config.yaml</code> Full config with computed values <code>labels_gt.train.0.slp</code> Training data split (ground truth) <code>labels_gt.val.0.slp</code> Validation data split (ground truth) <code>labels_pr.train.slp</code> Predictions on training data <code>labels_pr.val.slp</code> Predictions on validation data <code>metrics.train.0.npz</code> Training metrics <code>metrics.val.0.npz</code> Validation metrics <code>training_log.csv</code> Loss/metrics per epoch"},{"location":"guides/training/#fine-tuning-transfer-learning","title":"Fine-tuning / Transfer Learning","text":"<p>Initialize with pre-trained weights:</p> <pre><code>model_config:\n  pretrained_backbone_weights: /path/to/best.ckpt\n  pretrained_head_weights: /path/to/best.ckpt\n</code></pre> <p>Works with:</p> <ul> <li> <p>Previous SLEAP-NN checkpoints</p> </li> <li> <p>Legacy SLEAP <code>.h5</code> files (UNet only)</p> </li> </ul>"},{"location":"guides/training/#resume-training","title":"Resume Training","text":"<p>Resume from a previous checkpoint:</p> <pre><code>sleap-nn train --config config.yaml \\\n    trainer_config.resume_ckpt_path=/path/to/checkpoint.ckpt\n</code></pre> <p>This restores both model weights and optimizer state.</p>"},{"location":"guides/training/#multi-gpu-training","title":"Multi-GPU Training","text":"<p>For multi-GPU and distributed training, see the dedicated guide:</p> <p> Multi-GPU Training Guide</p>"},{"location":"guides/training/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/training/#enable-caching","title":"Enable Caching","text":"<pre><code>data_config:\n  data_pipeline_fw: torch_dataset_cache_img_memory  # RAM caching\n  # or\n  data_pipeline_fw: torch_dataset_cache_img_disk    # Disk caching\n</code></pre> <p>With caching, you can use <code>num_workers &gt; 0</code>:</p> <pre><code>trainer_config:\n  train_data_loader:\n    num_workers: 4\n</code></pre> <p>Workers without caching</p> <p>Keep <code>num_workers: 0</code> when not using caching.</p>"},{"location":"guides/training/#understanding-training","title":"Understanding Training","text":""},{"location":"guides/training/#epochs-and-batches","title":"Epochs and Batches","text":"<p>Training occurs in epochs, where one epoch consists of the larger of:</p> <ul> <li>(number of training images) / (batch size), or</li> <li>200 batches</li> </ul> <p>With larger datasets, one epoch equals one pass over the training data.</p>"},{"location":"guides/training/#early-stopping","title":"Early Stopping","text":"<p>By default, training stops early when a plateau is detected in the validation loss to prevent overfitting. You can disable this or set a fixed number of epochs:</p> <pre><code>trainer_config:\n  max_epochs: 200\n  early_stopping:\n    stop_training_on_plateau: false  # Disable early stopping\n</code></pre>"},{"location":"guides/training/#augmentation-strategy","title":"Augmentation Strategy","text":"<p>During training, augmentations are applied to raw images and poses to generate variants of labeled data. This promotes generalization.</p> <p>Rotation recommendations:</p> <ul> <li>Overhead/top-down view: Use full rotation range (-180\u00b0 to 180\u00b0)</li> <li>Side view: Use limited rotation (-15\u00b0 to 15\u00b0)</li> </ul> <pre><code>data_config:\n  augmentation_config:\n    geometric:\n      rotation_min: -180.0\n      rotation_max: 180.0\n</code></pre>"},{"location":"guides/training/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with default configurations</li> <li>Cache Data and increase <code>num_workers</code>: Use caching for faster training</li> <li>Use Augmentation: Always enable for better generalization</li> <li>Early Stopping: Prevents overfitting</li> </ol>"},{"location":"guides/training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/training/#out-of-memory","title":"Out of Memory","text":"<ul> <li>Reduce <code>batch_size</code></li> <li>Reduce model size (fewer filters)</li> <li>Reduce image size with <code>preprocessing.scale</code></li> </ul>"},{"location":"guides/training/#slow-training","title":"Slow Training","text":"<ul> <li>Enable caching</li> <li>Increase <code>num_workers</code> (with caching)</li> <li>Check GPU utilization</li> </ul>"},{"location":"guides/training/#poor-performance","title":"Poor Performance","text":"<ul> <li>Increase training data</li> <li>Adjust augmentation</li> <li>Try different architectures</li> <li>Tune hyperparameters</li> </ul>"},{"location":"guides/training/#next-steps","title":"Next Steps","text":"<ul> <li> Running Inference</li> <li> Configuration Reference</li> <li> Model Architectures</li> </ul>"},{"location":"help/faq/","title":"FAQ","text":"<p>Frequently asked questions about SLEAP-NN.</p>"},{"location":"help/faq/#general","title":"General","text":"What is SLEAP-NN? <p>SLEAP-NN is the PyTorch-based neural network backend for SLEAP. It handles training and inference for animal pose estimation models.</p> How is SLEAP-NN different from SLEAP? <ul> <li>SLEAP (&lt;v1.5): TensorFlow backend, GUI labeling tool</li> <li>SLEAP-NN: PyTorch backend, CLI-focused, faster training, multi-GPU support. This is currently the neural network backend for SLEAP (&gt;=v1.5)</li> </ul> Can I use my existing SLEAP data? <p>Yes! SLEAP-NN reads <code>.slp</code> and <code>.pkg.slp</code> files created by SLEAP.</p>"},{"location":"help/faq/#migrating-from-sleap-1x","title":"Migrating from SLEAP &lt;=v1.4","text":"Can I use my SLEAP &lt;=v1.4 trained models? <p>Yes, but only UNet backbone models. Load them like any other model: <pre><code>sleap-nn track -i video.mp4 -m /path/to/sleap_model/\n</code></pre> The directory should contain <code>best_model.h5</code> and <code>training_config.json</code>.</p> Can I convert SLEAP config files? <p>Yes: <pre><code>from sleap_nn.config.training_job_config import TrainingJobConfig\nfrom omegaconf import OmegaConf\n\nconfig = TrainingJobConfig.load_sleap_config(\"config.json\")\nOmegaConf.save(config, \"config.yaml\")\n</code></pre></p> What features are new in SLEAP-NN? <ul> <li>Multi-GPU training (DDP)</li> <li>Swin Transformer and ConvNeXt backbones</li> <li>ONNX/TensorRT export</li> <li>Faster augmentation (Skia backend)</li> <li>Better WandB integration</li> </ul>"},{"location":"help/faq/#installation","title":"Installation","text":"Which Python version should I use? <p>Python 3.11, 3.12, or 3.13. Python 3.14 is not yet supported.</p> Do I need a GPU? <p>GPUs significantly speed up training and inference but aren't required. CPU-only works but is slower.</p> Which CUDA version do I need? <p>SLEAP-NN supports CUDA 11.8, 12.8, and 13.0. Use <code>--torch-backend auto</code> to auto-detect: <pre><code>uv tool install sleap-nn[torch] --torch-backend auto\n</code></pre></p> How do I check if my GPU is working? <p><pre><code>sleap-nn system\n</code></pre> This shows GPU details and runs diagnostic tests.</p>"},{"location":"help/faq/#training","title":"Training","text":"How much training data do I need? <ul> <li>Minimum: 50-100 labeled frames</li> <li>Good: 200-500 labeled frames</li> <li>Better: 1000+ for complex scenarios</li> </ul> <p>More diverse poses and scenarios improve generalization.</p> How do I know when training is done? <ul> <li>Enable early stopping (default)</li> <li>Watch validation loss plateau</li> <li>Use WandB for detailed monitoring</li> </ul> Can I resume training? <p>Yes: <pre><code>sleap-nn train --config config.yaml \\\n    trainer_config.resume_ckpt_path=/path/to/checkpoint.ckpt\n</code></pre></p> How do I use multiple GPUs? <p><pre><code>trainer_config:\n  trainer_devices: 4\n  trainer_strategy: ddp\n</code></pre> See the Multi-GPU Training Guide for detailed setup and troubleshooting.</p>"},{"location":"help/faq/#inference","title":"Inference","text":"How do I speed up inference? <ol> <li>Increase batch size: <code>--batch_size 8</code></li> <li>Use GPU: <code>--device cuda</code></li> <li>Export to TensorRT: <code>sleap-nn export</code> + <code>sleap-nn predict</code></li> </ol> How do I run on specific frames? <pre><code>sleap-nn track -i video.mp4 -m models/ --frames 0-100,500-600\n</code></pre> How do I limit detected instances? <pre><code>sleap-nn track -i video.mp4 -m models/ --max_instances 5\n</code></pre>"},{"location":"help/faq/#models","title":"Models","text":"Which model type should I use? Scenario Model One animal Single Instance Multiple, not touching Top-Down Multiple, overlapping Bottom-Up Known identities Multi-Class Which backbone should I use? <ul> <li>UNet: Most flexible, works for any resolution</li> <li>ConvNeXt: Good with pretrained weights</li> <li>Swin Transformer: Best for large images, highest memory</li> </ul> What is sigma in the config? <p>Sigma controls the Gaussian spread for confidence maps: - Larger (5-10): Easier to learn, less precise - Smaller (1-3): More precise, harder to learn</p>"},{"location":"help/faq/#troubleshooting","title":"Troubleshooting","text":"Training is very slow <ul> <li>Enable caching: <code>data_config.data_pipeline_fw: torch_dataset_cache_img_memory</code></li> <li>Reduce image size: <code>data_config.preprocessing.scale: 0.5</code></li> <li>Check GPU is being used: <code>sleap-nn system</code></li> </ul> Out of GPU memory <ul> <li>Reduce batch size</li> <li>Reduce model size (<code>filters: 16</code>)</li> <li>Reduce image resolution</li> </ul> Poor predictions <ul> <li>Check training loss - did it converge?</li> <li>Verify preprocessing matches training</li> <li>Add more training data</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Technical reference documentation.</p> Reference Description CLI All command-line options Model Types Single instance, top-down, bottom-up explained Evaluation Metrics OKS, PCK, distance, and VOC metrics explained FAQ Common questions"},{"location":"reference/api/","title":"Python API","text":"<p>Use SLEAP-NN programmatically in Python.</p>"},{"location":"reference/api/#training","title":"Training","text":""},{"location":"reference/api/#using-config-file","title":"Using Config File","text":"<pre><code>from omegaconf import OmegaConf\nfrom sleap_nn.training.model_trainer import ModelTrainer\n\n# Load config\nconfig = OmegaConf.load(\"config.yaml\")\n\n# Create trainer and train\ntrainer = ModelTrainer.get_model_trainer_from_config(config=config)\ntrainer.train()\n</code></pre>"},{"location":"reference/api/#with-custom-labels","title":"With Custom Labels","text":"<pre><code>import sleap_io as sio\nfrom omegaconf import OmegaConf\nfrom sleap_nn.training.model_trainer import ModelTrainer\n\n# Load labels\ntrain_labels = sio.load_slp(\"train.slp\")\nval_labels = sio.load_slp(\"val.slp\")\n\n# Load config and train\nconfig = OmegaConf.load(\"config.yaml\")\ntrainer = ModelTrainer.get_model_trainer_from_config(\n    config=config,\n    train_labels=[train_labels],\n    val_labels=[val_labels]\n)\ntrainer.train()\n</code></pre>"},{"location":"reference/api/#simplified-training","title":"Simplified Training","text":"<pre><code>from sleap_nn.train import train\n\ntrain(\n    train_labels_path=[\"train.slp\"],\n    backbone_config=\"unet_medium_rf\",\n    head_configs=\"bottomup\",\n    save_ckpt=True,\n    max_epochs=100,\n)\n</code></pre>"},{"location":"reference/api/#inference","title":"Inference","text":""},{"location":"reference/api/#basic-inference","title":"Basic Inference","text":"<pre><code>from sleap_nn.predict import run_inference\n\nlabels = run_inference(\n    data_path=\"video.mp4\",\n    model_paths=[\"models/bottomup/\"],\n    output_path=\"predictions.slp\",\n    make_labels=True,\n)\n</code></pre>"},{"location":"reference/api/#get-raw-outputs","title":"Get Raw Outputs","text":"<pre><code>results = run_inference(\n    data_path=\"video.mp4\",\n    model_paths=[\"models/bottomup/\"],\n    make_labels=False,\n    return_confmaps=True,\n)\n\n# results is a list of dicts with predictions per frame\nfor frame_result in results:\n    peaks = frame_result[\"peaks\"]\n    confmaps = frame_result[\"confmaps\"]\n</code></pre>"},{"location":"reference/api/#with-options","title":"With Options","text":"<pre><code>labels = run_inference(\n    data_path=\"video.mp4\",\n    model_paths=[\"models/centroid/\", \"models/instance/\"],\n    output_path=\"predictions.slp\",\n    batch_size=8,\n    device=\"cuda:0\",\n    peak_threshold=0.3,\n    make_labels=True,\n)\n</code></pre>"},{"location":"reference/api/#evaluation","title":"Evaluation","text":"<pre><code>import sleap_io as sio\nfrom sleap_nn.evaluation import Evaluator\n\n# Load labels\ngt = sio.load_slp(\"ground_truth.slp\")\npred = sio.load_slp(\"predictions.slp\")\n\n# Evaluate\nevaluator = Evaluator(gt, pred)\nmetrics = evaluator.evaluate()\n\n# Access results\nprint(f\"OKS mAP: {metrics['voc_metrics']['oks_voc.mAP']:.3f}\")\nprint(f\"Mean OKS: {metrics['mOKS']:.3f}\")\nprint(f\"Distance 90th %ile: {metrics['distance_metrics']['p90']:.2f} px\")\n</code></pre>"},{"location":"reference/api/#export","title":"Export","text":""},{"location":"reference/api/#to-onnx","title":"To ONNX","text":"<pre><code>from sleap_nn.export import export_to_onnx\n\nexport_to_onnx(\n    model,\n    output_path=\"model.onnx\",\n    input_shape=(1, 1, 192, 192),\n    input_dtype=\"uint8\",\n)\n</code></pre>"},{"location":"reference/api/#to-tensorrt","title":"To TensorRT","text":"<pre><code>from sleap_nn.export import export_to_tensorrt\n\nexport_to_tensorrt(\n    model,\n    output_path=\"model.trt\",\n    input_shape=(1, 1, 192, 192),\n    precision=\"fp16\",\n)\n</code></pre>"},{"location":"reference/api/#inference-with-exported-model","title":"Inference with Exported Model","text":"<pre><code>from sleap_nn.export.predictors import ONNXPredictor\nimport numpy as np\n\npredictor = ONNXPredictor(\"model.onnx\")\n\n# Prepare frames (uint8, NCHW format)\nframes = np.random.randint(0, 256, (4, 1, 192, 192), dtype=np.uint8)\n\n# Predict\noutputs = predictor.predict(frames)\npeaks = outputs[\"peaks\"]      # (B, N_nodes, 2)\npeak_vals = outputs[\"peak_vals\"]  # (B, N_nodes)\n</code></pre>"},{"location":"reference/api/#configuration","title":"Configuration","text":""},{"location":"reference/api/#load-config","title":"Load Config","text":"<pre><code>from omegaconf import OmegaConf\n\nconfig = OmegaConf.load(\"config.yaml\")\n\n# Access values\nprint(config.trainer_config.max_epochs)\nprint(config.model_config.backbone_config.unet.filters)\n</code></pre>"},{"location":"reference/api/#modify-config","title":"Modify Config","text":"<pre><code>config.trainer_config.max_epochs = 200\nconfig.trainer_config.optimizer.lr = 0.0005\n</code></pre>"},{"location":"reference/api/#save-config","title":"Save Config","text":"<pre><code>OmegaConf.save(config, \"modified_config.yaml\")\n</code></pre>"},{"location":"reference/api/#convert-legacy-config","title":"Convert Legacy Config","text":"<pre><code>from sleap_nn.config.training_job_config import TrainingJobConfig\nfrom omegaconf import OmegaConf\n\nconfig = TrainingJobConfig.load_sleap_config(\"config.json\")\nOmegaConf.save(config, \"config.yaml\")\n</code></pre>"},{"location":"reference/api/#data-io","title":"Data I/O","text":"<p>SLEAP-NN uses sleap-io for data handling.</p>"},{"location":"reference/api/#load-labels","title":"Load Labels","text":"<pre><code>import sleap_io as sio\n\nlabels = sio.load_slp(\"labels.slp\")\nprint(f\"Videos: {len(labels.videos)}\")\nprint(f\"Labeled frames: {len(labels)}\")\nprint(f\"Skeleton: {labels.skeleton}\")\n</code></pre>"},{"location":"reference/api/#access-predictions","title":"Access Predictions","text":"<pre><code>for lf in labels:\n    print(f\"Frame {lf.frame_idx}:\")\n    for inst in lf.instances:\n        print(f\"  Instance: {len(inst.points)} points\")\n</code></pre>"},{"location":"reference/api/#save-labels","title":"Save Labels","text":"<pre><code>sio.save_slp(labels, \"output.slp\")\n</code></pre>"},{"location":"reference/api/#full-api-reference","title":"Full API Reference","text":"<p>For complete API documentation, see the auto-generated reference:</p> <p> Full API Documentation</p>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Complete command-line interface documentation.</p>"},{"location":"reference/cli/#commands","title":"Commands","text":"Command Description <code>sleap-nn train</code> Train models <code>sleap-nn track</code> Run inference/tracking <code>sleap-nn eval</code> Evaluate predictions <code>sleap-nn export</code> Export to ONNX/TensorRT <code>sleap-nn predict</code> Inference on exported models <code>sleap-nn system</code> System diagnostics"},{"location":"reference/cli/#sleap-nn-train","title":"<code>sleap-nn train</code>","text":"<p>Train pose estimation models.</p> <pre><code>sleap-nn train --config CONFIG_PATH [OPTIONS] [OVERRIDES]\n</code></pre>"},{"location":"reference/cli/#options","title":"Options","text":"Option Short Description <code>--config</code> Path to config YAML file <code>--config-dir</code> <code>-d</code> Directory containing config file <code>--config-name</code> <code>-c</code> Config file name (without .yaml) <code>--video-paths</code> <code>-v</code> Replace video paths (multiple allowed) <code>--video-path-map</code> Map old path to new (OLD NEW) <code>--prefix-map</code> Map path prefix (OLD_PREFIX NEW_PREFIX)"},{"location":"reference/cli/#examples","title":"Examples","text":"<pre><code># Simple\nsleap-nn train --config config.yaml\n\n# With directory/name\nsleap-nn train -d /configs -c my_config\n\n# Override values\nsleap-nn train --config config.yaml trainer_config.max_epochs=200\n\n# Remap video paths\nsleap-nn train --config config.yaml --prefix-map /old/data /new/data\n</code></pre>"},{"location":"reference/cli/#sleap-nn-track","title":"<code>sleap-nn track</code>","text":"<p>Run inference and/or tracking.</p> <pre><code>sleap-nn track --data_path INPUT --model_paths MODEL [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#essential-options","title":"Essential Options","text":"Option Short Description Default <code>--data_path</code> <code>-i</code> Video or labels file Required <code>--model_paths</code> <code>-m</code> Model directory (multiple for top-down) Required* <code>--output_path</code> <code>-o</code> Output file path <code>&lt;input&gt;.predictions.slp</code> <code>--device</code> <code>-d</code> Device (auto/cuda/cpu/mps) <code>auto</code> <code>--batch_size</code> <code>-b</code> Batch size <code>4</code> <code>--tracking</code> <code>-t</code> Enable tracking <code>false</code> <p>*Not required for track-only mode.</p>"},{"location":"reference/cli/#output-options","title":"Output Options","text":"Option Description Default <code>--gui</code> Output JSON progress for GUI integration (instead of Rich progress bar) <code>false</code>"},{"location":"reference/cli/#data-selection","title":"Data Selection","text":"Option Description <code>--frames</code> Frame indices (e.g., <code>0-100,200-300</code>) <code>--video_index</code> Video index in multi-video .slp <code>--only_labeled_frames</code> Only labeled frames <code>--only_suggested_frames</code> Only suggested frames <code>--exclude_user_labeled</code> Skip user-labeled frames <code>--only_predicted_frames</code> Only frames with predictions"},{"location":"reference/cli/#filtering","title":"Filtering","text":"Option Description Default <code>--filter_overlapping</code> Remove duplicate instances <code>false</code> <code>--filter_overlapping_method</code> <code>iou</code> or <code>oks</code> <code>iou</code> <code>--filter_overlapping_threshold</code> Similarity threshold <code>0.8</code> <code>--max_instances</code> Maximum instances per frame None"},{"location":"reference/cli/#tracking","title":"Tracking","text":"Option Description Default <code>--tracking</code> Enable tracking <code>false</code> <code>--tracking_window_size</code> Frames to look back <code>5</code> <code>--candidates_method</code> <code>fixed_window</code> or <code>local_queues</code> <code>fixed_window</code> <code>--features</code> <code>keypoints</code>/<code>centroids</code>/<code>bboxes</code>/<code>image</code> <code>keypoints</code> <code>--scoring_method</code> <code>oks</code>/<code>cosine_sim</code>/<code>iou</code>/<code>euclidean_dist</code> <code>oks</code> <code>--use_flow</code> Enable optical flow <code>false</code>"},{"location":"reference/cli/#examples_1","title":"Examples","text":"<pre><code># Basic inference\nsleap-nn track -i video.mp4 -m models/bottomup/\n\n# Top-down (two models)\nsleap-nn track -i video.mp4 -m models/centroid/ -m models/instance/\n\n# With tracking\nsleap-nn track -i video.mp4 -m models/bottomup/ -t\n\n# Track-only (no inference)\nsleap-nn track -i labels.slp -t\n\n# Filter overlapping + tracking\nsleap-nn track -i video.mp4 -m models/ --filter_overlapping -t\n</code></pre>"},{"location":"reference/cli/#sleap-nn-eval","title":"<code>sleap-nn eval</code>","text":"<p>Evaluate predictions against ground truth.</p> <pre><code>sleap-nn eval --ground_truth_path GT --predicted_path PRED [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#options_1","title":"Options","text":"Option Short Description Default <code>--ground_truth_path</code> <code>-g</code> Ground truth labels Required <code>--predicted_path</code> <code>-p</code> Predicted labels Required <code>--save_metrics</code> <code>-s</code> Save metrics to .npz None <code>--oks_stddev</code> OKS standard deviation <code>0.05</code> <code>--match_threshold</code> Instance matching threshold <code>0.0</code> <code>--user_labels_only</code> Only evaluate user-labeled <code>false</code>"},{"location":"reference/cli/#example","title":"Example","text":"<pre><code>sleap-nn eval -g ground_truth.slp -p predictions.slp -s metrics.npz\n</code></pre>"},{"location":"reference/cli/#sleap-nn-export","title":"<code>sleap-nn export</code>","text":"<p>Export models to ONNX/TensorRT.</p> <pre><code>sleap-nn export MODEL_PATH [MODEL_PATH_2] -o OUTPUT_DIR [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#options_2","title":"Options","text":"Option Short Description Default <code>--output-dir</code> <code>-o</code> Output directory Required <code>--format</code> <code>-f</code> <code>onnx</code>/<code>tensorrt</code>/<code>both</code> <code>onnx</code> <code>--precision</code> TensorRT: <code>fp32</code>/<code>fp16</code> <code>fp16</code> <code>--max-instances</code> <code>-n</code> Max instances per frame <code>20</code> <code>--max-batch-size</code> <code>-b</code> Max batch size <code>8</code>"},{"location":"reference/cli/#examples_2","title":"Examples","text":"<pre><code># ONNX only\nsleap-nn export models/bottomup -o exports/ --format onnx\n\n# Both formats\nsleap-nn export models/bottomup -o exports/ --format both\n\n# Top-down (two models)\nsleap-nn export models/centroid models/instance -o exports/\n</code></pre>"},{"location":"reference/cli/#sleap-nn-predict","title":"<code>sleap-nn predict</code>","text":"<p>Run inference on exported models.</p> <pre><code>sleap-nn predict EXPORT_DIR INPUT_PATH [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#options_3","title":"Options","text":"Option Short Description Default <code>--output</code> <code>-o</code> Output .slp path <code>&lt;input&gt;.predictions.slp</code> <code>--runtime</code> <code>-r</code> <code>auto</code>/<code>onnx</code>/<code>tensorrt</code> <code>auto</code> <code>--batch-size</code> <code>-b</code> Batch size <code>4</code> <code>--n-frames</code> <code>-n</code> Frames to process (0=all) <code>0</code> <code>--device</code> <code>auto</code>/<code>cuda</code>/<code>cpu</code> <code>auto</code>"},{"location":"reference/cli/#example_1","title":"Example","text":"<pre><code>sleap-nn predict exports/model video.mp4 -o predictions.slp --runtime tensorrt\n</code></pre>"},{"location":"reference/cli/#sleap-nn-system","title":"<code>sleap-nn system</code>","text":"<p>Display system information and GPU diagnostics.</p> <pre><code>sleap-nn system\n</code></pre> <p>Shows: - Python version - PyTorch version and build - CUDA/cuDNN versions - GPU details (name, memory, compute capability) - Driver compatibility - Installed package versions</p>"},{"location":"reference/cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>CUDA_VISIBLE_DEVICES</code> Control visible GPUs <code>WANDB_API_KEY</code> WandB API key"},{"location":"reference/cli/#global-options","title":"Global Options","text":"<pre><code>sleap-nn --version  # Show version\nsleap-nn --help     # Show help\n</code></pre>"},{"location":"reference/evaluation_metrics/","title":"Evaluation Metrics","text":"<p>Here, we explain the various evaluation metrics we output at the end of running inference with a trained SLEAP model. We report 6 broad categories of metrics:</p> <ul> <li>Distance metrics</li> <li>Object Keypoint similarity (OKS)</li> <li>Percentage of Correct Keypoints (PCK) metrics</li> <li>Visibility metrics</li> <li>VOC metrics</li> <li>Centroid metrics (centroid models only)</li> </ul>"},{"location":"reference/evaluation_metrics/#distance-metrics","title":"Distance Metrics","text":"<p>This metric computes the Euclidean distance between pairs of predicted and ground-truth (gt) instances. For each instance pair, we calculate the L2 norm of the difference between the predicted and corresponding ground-truth keypoints. The following statistics are reported:</p> <ul> <li> <p><code>Avg_dist</code>: Mean Euclidean distance across all (pred, gt) pairs.</p> </li> <li> <p><code>Dist@k</code>: Percentile-based distance metrics which includes the distance at the 50<sup>th</sup>, 75<sup>th</sup>, 90<sup>th</sup>, 95<sup>th</sup>, and 99<sup>th</sup> percentiles (denoted as p50, p75, p90, p95, p99).</p> </li> </ul> <p>These metrics provide insight into the distribution of how far off the predictions are from the ground-truth key-points.</p>"},{"location":"reference/evaluation_metrics/#object-keypoint-similarity-oks","title":"Object-keypoint similarity (OKS)","text":"<p>This returns the mean OKS score between every pair of ground truth and predicted instance, ranging from 0 to 1.0 and 1.0 indicating a perfect match. OKS provides a measure of similarity between ground-truth and predicted pose by taking into account the instance size (scale) and node visibility.</p> <p>OKS is computed by measuring the Euclidean distance between each predicted keypoint and its corresponding ground-truth keypoint. This distance is then normalized based on the scale of the object (bounding box area for the instance) and standard-deviation that defines the spread in the localization accuracy of each node. For each node, keypoint similarity is computed by taking the negative exponent of the normalized distance. Mean OKS is the average of keypoint similarities across all visible nodes.</p> <p>The implementation is based off of the descriptions in: Ronch &amp; Perona. \"Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation.\" ICCV (2017).</p>"},{"location":"reference/evaluation_metrics/#percentage-of-correct-keypoints-pck-metrics","title":"Percentage of Correct Keypoints (PCK) metrics","text":"<p>This metric measures the fraction of keypoints that fall within a certain pixel distance (threshold) from the ground-truth location. This is useful to evaluate how precise the predicted points are. The following are generated using PCK metric:</p> <ul> <li><code>PCKs</code>: PCK on each predicted instances for each node at different thresholds. (Thresholds: [1, 2, 3, ..., 10])</li> <li><code>mPCK part</code>: Mean PCK per node averaged over all predicted instances across thresholds.</li> <li><code>mPCK</code>: Mean PCK across all predicted nodes and thresholds.</li> </ul>"},{"location":"reference/evaluation_metrics/#visibility-metrics","title":"Visibility metrics","text":"<p>This metric evaluates the visibility accuracy of the predicted nodes. It measures how well the model identifies whether the keypoint is present or missing - independent of its spatial accuracy (i.e. distance from ground-truth). This is useful for evaluating models on datasets with occlusions (NaN nodes).</p> <p>The following statistics are computed across all matched instance pairs:</p> <ul> <li><code>True positives (TP)</code>: Node is visible in both ground-truth and prediction.</li> <li><code>False positives (FP)</code>: Node is missing in ground-truth but visible in prediction.</li> <li><code>True negatives (TN)</code>: Node is missing in both ground-truth and prediction.</li> <li><code>False negatives (FN)</code>: Node is visible in ground-truth but missing in prediction.</li> <li><code>Precision (TP / (TP + FP))</code>: Proportion of predicted visible nodes that are correct.</li> <li><code>Recall (TP / (TP + FN))</code>: Proportion of actual visible nodes that were correctly predicted.</li> </ul>"},{"location":"reference/evaluation_metrics/#voc-metrics","title":"VOC metrics","text":"<p>The following VOC-style metrics are generated using either OKS or PCK as the matching scores and a set of thresholds where a predicted instance is considered as a True Positive if its match score is greater than the threshold else it is counted as a False Positive.</p> <ul> <li><code>Average Precision (AP)</code>: Average of best precisions over fixed set of recall thresholds, at each match score threshold.</li> <li><code>Average Recall (AR)</code>: Maximum recall achieved at each match score threshold.</li> <li><code>Mean Average Precision (mAP)</code>: Mean of average precisions across match thresholds.</li> <li><code>Mean Average Recall (mAR)</code>: Mean of average recalls across match thresholds.</li> </ul> <p>To learn how to generate these metrics using sleap-nn, see the Evaluation guide.</p>"},{"location":"reference/evaluation_metrics/#centroid-metrics","title":"Centroid Metrics","text":"<p>Centroid models predict a single point (centroid) per instance rather than full pose skeletons. These models use specialized distance-based metrics that are more appropriate for point detection tasks than OKS/PCK metrics.</p>"},{"location":"reference/evaluation_metrics/#prediction-matching","title":"Prediction Matching","text":"<p>Predictions are matched to ground truth using the Hungarian algorithm (optimal bipartite matching) to minimize total distance. A <code>match_threshold</code> parameter (default: 50 pixels) determines the maximum distance for a valid match:</p> <ul> <li>True Positive (TP): Prediction matched to GT within threshold</li> <li>False Positive (FP): Prediction with no GT match within threshold</li> <li>False Negative (FN): GT with no prediction match within threshold</li> </ul>"},{"location":"reference/evaluation_metrics/#distance-metrics_1","title":"Distance Metrics","text":"<p>For matched prediction-GT pairs, the following distance statistics are computed:</p> Metric Description <code>centroid_dist_avg</code> Mean Euclidean distance (pixels) <code>centroid_dist_median</code> Median distance <code>centroid_dist_p90</code> 90<sup>th</sup> percentile distance <code>centroid_dist_p95</code> 95<sup>th</sup> percentile distance <code>centroid_dist_max</code> Maximum distance"},{"location":"reference/evaluation_metrics/#detection-metrics","title":"Detection Metrics","text":"<p>These metrics evaluate how well the model detects instances:</p> Metric Formula Description <code>centroid_precision</code> TP / (TP + FP) Proportion of predictions that are correct <code>centroid_recall</code> TP / (TP + FN) Proportion of GT instances that were detected <code>centroid_f1</code> 2 * P * R / (P + R) Harmonic mean of precision and recall"},{"location":"reference/evaluation_metrics/#count-metrics","title":"Count Metrics","text":"Metric Description <code>centroid_n_tp</code> Number of true positives <code>centroid_n_fp</code> Number of false positives <code>centroid_n_fn</code> Number of false negatives"},{"location":"reference/evaluation_metrics/#when-to-use","title":"When to Use","text":"<p>Centroid metrics are automatically used during epoch-end evaluation when training centroid models. They can also be used for post-training evaluation of centroid model predictions.</p> <p>Adjusting match_threshold</p> <p>The <code>match_threshold</code> parameter should be set based on your expected centroid accuracy:</p> <ul> <li>Smaller threshold (e.g., 20px): Stricter matching, may undercount true positives</li> <li>Larger threshold (e.g., 100px): More lenient, may incorrectly match distant predictions</li> </ul> <p>For most applications, the default of 50 pixels works well.</p>"},{"location":"reference/models/","title":"Model Types","text":"<p>Understand the different model architectures in SLEAP-NN.</p>"},{"location":"reference/models/#overview","title":"Overview","text":"Model Type Animals Occlusion Training Use Case Single Instance 1 N/A 1 model Isolated animals Top-Down Many Some 2 models Multiple non-overlapping and animal sizes are smaller compared to the whole image Bottom-Up Many Heavy 1 model Crowded scenes Multi-Class Many Varies 1-2 models Known identities"},{"location":"reference/models/#single-instance","title":"Single Instance","text":"<p>One animal per frame.</p> <pre><code>Image \u2192 Backbone \u2192 Confidence Maps \u2192 Peaks\n</code></pre>"},{"location":"reference/models/#when-to-use","title":"When to Use","text":"<ul> <li>Single animal videos</li> <li>No need for tracking</li> <li>Fastest training and inference</li> </ul>"},{"location":"reference/models/#configuration","title":"Configuration","text":"<pre><code>head_configs:\n  single_instance:\n    confmaps:\n      sigma: 5.0\n      output_stride: 2\n</code></pre>"},{"location":"reference/models/#inference","title":"Inference","text":"<pre><code>sleap-nn track -i video.mp4 -m models/single_instance/\n</code></pre>"},{"location":"reference/models/#top-down","title":"Top-Down","text":"<p>Two-stage: detect centers, then estimate pose.</p> <p></p> <pre><code>Stage 1: Image \u2192 Backbone \u2192 Centroid Map \u2192 Instance Centers\nStage 2: Crop \u2192 Backbone \u2192 Confidence Maps \u2192 Keypoints\n</code></pre>"},{"location":"reference/models/#when-to-use_1","title":"When to Use","text":"<ul> <li>Multiple animals that are clearly separated</li> <li>Animals vary in size (centroid crops normalize scale)</li> <li>Need precise localization per individual</li> </ul>"},{"location":"reference/models/#centroid-model-tips","title":"Centroid Model Tips","text":"<p>Sigma for centroid detection</p> <p>Increasing <code>sigma</code> makes centroid confidence maps coarser\u2014easier to detect animals but less precise. This is often a good trade-off for the centroid stage.</p> <p>Use a specific anchor part</p> <p>Choosing a specific node as the centroid (e.g., thorax) leads to more consistent results than using the bounding box center, which often falls on different parts of the animal. This matters because the centered instance model depends on consistent positioning within the crop.</p> <p>Reduce resolution for centroid model</p> <p>Since centroids can be detected coarsely, you can reduce input image resolution (<code>preprocessing.scale</code>) to save computation. This is especially useful early in labeling when training data is limited.</p> <p>Crop size</p> <p>Set crop size large enough to include the whole animal in the centered instance crops.</p>"},{"location":"reference/models/#configuration_1","title":"Configuration","text":"<p>Centroid model: <pre><code>head_configs:\n  centroid:\n    confmaps:\n      anchor_part: null  # Use bbox center\n      sigma: 5.0\n</code></pre></p> <p>Instance model: <pre><code>head_configs:\n  centered_instance:\n    confmaps:\n      anchor_part: null\n      sigma: 5.0\n\ndata_config:\n  preprocessing:\n    crop_size: 256\n</code></pre></p>"},{"location":"reference/models/#training","title":"Training","text":"<p>Train two models separately:</p> <pre><code>sleap-nn train --config centroid_config.yaml\nsleap-nn train --config instance_config.yaml\n</code></pre>"},{"location":"reference/models/#inference_1","title":"Inference","text":"<pre><code>sleap-nn track -i video.mp4 \\\n    -m models/centroid/ \\\n    -m models/centered_instance/\n</code></pre>"},{"location":"reference/models/#bottom-up","title":"Bottom-Up","text":"<p>Detect all keypoints, then group into instances.</p> <p></p> <pre><code>Image \u2192 Backbone \u2192 [Confidence Maps + Part Affinity Fields] \u2192 Grouping \u2192 Instances\n</code></pre>"},{"location":"reference/models/#when-to-use_2","title":"When to Use","text":"<ul> <li>Animals frequently occlude each other</li> <li>Many animals in frame (more efficient than top-down)</li> <li>Animals touching/interacting</li> <li>Uniform animal sizes</li> </ul> <p>Try both approaches</p> <p>Top-down works better for some datasets while bottom-up works better for others. To maximize accuracy, try both and compare results.</p>"},{"location":"reference/models/#how-it-works","title":"How It Works","text":"<ol> <li>Confidence maps: Locate all keypoints of all animals</li> <li>Part Affinity Fields (PAFs): Encode connections between keypoints</li> <li>Grouping: Hungarian matching to assemble instances</li> </ol>"},{"location":"reference/models/#configuration_2","title":"Configuration","text":"<pre><code>head_configs:\n  bottomup:\n    confmaps:\n      sigma: 2.5\n      output_stride: 4\n      loss_weight: 1.0\n    pafs:\n      sigma: 75.0\n      output_stride: 8\n      loss_weight: 1.0\n</code></pre>"},{"location":"reference/models/#inference_2","title":"Inference","text":"<pre><code>sleap-nn track -i video.mp4 -m models/bottomup/\n</code></pre>"},{"location":"reference/models/#multi-class-identity-models","title":"Multi-Class (Identity Models)","text":"<p>Pose estimation + supervised identity prediction.</p> <p>Use when you have labeled identity/track information in training data.</p>"},{"location":"reference/models/#multi-class-bottom-up","title":"Multi-Class Bottom-Up","text":"<pre><code>head_configs:\n  multi_class_bottomup:\n    confmaps:\n      sigma: 5.0\n      loss_weight: 1.0\n    class_maps:\n      classes: null  # Infer from track names\n      sigma: 5.0\n      loss_weight: 1.0\n</code></pre>"},{"location":"reference/models/#multi-class-top-down","title":"Multi-Class Top-Down","text":"<pre><code>head_configs:\n  multi_class_topdown:\n    confmaps:\n      sigma: 5.0\n      loss_weight: 1.0\n    class_vectors:\n      classes: null\n      num_fc_layers: 1\n      num_fc_units: 64\n      loss_weight: 1.0\n</code></pre>"},{"location":"reference/models/#backbones","title":"Backbones","text":""},{"location":"reference/models/#unet","title":"UNet","text":"<ul> <li>Most flexible</li> <li>Works at any resolution</li> <li>Configurable depth/width</li> </ul> <pre><code>backbone_config:\n  unet:\n    filters: 32\n    max_stride: 16\n</code></pre>"},{"location":"reference/models/#convnext","title":"ConvNeXt","text":"<ul> <li>Modern CNN architecture</li> <li>ImageNet pretrained weights</li> <li>Good for transfer learning</li> </ul> <pre><code>backbone_config:\n  convnext:\n    model_type: tiny\n    pre_trained_weights: ConvNeXt_Tiny_Weights\n</code></pre>"},{"location":"reference/models/#swin-transformer","title":"Swin Transformer","text":"<ul> <li>Vision transformer</li> <li>Best for global context</li> <li>Highest memory usage</li> </ul> <pre><code>backbone_config:\n  swint:\n    model_type: tiny\n    pre_trained_weights: Swin_T_Weights\n</code></pre>"},{"location":"reference/models/#choosing-a-model","title":"Choosing a Model","text":"<pre><code>graph TD\n    A[How many animals?] --&gt;|One| B[Single Instance]\n    A --&gt;|Multiple| C[Do they overlap?]\n    C --&gt;|No/Rarely| D[Top-Down]\n    C --&gt;|Yes/Often| E[Bottom-Up]\n    D --&gt; F[Need identity?]\n    E --&gt; F\n    F --&gt;|Yes| G[Multi-Class variant]\n    F --&gt;|No| H[Standard variant]\n</code></pre>"},{"location":"reference/models/#quick-guidelines","title":"Quick Guidelines","text":"Scenario Recommendation Single fly in chamber Single Instance 2-3 mice, separate areas Top-Down Social behavior, touching Bottom-Up Same individuals across sessions Multi-Class"},{"location":"reference/models/#performance-comparison","title":"Performance Comparison","text":"<p>Approximate training times on RTX 3090 (1000 labeled frames):</p> Model Training Time Inference Speed Single Instance ~30 min ~500 FPS Top-Down ~1 hr (2 models) ~100 FPS Bottom-Up ~1 hr ~80 FPS"},{"location":"reference/models/#training-options","title":"Training Options","text":"<p>Key hyperparameters to configure when training models.</p>"},{"location":"reference/models/#batch-size","title":"Batch Size","text":"<p>Number of examples per training step.</p> Setting Effect Higher Better generalization, requires more GPU memory Lower May overfit, useful when few varied examples <p>Tip: Reduce batch size if you run out of GPU memory.</p>"},{"location":"reference/models/#receptive-field","title":"Receptive Field","text":"<p>Controls how much context the model sees around each pixel. Determined by max stride and input scaling.</p> Parameter Description <code>max_stride</code> Larger stride = larger receptive field, but more parameters <code>input_scale</code> Downsampling increases receptive field relative to original size <p>Rule of thumb: Receptive field should be approximately as large as your animal.</p> <p>For top-down models:</p> <ul> <li>Centroid model: Use larger receptive field (more downsampling OK)</li> <li>Centered instance model: Smaller receptive field to preserve details</li> </ul>"},{"location":"reference/models/#augmentation","title":"Augmentation","text":"<p>Data augmentation helps train more robust models.</p> Type Recommended Settings Rotation Side view: -15\u00b0 to 15\u00b0, Top view: -180\u00b0 to 180\u00b0 Brightness/Contrast Enable if test videos have different lighting Scale Small variations (0.9-1.1) for size robustness"},{"location":"reference/models/#online-hard-keypoint-mining-ohkm","title":"Online Hard Keypoint Mining (OHKM)","text":"<p>Enable for skeletons with many joints. Makes \"hard\" joints contribute more to the loss, improving accuracy on difficult keypoints.</p> <p>See Shrivastava et al., 2016 for details.</p>"},{"location":"reference/models/#tips","title":"Tips","text":"<p>Start simple</p> <p>Try Single Instance or Top-Down first. Only use Bottom-Up if needed.</p> <p>More data helps Bottom-Up</p> <p>PAF learning benefits from diverse poses and interactions.</p> <p>Anchor part for Top-Down</p> <p>Set <code>anchor_part</code> to a reliable body part (e.g., \"thorax\") for better cropping.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Step-by-step tutorials to get you started with sleap-nn.</p> Tutorial Description Quick Start Train and run inference in 5 minutes Your First Model Complete walkthrough from data to predictions Example Notebooks Interactive notebooks for hands-on learning"},{"location":"tutorials/#learning-path","title":"Learning Path","text":"<p>New to sleap-nn? Start here:</p> <ol> <li>Quick Start - Get up and running fast</li> <li>Your First Model - Understand the full workflow</li> <li>Example Notebooks - Explore interactive examples</li> </ol> <p>Once you're comfortable, move on to the Guides for specific tasks.</p>"},{"location":"tutorials/notebooks/","title":"Example Notebooks","text":"<p>Interactive tutorials using marimo notebooks.</p>"},{"location":"tutorials/notebooks/#accessing-notebooks","title":"Accessing Notebooks","text":"<p>The example notebooks are in the sleap-nn GitHub repository in the <code>example_notebooks/</code> folder. They use marimo which provides a sandboxed environment that automatically handles all dependencies.</p>"},{"location":"tutorials/notebooks/#prerequisites","title":"Prerequisites","text":"<p>Install uv:</p> <pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre>"},{"location":"tutorials/notebooks/#available-notebooks","title":"Available Notebooks","text":"<p>Download notebooks from here.</p>"},{"location":"tutorials/notebooks/#training-demo","title":"Training Demo","text":"<p>End-to-end demo on creating config files and running training, inference, and evaluation.</p> <pre><code>uvx marimo edit --sandbox training_demo.py\n</code></pre>"},{"location":"tutorials/notebooks/#augmentation-guide","title":"Augmentation Guide","text":"<p>Visualize different data augmentation techniques available in sleap-nn.</p> <pre><code>uvx marimo edit --sandbox augmentation_guide.py\n</code></pre>"},{"location":"tutorials/notebooks/#receptive-field-guide","title":"Receptive Field Guide","text":"<p>Visualize how receptive field changes with different config parameters.</p> <pre><code>uvx marimo edit --sandbox receptive_field_guide.py\n</code></pre>"},{"location":"tutorials/notebooks/#running-notebooks","title":"Running Notebooks","text":"<p>Each notebook runs in a sandboxed environment - no need to install sleap-nn separately:</p> <pre><code># Navigate to your notebooks folder\ncd example_notebooks/\n\n# Run any notebook\nuvx marimo edit --sandbox &lt;notebook_name&gt;.py\n</code></pre> <p>The <code>--sandbox</code> flag creates an isolated environment with all required dependencies.</p>"}]}